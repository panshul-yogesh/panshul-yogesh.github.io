{
  "title": "Create local persistent volumes on worker nodes",
  "content": "<div class=\"mw-parser-output\">\n<div>If you want to install OPTIC Data Lake on embedded Kubernetes, the OPTIC Data Lake message bus (pulsar) requires Local Persistent Volumes (LPVs) for runtime data persistence. This storage is allocated from volumes available to your worker nodes. There are three LPVs required on each worker node (the ledger, journal, and zookeeper LPV). </div>\n<p>You can find the exact number of required pulsar bookkeeper/broker replicas for your deployment in the SizCalc (Results) sheet of the sizing calculator. This is also the minimum number of worker nodes where you need to create LPVs because each replica binds to a different worker node. You might consider having an extra worker where you have created LPVs but not used, in case one of the other workers fails and you can then move the pulsar pods to this already prepared worker node. </p>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><div>\n<p>The pulsar replicas run on different worker nodes and bind to their own set of volumes. Create pulsar LPVs for each OPTIC Data Lake deployment on each worker node if you deploy OPTIC Data Lake more than one time in the same cluster.  Version 25.2 onwards, OPTIC Data Lake is deployed with Operations Platform. In earlier versions, you would choose an application to be the OPTIC Data Lake provider.</p>\n</div></div></div>\n<p>During this step, you will be configuring the LPVs on each worker node that will host the pulsar replicas. The options set up volumes in slightly different ways:</p>\n<ul style=\"padding-left: 0px;\">\n<li style=\"margin-left: 28px; position: relative;\">\n<p><strong>Configure separate disk devices for each LPV on every worker node</strong>: For best performance, allocate and use three separate disk devices per worker node, one for each pulsar LPV as shown in the sizing calculator. Add empty disk devices to all your worker systems, each sized so that enough free space will result when a filesystem is installed to accommodate the output from your sizing calculator for the ledger, journal, zookeeper LPVs according to your expected scale. This is shown in the SizCalc (Results) sheet and also the K8S Cluster sheet for OPTIC Data Lake Ledger, Zookeeper, and Journal storage size in GB units.</p>\n</li>\n<li style=\"margin-left: 28px; position: relative;\">\n<p><strong>Use a single separate disk device for all LPVs on every worker node</strong>: One separate disk device common for all three pulsar LPVs on each worker node is also supported. You will add one empty disk device on your worker systems which should be sized for filesystem free space more than the sum of the sizes for OPTIC Data Lake Ledger, Zookeeper, and Journal storage size, based on the output from your sizing calculator.</p>\n</li>\n<li style=\"margin-left: 28px; position: relative;\">\n<p><strong>Use the space under one of your existing worker disks or filesystems</strong>: Using an existing disk device that already has a mounted filesystem to use for pulsar LPVs may result in lower performance. It is best to use a disk which isn’t used for the root filesystem for this purpose. Note that LPV space usage impacts the chosen filesystem space and performance.</p>\n</li>\n</ul>\n<p>Depending on your choice of setting up volumes, do one of the following to prepare the LPVs for your software installation. In all cases, note that:</p>\n<ul style=\"padding-left: 0px;\">\n<li style=\"margin-left: 28px; position: relative;\">The default OPTIC Data Lake path to mount the pulsar LPV filesystems is under /mnt/disks. You can choose a non-default OPTIC Data Lake path but it must be identical on all the workers where you create pulsar LPVs. The OPTIC Data Lake path should be owned by root and have 0755 permissions. When deploying OPTIC Data Lake for the first time, make sure these directories are empty.</li>\n<li style=\"margin-left: 28px; position: relative;\">The three directories under the OPTIC Data Lake path that you create for LPVs will be owned by SYSTEM_USER_ID and SYSTEM_GROUP_ID. For the ID values, see the  install.properties file you used for OMT installation. The default values are 1999 and 1999 respectively. The LPV directory permissions will be set below to 755.</li>\n<li style=\"margin-left: 28px; position: relative;\">The filesystems hosting the LPVs can be either ext4 or xfs.</li>\n<li style=\"margin-left: 28px; position: relative;\">Each of the three directories you create under the OPTIC Data Lake path must be separate mount points, with corresponding lines in your /etc/fstab. This is to make sure all three directories are seen in the output of the \"mount\" command even after reboots.</li>\n</ul>\n<h2><a id=\"Configure-separate-disk-devices-for-each-LPV-on-worker-nodes\" name=\"Configure-separate-disk-devices-for-each-LPV-on-worker-nodes\" title=\"Configure separate disk devices for each LPV on worker nodes\"></a>Configure separate disk devices for each LPV on worker nodes</h2>\n<p>This section covers the recommended procedure for production use where you have added three additional disk devices, one for each on all your worker nodes that will host pulsar replicas.</p>\n<ol style=\"padding-left: 0px;\">\n<li style=\"--number-width: 10px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Log on to a worker node as root or <code>SUDO</code> as root.</li>\n<li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following command to create a new directory as the OPTIC Data Lake path, under which you will mount filesystems:<br/>\n<code>mkdir -p &lt;mount path&gt;</code><br/>\n\tFor example:<br/>\n<code>mkdir -p /mnt/disks</code>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-important-icon\"></div><div class=\"admonition-content admonition-important-content\"><div>The default OPTIC Data Lake path is <code>/mnt/disks</code>. You can use a different OPTIC Data Lake path to mount LPV filesystems. If you use a non-default mount path, you must create a YAML file and provide the path during the helm install. </div></div></div>\n</li>\n<li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the <code>lsblk</code> command to list the new empty disks that your sysadmin has created. For example, the sizing calculator requires 110GB for Ledger, 22GB for Zookeeper, and 22GB for Journal for each replica. To ensure that amount of free space, add three hard disks, one disk of 120GB and two disks of 25GB each in size. The result may appear similar to the following:\n\t<pre><code># lsblk\n\nNAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\n\nsda             8:0    0   60G  0 disk\n\n├─sda1          8:1    0    1G  0 part /boot\n\n└─sda2          8:2    0   59G  0 part\n\n  ├─rhel-root 253:0    0 35.6G  0 lvm  /\n\n  ├─rhel-swap 253:1    0    6G  0 lvm  [SWAP]\n\n  └─rhel-home 253:2    0 17.4G  0 lvm  /home\n\nsdb             8:16   0  120G  0 disk\n\nsdc             8:32   0   25G  0 disk\n\nsdd             8:48   0   25G  0 disk\n\nsr0            11:0    1 1024M  0 rom\n</code></pre>\n</li>\n<li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run either <code>mkfs.ext4</code> or <code>mkfs.xfs</code> to format the new disks or partitions to then mount as a filesystem, being careful to specify the unused disks or partitions you intend to use.<br/>\n\tFor example, to create 3 LPVs:\n\t<pre><code>mkfs.ext4 /dev/sdb\nmkfs.ext4 /dev/sdc\nmkfs.ext4 /dev/sdd\n</code></pre>\n</li>\n<li style=\"--number-width: 12px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Use your organization's standard conventions for your Linux systems to create the mountpoints under your chosen OPTIC Data Lake path, then mount the new volumes and add entries for them in /etc/fstab this will preserve the mounts after reboots. We recommend that you use UUIDs for the new volumes to mount.<br/>\n\tFor example:\n\t<pre><code>mkdir -p /mnt/disks/lpv1\n\nUUID1=$(sudo blkid -s UUID -o value /dev/sdb)\n\nmount -t ext4 UUID=$UUID1 /mnt/disks/lpv1\n\necho \"UUID=$UUID1 /mnt/disks/lpv1 ext4 defaults 0 2\" | sudo tee -a /etc/fstab\n\nmkdir -p /mnt/disks/lpv2\n\nUUID2=$(sudo blkid -s UUID -o value /dev/sdc)\n\nmount -t ext4 UUID=$UUID2 /mnt/disks/lpv2\n\necho \"UUID=$UUID2 /mnt/disks/lpv2 ext4 defaults 0 2\" | sudo tee -a /etc/fstab\n\nmkdir -p /mnt/disks/lpv3\n\nUUID3=$(sudo blkid -s UUID -o value /dev/sdd)\n\nmount -t ext4 UUID=$UUID3 /mnt/disks/lpv3\n\necho \"UUID=$UUID3 /mnt/disks/lpv3 ext4 defaults 0 2\" | sudo tee -a /etc/fstab</code></pre>\n\tThe resulting lines in <code>/etc/fstab</code> may appear similar to the following:\n\n\t<pre><code>UUID=76302901-08be-4d4a-97c8-1226b502ebbe /mnt/disks/lpv1 ext4 defaults 0 2\n\nUUID=5cecdc64-f232-448f-9051-fbd200aa2a80 /mnt/disks/lpv2 ext4 defaults 0 2\n\nUUID=e710d0d3-7a44-4188-a649-eb7ca30c1eec /mnt/disks/lpv3 ext4 defaults 0 2\n\n</code></pre>\n</li>\n<li style=\"--number-width: 12px; --number-spacing: 24px; margin-left: 36px; position: relative;\">You can run <code>df -h</code> command to ensure that the newly created file systems are mounted correctly and the free space available for each file system is equal to or greater than the minimum size recommended by the sizing calculator. The result may appear similar to the following:\n\t<pre><code># df -h |grep -E 'Filesystem|lpv'\n\nFilesystem             Size  Used Avail Use% Mounted on\n/dev/sdb               118G   24K  112G   1% /mnt/disks/lpv1\n/dev/sdc                25G   24K   24G   1% /mnt/disks/lpv2\n/dev/sdd                25G   24K   24G   1% /mnt/disks/lpv3</code></pre>\n</li>\n<li style=\"--number-width: 10px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run commands similar to the following to change the ownership of the three directories under the OPTIC Data Lake path. The user and system group ID were defined in your <code>install.properties</code> file. The defaults are 1999 for both. The parent directory needs read and execute permissions for all:<br/>\n<br/>\n<code>cd &lt;OPTIC DL path&gt;; chown -R &lt;SYSTEM_USER_ID&gt;:&lt;SYSTEM_GROUP_ID&gt; *; chmod -R 755 &lt;OPTIC DL path&gt;; ls -la</code><br/>\n<br/>\n\tFor example: if the system user id is 1999 and the system group id is 1999:\n\t<pre><code>cd /mnt/disks; chown -R 1999:1999 *; chmod -R 755 /mnt/disks; ls -la   </code></pre>\n<br/>\n\tThe result should appear similar to the following:\n\t<pre><code># cd /mnt/disks; chown -R 1999:1999 *; chmod -R 755 /mnt/disks; ls -la\ntotal 12\ndrwxr-xr-x  5 root root   42 Aug 14 17:02 .\ndrwxr-xr-x. 3 root root   19 Aug 14 17:02 ..\ndrwxr-xr-x  3 1999 1999 4096 Aug 14 17:01 lpv1\ndrwxr-xr-x  3 1999 1999 4096 Aug 14 17:01 lpv2\ndrwxr-xr-x  3 1999 1999 4096 Aug 14 17:01 lpv3</code></pre>\n</li>\n<li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Repeat steps 1 to 7, to mount new disks on all the worker nodes you will be using for OPTIC Data Lake LPVs.</li>\n</ol>\n<h2><a id=\"Configure-a-single-separate-disk-device-for-all-LPVs-on-every-worker-node\" name=\"Configure-a-single-separate-disk-device-for-all-LPVs-on-every-worker-node\" title=\"Configure a single separate disk device for all LPVs on every worker node\"></a>Configure a single separate disk device for all LPVs on every worker node</h2>\n<p>This section covers the case where you have added one additional separate disk device on each worker node to share for all the LPVs (on ESX they would be new \"hard disks\"). Make sure that the size of the new disk is at least the size of the sum of all the intended LPVs from your sizing calculator.</p>\n<ol style=\"padding-left: 0px;\">\n<li style=\"--number-width: 10px; --number-spacing: 24px; margin-left: 36px; position: relative;\"><span id=\"cke_bm_6743S\" style=\"display: none;\"> </span>Log on to a worker node as root or <code>SUDO</code> as root.</li>\n<li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the <code>l<span id=\"cke_bm_6743E\" style=\"display: none;\"> </span>sblk</code> command to list the new empty disk that your sysadmin has created or that you create now using vCenter or other tools. The result will appear similar to the following:\n\t<pre><code># lsblk\n\nNAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\n\nsda             8:0    0   60G  0 disk\n\n├─sda1          8:1    0    1G  0 part /boot\n\n└─sda2          8:2    0   59G  0 part\n\n  ├─rhel-root 253:0    0 35.6G  0 lvm  /\n\n  ├─rhel-swap 253:1    0    6G  0 lvm  [SWAP]\n\n  └─rhel-home 253:2    0 17.4G  0 lvm  /home\n\nsdb             8:16   0  170G  0 disk\n\nsr0            11:0    1 1024M  0 rom</code></pre>\n</li>\n<li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\"><span id=\"cke_bm_9317C\" style=\"display: none;\"> </span>Run either <code>mkfs.ext4</code> or <code>mkfs.xfs</code> to format the new disk or partition to then mount as a filesystem, being careful to specify the unused disks or partitions you intend to use.<br/>\n\tFor example:<br/>\n<code>mkfs.ext4 /dev/sdb</code></li>\n<li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Use your organization's standard conventions for your Linux systems it in /etc/fstab which will preserve the mount after reboots. It is recommended you use UUIDs for the new volumes to mount..<br/>\n\tFor example:\n\t<pre><code>mkdir -p /mnt/onelpvdisk\n\nUUID=$(sudo blkid -s UUID -o value /dev/sdb)\n\nmount -t ext4 UUID=$UUID /mnt/onelpvdisk\n\necho \"UUID=$UUID /mnt/onelpvdisk ext4 defaults 0 2\" | sudo tee -a /etc/fstab\n</code></pre>\n</li>\n<li style=\"--number-width: 12px; --number-spacing: 24px; margin-left: 36px; position: relative;\">\n<p>You can run <code>df -h</code> command to ensure that the newly created file systems are mounted correctly and the free space available for each filesystem meets the minimum size recommended by the sizing calculator. The result may appear similar to the following:</p>\n<pre><code># df -h |grep -E 'Filesystem|mnt'\n\nFilesystem             Size  Used Avail Use% Mounted on\n\n/dev/sdb               167G   28K  158G   1% /mnt/onelpvdisk</code></pre>\n</li>\n<li style=\"--number-width: 12px; --number-spacing: 24px; margin-left: 36px; position: relative;\">You now need to create three directories under the new filesystem as well as in the OPTIC Data Lake path, which you will use to mount special “bind” filesystem directories under it, one each for the LPVs as indicated by your sizing calculator output.\n\t<div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><div>The default OPTIC Data Lake path is <code>/mnt/disks</code>. You can use a different OPTIC Data Lake path to mount pulsar LPV filesystems, but if you use a non-default mount path, then you must create a YAML file and provide the path during the helm install. </div></div></div>\n<p>For example:</p>\n<pre><code>cd /mnt/disks\nfor lpv in lpv1 lpv2 lpv3\n\ndo\n\n    mkdir /mnt/onelpvdisk/$lpv\n\n    mkdir -p /mnt/disks/$lpv\n\n    echo \"/mnt/onelpvdisk/$lpv /mnt/disks/$lpv none bind\" &gt;&gt; /etc/fstab\n\ndone\n\nmount -a\n\nmount | grep lpv\n</code></pre>\n<p>For example, the new filesystems are shown as following:</p>\n<pre><code># mount | grep lpv\n\nmount: (hint) your fstab has been modified, but systemd still uses\n\n       the old version; use 'systemctl daemon-reload' to reload.\n\n/dev/sdb on /mnt/onelpvdisk type ext4 (rw,relatime)\n\n/dev/sdb on /mnt/disks/lpv1 type ext4 (rw,relatime)\n\n/dev/sdb on /mnt/disks/lpv2 type ext4 (rw,relatime)\n\n/dev/sdb on /mnt/disks/lpv3 type ext4 (rw,relatime)</code></pre>\n</li>\n<li style=\"--number-width: 10px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following command to change the ownership of the new directories under the OPTIC Data Lake path. The user and system group ID were defined in your <code>install.properties</code> file and the defaults are 1999 for both. The parent directory needs read and execute permissions for all: <br/>\n<code>cd &lt;OPTIC DL path&gt;; chown -R &lt;SYSTEM_USER_ID&gt;:&lt;SYSTEM_GROUP_ID&gt; *; chmod -R 755 &lt;OPTIC DLSO path&gt;; ls -l</code><br/>\n<br/>\n\tFor example: if the system user id is 1999 and the system group id is 1999,\n\t<pre><code>cd /mnt/disks; chown -R 1999:1999 *; chmod -R 755 /mnt/disks; ls -l    </code></pre>\n<br/>\n\tThe result should appear similar to the following:\n\t<pre><code># cd /mnt/disks; chown -R 1999:1999 *; chmod -R 755 /mnt/disks; ls -l\n\ntotal 12\n\ndrwxr-xr-x 2 1999 1999 4096 Aug 20 11:36 lpv1\n\ndrwxr-xr-x 2 1999 1999 4096 Aug 20 11:36 lpv2\n\ndrwxr-xr-x 2 1999 1999 4096 Aug 20 11:36 lpv3</code></pre>\n</li>\n<li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Repeat steps 1 to 8, to mount the new disks on all the worker nodes you will be using for OPTIC Data Lake LPVs.</li>\n</ol>\n<h2><a id=\"Use-existing-disk-devices-on-worker-nodes\" name=\"Use-existing-disk-devices-on-worker-nodes\" title=\"Use existing disk devices on worker nodes\"></a>Use existing disk devices on worker nodes</h2>\n<p>You can use existing mounted filesystems to create local persistent volumes, but this may result in lower performance and the LPV space usage can affect other processes using that mount point. You may choose to allocate space from any existing filesystem that has ample free space, well in excess of the sum of all LPV sizes from the sizing calculator. We recommend not using the root file system for this purpose. <br/>\nIn the following example, your Sizing Calculator has determined the minimum recommended OpenSearch LPV size for masters and nodes as 50 GB and 50 GB, respectively. And pulsar LPV size for ledger, journal and zookeeper as 25 GB, 5 GB, and 5 GB respectively. Ensure that the free space available in the final file system for LPV should match or exceed the size requirement as recommended by the sizing calculator. <br/>\nNote that for Pulsar the final file system should have 35GB minimum (25+5+5)</p>\n<p>Follow these steps:</p>\n<ol style=\"padding-left: 0px;\">\n<li style=\"--number-width: 10px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Log on to a worker node as root or <code>SUDO</code>.</li>\n<li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">\n<p>Run the following commands:<br/>\n<br/>\n\tIn the below example, use the <code>/mnt/pulsar</code> directory with minimum size as 35GB. <br/>\n\tCreate three folders inside<code> /mnt/pulsar</code>, one each for ledger, journal and zookeeper. Similarly create /mnt/opensearch-masters and /mnt/opensearch-nodes having minimum 50GB each. Create one folder inside both these directories.</p>\n<pre><code>mkdir -p /mnt/pulsar/ledger \n\nmkdir -p /mnt/pulsar/journal \n\nmkdir -p  /mnt/pulsar/zookeeper</code></pre>\n</li>\n<li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following command to change the ownership of  your chosen OPTIC Data Lake path and its sub directories to have read and execute permissions for all:\n\t<pre><code>chown -R 1999:1999 /mnt/pulsar/*; chmod -R 755 /mnt/pulsar; ls -la /mnt/pulsar \n\ntotal 20 \n\ndrwxr-xr-x  5 root root 4096 Sep  6 11:48 . \n\ndrwxr-xr-x. 3 root root 4096 Sep  6 10:43 .. \n\ndrwxr-xr-x  3 1999 1999 4096 Sep  6 10:43 journal \n\ndrwxr-xr-x  3 1999 1999 4096 Sep  6 11:41 ledger \n\ndrwxr-xr-x  3 1999 1999 4096 Sep  6 11:48 zookeeper</code></pre>\n\t </li>\n<li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Repeat steps 1 to 4 on the other worker nodes you will use for OPTIC Data Lake if there will be more than one worker node.</li>\n</ol>\n</div>",
  "modifiedon": "2026-01-30 13:52:30"
}