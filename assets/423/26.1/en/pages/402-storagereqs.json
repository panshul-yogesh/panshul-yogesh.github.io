{
  "title": "Storage related infrastructure",
  "content": "<div class=\"mw-parser-output\">\n<p>This topic provides you the required specifications to configure the storage server requirements. </p>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-important-icon\"></div><div class=\"admonition-content admonition-important-content\"><div>Skip this section if you have already set up the required storage infrastructure, either by using the ITOM Cloud Deployment Toolkit or by following the system requirements topic.</div></div></div>\n<p>If a container stops or restarts, all changes made inside the container are lost. To save information such as configuration files and databases, the information must be stored external to the container in an entity called Persistent Volume (PV) within the cluster. <span style=\"background-color: transparent;\">To decide the size for each of the disks, refer to the sizing calculator. </span></p>\n<p>The cluster components (say a Pod) access a storage volume in the following flow:</p>\n<pre>Pod -&gt; PVC -&gt; PV-&gt; Storage ( for example, NFS)</pre>\n<p>A PV is a storage object in a cluster that maps to a physical storage for example, an NFS server volume. When you create a PV, the mapped physical storage volume becomes available in the Kubernetes cluster. The cluster components use this storage for persistent data storage. However, they don't use a PV for their storage directly. They in turn use an object called Persistent Volume Claim (PVC). A PVC is an intermediary between Operations Platform requesting a particular storage of a specific type and the PV representing a physical volume in the cluster.</p>\n<p>PVCs are owned by Operations Platform and are automatically created when you deploy Operations Platform. However, a PVC must be bound to a PV and this binding is done by Kubernetes. To enable kubernetes to bind a PV to a PVC the PV’s either have to be created manually upfront, or through a storage provisioner. If you choose to use a storage provisioner, Kubernetes uses the provisioner to create PV’s on demand based on PVCs that are created while deploying. The following two sections describe two ways of creating PVs.</p>\n<h2>Static provisioning without storage provisioner</h2>\n<p>In this scenario, the PVC is directly bound to the persistent volumes. You must create the PVs before deploying Operations Platform. The following diagram illustrates the relationship between the physical storage and the Kubernetes storage objects.</p>\n<div><a class=\"image\" href=\"\"> <img alt=\"Kubernetes Storage Concepts\" border=\"1\" file=\"/mediawiki/images/e/e4/Kube_Storage_Concepts.PNG\" hspace=\"0\" src=\"../../../images/Kube_Storage_Concepts_cfdc8b7a.png\" style=\"width:250px;height:254px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:1px solid black;\" vspace=\"0\"/> </a></div>\n<h2>Dynamic provisioning with storage provisioner</h2>\n<p>You can also set up dynamic volume provisioning, which creates storage volumes (the PVs) on demand. For more information about dynamic volume provisioning, see <a href=\"https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/\" rel=\"nofollow\" target=\"1\" title=\"Kubernetes documentation\">Kubernetes documentation</a>.  If you use dynamic volume provisioning to create the volumes on demand, make sure that the provisioner is able to offer the storage with the <code>ReadWriteOnce</code> and <code>ReadWriteMany</code> access modes. For more information on the access modes,  see <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\" target=\"_blank\" title=\"Kubernetes documentation\">Kubernetes documentation</a>.</p>\n<p>You can also have multiple storage provisioners, for example, one storage provisioner which is able to create PVs with access mode <code>ReadWriteMany</code> and another one with access mode <code>ReadWriteMany</code>. Refer to the documentation of your storage provisioner to see what type of access modes it supports. For more information on the access modes,  see <a href=\"https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner\" title=\"Kubernetes documentation\">Kubernetes documentation</a>.</p>\n<p>When you use a storage provisioner, the PVC is configured to request PVs of a specific storage class. The dynamic storage provisioner responsible for a specific storage class will be triggered to create a new volume. As a result a new PV is created pointing to the dynamically created volume. The PV will be marked with the same storageclass as the PVC which requested the PV. The advantage is that you can define a storage class that maps to different characteristics. The following diagram illustrates how the PVC accesses the persistent volumes through a storage class.</p>\n<div><a class=\"image\" href=\"\"> <img alt=\"Kube_Storage_Concepts_storeclass\" border=\"1\" file=\"/mediawiki/images/e/e4/Kube_Storage_Concepts_storeclass.PNG\" hspace=\"0\" src=\"../../../images/Kube_Storage_Concepts_storeclass_b76f2434.png\" style=\"width:508px;height:239px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:1px solid black;\" vspace=\"0\"/> </a></div>\n<p><span style='color: rgb(51, 51, 51); font-family: Roboto, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 16px;'>For detailed information on storage concepts in Kubernetes see </span><a href=\"https://kubernetes.io/docs/concepts/storage/\" style='font-family: Roboto, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 16px;' target=\"_blank\" title=\"Storage\">Storage</a><span style='color: rgb(51, 51, 51); font-family: Roboto, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; font-size: 16px;'>.</span></p>\n<h2>Storage configuration guidelines</h2>\n<p>The following table provides a mapping of supported storage types based on the storage selector</p>\n<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\">\n<thead>\n<tr>\n<th scope=\"col\">Storage selector</th>\n<th scope=\"col\">Access Mode</th>\n<th scope=\"col\">Qualified storage types</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Default RWO</td>\n<td>ReadWriteOnce</td>\n<td>NFS, Ceph RBD, EBS, Azure Disk, standard-rwo</td>\n</tr>\n<tr>\n<td>Default RWX</td>\n<td>ReadWriteMany</td>\n<td>NFS, EBS, Azure Disk, Cephfs, standard</td>\n</tr>\n<tr>\n<td>Fast RWO</td>\n<td>ReadWriteOnce</td>\n<td>NFS, EBS, Azure Disk, premium-rwo</td>\n</tr>\n</tbody>\n</table>\n<p> The following tables provides a mapping of capabilities and storage selectors for Operations Platform:</p>\n<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\">\n<thead>\n<tr>\n<th scope=\"col\">PVC</th>\n<th scope=\"col\">Storage class selector helm parameter</th>\n<th scope=\"col\">Capabilities</th>\n<th scope=\"col\">Default value</th>\n</tr>\n<tr>\n<td>configvolumeclaim</td>\n<td>global.persistence.storageClasses.default-rwx</td>\n<td>all capabilities</td>\n<td></td>\n</tr>\n<tr>\n<td>datavolumeclaim</td>\n<td>global.persistence.storageClasses.default-rwx</td>\n<td>all capabilities</td>\n<td></td>\n</tr>\n<tr>\n<td>dbvolumeclaim</td>\n<td>global.persistence.storageClasses.default-rwx</td>\n<td>all capabilities</td>\n<td></td>\n</tr>\n<tr>\n<td>logvolumeclaim</td>\n<td>global.persistence.storageClasses.default-rwx</td>\n<td>all capabilities</td>\n<td></td>\n</tr>\n</thead>\n</table>\n<p>The following tables provides a mapping of capabilities and storage selectors for AI Operations Management:</p>\n<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\">\n<thead>\n<tr>\n<th scope=\"col\">PVC</th>\n<th scope=\"col\">Storage class selector helm parameter</th>\n<th scope=\"col\">Capabilities</th>\n<th scope=\"col\">Default value</th>\n</tr>\n<tr>\n<td>omi-artemis-pvc</td>\n<td>global.persistence.storageClasses.default-rwo</td>\n<td>Only if OBM is selected</td>\n<td></td>\n</tr>\n<tr>\n<td>pvc-omi-0, pvc-omi-1*</td>\n<td>global.persistence.storageClasses.default-rwo</td>\n<td>Only if OBM is selected</td>\n<td></td>\n</tr>\n<tr>\n<td>itomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-n</td>\n<td>itomdipulsar.bookkeeper.volumes.journal.storageClassName</td>\n<td>Only if AEC or Reporting is selected</td>\n<td>fast-disks</td>\n</tr>\n<tr>\n<td>itomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-n</td>\n<td>itomdipulsar.bookkeeper.volumes.ledgers.storageClassName</td>\n<td>Only if AEC or Reporting is selected</td>\n<td>fast-disks</td>\n</tr>\n<tr>\n<td>itomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-n</td>\n<td>itomdipulsar.zookeeper.volumes.data.storageClassName</td>\n<td>Only if AEC or Reporting is selected</td>\n<td>fast-disks</td>\n</tr>\n</thead>\n</table>\n<p>Based on the above requirements, make sure that you have the required storage set up.</p>\n<h3>AWS</h3>\n<table>\n<thead>\n<tr>\n<th scope=\"col\">Storage type</th>\n<th scope=\"col\">Notes</th>\n</tr>\n<tr>\n<td>Amazon Elastic File System (Amazon EFS)</td>\n<td>Make sure you configure Amazon EFS with two private subnets, Amazon EFS <strong>performance mode </strong>as General Purpose and <strong>throughput mode</strong> as Bursting Throughput.</td>\n</tr>\n<tr>\n<td>Amazon Elastic Block Store (Amazon EBS)</td>\n<td>\n<p>You will require EBS if you plan to use OPTIC Reporting or any of the capabilities with OPTIC DL. You must deploy the Amazon EBS Container Storage Interface (CSI) <b><em>without</em> </b>the tags.<br/>\n\t\t\tThe OPTIC DL Message Bus uses Amazon Web Services Elastic Block Store (AWS EBS) as the storage with dynamic provisioning. It's recommended to use <strong>io2</strong> and <strong>gp3</strong> EBS volume types. <strong>io2</strong> volumes are supported in the specific regions in AWS. See <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html\" title=\"AWS documentation\">AWS documentation</a> for a list of supported regions.</p>\n<p>If you plan to use an existing OPTIC Data Lake from another deployment, then there is no need to deploy EBS again for OPTIC Data Lake.</p>\n</td>\n</tr>\n</thead>\n</table>\n<h3>Azure</h3>\n<table>\n<thead>\n<tr>\n<th scope=\"col\">Storage type</th>\n<th scope=\"col\">Notes</th>\n</tr>\n<tr>\n<td>Azure File Share</td>\n<td>Premium Azure File Share with <b>&lt;share_quota&gt;</b> greater than 0, and less than or equal to <strong>5 TB</strong> (<code>5120</code>). </td>\n</tr>\n<tr>\n<td>Azure disks</td>\n<td>You will require Azure disks if you plan to use OPTIC Reporting, Hyperscale Observability, AEC, and OBM.</td>\n</tr>\n</thead>\n</table>\n<p>For more information on creating Azure storage see <a href=\"https://learn.microsoft.com/en-us/azure/storage/files/storage-how-to-create-file-share?tabs=azure-portal\" title=\"Azure documentation\">Azure documentation</a>.</p>\n<h3>Google Kubernetes Engine</h3>\n<table>\n<thead>\n<tr>\n<th scope=\"col\">Storage type</th>\n<th scope=\"col\">Notes</th>\n</tr>\n<tr>\n<td>Filestore</td>\n<td>\n<p>Filestore with either of the following:</p>\n<p>Basic HDD with capacity in the range from 1 TiB to 63.9 TiB</p>\n<p>Basic SSD with capacity in the range from 2.5 TiB to 63.9 TiB</p>\n</td>\n</tr>\n</thead>\n</table>\n<h3>Red Hat OpenShift</h3>\n<table>\n<thead>\n<tr>\n<th scope=\"col\">Storage type</th>\n<th scope=\"col\">Notes</th>\n</tr>\n<tr>\n<td>\n<p>Ceph File System (<em>CephFS</em>)</p>\n</td>\n<td>\n<p>To set up OpenShift Data Foundation on an OpenShift Container Platform with worker nodes on a Red Hat Enterprise Linux, you must install Local Operator and OpenShift Data Foundation Operator. See <a href=\"https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.17\" title=\"Product Documentation for Red Hat OpenShift Container Storage\">Red Hat OpenShift Data Foundation</a> for details.</p>\n<p>Configure additional <strong><code>cephfs</code></strong> disks on all worker nodes to persist data across containers started and stopped, and for shared data access between containers. To decide the size for each of the disks, refer to the sizing calculator.</p>\n<p>You can set up the OpenShift Data Foundation (ODF) either internally, within the OpenShift cluster, or externally. <br/>\n\t\t\tWhen using OpenShift Data Foundation as storage, there is no need to create persistent volumes. During deployment, OpenShift Data Foundation will automatically create the required persistent volumes, based on the storage class provided as input.</p>\n<p>You must make sure that the following <strong>StorageClasses</strong> are available:</p>\n<ol>\n<li><code>ocs-storagecluster-cephfs</code></li>\n<li><code>ocs-storagecluster-ceph-rbd</code></li>\n</ol>\n</td>\n</tr>\n<tr>\n<td>Ceph RBD</td>\n<td>\n<p>If you plan to deploy OPTIC Reporting, Automatic Event Correlation, Hyperscale Observability, OBM, or Anomaly Detection, it's mandatory to configure OpenShift Data Foundation as some OPTIC Reporting components require persistent volumes from the <code>ocs-storagecluster-ceph-rbd</code> storage class.</p>\n</td>\n</tr>\n</thead>\n</table>\n<p>If you use dynamic volume provisioning to create the volumes on demand, please make sure that your provisioner is able to offer the storage with access mode ReadWriteOnce and ReadWriteMany. For more information on the access modes refer to the <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes\" target=\"_blank\" title=\"Kubernetes documentation\">Kubernetes documentation</a>.</p>\n<h2>High availability for storage</h2>\n<p>You can have data replication on the storage level and Operations Platform level using High Availability.</p>\n<p>There are multiple ways to achieve HA and prevent data loss of a service based on the state of storage.  You must follow the storage guidelines and recommendations of your organization.</p>\n<p>In addition, when you are preparing to deploy, while you can set any <code>storage-class</code> supported for the Kubernetes distribution you are using, it is important to understand the storage topology underneath the volumes. Following are High Availability recommendations if you are deploying on embedded Kubernetes and Amazon Elastic Kubernetes Services (Amazon EKS).</p>\n<ul>\n<li>If you are deploying a capability such as OPTIC reporting, that requires local persistent volumes (LPVs) and you need to set up High Availability (HA) in an embedded Kubernetes environment, you must allocate the LPVs to at least three worker nodes and also allocate an additional worker node if one of the three workers is lost out of the cluster permanently.</li>\n<li>If you are deploying in Amazon Elastic Kubernetes Services (Amazon EKS) environment, make sure that you’re running nodes in different zones. The term zone here represents a logical failure domain.  You must use a topology aware volume provisioner. For more information on related concepts, see <a href=\"https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone\" title=\"https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone\">https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone</a>.</li>\n</ul>\n</div>",
  "modifiedon": "2026-01-30 14:51:17"
}