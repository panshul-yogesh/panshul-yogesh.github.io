{
  "title": "Sizing considerations for Azure deployment",
  "content": "<div class=\"mw-parser-output\">\n \n\n<p>The performance information provided here is based on tests in an out-of-box environment and for your reference only. Your implementation may consume more resources or require more resources to perform in an acceptable manner.</p>\n<h2 class=\"mw-headline\" id=\"Suite_size_definitions\"><a id=\"SMA_size_definitions\" name=\"SMA_size_definitions\" title=\"SMA size definitions\"></a>SMA size definitions</h2>\n<p>When you run the suite installer, you will need to select a suite size: <strong>Small</strong>, <strong>Medium</strong>, or <strong>Large</strong>. Different suite sizes require different hardware configurations. </p>\n<p>The following table describes the available suite sizes.</p>\n<table>\n<tbody>\n<tr>\n<th>Suite size</th>\n<th>Small</th>\n<th>Medium</th>\n<th>Large</th>\n<th>Notes</th>\n</tr>\n<tr>\n<td>Maximum number of concurrent users (including both ESS and IT agent users)</td>\n<td>100~400</td>\n<td>400~1000</td>\n<td>1000~3000</td>\n<td>Concurrent users are active users who have logged in to the system to perform operations and consume system resources. There are two types of concurrent users, service portal users and IT agent users. Workload for each user type is defined as:\n\t\t\t<ul style=\"padding-left: 0px;\"><li style=\"margin-left: 28px; position: relative;\">For service portal users: After logging in to the system, each portal user has an about 30 minutes duration session on the system and will create 1 ~ 2 orders or requests.</li><li style=\"margin-left: 28px; position: relative;\">For IT agent users: IT agent users have a longer session duration, and each of them will be active for about 1.5 hours after logon and handle 4 ~ 6 tickets.</li></ul>\n<p>Based on the above workload assumption, the business operation throughput can be calculated. For example, for 400 service portal users, the number of newly created orders or requests will be 800 ~ 1600 per hour, while for 400 IT agent users, the number of handled tickets will be 1000 ~ 1600 per hour. If you have a much larger business operation throughput than the numbers provided here, consider adding more worker nodes.</p>\n<p><b>Notes: </b></p>\n<p>The sizing numbers are based on an assumption that there are only standard Service Management tenant users. If you are using one or more Service Manager tenants or using both SMA and Service Manager tenants, user count conversion is needed.</p>\n<p>In general, 1 Service Manager user = 0.7 standard SMA users, because a Service Manager tenant consumes only Service Portal in the container when SMA is integrated with external Service Manager.</p>\n<p>For example, if an SMA deployment needs to support 400 Service Manager tenant concurrent users and 100 Service Management tenant concurrent users, the actual user count is: 400 X 0.7 + 100 = 380. So you need to select the Small size.</p>\n</td>\n</tr>\n<tr>\n<td>Maximum number of records in Smart Analytics</td>\n<td>1 Million</td>\n<td>2 Million</td>\n<td>2~4 Million</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<h2 class=\"mw-headline\" id=\"Sizing_for_SMA_deployment\"><a id=\"Hardware_requirements_for_SMA_SMA_only_\" name=\"Hardware_requirements_for_SMA_SMA_only_\" title=\"Hardware requirements for SMA (SMA only)\"></a>Hardware requirements for SMA (SMA only)</h2>\n<p>The following table lists the minimum resources needed for setting up an SMA environment in the cloud.<br/>\nA standard deployment includes control plane nodes (owned by Azure), worker nodes, NetApp Files or Azure Files &amp; Azure Disks as storage, and PostgreSQL as databases.<br/>\n<strong>Note</strong>: Details of the resources required for the Large profile are currently not available. </p>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\" style=\"text-align: center;\"><b>Suite size (sizing profile)</b></th>\n<th rowspan=\"2\" style=\"text-align: center;\">SMA control plane node</th>\n<th colspan=\"3\" style=\"text-align: center;\">SMA worker node</th>\n<th colspan=\"2\" style=\"text-align: center;\">SMA AFS</th>\n<th colspan=\"4\" style=\"text-align: center;\">SMA Database</th>\n</tr>\n<tr>\n<th style=\"text-align: center;\">Type</th>\n<th style=\"text-align: center;\">Volume</th>\n<th style=\"text-align: center;\">Quantity</th>\n<th style=\"text-align: center;\">Type</th>\n<th style=\"text-align: center;\">Volume**</th>\n<th style=\"text-align: center;\">Type</th>\n<th style=\"text-align: center;\">Storage</th>\n<th colspan=\"2\" rowspan=\"1\" style=\"text-align: center;\">Quantity</th>\n</tr>\n<tr>\n<td rowspan=\"2\">Small</td>\n<td rowspan=\"2\">Owned by Azure</td>\n<td rowspan=\"2\">Standard_D8s_v3</td>\n<td rowspan=\"2\">Disk: 100 GB (Premium SSD)</td>\n<td rowspan=\"2\">3*</td>\n<td>Azure NetApp Files (Standard)***</td>\n<td>\n<p>4 TB***** </p>\n</td>\n<td rowspan=\"2\">Standard_D8s_v3 (Premium SSD)******</td>\n<td rowspan=\"2\">\n<p><span style=\"background-color: transparent;\">256 GB</span></p>\n</td>\n<td colspan=\"2\" rowspan=\"2\">1</td>\n</tr>\n<tr>\n<td>Azure File (Premium) &amp; Azure Disks****</td>\n<td>1 TB for Azure File + 1 TB at most for Azure Disks</td>\n</tr>\n<tr>\n<td rowspan=\"2\">Medium</td>\n<td rowspan=\"2\">Owned by Azure</td>\n<td rowspan=\"2\">Standard_D8s_v3</td>\n<td rowspan=\"2\">Disk: 100 GB (Premium SSD)</td>\n<td rowspan=\"2\">4*</td>\n<td>Azure NetApp Files (Premium)***</td>\n<td>\n<p>4 TB*****</p>\n</td>\n<td rowspan=\"2\">Standard_D16s_v3 (Premium SSD)******</td>\n<td rowspan=\"2\">\n<p><span style=\"background-color: transparent;\">512 GB</span></p>\n</td>\n<td colspan=\"2\" rowspan=\"2\">1</td>\n</tr>\n<tr>\n<td>Azure File (Premium) &amp; Azure Disks****</td>\n<td>1 TB for Azure File + 1 TB at most for Azure Disks</td>\n</tr>\n</tbody>\n</table>\n<p><b>Notes:</b></p>\n<p>* This is the minimum resources requirements for SMA deployment. According to sizing definition benchmark results, our lab tests indicate the average CPU utilization is 40 ~ 60%, and the memory utilization is 60 ~ 75%. If one node is down, it may cause services to be unavailable. From node high availability consideration, it's recommended that you add at least one additional node or more depending on your actual environments such as ticket size, user access rate, and integrations.</p>\n<p>** The volume sizes may grow on a daily basis. Check <b>Storage requirement </b>below for more details.</p>\n<h3 class=\"mw-headline\" id=\"Disk_usage\"><a id=\"Storage_requirement\" name=\"Storage_requirement\" title=\"Storage requirement\"></a>Storage requirement</h3>\n<table>\n<tbody>\n<tr>\n<th>Number of requests in database</th>\n<th>AFS size</th>\n</tr>\n<tr>\n<td>100 K</td>\n<td>160 GB</td>\n</tr>\n<tr>\n<td>300 K</td>\n<td>600 GB</td>\n</tr>\n</tbody>\n</table>\n<p>If you use Azure Files as storage, Azure Disks are used to store the Smart Analytics and RabbitMQ data, which will provision disk dynamically and consume 1 TB of disk usage at most.</p>\n<p>*** Check the <b>Minimal Azure NetApp Files Service Level</b> below for the details to prepare Azure NetApp Files for SMA. The volume quota is used to reserve better performance on storage.</p>\n<h3 class=\"mw-headline\" id=\"Minimal_Azure_NetApp_Files_Service_Level\"><a id=\"Minimal_Azure_NetApp_Files_Service_Level\" name=\"Minimal_Azure_NetApp_Files_Service_Level\" title=\"Minimal Azure NetApp Files Service Level\"></a>Minimal Azure NetApp Files Service Level</h3>\n<table>\n<tbody>\n<tr>\n<th style=\"text-align: center;\"><b>Suite size (sizing profile)</b></th>\n<th style=\"text-align: center;\">Minimal Azure NetApp File Service Level</th>\n<th style=\"text-align: center;\">Volume Quota</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>Standard</td>\n<td>&gt;=4 TB (3 TB quota for Smart Analytics, 1 TB quota for OMT and SMA)</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>Premium</td>\n<td>&gt;=4 TB (2 TB quota for Smart Analytics, 2 TB quota for OMT and SMA)</td>\n</tr>\n</tbody>\n</table>\n<p>**** Check the <b>Minimal Azure File Service Level</b> below for the details to prepare Azure File for SMA. It's not recommended to deploy the large size using Azure Files.</p>\n<h3 class=\"mw-headline\" id=\"Minimal_Azure_File_Service_Level\"><a id=\"Minimal_Azure_Files_Service_Level\" name=\"Minimal_Azure_Files_Service_Level\" title=\"Minimal Azure Files Service Level\"></a>Minimal Azure Files Service Level</h3>\n<table>\n<tbody>\n<tr>\n<th style=\"text-align: center;\"><b>Suite size (sizing profile)</b></th>\n<th style=\"text-align: center;\">Minimal Azure File Service Level</th>\n<th style=\"text-align: center;\">Volume Quota</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>Premium</td>\n<td>&gt;=1 TB</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>Premium</td>\n<td>&gt;=1 TB</td>\n</tr>\n</tbody>\n</table>\n<p>***** 4 TB is the minimum allowed size for Azure NetApp Files. See <a href=\"https://docs.microsoft.com/en-us/azure/azure-netapp-files/azure-netapp-files-set-up-capacity-pool#steps\" title=\"Set up a capacity pool\">Set up a capacity pool</a> for more information.</p>\n<p>****** Azure Database for PostgreSQL Flexible Server is supported and recommended. See more details in <a href=\"/doc/423/26.1/akssupportmatrix#Databases\" title=\"Support matrix for Azure deployment\">Support matrix for Azure deployment</a>.</p>\n<p>The sizes are for your reference only. They may vary depending on your ticket size, user access rate, integrations, and more.</p>\n<h2><a id=\"Hardware_requirements_for_bastion\" name=\"Hardware_requirements_for_bastion\" title=\"Hardware requirements for bastion\"></a>Hardware requirements for bastion</h2>\n<p>The following table lists the minimum resources needed for setting up a bastion in the cloud.</p>\n<table>\n<tbody>\n<tr>\n<th><b>Bastion instance type</b></th>\n<th><b>Boot disk type</b></th>\n<th><b>Boot disk size</b></th>\n</tr>\n<tr>\n<td>Standard B1ms</td>\n<td>Premium SSD</td>\n<td>128 GB</td>\n</tr>\n</tbody>\n</table>\n<p><b>Notes:</b> You can switch to better instance types for better performance.</p>\n<h2 id=\"Additional_hardware_requirements_for_OO\"><a id=\"Additional_hardware_requirements_for_OO_Containerized\" name=\"Additional_hardware_requirements_for_OO_Containerized\" title=\"Additional hardware requirements for OO Containerized\"></a>Additional hardware requirements for OO Containerized</h2>\n<p>The following sizing requirements denote the target deployment size for a dedicated OO Containerized database, based on the number of tenants. This is not based on concurrent users.</p>\n<p>To deploy OO Containerized for a 5-tenant support, use the <strong>oo_size_values.yaml</strong> file provided <a href=\"/doc/423/26.1/399-oocpreparecreateyaml\" title=\"here\">here</a>.</p>\n<p>To deploy OO Containerized for a 20-tenant support, use the <strong>oo_size_values.yaml</strong> file provided <a href=\"/doc/423/26.1/399-oocpreparecreateyaml\" title=\"here\">here</a>.<strong> </strong></p>\n<table>\n<tbody>\n<tr>\n<th colspan=\"1\" rowspan=\"2\" style=\"text-align: center;\">Target size</th>\n<th colspan=\"1\" rowspan=\"2\" style=\"text-align: center;\">Number of OO Containerized tenants</th>\n<th colspan=\"4\" style=\"text-align: center;\">AKS nodes configuration for OO Containerized</th>\n<th colspan=\"2\" rowspan=\"1\" style=\"text-align: center;\">OO Containerized AFS </th>\n<th colspan=\"6\" style=\"text-align: center;\">OO Containerized Database on Azure DB for PostgreSQL - Flexible server</th>\n</tr>\n<tr>\n<th style=\"text-align: center;\">AKS VM type</th>\n<th style=\"text-align: center;\">Configuration</th>\n<th style=\"text-align: center;\">Storage</th>\n<th style=\"text-align: center;\">Quantity</th>\n<th style=\"text-align: center;\">Configuration</th>\n<th style=\"text-align: center;\">Additional storage</th>\n<th style=\"text-align: center;\"> Type</th>\n<th style=\"text-align: center;\">vCPU</th>\n<th style=\"text-align: center;\">Memory</th>\n<th style=\"text-align: center;\">Storage</th>\n<th style=\"text-align: center;\">Quantity</th>\n<th style=\"text-align: center;\">max_connections</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>1 to 5</td>\n<td>Standard D8s v3</td>\n<td>8 vCPU 32 GB RAM</td>\n<td>100 GB</td>\n<td>2</td>\n<td>OO Containerized shares the same AFS server with the suite.</td>\n<td>500 GB</td>\n<td>\n<p>Standard <span style=\"background-color: transparent;\">D4s_v3</span></p>\n<p>(Premium SSD)**</p>\n</td>\n<td>4</td>\n<td>16 GB</td>\n<td>750 GB ~ 5 TB</td>\n<td>1</td>\n<td>\n<p><span style=\"background-color: transparent;\">1400</span><sup style=\"background-color: transparent;\">#</sup></p>\n</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>6 to 20</td>\n<td>Standard D8s v3</td>\n<td>8 vCPU 32 GB RAM</td>\n<td>100 GB</td>\n<td>8</td>\n<td>OO Containerized shares the same AFS server with the suite.</td>\n<td>500 GB</td>\n<td>\n<p>Standard D16s_v3</p>\n<p>(Premium SSD)**</p>\n</td>\n<td>16</td>\n<td>64 GB</td>\n<td>750 GB ~ 5 TB</td>\n<td>1</td>\n<td><span style=\"background-color: transparent;\">5710</span><sup style=\"background-color: transparent;\">##</sup></td>\n</tr>\n</tbody>\n</table>\n<p>#The max_connections value for the OO Containerized database should be set to 1400 or more.</p>\n<p>##The max_connections value for the OO Containerized database should be set to 5710 or more.</p>\n<p>**Azure Database for PostgreSQL Flexible Server is supported and recommended. See more details in the <a href=\"/doc/423/26.1/akssupportmatrix#Databases\" title=\"Support matrix for Azure deployment\">Support matrix for Azure deployment</a>.</p>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><div>For AFS and Postgres HA configuration, follow the recommendations provided in vendor-specific documentation. Example: To configure Postgres HA, see  <a href=\"/doc/423/26.1/hasqlpatroni\" title=\"High Availability PostgreSQL with Patroni\">High Availability PostgreSQL with Patroni</a>.</div></div></div>\n<h2><a id=\"Additional_hardware_requirements_for_DND_OO_Containerized\" name=\"Additional_hardware_requirements_for_DND_OO_Containerized\" title=\"Additional hardware requirements for DND &amp; OO Containerized\"></a>Additional hardware requirements for DND &amp; OO Containerized</h2>\n<p>The system requirements for SMA and Design and Deploy (DND) can be planned while installing SMA or at a later point when you are deploying DND on an existing SMA setup. <br/>\nFirst, make sure that you have determined your SMA deployment size by using the table in the \"SMA size definitions\" section. <br/>\n<br/>\nThen, use the following table to determine your DND deployment size. Note that this table is applicable to a single DND instance. If you will enable DND for multiple tenants (one DND Instance for each tenant), you need to determine the deployment size of each individual DND instance. </p>\n<table>\n<thead>\n<tr>\n<th scope=\"col\">DND deployment size</th>\n<th scope=\"col\">Small</th>\n<th scope=\"col\">Medium</th>\n<th scope=\"col\">Large</th>\n<th scope=\"col\">Notes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cloud subscriptions per hour</td>\n<td>30</td>\n<td>60</td>\n<td>-</td>\n<td colspan=\"1\" rowspan=\"5\">The numbers in the table are meant to be approximate for a given deployment environment, but not exact. For example, a Small environment may only create 5 offerings, or 10 per category, although the table lists a representative number of 20. In your anticipated environment, you may find the expected numbers of tenants to span multiple profile definitions (for example, the number of active subscriptions may fit the Small deployment, while the number of offerings may fit the Medium profile). In such a case, select the profile that seems most representative of your anticipated environment, with the number of total subscriptions of particular importance. The sizing configuration is based on the results of in-house performance testing against these deployment environments. In general, you should find the need to tune the configuration parameters after the installation, as the components can be scaled manually as needed, to support the size of the environment you have specified.</td>\n</tr>\n<tr>\n<td>Number of active subscriptions</td>\n<td>1,000</td>\n<td>7,500</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Number of canceled subscriptions</td>\n<td>10,000</td>\n<td>50,000</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Number of offerings per category</td>\n<td>20</td>\n<td>20</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Total number of designs</td>\n<td>35</td>\n<td>75</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<p>Use the table below to plan the additional hardware resources for each individual DND instance. The additional hardware resources for DND are the total of the additional hardware resources for all DND instances. The table includes the hardware requirements for OO also as it is required for DND. </p>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><div><strong>Important: </strong>When you are installing DND functionality on an existing setup, you need to reconfigure the hardware resources as per the recommendations given in the below tables to accommodate the required additional resources. </div></div></div>\n<p>For OO, choose one of the following options for setting up a database:</p>\n<ul style=\"padding-left: 0px;\"><li style=\"margin-left: 28px; position: relative;\">To set up OO Containerized with a common database, you can choose to connect OO Containerized to the same database instance as the rest of the suite components. For hardware requirements, see the table below <strong>Standard hardware requirements with a common database.</strong><br/>\n<strong>Important:</strong>\n<ul style=\"padding-left: 0px;\"><li style=\"margin-left: 40px; position: relative;\">OO Containerized supports the database servers recommended by the rest of the suite components (that is, Standard D8S_V3), and if you choose to connect OO Containerized to the same database server as the rest of the suite components, make sure the common database server sums up both OO Containerized and the suite requirements. </li><li style=\"margin-left: 40px; position: relative;\">OO Containerized shares the same <strong>AFS</strong> server with the suite.</li></ul>\n</li><li style=\"margin-left: 28px; position: relative;\">To determine standard hardware requirements with a dedicated database for OO Containerized, see <strong>Additional hardware requirements for OO Containerized with a dedicated database</strong> section.</li></ul>\n<p>To deploy OO Containerized for a 5-tenant support, use the <strong>oo_size_values.yaml</strong> file provided <a href=\"/doc/423/26.1/399-oocpreparecreateyaml\" title=\"here\">here</a>.</p>\n<p>To deploy OO Containerized for a 20-tenant support, use the <strong>oo_size_values.yaml</strong> file provided <a href=\"/doc/423/26.1/399-oocpreparecreateyaml\" title=\"here\">here</a>.<strong> </strong></p>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\"><b>DND deployment size</b></th>\n<th rowspan=\"2\">Number of OO Containerized tenants</th>\n<th rowspan=\"2\">SMA control plane node</th>\n<th colspan=\"3\">SMA worker node</th>\n<th>AFS</th>\n<th colspan=\"4\" rowspan=\"1\"> DND and OO Containerized database - PostgreSQL</th>\n</tr>\n<tr>\n<th>Type</th>\n<th>Volume</th>\n<th>Quantity</th>\n<th>Additional configuration</th>\n<th>\n<p>Configuration</p>\n</th>\n<th>Storage</th>\n<th>Quantity</th>\n<th>max_connections</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>1 to 5</td>\n<td>Owned by Azure</td>\n<td>\n<p>Standard</p>\n<p>D8s v3</p>\n</td>\n<td>\n<p>Disk: 100 GB (Premium SSD)</p>\n</td>\n<td>3 ~ 4*</td>\n<td colspan=\"1\" rowspan=\"2\">\n<p>500 GB</p>\n<p>DND and OO share AFS server with the suite.</p>\n</td>\n<td>\n<p>Standard  D16s_v3<sup>#</sup></p>\n<p>(Premium SSD)**</p>\n</td>\n<td>1 TB ~ 5.2 TB</td>\n<td>1</td>\n<td>\n<p>3400<sup>#</sup></p>\n</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>6 to 20</td>\n<td>Owned by Azure</td>\n<td>\n<p>Standard</p>\n<p>D8s v3</p>\n</td>\n<td>Disk: 200 GB (Premium SSD)</td>\n<td>9 ~ 10*</td>\n<td>\n<p>Standard D32s_v3</p>\n<p>(Premium SSD)**</p>\n</td>\n<td>1.6 TB ~ 5.8 TB</td>\n<td>1</td>\n<td>7710<sup>##</sup></td>\n</tr>\n</tbody>\n</table>\n<p>* This is the minimum resources requirements for the deployment. According to the sizing definition benchmark results, our lab tests indicate the average CPU utilization is 40 ~ 60%, and the memory utilization is 60 ~ 75%. If one node is down, it may cause services to be unavailable. From node high availability consideration, it's recommended that you add at least one additional node or more depending on your actual environments such as ticket size, user access rate, and integrations.</p>\n<p>#The <code>max_connections</code> value for the combined database should be equal to or greater than 3400.</p>\n<p>##The<code> max_connections</code> value for the combined database should be equal to or greater than 7710.</p>\n<p>**Azure Database for PostgreSQL Flexible Server is supported and recommended. See more details in the <a href=\"/doc/423/26.1/akssupportmatrix#Databases\" title=\"Support matrix for Azure deployment\">Support matrix for Azure deployment</a>.</p>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><div>For AFS and Postgres HA configuration, follow the recommendations provided in vendor-specific documentation. Example: To configure Postgres HA, see  <a href=\"/doc/423/26.1/hasqlpatroni\" title=\"High Availability PostgreSQL with Patroni\">High Availability PostgreSQL with Patroni</a>.</div></div></div>\n<p><strong>Additional hardware requirements for OO Containerized with a dedicated database</strong></p>\n<p>The following sizing requirements denote the target deployment size for a dedicated OO database, based on the number of tenants. This is not based on concurrent users.</p>\n<table>\n<tbody>\n<tr>\n<th colspan=\"1\" rowspan=\"2\">Target size</th>\n<th rowspan=\"2\">Number of OO Containerized tenants</th>\n<th colspan=\"5\" rowspan=\"1\">Database</th>\n</tr>\n<tr>\n<th colspan=\"2\" rowspan=\"1\">Configuration</th>\n<th><b>Storage</b></th>\n<th>Quantity</th>\n<th>max_connections</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>1 to 5</td>\n<td colspan=\"2\" rowspan=\"1\">Standard D4s_V3 (Premium SSD)**</td>\n<td>750 GB ~ 5 TB</td>\n<td>1</td>\n<td>1400<sup>#</sup></td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>6 to 20</td>\n<td colspan=\"2\" rowspan=\"1\">\n<p>Standard D16s_v3</p>\n<p>(Premium SSD)**</p>\n</td>\n<td>750 GB ~ 5 TB</td>\n<td>1</td>\n<td>5710<sup>##</sup></td>\n</tr>\n</tbody>\n</table>\n<p>#The max_connections value for the combined database should be equal to or greater than 1400</p>\n<p>##The <code>max_connections</code> value for the combined database should be equal to or greater than 5710</p>\n<p>**Azure Database for PostgreSQL Flexible Server is supported and recommended. See more details in the <a href=\"/doc/423/26.1/akssupportmatrix#Databases\" title=\"Support matrix for Azure deployment\">Support matrix for Azure deployment</a>.</p>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><div>For Postgres HA configuration, follow the recommendations provided in vendor-specific documentation. Example: To configure Postgres HA, see  <a href=\"/doc/423/26.1/hasqlpatroni\" title=\"High Availability PostgreSQL with Patroni\">High Availability PostgreSQL with Patroni</a>.</div></div></div>\n<h2><a id=\"Additional_hardware_requirements_for_CMP_FinOps\" name=\"Additional_hardware_requirements_for_CMP_FinOps\" title=\"Additional hardware requirements for CMP FinOps\"></a>Additional hardware requirements for CMP FinOps</h2>\n<p>Use the table below to plan the additional hardware resources for CMP FinOps.</p>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\"><b>Suite size (sizing profile)</b></th>\n<th rowspan=\"2\">\n<p>Number of Cloud Cost Provider Integrations</p>\n<p>(Total number of integrations across all CMP FinOps tenants)</p>\n</th>\n<th colspan=\"3\">Worker node</th>\n<th colspan=\"1\" rowspan=\"2\">Showback replica count</th>\n<th colspan=\"3\" rowspan=\"1\">Vertica EON database</th>\n</tr>\n<tr>\n<th>Azure Instance type</th>\n<th>Volume</th>\n<th>Quantity</th>\n<th>Azure Instance type</th>\n<th>Volume</th>\n<th>Vertica node count</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>1 to 5</td>\n<td>8 vCPU\n\t\t\t<p>32 GiB Memory</p>\n<p>(Standard D8s_v3)</p>\n</td>\n<td colspan=\"1\" rowspan=\"3\">Disk: 100 GB</td>\n<td>2-4</td>\n<td>2-4</td>\n<td>\n<p>*16 vCPU<br/>\n\t\t\t128 GiB memory</p>\n<p>(Standard D32s_v3)</p>\n</td>\n<td colspan=\"1\" rowspan=\"3\">\n<p>Depot storage: 240 GB</p>\n<p>Catalog storage: 50 GB</p>\n<p>Temporary data storage: 100 GB</p>\n</td>\n<td>3</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>6 to 20</td>\n<td colspan=\"1\" rowspan=\"2\">\n<p>16 vCPU<br/>\n\t\t\t64 GiB memory</p>\n<p>(Standard D16s_v3)</p>\n</td>\n<td>4-6</td>\n<td>4-6</td>\n<td rowspan=\"2\">\n<p>*32 vCPU<br/>\n\t\t\t256 GiB memory</p>\n<p>(Standard D64s_v3)</p>\n</td>\n<td>3-6</td>\n</tr>\n<tr>\n<td>Large</td>\n<td>21 to 30</td>\n<td>6-8</td>\n<td>6-8</td>\n<td>6-9</td>\n</tr>\n</tbody>\n</table>\n<div>*This recommendation is based on our lab tests results. Per <a href=\"https://learn.microsoft.com/en-us/azure/virtual-machines/dv3-dsv3-series\" title=\"Azure sizing guide\">Azure sizing guide</a>, the Azure instance type mentioned are the closest sizing to our lab tests configuration.</div>\n<p>Average load expected on CMP FinOps: 60,000,000 billing lines per month</p>\n<p><strong>Note</strong>: For Vertica HA configuration, follow the recommendations provided in vendor-specific documentation. </p>\n<h3><a id=\"Performance_tuning_for_CMP_FinOps\" name=\"Performance_tuning_for_CMP_FinOps\" title=\"Performance tuning for CMP FinOps\"></a>Performance tuning for CMP FinOps</h3>\n<p>This section contains configuration changes required to support a higher number of Cloud Cost Provider integrations (small, medium, and large sizing profiles). </p>\n<h4><a id=\"Configure_node_group_on_your_Azure_cluster\" name=\"Configure_node_group_on_your_Azure_cluster\" title=\"Configure node group on your Azure cluster\"></a>Configure node group on your Azure cluster</h4>\n<p>On your Azure cluster, configure the following parameters as part of node grouping based on your sizing profile.</p>\n<ul style=\"padding-left: 0px;\"><li style=\"margin-left: 28px; position: relative;\">Node group name. Example: <code>cgro</code></li><li style=\"margin-left: 28px; position: relative;\">Desired size (node count). Example: 8</li><li style=\"margin-left: 28px; position: relative;\">Add Kubernetes label. Example: Worker cgro.</li></ul>\n<h4><a id=\"Update_the_nodeSelector_parameter\" name=\"Update_the_nodeSelector_parameter\" title=\"Update the nodeSelector parameter\"></a>Update the nodeSelector parameter</h4>\n<p>To run the CGRO service on specified nodes only, set the  <code>nodeSelector.Worker</code> parameter's value to the label of the worker nodes created for the CGRO service.</p>\n<p>Complete the following steps to update the <code>nodeSelector</code><strong> </strong>parameter.</p>\n<ol style=\"padding-left: 0px;\"><li style=\"--number-width: 10px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Extract the deployment values to a file named <b style=\"\">cgro-showback.yaml.</b>\n<pre><code>helm get values &lt;release name&gt; -n &lt;suite namespace&gt; &gt; cgro-showback.yaml\n</code></pre>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Locate the  <code>itom-cgro-showback</code> parameter and copy it along with its keys to a new file named <strong>cgro_nodeSelector.yaml. </strong>The content to copy will resemble the following:\n\t<pre><code>itom-cgro-showback:\n  replicas: 1\n  global:\n    nodeSelector:</code></pre>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Update the  <strong>cgro_nodeSelector.yaml </strong>file to make the following changes:\n\t<ol style=\"padding-left: 0px;\"><li style=\"--number-width: 21px; --number-spacing: 25px; margin-left: 49px; position: relative;\">Add a new top-level key <code>cgro:</code>, move the contents of  <code>itom-cgro-showback:</code> parameter under it.</li><li style=\"--number-width: 25px; --number-spacing: 29px; margin-left: 53px; position: relative;\">Update the <code>NodeSelector</code> parameter to include the worker node label. See the following sample for the updated content. In the sample, the value 'cgro' in <code>Worker: cgro</code> is the worker nodes label name.</li></ol>\n<pre>cgro:\n  itom-cgro-showback:\n    replicas: 1\n    global:\n      nodeSelector:\n        Worker: \"cgro\"</pre>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following command to apply this update:\n\t<pre>helm get values &lt;release name&gt; -n &lt;suite namespace&gt; &gt; my-values.yaml\nhelm upgrade &lt;release name&gt; &lt;ESM helm chart path&gt; -n &lt;suite namespace&gt; -f  my-values.yaml -f cgro_nodeSelector.yaml\n</pre>\n\tSee the following sample command:\n\n\t<pre>helm upgrade sma ESM_Helm_Chart-2x.x.x\\charts\\esm-1.0.0+2x.x-xxx.tgz -n itsma-xxx -f my-values.yaml -f cgro_nodeSelector.yaml</pre>\n</li><li style=\"--number-width: 12px; --number-spacing: 24px; margin-left: 36px; position: relative;\">To update the <code>CGRO_SHOWBACK_COLLECT_COPY_STREAM_RESOURCE_POOL_NAME</code> setting in the <strong>itom-cgro-showback-deployment</strong>:\n\t<ol style=\"padding-left: 0px;\"><li style=\"--number-width: 21px; --number-spacing: 25px; margin-left: 49px; position: relative;\">Copy the following content to a new file named <strong>cgro_copy_stream.yaml</strong>:\n\t\t<pre><code>cgro:\n  itom-cgro-showback:\n    collect:\n      copy_stream_resource_pool_name: \"itom_finops_etl\"</code></pre>\n</li><li style=\"--number-width: 25px; --number-spacing: 29px; margin-left: 53px; position: relative;\">Change the value of the <code>copy_stream_resource_pool_name</code> property to the required value and save the file.</li></ol>\n</li><li style=\"--number-width: 12px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following command to apply this update:\n\t<pre><code>helm upgrade &lt;release name&gt; &lt;ESM helm chart path&gt; -n &lt;suite namespace&gt; -f my-values.yaml -f cgro_copy_stream.yaml</code></pre>\n</li></ol>\n<h4><a id=\"Separate_ETL_and_query_workloads\" name=\"Separate_ETL_and_query_workloads\" title=\"Separate ETL and query workloads\"></a><span style=\"font-size: clamp(1.125rem, 13.2632px + 0.986842vw, 1.5rem);\">Separate ETL and query workloads</span></h4>\n<p>Perform the following tasks to divide ETL workloads and query workloads into separate resource pools.</p>\n<ol style=\"padding-left: 0px;\"><li style=\"--number-width: 10px; --number-spacing: 24px; margin-left: 36px; position: relative;\">On the Vertica database, as a database administrator, run the following command to create a custom ETL resource pool.\n\t<p>For small profile:</p>\n<pre> CREATE RESOURCE POOL itom_finops_etl MAXMEMORYSIZE '40%' PLANNEDCONCURRENCY 64 MAXCONCURRENCY 64;</pre>\n<p>For medium and large proflies:</p>\n<pre>CREATE RESOURCE POOL itom_finops_etl MAXMEMORYSIZE '40%' PLANNEDCONCURRENCY 128 MAXCONCURRENCY 128;</pre>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following commands to validate that the new resource pools are created:\n\t<pre>SELECT * from RESOURCE_POOLS\nSELECT * from RESOURCE_POOL_STATUS</pre>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Perform the following configuration changes on the Vertica database as a Vertica Database Administrator:\n\t<p>For small profile:</p>\n<pre>ALTER RESOURCE POOL general PLANNEDCONCURRENCY 32;\nALTER RESOURCE POOL general MAXCONCURRENCY 32;\n</pre>\n<p>For medium and large profiles:</p>\n<pre>ALTER RESOURCE POOL general PLANNEDCONCURRENCY 64;\nALTER RESOURCE POOL general MAXCONCURRENCY 64;</pre>\n</li></ol>\n<ol start=\"4\" style=\"padding-left: 0px;\"><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following commands on the bastion host to restart the pods:\n\t<pre>kubectl scale deployment itom-cgro-costpolicy --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-costpolicy --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-insights --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-insights --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-insights-gateway --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-insights-gateway --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-policy-gateway --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-policy-gateway --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-showback --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-showback --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-showback-gateway --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-showback-gateway --replicas=1 -n &lt;itsma_namespace&gt;\n</pre>\n</li><li style=\"--number-width: 12px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Scale up the showback pod as needed. For example, run the following command on the bastion host to scale up the replica to 2.\n\t<pre>kubectl scale deployment itom-cgro-showback --replicas=2 -n &lt;itsma_namespace&gt;</pre>\n\tSee the table above for specific replica counts based on workloads. </li></ol>\n<p><span style=\"color: rgb(0, 115, 231); font-family: Roboto, sans-serif; font-size: 24px;\">Additional hardware requirements for SAM</span></p>\n<p>Use the table below to plan the additional hardware resources for Software Asset Management (SAM).</p>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\" style=\"text-align: center;\"><b>SAM deployment size</b></th>\n<th colspan=\"3\" style=\"text-align: center;\">SMA worker node</th>\n<th style=\"text-align: center;\">AFS</th>\n</tr>\n<tr>\n<th style=\"text-align: center;\">Type</th>\n<th style=\"text-align: center;\">Volume</th>\n<th style=\"text-align: center;\">Quantity</th>\n<th style=\"text-align: center;\">Configuration</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>Standard D8s v3</td>\n<td>Disk: 100 GB (Premium SSD)</td>\n<td>1</td>\n<td colspan=\"1\" rowspan=\"3\">No additional storage is required for SAM. SAM shares the SMA storage resources.</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>Standard D8s v3</td>\n<td>Disk: 100 GB (Premium SSD)</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Large</td>\n<td>Standard D8s v3</td>\n<td>Disk: 100 GB (Premium SSD)</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Additional_hardware_requirements_for_CMS\"><a id=\"Additional_hardware_requirements_for_UCMDB\" name=\"Additional_hardware_requirements_for_UCMDB\" title=\"Additional hardware requirements for UCMDB\"></a>Additional hardware requirements for UCMDB</h2>\n<p>Native SACM and SAM require you to install UCMDB (UCMDB Server and UCMDB Gateway). You have two options:</p>\n<ul style=\"padding-left: 0px;\"><li style=\"margin-left: 28px; position: relative;\">Classic UCMDB + Classic UCMDB Gateway</li><li style=\"margin-left: 28px; position: relative;\">Containerized UCMDB (which comes with the UCMDB Server and UCMDB Gateway)</li></ul>\n<p>See <a href=\"/integration/Service_Management_Automation_X/all/Universal_Discovery_and_CMDB\" title=\"here\">here</a> for the supported <span style=\"font-size:11.0pt\"><span style=\"font-family:Calibri\">UCMDB </span></span>versions in this release.</p>\n<h3><a id=\"Classic_UCMDB\" name=\"Classic_UCMDB\" title=\"Classic UCMDB\"></a>Classic UCMDB</h3>\n<p>If you plan to use classic UD/UCMDB, install UCMDB Gateway on the UCMDB Server, and add the following minimal resources for UCMDB Gateway (for classic UD/UCMDB hardware resource requirements, see <a href=\"/doc/401/25.4/sysreqshardware\" title=\"Classic UD/UCMDB sizing recommendations\">Classic UCMDB sizing recommendations</a>):</p>\n<table>\n<tbody>\n<tr>\n<th><b>Deployment</b></th>\n<th><b>CPU</b></th>\n<th><b>RAM</b></th>\n<th><b>Disk Space</b></th>\n</tr>\n<tr>\n<td><b>All sizes</b></td>\n<td>4 cores</td>\n<td>2 GB (allocated for UCMDB Gateway only)</td>\n<td>100 GB</td>\n</tr>\n</tbody>\n</table>\n<h3><a id=\"Containerized_UCMDB\" name=\"Containerized_UCMDB\" title=\"Containerized UCMDB\"></a>Containerized UCMDB</h3>\n<p>If you plan to install the containerized UCMDB (as the next deployment on the same OMT), use the following guidelines to determine the additional hardware resources required for UCMDB.</p>\n<p id=\"Determine_the_deployment_size\"><strong>1. Determine the deployment size</strong></p>\n<p>When installing UCMDB, you will need to select a deployment size: Small, Medium, or Large, so that the UCMDB deployment is automatically scaled according to the selected size. Use the following table to determine your deployment size.</p>\n<table>\n<thead>\n<tr>\n<th scope=\"col\">Deployment size </th>\n<th scope=\"col\">Small</th>\n<th scope=\"col\">Medium</th>\n<th scope=\"col\">Large</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Maximum number of CIs and Relationships in UCMDB</td>\n<td>10 million</td>\n<td>60 million</td>\n<td>200 million</td>\n</tr>\n</tbody>\n</table>\n<p id=\"Determine_the_hardware_resource_requirements\"><strong>2. Determine the hardware resource requirements</strong></p>\n<p>The following table lists the minimum resources needed for setting up a <span style=\"font-size:11.0pt\"><span style=\"font-family:Calibri\">UCMDB </span></span>environment in the cloud.</p>\n<p>A standard deployment includes control plane nodes (owned by Azure), worker nodes, Azure Files as storage, and PostgreSQL as databases.</p>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\">Suite size<br/>\n\t\t\t(sizing profile)</th>\n<th rowspan=\"2\">\n<p><span style=\"font-size:11.0pt\"><span style=\"font-family:Calibri\">UCMDB </span></span></p>\n\t\t\tcontrol plane node</th>\n<th colspan=\"3\">\n<p><span style=\"font-size:11.0pt\"><span style=\"font-family:Calibri\">UCMDB </span></span>worker node</p>\n</th>\n<th colspan=\"2\">\n<p><span style=\"font-size:11.0pt\"><span style=\"font-family:Calibri\">UCMDB </span></span>AFS</p>\n</th>\n<th colspan=\"3\">\n<p><span style=\"font-size:11.0pt\"><span style=\"font-family:Calibri\">UCMDB </span></span>Database</p>\n</th>\n</tr>\n<tr>\n<th>Type</th>\n<th>Volume</th>\n<th>Quantity</th>\n<th>Type</th>\n<th>Volume</th>\n<th>Type</th>\n<th>Storage</th>\n<th>Quantity</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>Owned by Azure</td>\n<td>Standard D8s v3</td>\n<td>Disk: 100 GB<br/>\n\t\t\t(Premium SSD)</td>\n<td>2</td>\n<td>\n<p>Azure File (Premium)</p>\n</td>\n<td>1 TB / 1 TB</td>\n<td>Standard D8S_V3<br/>\n\t\t\t(Premium SSD) <span style=\"font-size: 13.3333px;\">*</span></td>\n<td>200 GB</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>Owned by Azure</td>\n<td>Standard D8s v3</td>\n<td>Disk: 100 GB<br/>\n\t\t\t(Premium SSD)</td>\n<td>3</td>\n<td>\n<p>Azure File (Premium) </p>\n</td>\n<td>1 TB / 1 TB</td>\n<td>Standard D16S_V3<br/>\n\t\t\t(Premium SSD) *</td>\n<td>400 GB</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Large</td>\n<td>Owned by Azure</td>\n<td>Standard D8s v3</td>\n<td>Disk: 100 GB<br/>\n\t\t\t(Premium SSD)</td>\n<td>4</td>\n<td>\n<p>Azure File (Premium) </p>\n</td>\n<td>1 TB / 1 TB</td>\n<td>Standard D16S_V3<br/>\n\t\t\t(Premium SSD) *</td>\n<td>800 GB</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<p>* Azure Database for PostgreSQL Flexible Server is supported and recommended. See more details in the <a href=\"/doc/423/26.1/akssupportmatrix#Databases\" title=\"Support matrix for Azure deployment\">Support matrix for Azure deployment</a>.</p>\n<h2 class=\"mw-headline\"><a id=\"Tuning_configurations\" name=\"Tuning_configurations\" title=\"Tuning configurations\"></a>Tuning configurations</h2>\n<p>​​​<span style=\"font-family: Roboto, sans-serif; font-size: 20px;\">Ingress and xruntime-ui tuning</span></p>\n<p>When you use Azure Files as the storage service and login or other operations are loading too many static resources, check if the response time of static resources is slow and scale out pairs of the <b>ingress</b> and <b>xruntime-ui</b> pods if needed:</p>\n<ol style=\"padding-left: 0px;\"><li style=\"--number-width: 10px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Log in to the service portal and press <i>F12</i> to access the developer console. Check the response time of each static resource as shown in the screenshot below:\n\n\t<div><a class=\"image\" href=\"\"> <img alt=\"staticRequestTime.png\" data-file-height=\"195\" data-file-width=\"1311\" src=\"../../../images/staticRequestTime_71d2a518.png\" style=\"width: 1311px; height: 195px;\"/> </a></div>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">If the response time of many static resources is more than 10s, run the following command to open the ingress configmap and add the <code>$request</code> field to log format as shown in the screenshot below:\n\t<pre>kubectl edit cm itom-ingress-controller-conf -n &lt;namespace&gt;</pre>\n<div><a class=\"image\" href=\"\"> <img alt=\"request.png\" data-file-height=\"361\" data-file-width=\"825\" src=\"../../../images/request_2880a3b9.png\" style=\"width: 825px; height: 361px;\"/> </a></div>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Access the ingress access log, locate the correct static resources, and check the response time as shown in the screenshot below:\n\t<div><a class=\"image\" href=\"\"> <img alt=\"AzureAccessLog.png\" data-file-height=\"171\" data-file-width=\"1211\" src=\"../../../images/AzureAccessLog_d5754005.png\" style=\"width: 1211px; height: 171px;\"/> </a></div>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">If the response time is still more than 10s, you can scale out pairs of the <b>ingress</b> and <b>xruntime-ui</b> pods by running the following commands:\n\t<pre>kubectl scale deploy itom-ingress-controller --replicas=&lt;number&gt; -n &lt;namespace&gt;\nkubectl scale deploy itom-xruntime-ui --replicas=&lt;number&gt; -n &lt;namespace&gt;</pre>\n<p>In these commands, &lt;number&gt; is the replicas number you want to reach by scaling out and &lt;namespace&gt;is your suite namespace.</p>\n<p><b>Note</b>: One pair of <b>ingress</b> and <b>xruntime-ui</b> pods can support 8~15 concurrent users' login in one minute. Scale out these two pods according to your requirement.</p>\n</li></ol>\n</div>",
  "modifiedon": "2025-10-24 08:51:12"
}