{
  "title": "Balance cluster resource usage",
  "content": "<div class=\"mw-parser-output\"><p class=\"mw-empty-elt\">\n</p><p><br/>\nIf the CPU and memory usage on a worker node in your k8s system reaches a percentage larger than 80%, your k8s system may have stability issues. You can check the resource availability of all worker nodes, and balance the resource usage of worker nodes if needed.</p>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"><span id=\"cke_bm_503C\" style=\"display: none;\"> </span></div>\n<div class=\"admonition-content admonition-note-content\">The CPU or memory usage for nodes is an indicator to be used when system instability is detected. While rebalancing nodes to a lower CPU or memory usage can fix stability issues, it's also possible and normal that you see the CPU or memory usage rises to above 80% without causing any stability issues; if no stability issues, resource balancing isn't needed. </div>\n</div>\n<h2 class=\"mw-headline\" id=\"Check_resource_availability_of_all_worker_nodes\">Check resource availability of all worker nodes</h2>\n<ol>\n<li>Check the CPU and Memory usage of the worker nodes by running the following command on one of the worker nodes.\n\t<pre>kubectl top nodes</pre>\n</li>\n<li>If any CPU or Memory usage value is higher than 80%, the system may have stability issues. We recommend you to take the actions below.<br/>\n\tIf the average CPU or Memory usage is higher than 80%, do the following:\n\t<ol>\n<li>Restart the SMA suite, and then check if the average usage becomes higher than 80% again after several peak times.</li>\n<li>If any average resource usage is still higher than 80%, add a new worker node and then follow the guidance in the \"Balance the resource usage\" section, to balance the resources until no resource usage is higher than 80%.</li>\n</ol>\n<p>If the average CPU or Memory usage is lower than 80%, while the CPU or Memory usage on a worker node is higher than 80%, follow the guidance in the \"Balance the resource usage\" section to balance the resources until no usage is higher than 80%.</p>\n</li>\n</ol>\n<h2 class=\"mw-headline\" id=\"Calculate_the_average_CPU_and_Memory_usage\">Calculate the average CPU and Memory usage</h2>\n<p>Calculate the average usage values as follows:</p>\n<ul>\n<li>Average CPU usage = SUM(CPU% of all nodes)/number of nodes</li>\n<li>Average Memory usage= SUM(MEMORY% of all nodes)/number of nodes</li>\n</ul>\n<h2 class=\"mw-headline\" id=\"Balance_the_resource_usage\">Balance the resource usage</h2>\n<p>Balance the resources based on the number of available pods as described below.</p>\n<ol>\n<li>Check the number of available pods for each deployment, by running the following command.\n\t<pre>kubectl get deployment -n &lt;suite namespace&gt;</pre>\n<p>Next, use the appropriate method depending on your number of available pods.</p>\n</li>\n<li>If the number of available pods is 1, run the rolling restart command to reschedule the pod to a new node.\n\t<pre>kubectl patch deployment &lt;deployment name&gt; --type json -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/LAST_ROLLING_RESTART\", \"value\": \"'$(date +%s)'\"}]' -n &lt;suite namespace&gt;\n</pre>\n<p>If the number of available pods is greater than 1, run the following command to directly delete the pod from the node that has high resource usage.</p>\n<pre>kubectl delete pod &lt;pod name&gt; -n &lt;suite namespace&gt;</pre>\n<p><b>List of balancible deployments</b></p>\n<p>You can balance the following deployments at any time (even during peak hours) - balance them first:</p>\n<ul>\n<li>itom-xruntime-help-center</li>\n<li>itom-xruntime-opb-ui</li>\n<li>itom-xruntime-ppo</li>\n<li>itom-bo-license-deployment</li>\n<li>itom-bo-user-offline-deployment</li>\n<li>itom-bo-config-deployment</li>\n<li>auth</li>\n<li>autopass-lm-v2</li>\n</ul>\n<p>You can balance the following deployments only during non-peak hours. After balancing the deployments listed above, if resource balancing is still needed, you can balance the following deployments during non-peak hours:</p>\n<ul>\n<li>itom-xruntime-platform</li>\n<li>itom-xruntime-platform-offline</li>\n<li>itom-xruntime-serviceportal</li>\n<li>itom-xruntime-gateway</li>\n<li>itom-xruntime-mobile-gateway</li>\n<li>idm</li>\n<li>itom-bo-ats-deployment</li>\n<li>itom-bo-user-deployment</li>\n<li>itom-bo-facade-deployment</li>\n<li>itom-bo-login-deployment</li>\n<li>smarta-admin-ui-backend</li>\n<li>smarta-data-source</li>\n<li>smarta-idolfarm-config</li>\n<li>smarta-installer</li>\n<li>smarta-ocr</li>\n<li>smarta-saw-dah</li>\n<li>smarta-saw-dih</li>\n<li>smarta-saw-proxy</li>\n<li>smarta-sawarc-dah</li>\n<li>smarta-sawarc-dih</li>\n<li>smarta-sawmeta-dah</li>\n<li>smarta-sawmeta-dih</li>\n<li>smarta-search</li>\n<li>smarta-smart-ticket</li>\n<li>smarta-smart-ticket-admin-ui</li>\n<li>smarta-smart-ticket-task</li>\n<li>smarta-stx-category</li>\n<li>smarta-stx-dah</li>\n<li>smarta-stx-imgsvr</li>\n</ul>\n</li>\n<li>Reallocate resources as follows:\n\t<ol start=\"1\">\n<li>Run the following command to obtain a pod distribution list:\n\t\t<pre>kubectl get pods -n &lt;suite namespace&gt; --sort-by=spec.nodeName -l \"itsmaService in (itom-auth,itom-autopass,itom-idm,itom-bo,itom-xruntime,itom-smartanalytics)\" -o wide</pre>\n</li>\n<li>From the pod list, find out the pods that are running on a node with high resource usage and also belong to a deployment on the list of balancible deployments.</li>\n<li>Determine the rebalancing method applicable for the pod (running the rolling restart command to reschedule the pod to a new node or deleting the pod), and rebalance the pod accordingly.</li>\n<li>Repeat the steps above until all resource usage values are less than 80%.</li>\n</ol>\n</li>\n</ol>\n<h2 class=\"mw-headline\" id=\"Example\">Example</h2>\n<p>The following is an example of how to rebalance resource usage.</p>\n<ol>\n<li>Run the following command on one of the worker nodes in the cluster.\n\t<pre>kubectl top nodes</pre>\n<p>In this example, the command output is shown in the following figure.<br/>\n<a class=\"image\" href=\"/file/images/d/dc/SMA_2019.02_resource_usage.PNG\" title=\"/file/images/d/dc/SMA_2019.02_resource_usage.PNG\"> <img alt=\"SMA 2019.02 resource usage.PNG\" data-file-height=\"67\" data-file-width=\"547\" height=\"67\" src=\"../../../images/SMA_2019.02_resource_usage_b6abaf6b.png\" width=\"547\"/> </a></p>\n</li>\n<li>Calculate the average CPU and Memory usage values as follows:\n\t<ul>\n<li>CPU average: (38%+22%)/2=30%</li>\n<li>Memory average: (87%+80%)/2=83.5%</li>\n</ul>\n</li>\n<li>Since the average Memory usage value is much higher than 80%, so we decide to add a new worker node.<br/>\n\tAfter adding the new node, the resource usage is shown as below.<br/>\n<a class=\"image\" href=\"/file/images/f/ff/SMA_2019.02_resource_usage2.PNG\" title=\"/file/images/f/ff/SMA_2019.02_resource_usage2.PNG\"> <img alt=\"SMA 2019.02 resource usage2.PNG\" data-file-height=\"81\" data-file-width=\"590\" height=\"81\" src=\"../../../images/SMA_2019.02_resource_usage2_967894b3.png\" width=\"590\"/> </a><br/>\n\tNext, we will follow the steps in the \"Balance the resource usage\" section to balance the resources.</li>\n<li>Since one node has a Memory usage higher than 80%, we need to immediately rebalance the resource usage even it's during peak hours. Run the following command to check the pod distribution:\n\t<pre>kubectl get pods -n &lt;suite namespace&gt; --sort-by=spec.nodeName -l \"itsmaService in (itom-auth,itom-idm,itom-bo,itom-xruntime,itom-idm,itom-smartanalytics)\" -o wide </pre>\n<p>The following figure shows the result.<br/>\n<a class=\"image\" href=\"/file/images/2/27/SMA_2019.02_resource_usage3.PNG\" title=\"/file/images/2/27/SMA_2019.02_resource_usage3.PNG\"> <img alt=\"SMA 2019.02 resource usage3.PNG\" data-file-height=\"532\" data-file-width=\"1302\" height=\"532\" src=\"../../../images/SMA_2019.02_resource_usage3_20201c82.png\" width=\"1302\"/> </a><br/>\n\tAll rebalancible resources are on a worker node with high resource usage, so we can rebalance all of them to the new node.</p>\n</li>\n<li>We need to decide on which method to use for resource rebalancing. To do this, run the following command to check the number of available pods for each of these deployments.\n\t<pre>[root@mfc-itsma-suite-vm-10 ~]# kubectl get deployment -n &lt;suite namespace&gt; --sort-by=metadata.name \nNAME                               READY   UP-TO-DATE   AVAILABLE  AGE\nauth                               1/1     1            1           21m\nautopass-lm-v2                     1/1     1            1           25h\nitom-bo-config-deployment          1/1     1            1           25h\nitom-bo-license-deployment         1/1     1            1           25h\nitom-bo-user-offline-deployment    1/1     1            1           354d\nitom-xruntime-help-center          1/1     1            1           354d\nitom-xruntime-opb-ui               1/1     1            1           354d\nitom-xruntime-ppo                  1/1     1            1           25h\n</pre>\n<p>In this example, the number of AVAILABLE pods for each deployment is 1. So we should use the rolling restart method for these deployments.</p>\n</li>\n<li>Run the rolling restart command for the deployments one by one to rebalance these deployments first during peak hours. <strong>Note</strong>: Do <strong>Not</strong> restart more than one pod at a time; wait for the pod to be fully restarted and for the old one to be deleted before running the next command. \n\t<pre>kubectl patch deployment auth --type json -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/LAST_ROLLING_RESTART\", \"value\": \"'$(date +%s)'\"}]' -n &lt;suite namespace&gt;\nkubectl patch deployment autopass-lm-v2 --type json -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/LAST_ROLLING_RESTART\", \"value\": \"'$(date +%s)'\"}]' -n &lt;suite namespace&gt;\nkubectl patch deployment itom-bo-config-deployment --type json -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/LAST_ROLLING_RESTART\", \"value\": \"'$(date +%s)'\"}]' -n &lt;suite namespace&gt;\nkubectl patch deployment itom-bo-license-deployment --type json -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/LAST_ROLLING_RESTART\", \"value\": \"'$(date +%s)'\"}]' -n &lt;suite namespace&gt;\nkubectl patch deployment itom-xruntime-help-center --type json -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/LAST_ROLLING_RESTART\", \"value\": \"'$(date +%s)'\"}]' -n &lt;suite namespace&gt;\nkubectl patch deployment itom-xruntime-opb-ui --type json -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/LAST_ROLLING_RESTART\", \"value\": \"'$(date +%s)'\"}]' -n &lt;suite namespace&gt;\nkubectl patch deployment itom-xruntime-ppo --type json -p='[{\"op\": \"add\", \"path\": \"/spec/template/metadata/labels/LAST_ROLLING_RESTART\", \"value\": \"'$(date +%s)'\"}]' -n &lt;suite namespace&gt;\n\n</pre>\n</li>\n<li>After all pods are rebalanced, run the following command to check the resource usage again:\n\t<pre>kubectl top nodes</pre>\n<p><a class=\"image\" href=\"/file/images/d/da/SMA_2019.02_resource_usage4.PNG\" title=\"/file/images/d/da/SMA_2019.02_resource_usage4.PNG\"> <img alt=\"SMA 2019.02 resource usage4.PNG\" data-file-height=\"81\" data-file-width=\"575\" height=\"81\" src=\"../../../images/SMA_2019.02_resource_usage4_3112e536.png\" width=\"575\"/> </a><br/>\n\tThe figure above shows that one node still has a memory usage value higher than 80%, so we need to rebalance the rest of the rebalancible deployments during non-peak hours.</p>\n</li>\n</ol>\n<p class=\"mw-empty-elt\"></p></div>",
  "modifiedon": "2025-10-24 08:51:12"
}