{
  "title": "EKS deployment FAQs",
  "content": "<div class=\"mw-parser-output\"><p>This topic answers certain most frequently asked questions about deploying the suite on AWS with EKS. </p><h2 class=\"mw-headline\" id=\"General\">General</h2><span class=\"snippet-start\" data-sname=\"423-25.4-423-25.3-AKSDeploymentFAQ|General|N\"></span><p>The following are general questions that apply to all supported cloud platforms.</p>\n<h3 id=\"Can_I_use_machine_types_other_than_those_specified_in_the_sizing_guide?\"><span class=\"mw-headline\" id=\"Can_I_use_machine_types_other_than_those_specified_in_the_sizing_guide.3F\">Can I use machine types other than those specified in the sizing guide?</span></h3>\n<p>No.</p>\n<p>One VM with 8vCPU can have better performance than 2 VMs with 4vCPU in some scenarios. VMs have their own limitations, so you can't improve system performance simply by adding more VMs. Additionally, all our sizing recommendations, tuning and tests are based on certain requirements for each node. You can't guarantee the performance if using VMs that don't meet the requirements for a node.</p>\n<h3 id=\"Can_I_share_storage_between_development_and_test_environments?\"><span class=\"mw-headline\" id=\"Can_I_share_storage_between_development_and_test_environments.3F\">Can I share storage between development and test environments?</span></h3>\n<p>Technically, you can. However, we don't recommend it.</p>\n<p>This is because all our sizing recommendations, tuning and tests are based on an assumption that each environment uses their dedicated storage. Sharing the same storage between two environments may cause the environments to interfere with each other, leading to poor stability and performance.  </p>\n<h3 id=\"Can_I_start_with_fewer_worker_nodes_and_add_more_later?\"><span class=\"mw-headline\" id=\"Can_I_start_with_fewer_worker_nodes_and_add_more_later.3F\">Can I start with fewer worker nodes and add more later?</span></h3>\n<p>Technically, you can. However, we strongly recommend that you install the suite according to our sizing recommendations. By providing enough worker nodes, you can avoid installation failures because of insufficient resources. </p>\n<h3 id=\"What_version_of_Kubernetes_should_I_choose?_Always_the_latest?\"><span class=\"mw-headline\" id=\"What_version_of_Kubernetes_should_I_choose.3F_Always_the_latest.3F\">What version of Kubernetes should I choose? Always the latest?</span></h3>\n<p>We use EKS, AKS, or GKE to fully manage Kubernetes in the suite. When you create or update the Kubernetes cluster, you need to select a Kubernetes version. Each release of Kubernetes may have some changes that may affect SMA deployment. For this reason, we verify a new Kubernetes version before SMA can adopt it. The latest version and any other versions that we haven't certified may be incompatible with SMA.</p>\n<p>Always select the Kubernetes version that's described in the suite installation documentation. This version has been certified for the current SMA version.  </p>\n<h3 id=\"What_is_the_minimum_disk_requirement_for_SMA?\"><span class=\"mw-headline\" id=\"What_is_the_minimum_disk_requirement_for_SMA.3F\">What is the minimum disk requirement for SMA?</span></h3>\n<p>We provide sizing recommendations for SMA deployment on different cloud platforms. The sizing recommendations provide the minimum resources needed for setting up an SMA environment on each cloud. You must install SMA according to the sizing recommendations to avoid installation failures caused by resource insufficiency; otherwise, even if you have successfully installed the suite, issues may still occur when the suite is running.</p>\n<h3 id=\"Can_I_change_the_SMA_sizing_profile_dynamically_after_installation?\"><span class=\"mw-headline\" id=\"Can_I_change_the_SMA_sizing_profile_dynamically_after_installation.3F\">Can I change the SMA sizing profile dynamically after installation?</span></h3>\n<p>Yes, you can, but only as part of a suite upgrade process. That is, you need to change the deployment size first and then upgrade the suite to a newer version. The new deployment size takes effect only when the suite upgrade process is completed. For details, see <a href=\"/doc/423/25.1/upgdmanagedk8ssuite\" title=\"SMAX:25.1/UpgdManagedK8SSuite\">Upgrade the suite (managed Kubernetes)</a>.</p>\n<h3 id=\"Can_SMA_share_the_Kubernetes_cluster_with_other_products?\"><span class=\"mw-headline\" id=\"Can_SMA_share_the_Kubernetes_cluster_with_other_products.3F\">Can SMA share the Kubernetes cluster with other products?</span></h3>\n<p>No.  When SMA shares the Kubernetes cluster with other products, they will compete for resources, which will impact the system stability and performance. Additionally, sharing the same Kubernetes cluster may cause incompatibility and security problems, because the products may require different versions of Kubernetes.</p>\n<h3 id=\"What_is_the_organization_name_for_an_image_repository?\"><span class=\"mw-headline\" id=\"What_is_the_organization_name_for_an_image_repository.3F\">What's the organization name for an image repository?</span></h3>\n<p>An organization is a collection of teams and repositories that can be managed together.</p>\n<p>All the images for SMA in each release should be pushed to the same organization. </p>\n<table>\n<tbody><tr>\n<th> Platform</th>\n<th>Image registry</th>\n</tr>\n<tr>\n<td>Azure</td>\n<td>The address of an image in an Azure container registry includes the following elements:\n\t\t\t<p><code>[loginUrl]/[namespace]/[artifact:][tag]</code></p>\n<ul>\n<li>loginUrl - The fully qualified name of the registry host. The registry host in an Azure container registry is in this format: <i>myregistry</i>.azurecr.io (all lowercase). You must specify the loginUrl when using Docker or other client tools to pull or push artifacts to an Azure container registry.</li>\n<li>namespace - Slash-delimited logical grouping of related images or artifacts - for example, for a workgroup or app.</li>\n<li>artifact - The name of a repository for a particular image or artifact.</li>\n<li>tag - A specific version of an image or artifact stored in a repository.</li>\n</ul>\n<p>An organization name in our documentation refers to a \"namespace\" in Azure.</p>\n<p>For example, the full name of an image in an Azure container registry might look like the following:</p>\n<p><em>itomsma.azurecr.io/hpeswitomsandbox/itom-idm:1.26.0</em></p>\n<p>The organization name in this example is hpeswitomsandbox.</p>\n</td>\n</tr>\n<tr>\n<td>GCP</td>\n<td>The address of an image in an GCP container registry includes the following elements:\n\t\t\t<p><code>gcr.io/[your-project-id]/[image]:[tag]</code></p>\n<p>An organization name in our documentation refers to a \"project-id\" in GCR. A project-id is the ID of your Google Cloud project. For example, the full name of an image in GCR may look like the following:</p>\n<p><em>gcr.io/itom-smax-nonprod/itom-cdf-controller:0.2.0-005</em></p>\n<p>The organization name in this example is itom-smax-nonprod.</p>\n</td>\n</tr>\n<tr>\n<td>AWS</td>\n<td>The address of an image in an AWS container registry includes the following elements:\n\t\t\t<p><code>[aws_account_id].dkr.ecr.[region].amazonaws.com/[Repository name]:[tag]</code></p>\n<p>You can push images to ECR like this:</p>\n<p><em>114706266704.dkr.ecr.us-east-1.amazonaws.com/hpeswitomsandbox/itom-autopass-lms:latest</em></p>\n<p>In this example, the organization name for ECR is hpeswitomsandbox.</p>\n</td>\n</tr>\n<tr>\n<td colspan=\"1\">Docker hub</td>\n<td colspan=\"1\">\n<p>Organizations are collections of teams and repositories that can be managed together. SMA uses an organization name of \"hpeswitom\" to manage all SMA repositories.</p>\n<p>You can pull images from Docker Hub like this:</p>\n<p><em>itom-docker.shcartifactory.swinfra.net/hpeswitom/vault:0.12.0-0016</em></p>\n<p>The organization name for all Docker Hub repositories for SMA is hpeswitom.</p>\n</td>\n</tr>\n</tbody></table>\n<h3 id=\"Is_the_load_balancer_working_on_layer_4_or_layer_7?_Why_do_we_need_both_layer_4_and_layer_7_load_balancers?\"><span class=\"mw-headline\" id=\"Is_the_load_balancer_working_on_layer_4_or_layer_7.3F_Why_do_we_need_both_layer_4_and_layer_7_load_balancers.3F\">Is the load balancer working on layer 4 or layer 7?  Why do we need both layer 4 and layer 7 load balancers?</span></h3>\n<p>Basically, we leverage solutions with both layer 4 and layer 7 load balancers. They work together to serve different purposes.</p>\n<p>A layer 7 load balancer is introduced for application level scenarios:</p>\n<ul>\n<li>Rule-based routing. In a production environment, there may be multiple applications behind the load balancer, such as SMA and HCM. You may need a load balancer to redirect you to different applications based on the context path. For example, requests against <a class=\"external text\" href=\"http://www.microfocus.com/saw\" rel=\"nofollow\" target=\"1\" title=\"www.microfocus.com/saw\">www.microfocus.com/saw</a> and <a class=\"external text\" href=\"http://www.microfocus.com/bo\" rel=\"nofollow\" target=\"1\" title=\"www.microfocus.com/bo\">www.microfocus.com/bo</a> should be redirected to SMA, and requests against <a class=\"external text\" href=\"http://www.microfocus.com/hcm\" rel=\"nofollow\" target=\"1\" title=\"www.microfocus.com/hcm\">www.microfocus.com/hcm</a> to HCM.</li>\n<li>Working with a Web Application Firewall for security considerations</li>\n<li>Providing SSL/TLS encryption at the load balancer level</li>\n</ul>\n<p>A layer 4 load balancer is introduced for infrastructure and network level scenarios:</p>\n<ul>\n<li>A layer 4 load balancer can work with cloud-native Kubernetes so that it can notice any node-level changes and update its backend pool</li>\n<li>It provides the capacity for handling thousands to millions of requests with very low latency</li>\n</ul>\n<h3 id=\"Is_it_possible_for_a_worker_node_to_run_out_of_disk_space_over_time?\"><span class=\"mw-headline\" id=\"Is_it_possible_for_a_worker_node_to_run_out_of_disk_space_over_time.3F\">Is it possible for a worker node to run out of disk space over time?</span></h3>\n<p>Yes, it's possible, but not very likely. There are always some unused files left on a worker node (for example, image files, old version resources). You can monitor your disk usage on a regular basis just in case. </p>\n<h3 id=\"Can_I_choose_for_a_worker_node_an_instance_type_other_than_the_one_required_in_the_sizing_guide?\"><span class=\"mw-headline\" id=\"Can_I_choose_for_a_worker_node_an_instance_type_other_than_the_one_required_in_the_sizing_guide.3F\">Can I choose for a worker node an instance type other than the one required in the sizing guide? </span></h3>\n<p>Technically, you can, but with certain limitations. You can only choose an instance type with the same or a higher configuration than the requirements (disk size, CPU, etc) documented in the sizing guide, to avoid problems caused by resource insufficiency.</p>\n<h3 id=\"Can_I_choose_other_operating_systems_than_CentOS_for_the_Bastion_node?\"><span class=\"mw-headline\" id=\"Can_I_choose_other_operating_systems_than_CentOS_for_the_Bastion_node.3F\">Can I choose other operating systems than CentOS for the Bastion node? </span></h3>\n<p>Technically, you can, but we recommend using CentOS. We use the bastion node as a jump box to connect to our Kubernetes cluster and other resources in the private subnet. Operating systems don't matter in this scenario. The SMA documentation provides steps that have been verified. So you can use the documentation for reference. </p>\n<h3 id=\"Can_I_use_the_same_PostgreSQL_server_for_all_SMA_services?\"><span class=\"mw-headline\" id=\"Can_I_use_the_same_PostgreSQL_server_for_all_SMA_services.3F\">Can I use the same PostgreSQL server for all SMA services? </span></h3>\n<p>Yes, our sizing recommendations are provided assuming all SMA services use the same PostgreSQL server. You can follow the suite documentation to prepare a PostgreSQL server that meets the requirements. </p>\n<h3 id=\"Do_I_have_to_clean_up_the_NFS_server_and_PostgreSQL_database_before_re-installation?\"><span class=\"mw-headline\" id=\"Do_I_have_to_clean_up_the_NFS_server_and_PostgreSQL_database_before_re-installation.3F\">Do I have to clean up the NFS server and PostgreSQL database before re-installation?</span></h3>\n<p>Yes, you do. The NFS server and PostgreSQL database keep some data from the last installation.</p>\n<ul>\n<li>If you don't clean up the old data in the NFS volumes, some services can't start because of the old dirty data.</li>\n<li>On the other hand, during the suite installation, the suite installer automatically creates certain databases and users in the PostgreSQL server. If you don't clean up these databases and users from the PostgreSQL server, the next installation will fail.</li>\n</ul>\n<h3 id=\"Why_should_I_specify_the_max_pods_per_node_when_creating_a_K8s_cluster?_How_do_I_calculate_this_value?\"><span class=\"mw-headline\" id=\"Why_should_I_specify_the_max_pods_per_node_when_creating_a_K8s_cluster.3F_How_do_I_calculate_this_value.3F\">Why should I specify the max pods per node when creating a K8s cluster? How do I calculate this value?</span></h3>\n<p>This parameter controls the maximum number of pods that are allowed to run on each node, so that you can balance your pods. If you can configure more nodes in the cluster than required in the sizing guide, you can adjust this parameter value. For example, for AKS, the value is 60. If the sizing guide states that you need to prepare 3 nodes for your specific sizing profile and you want to use 4 nodes for better stability, you can recalculate the parameter value as follows: 3*60/4 = 45. So you can specify a value of 45 to better balance the pods.</p>\n<h3 id=\"_What_kind_of_data_is_stored_on_NFS_storage?\"><span class=\"mw-headline\" id=\"_What_kind_of_data_is_stored_on_NFS_storage.3F\"> What kind of data is stored on NFS storage?</span></h3>\n<table>\n<tbody><tr>\n<th>Data</th>\n<th>Description</th>\n<th>Details</th>\n</tr>\n<tr>\n<td>Certificates</td>\n<td>Certificates for authentication and authorization</td>\n<td>The &lt;config-volume&gt;/certificate folder stores all certificate file for authentication/authorization between components in SMA.</td>\n</tr>\n<tr>\n<td>YAML files</td>\n<td>Kubernetes resources' YAML files</td>\n<td>The itom-vol-claim volume stores certain YAML templates used by OMT.\n\t\t\t<p>The OMT core volume is used to retain infra component data such as suitedb, internal image registry, idm, and YAML templates.</p>\n<p>The SMA global volume stores all services' yaml files.</p>\n</td>\n</tr>\n<tr>\n<td colspan=\"1\">Log files</td>\n<td colspan=\"1\">System, container, Kubernetes logs, and so on</td>\n<td colspan=\"1\">The itom-logging-vol volume stores OMT deployment log, container log. system log, and Kubernetes log.\n\t\t\t<p>The SMA global volume stores all services logs.</p>\n</td>\n</tr>\n<tr>\n<td colspan=\"1\">Attachments</td>\n<td colspan=\"1\">Attachments in your tickets</td>\n<td colspan=\"1\">The &lt;data-volume&gt;/share1 and &lt;data-volume&gt;/share2 folders store attachments in your tickets.</td>\n</tr>\n<tr>\n<td colspan=\"1\">Rabbitmq data</td>\n<td colspan=\"1\">Some message queue data</td>\n<td colspan=\"1\">The SMA rabbitmq volumes store some rabbitmq data related to message queuing.</td>\n</tr>\n<tr>\n<td colspan=\"1\">Search service data</td>\n<td colspan=\"1\">Data related to the search functionality</td>\n<td colspan=\"1\">For example, the SMA Smart Analytics volume.</td>\n</tr>\n</tbody></table>\n<p><br/>\n</p><span class=\"snippet-end\" data-sname=\"423-25.4-423-25.3-AKSDeploymentFAQ|General|N\"></span><h2 class=\"mw-headline\" id=\"EKS\">EKS</h2><p>The following are questions specific to deployments on AWS with EKS.</p><h3 id=\"Do_I_really_need_500_private_IPs_for_SMA_deployment_on_AKS?\">Do I really need 500 private IPs for SMA deployment on EKS?</h3><p>Yes. AWS uses CNI network plugin for EKS, which allocates a private IP address for each pod in the same IP space as worker node. A cluster of full capacity for SMA (plus the EKS system pods) has about 200 pods, which means 200 IPs are needed. However, during the system upgrade, because of the rolling update strategy, the number of pods will be doubled. That's why 500 IPs are required to make sure the upgrade will succeed.</p><h3><br/>\nIs the AWS EKS solution an HA solution? There are only a few worker nodes suggested in the sizing guide for EKS deployment. Can they guarantee HA for a specific deployment?</h3><p>Yes, it's an HA solution. The EKS solution is leveraging the auto scaling group to guarantee node level fail-recovery. That is, once a worker node isn't healthy or has crashed, a new node will be added to the group to replace it. It guarantees HA for your deployment. If you have more budget on computing resources, you may add more worker nodes to improve availability.</p><h3 id=\"Can_I_change_the_RDS_instance_type_dynamically_without_downtime?\"><span class=\"mw-headline\" id=\"Can_I_change_the_RDS_instance_type_dynamically_without_downtime.3F\">Can I change the RDS instance type dynamically without downtime?</span></h3><p>It may be covered by the AWS side. It depends on how you configure your RDS structure.</p><p>There is minimal downtime when you are scaling up on a Multi-AZ environment because the standby database gets upgraded first, then a failover will occur to the newly sized database.</p><p>However, a Single-AZ instance will be unavailable during the scaling operation.</p><h3 id=\"Do_we_still_need_to_write_500_GB_dummy_data_in_EFS_on_EKS_to_improve_EFS_performance?_What's_the_suggested_size_for_dummy_data?\"><span class=\"mw-headline\" id=\"Do_we_still_need_to_write_500_GB_dummy_data_in_EFS_on_EKS_to_improve_EFS_performance.3F_What.27s_the_suggested_size_for_dummy_data.3F\">Do we still need to write 500 GB dummy data in EFS on EKS to improve EFS performance? What's the suggested size for dummy data?</span></h3><p>Yes. The dummy data is used to make sure you have enough credits for continuously writing with a good throughput rate. For a more precise dummy data size, you can keep monitoring the EFS Burst Credits by following the link below:</p><p><a href=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\" title=\"https://docs.aws.amazon.com/efs/latest/ug/performance.html\">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p><p>Another way to optimize this is to use provisioned throughput mode.</p><div class=\"transcontent stn-end--EKS\"></div></div>",
  "modifiedon": "2025-10-24 08:51:12"
}