{
  "title": "Sizing considerations for EKS deployment",
  "content": "<div class=\"mw-parser-output\"><br/><p>The performance information provided here is based on tests in an out-of-box environment and for your reference only. Your implementation may consume more resources or require more resources to perform in an acceptable manner.</p>\n<h2 class=\"mw-headline\" id=\"Suite_size_definitions\">Suite size definitions</h2>\n<p>When you run the suite installer, you will need to select a suite size: <strong>Small</strong>, <strong>Medium</strong>, or <strong>Large</strong>. Different suite sizes require different hardware configurations.</p>\n<p>The following table describes the available suite sizes.</p>\n<table>\n<tbody>\n<tr>\n<th>Suite size</th>\n<th>Small</th>\n<th>Medium</th>\n<th>Large</th>\n<th>Notes</th>\n</tr>\n<tr>\n<td>Maximum number of concurrent users (including both ESS and IT agent users)</td>\n<td>100~400</td>\n<td>400~1000</td>\n<td>1000~3000</td>\n<td>Concurrent users are active users who have logged in to the system to perform operations and consume system resources. There are two types of concurrent users, service portal users and IT agent users. The workload for each user type is defined as:\n\t\t\t<ul>\n<li>For service portal users: After logging in to the system, each portal user has about 30-minute session duration on the system and will create 1 ~ 2 orders or requests.</li>\n<li>For IT agent users: IT agent users have longer session duration, and each of them will be active for about 1.5 hours after logon and handle 4 ~ 6 tickets.</li>\n</ul>\n<p>Based on this workload assumption, the business operation throughput can be calculated. For example, for 400 service portal users, the number of newly created orders or requests will be 800 ~ 1600 per hour, while for 400 IT agent users, the number of handled tickets will be 1000 ~ 1600 per hour. If you have a much larger business operation throughput than the numbers provided here, consider adding more worker nodes.</p>\n<p><b>Notes: </b></p>\n<p>The sizing numbers are based on an assumption that there are only standard Service Management tenant users. If you are using one or more Service Manager tenants or using both SMA and Service Manager tenants, user count conversion is needed.</p>\n<p>In general, 1 Service Manager user = 0.7 standard SMA users, because a Service Manager tenant consumes only Service Portal in the container when SMA is integrated with external Service Manager.</p>\n<p>For example, if an SMA deployment needs to support 400 Service Manager tenant concurrent users and 100 Service Management tenant concurrent users, the actual user count is 400 X 0.7 + 100 = 380. So you need to select the Small size.</p>\n</td>\n</tr>\n<tr>\n<td>Maximum number of records in Smart Analytics</td>\n<td>1 Million</td>\n<td>2 Million</td>\n<td>2~4 Million</td>\n<td> </td>\n</tr>\n</tbody>\n</table>\n<h2 class=\"mw-headline\" id=\"Sizing_for_SMA_deployment\">Hardware requirements for Service Management (only)</h2>\n<p>The following table lists the minimum resources needed for setting up Service Management  in the cloud with high availability.<br/>\nA standard deployment includes control plane nodes (owned by AWS), worker nodes, EFS as storage, and RDS as databases.</p>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\" style=\"text-align: center;\"><b>Suite size (sizing profile)</b></th>\n<th rowspan=\"2\" style=\"text-align: center;\">Concurrent users</th>\n<th rowspan=\"2\" style=\"text-align: center;\">control plane node</th>\n<th colspan=\"3\" style=\"text-align: center;\">worker node</th>\n<th colspan=\"2\" style=\"text-align: center;\">EFS</th>\n<th colspan=\"3\" style=\"text-align: center;\">Database</th>\n</tr>\n<tr>\n<th style=\"text-align: center;\">Type</th>\n<th style=\"text-align: center;\">Volume</th>\n<th style=\"text-align: center;\">Quantity</th>\n<th style=\"text-align: center;\">Type</th>\n<th style=\"text-align: center;\">Volume</th>\n<th style=\"text-align: center;\">Type</th>\n<th style=\"text-align: center;\">Storage</th>\n<th style=\"text-align: center;\">Quantity</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>100~400</td>\n<td>Owned by AWS</td>\n<td>m5.2xlarge</td>\n<td>Disk: 100 GB</td>\n<td>3</td>\n<td>EFS</td>\n<td>Bursting mode with 500 GB storage</td>\n<td>AWS RDS db.m6g.2xlarge*</td>\n<td>200 GB</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>400~1000</td>\n<td>Owned by AWS</td>\n<td>m5.2xlarge</td>\n<td>Disk: 100 GB</td>\n<td>4 ~ 5</td>\n<td>EFS</td>\n<td>Bursting mode with 2T storage</td>\n<td>AWS RDS db.m6g.4xlarge*</td>\n<td>300 GB</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Large</td>\n<td>1000~3000</td>\n<td>Owned by AWS</td>\n<td>m5.2xlarge</td>\n<td>Disk: 100 GB</td>\n<td>6 ~ 9</td>\n<td>EFS</td>\n<td>Provisioned mode with &gt;= 128 MiB/s</td>\n<td>AWS RDS db.m6g.8xlarge*</td>\n<td>400 GB</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Note</strong>:</p>\n<p><sup>*</sup> For EBS backend storage, including EBS backend storage used for OS storage and overlay2, and EBS backend RDS services, you may switch to IO1 instead of GP2 or GP3 for better performance. For more details, see the AWS documents <a class=\"external text\" href=\"https://aws.amazon.com/ebs/details/\" rel=\"nofollow\" target=\"1\" title=\"EBS storage\">EBS storage</a> and <a class=\"external text\" href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\" rel=\"nofollow\" target=\"1\" title=\"RDS Storage\">RDS Storage</a>. </p>\n<h3>Storage requirements</h3>\n<table>\n<tbody>\n<tr>\n<th>Number of requests in the database</th>\n<th>EFS size</th>\n</tr>\n<tr>\n<td>100 K</td>\n<td>160 GB</td>\n</tr>\n<tr>\n<td>300 K</td>\n<td>600 GB</td>\n</tr>\n</tbody>\n</table>\n<p>The sizes are for your reference only. They may vary depending on your ticket size, user access rate, integrations, and so on.</p>\n<h2 class=\"mw-headline\" id=\"Sizing_for_Bastion\">Hardware requirements for bastion</h2>\n<p>The following table lists the minimum resources needed for setting up a bastion in the cloud. </p>\n<table>\n<tbody>\n<tr>\n<th><b>Bastion instance type</b></th>\n<th><b>Boot disk type</b></th>\n<th><b>Boot disk</b></th>\n</tr>\n<tr>\n<td><b> </b>t2.micro,\n\t\t\t<p> m5.large</p>\n</td>\n<td>EBS/General purpose SSD (gp3)</td>\n<td>100 GB</td>\n</tr>\n</tbody>\n</table>\n<p> <b>Note:</b> t2.micro is for daily operations while m5.large is for migration and other operations that require more computing resources and network bandwidth.</p>\n<h2 id=\"Additional_hardware_requirements_for_OO\">Additional hardware requirements for OO Containerized</h2>\n<p>The following sizing requirements denote the target deployment size for a dedicated OO Containerized database, based on the number of tenants. This is not based on concurrent users.</p>\n<p>The OO Containerized deployment sizing depends on the number of tenants in your system. You specify a deployment size with the <code>global.oo.size</code> parameter when installing OO Containerized. For details, see <a href=\"/doc/423/26.1/installooconeks\" title=\"Install OO on AWS\">Install OO Containerized on AWS</a>.</p>\n<table>\n<tbody>\n<tr>\n<th colspan=\"1\" rowspan=\"2\">Target size</th>\n<th colspan=\"1\" rowspan=\"2\">Number of OO Containerized tenants</th>\n<th colspan=\"4\">EKS nodes configuration</th>\n<th colspan=\"2\" rowspan=\"1\">EFS </th>\n<th colspan=\"6\">RDS Database Instance</th>\n</tr>\n<tr>\n<th>AWS Instance Type</th>\n<th>Configuration</th>\n<th>Storage</th>\n<th>Quantity</th>\n<th>Configuration</th>\n<th>Additional storage</th>\n<th>RDS Instance Type</th>\n<th>vCPU</th>\n<th>Memory</th>\n<th>Storage</th>\n<th>Quantity</th>\n<th>max_connections</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>1 to 5</td>\n<td>m5.2xlarge</td>\n<td>8 vCPU 32 GB RAM</td>\n<td>100 GB</td>\n<td>2</td>\n<td>OO shares the same EFS server with the suite.</td>\n<td>500 GB</td>\n<td>AWS RDS db.r5.large</td>\n<td>2</td>\n<td>\n<p>16 GB</p>\n</td>\n<td>750 GB ~ 5 TB</td>\n<td>1</td>\n<td>1400<sup>#</sup></td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>6 to 20</td>\n<td>m5.2xlarge</td>\n<td>8 vCPU 32 GB RAM</td>\n<td>100 GB</td>\n<td>8</td>\n<td>OO shares the same EFS server with the suite.</td>\n<td>500 GB</td>\n<td>AWS RDS db.r5.2xlarge</td>\n<td>8</td>\n<td>\n<p>64 GB</p>\n</td>\n<td>750 GB ~ 5 TB</td>\n<td>1</td>\n<td>5710<sup>##</sup></td>\n</tr>\n</tbody>\n</table>\n<p>#The max_connections value for the OO Containerized database should be set to 1400 or more.</p>\n<p>##The max_connections value for the OO Containerized database should be set to 5710 or more.</p>\n<h2>Additional hardware requirements for DND and OO Containerized</h2>\n<p>The system requirements for SMA and Design and Deploy (DND) can be planned while installing SMA or at a later point when you are deploying DND on an existing SMA setup. The table includes the hardware requirements for OO Containerized also as it is mandatory for DND to work.<br/>\nA standard deployment includes control plane nodes (owned by AWS), worker nodes, EFS as storage, and RDS as databases.</p>\n<p>First, make sure that you have determined your SMA deployment size by using the table in the \"SMA size definitions\" section. Then, use the following table to determine your DND deployment size. Note that this table applies for one DND instance. If you will enable DND for multiple tenants (one DND Instance for each tenant), you need to determine the deployment size of each individual DND instance.  </p>\n<table>\n<thead>\n<tr>\n<th scope=\"col\">DND deployment size</th>\n<th scope=\"col\">Small</th>\n<th scope=\"col\">Medium</th>\n<th scope=\"col\">Large</th>\n<th scope=\"col\">Notes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Number of cloud subscriptions per hour</td>\n<td>30</td>\n<td>60</td>\n<td>-</td>\n<td colspan=\"1\" rowspan=\"5\">The numbers in the table are meant to be approximate for a given deployment environment, but not exact. For example, a Small environment may only create 5 offerings, or 10 per category, although the table lists a representative number of 20. In your anticipated environment, you may find the expected numbers of tenants to span multiple profile definitions (for example, the number of active subscriptions may fit the Small deployment, while the number of offerings may fit the Medium profile). In such a case, select the profile that seems most representative of your anticipated environment, with the number of total subscriptions of particular importance. The sizing configuration is based on the results of in-house performance testing against these deployment environments. In general, you should find the need to tune the configuration parameters after the installation, as the components can be scaled manually as needed, to support the size of the environment you have specified.</td>\n</tr>\n<tr>\n<td>Number of active subscriptions</td>\n<td>1,000</td>\n<td>7,500</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Number of canceled subscriptions</td>\n<td>10,000</td>\n<td>50,000</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Number of offerings per category</td>\n<td>20</td>\n<td>20</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Total number of designs</td>\n<td>35</td>\n<td>75</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<p>Before enabling and using the Design and Deploy functionality, you should install and enable Operations Orchestration(OO) Containerized capability. </p>\n<p>Use the table below to plan the additional hardware resources for each individual DND instance. The additional hardware resources for DND are the total of the additional hardware resources for all DND instances.</p>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"> </div>\n<div class=\"admonition-content admonition-note-content\"><strong>Important: </strong>When you are installing DND functionality on an existing setup, you need to reconfigure the hardware resources as per the recommendations given in the below tables to accommodate the required additional resources. </div>\n</div>\n<p>For OO Containerized, choose one of the following options for setting up a database:</p>\n<ul>\n<li>To set up OO Containerized with a common database, you can choose to connect OO Containerized to the same database instance as the rest of the suite components. For hardware requirements, see the table below <strong>Standard hardware requirements with a common database.</strong><br/>\n<strong>Important:</strong>\n<ul>\n<li>On any AWS RDS instance type, the default value of the <code>max_connections</code> parameter is calculated based on formula: <strong>LEAST({DBInstanceClassMemory/9531392}, 5000)</strong>. For details, see Amazon documentation <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Limits.html\" title=\"here\">here</a>.</li>\n<li>To set the <code>max_connections</code> parameter of the RDS instance of OO Containerized to the values specified in the following table, you need to edit the parameter group associated with the RDS instance and, if the RDS instance was created before the parameter change, restart the RDS instance.</li>\n</ul>\n</li>\n<li>To determine standard hardware requirements with a dedicated database for OO Containerized, see <strong>Additional hardware requirements for OO Containerized with a dedicated database</strong> section.</li>\n</ul>\n<p>The OO Containerized deployment sizing depends on the number of tenants in your system. You specify a deployment size with the <code>global.oo.size</code> parameter when installing OO Containerized. For details, see <a href=\"/doc/423/26.1/installooconeks\" title=\"Install OO on AWS\">Install OO Containerized on AWS</a>.</p>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\"><b>Suite size (sizing profile)</b></th>\n<th rowspan=\"2\">Number of OO Containerized tenants</th>\n<th rowspan=\"2\"> Control plane node</th>\n<th colspan=\"3\"> Worker node</th>\n<th colspan=\"2\">EFS</th>\n<th colspan=\"3\">DND and OO Containerized RDS Database - PostgreSQL</th>\n<th> </th>\n</tr>\n<tr>\n<th>Type</th>\n<th>Volume</th>\n<th>Quantity</th>\n<th>Type</th>\n<th>Volume</th>\n<th>Type</th>\n<th>Storage</th>\n<th>Quantity</th>\n<th>max_connections</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>1 to 5</td>\n<td>Owned by AWS</td>\n<td>m5.2xlarge</td>\n<td>Disk: 100 GB</td>\n<td>3 ~ 4</td>\n<td>EFS</td>\n<td>Bursting mode with 1 TB storage</td>\n<td>AWS RDS db.m6g.4xlarge*</td>\n<td>1 TB ~ 5.2 TB</td>\n<td>1</td>\n<td>3400<sup>#</sup></td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>6 to 20</td>\n<td>Owned by AWS</td>\n<td>m5.2xlarge</td>\n<td>Disk: 100 GB</td>\n<td>9 ~ 10</td>\n<td>EFS</td>\n<td>Bursting mode with 2.5 TB storage</td>\n<td>AWS RDS db.m6g.8xlarge*</td>\n<td>1.6 TB ~ 5.8 TB</td>\n<td>1</td>\n<td>7710<sup>##</sup></td>\n</tr>\n</tbody>\n</table>\n<p>#The max_connections value for Service Management and DND database should be equal to or greater than 3400.</p>\n<p>##The max_connections value for Service Management and DND database should be equal to or greater than 7710 or more.</p>\n<p><sup>*</sup> For EBS backend storage, including EBS backend storage used for OS storage and overlay2, and EBS backend RDS services, you may switch to IO1 instead of GP2 or GP3 for better performance. For more details, see <a class=\"external text\" href=\"https://aws.amazon.com/ebs/details/\" rel=\"nofollow\" target=\"1\" title=\"EBS storage\">EBS storage</a> and <a class=\"external text\" href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\" rel=\"nofollow\" target=\"1\" title=\"RDS Storage\">RDS Storage</a>. </p>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"> </div>\n<div class=\"admonition-content admonition-note-content\">For EFS and Postgres HA configuration, follow the recommendations provided in vendor-specific documentation. Example: To configure Postgres HA, see  <a href=\"/doc/423/26.1/hasqlpatroni\" title=\"High Availability PostgreSQL with Patroni\">High Availability PostgreSQL with Patroni</a>.</div>\n</div>\n<h3 id=\"Additional_hardware_requirements_for_OO_Containerized_with_a_dedicated_database\">Additional hardware requirements for OO Containerized with a dedicated database</h3>\n<p>The following sizing requirements denote the target deployment size for a dedicated OO Containerized database, based on the number of tenants. This is not based on concurrent users.</p>\n<table>\n<tbody>\n<tr>\n<th colspan=\"1\" rowspan=\"2\">Target size</th>\n<th rowspan=\"2\">Number of OO Containerized tenants</th>\n<th colspan=\"5\" rowspan=\"1\">OO Containerized RDS Database Instance - PostgreSQL</th>\n</tr>\n<tr>\n<th colspan=\"2\" rowspan=\"1\">Configuration</th>\n<th><b>Storage</b></th>\n<th>Quantity</th>\n<th>max_connections</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>1 to 5</td>\n<td colspan=\"2\" rowspan=\"1\">\n<p>AWS RDS <span style=\"background-color: transparent;\">db.r5.large</span></p>\n</td>\n<td>750 GB ~ 5 TB</td>\n<td>1</td>\n<td>1400<sup>#</sup></td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>6 to 20</td>\n<td colspan=\"2\" rowspan=\"1\">\n<p>AWS RDS db.r5.2xlarge</p>\n</td>\n<td>750 GB ~ 5 TB</td>\n<td>1</td>\n<td>5710<sup>##</sup></td>\n</tr>\n</tbody>\n</table>\n<p>#The max_connections value for the combined database should be equal to or greater than 1400</p>\n<p>##The <code>max_connections</code> value for the combined database should be equal to or greater than 5710</p>\n<p><sup>*</sup> For EBS backend storage, including EBS backend storage used for OS storage and overlay2, and EBS backend RDS services, you may switch to IO1 instead of GP2 or GP3 for better performance. For more details, see <a class=\"external text\" href=\"https://aws.amazon.com/ebs/details/\" rel=\"nofollow\" target=\"1\" title=\"EBS storage\">EBS storage</a> and <a class=\"external text\" href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\" rel=\"nofollow\" target=\"1\" title=\"RDS Storage\">RDS Storage</a>. </p>\n<h2>Additional hardware requirements for CMP FinOps</h2>\n<p>Use the table below to plan the additional hardware resources for CMP FinOps.</p>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\"><b>Suite size (sizing profile)</b></th>\n<th rowspan=\"2\">\n<p>Number of Cloud Cost Provider Integrations</p>\n<p>(Total number of integrations across all CMP FinOps tenants)</p>\n</th>\n<th colspan=\"3\">Worker node</th>\n<th colspan=\"1\" rowspan=\"2\">\n<p>Showback</p>\n<p>replica count</p>\n</th>\n<th rowspan=\"2\">\n<p>Insights</p>\n<p>Replica Count</p>\n</th>\n<th colspan=\"3\" rowspan=\"1\">Vertica EON database</th>\n</tr>\n<tr>\n<th>Amazon Machines Image (AMI) type</th>\n<th>Volume</th>\n<th>Quantity</th>\n<th>AMI type</th>\n<th>Volume</th>\n<th>Vertica node count</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>1 to 5</td>\n<td>\n<p>8 vCPU<br/>\n\t\t\t32 GiB memory</p>\n<p>(m5.2x large)</p>\n</td>\n<td colspan=\"1\" rowspan=\"3\">Disk: 100 GB</td>\n<td>2-4</td>\n<td>2-4</td>\n<td>1-2</td>\n<td>\n<p>16 vCPU<br/>\n\t\t\t128 GiB memory</p>\n<p>(r5.4x large)</p>\n</td>\n<td colspan=\"1\" rowspan=\"3\">\n<p>Depot storage: 240 GB</p>\n<p>Catalog storage: 50 GB</p>\n<p>Temporary data storage: 100 GB</p>\n</td>\n<td>3</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>6 to 20</td>\n<td colspan=\"1\" rowspan=\"2\">\n<p>16 vCPU<br/>\n\t\t\t64 GiB memory</p>\n<p>(m5.4xlarge)</p>\n</td>\n<td>4-6</td>\n<td>4-6</td>\n<td>2-4</td>\n<td rowspan=\"2\">\n<p>32 vCPU<br/>\n\t\t\t256 GiB memory</p>\n<p>(r5.8xlarge)</p>\n</td>\n<td>3-6</td>\n</tr>\n<tr>\n<td>Large</td>\n<td>21 to 30</td>\n<td>6-8</td>\n<td>6-8</td>\n<td>4-6</td>\n<td>6-9</td>\n</tr>\n</tbody>\n</table>\n<p>Average load expected on CMP FinOps: 60,000,000 billing lines per month</p>\n<p><strong>Note</strong>: For Vertica HA configuration, follow the recommendations provided in vendor-specific documentation. </p>\n<h3>Performance tuning for CMP FinOps</h3>\n<p>This section contains configuration changes required to support a higher number of Cloud Cost Provider integrations (small, medium, and large sizing profiles).</p>\n<p>Complete the steps (Configure node group, Update <code>nodeSelector</code> parameter, and Separate ETL and query workloads) below to support a higher number of Cloud Cost Provider integrations. </p>\n<h4>Configure node group on your AWS cluster</h4>\n<p>On your AWS cluster, configure the following parameters as part of node grouping based on your sizing profile.</p>\n<ul>\n<li>Node group name: Specify a node group name, example: <code>cgro.</code></li>\n<li>Desired size (node count): Specify the current number of nodes that the node group should maintain. Example: 8.</li>\n<li>Add Kubernetes label. Example: Worker cgro.</li>\n</ul>\n<p>For information, see <a href=\"https://docs.aws.amazon.com/eks/index.html\" title=\"Amazon EKS documentation\">Amazon EKS documentation</a>.</p>\n<h4>Update the nodeSelector parameter</h4>\n<p>To run the CGRO service on specified nodes only, set the  <code>nodeSelector.Worker</code> parameter's value to the label of the worker nodes created for the CGRO service.</p>\n<p>Complete the following steps to update the <code>nodeSelector</code><strong> </strong>parameter.</p>\n<ol>\n<li>Extract the deployment values to a file named <b style=\"\">cgro-showback.yaml.</b>\n<pre><code>helm get values &lt;release name&gt; -n &lt;suite namespace&gt; &gt; cgro-showback.yaml\n</code></pre>\n</li>\n<li>Locate the  <code>itom-cgro-showback</code> parameter and copy it along with its keys to a new file named <strong>cgro_nodeSelector.yaml. </strong>The content to copy will resemble the following:\n\t<pre><code>itom-cgro-showback:\n  replicas: 1\n  global:\n    nodeSelector:</code></pre>\n</li>\n<li>Update the  <strong>cgro_nodeSelector.yaml </strong>file to make the following changes:\n\t<ol>\n<li>Add a new top-level key <code>cgro:</code>, move the contents of  <code>itom-cgro-showback:</code> parameter under it.</li>\n<li>Update the <code>NodeSelector</code> parameter to include the worker node label. See the following sample for the updated content. In the sample, the value 'cgro' in <code>Worker: cgro</code> is the worker nodes label name.</li>\n</ol>\n<pre>cgro:\n  itom-cgro-showback:\n    replicas: 1\n    global:\n      nodeSelector:\n        Worker: \"cgro\"</pre>\n</li>\n<li>Run the following command to apply this update:\n\t<pre>helm get values &lt;release name&gt; -n &lt;suite namespace&gt; &gt; my-values.yaml\nhelm upgrade &lt;release name&gt; &lt;ESM helm chart path&gt; -n &lt;suite namespace&gt; -f  my-values.yaml -f cgro_nodeSelector.yaml\n</pre>\n\tSee the following sample command:\n\n\t<pre>helm upgrade sma ESM_Helm_Chart-2x.x.x\\charts\\esm-1.0.0+2x.x-xxx.tgz -n itsma-xxx -f my-values.yaml -f cgro_nodeSelector.yaml</pre>\n</li>\n<li>To update the <code>CGRO_SHOWBACK_COLLECT_COPY_STREAM_RESOURCE_POOL_NAME</code> setting in the <strong>itom-cgro-showback-deployment</strong>:\n\t<ol>\n<li>Copy the following content to a new file named <strong>cgro_copy_stream.yaml</strong>:\n\t\t<pre><code>cgro:\n  itom-cgro-showback:\n    collect:\n      copy_stream_resource_pool_name: \"itom_finops_etl\"</code></pre>\n</li>\n<li>Change the value of the <code>copy_stream_resource_pool_name</code> property to the required value and save the file.</li>\n</ol>\n</li>\n<li>Run the following command to apply this update:\n\t<pre><code>helm upgrade &lt;release name&gt; &lt;ESM helm chart path&gt; -n &lt;suite namespace&gt; -f my-values.yaml -f cgro_copy_stream.yaml</code></pre>\n</li>\n</ol>\n<h4>Separate ETL and query workloads</h4>\n<p>Perform the following tasks to divide ETL workloads and query workloads into separate resource pools.</p>\n<ol>\n<li>On the Vertica database, as a database administrator, run the following command to create a custom ETL resource pool.\n\t<p>For small profile:</p>\n<pre> CREATE RESOURCE POOL itom_finops_etl MAXMEMORYSIZE '40%' PLANNEDCONCURRENCY 64 MAXCONCURRENCY 64;</pre>\n<p>For medium and large proflies:</p>\n<pre>CREATE RESOURCE POOL itom_finops_etl MAXMEMORYSIZE '40%' PLANNEDCONCURRENCY 128 MAXCONCURRENCY 128;</pre>\n</li>\n<li>Run the following commands to validate that the new resource pools are created:\n\t<pre>SELECT * from RESOURCE_POOLS\nSELECT * from RESOURCE_POOL_STATUS</pre>\n</li>\n<li>Perform the following configuration changes on the Vertica database as a Vertica Database Administrator according to your profile size:\n\t<p>For small profile:</p>\n<pre>ALTER RESOURCE POOL general PLANNEDCONCURRENCY 32;\nALTER RESOURCE POOL general MAXCONCURRENCY 32;\n</pre>\n<p>For medium and large profiles:</p>\n<pre>ALTER RESOURCE POOL general PLANNEDCONCURRENCY 64;\nALTER RESOURCE POOL general MAXCONCURRENCY 64;</pre>\n</li>\n</ol>\n<ol start=\"4\">\n<li>Run the following commands on the bastion host to restart the pods:\n\t<pre>kubectl scale deployment itom-cgro-costpolicy --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-costpolicy --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-insights --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-insights --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-insights-gateway --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-insights-gateway --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-policy-gateway --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-policy-gateway --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-showback --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-showback --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-showback-gateway --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-showback-gateway --replicas=1 -n &lt;itsma_namespace&gt;\n</pre>\n</li>\n<li>Scale up the showback pod as needed. For example, run the following command on the bastion host to scale up the replica to 2.\n\t<pre>kubectl scale deployment itom-cgro-showback --replicas=2 -n &lt;itsma_namespace&gt;</pre>\n\tSee the table above for specific replica counts based on workloads. </li>\n</ol>\n<h2>Additional hardware requirements for SAM</h2>\n<p>Use the table below to plan the additional hardware resources for Software Asset Management (SAM).</p>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\" style=\"text-align: center;\"><b>SAM deployment size</b></th>\n<th rowspan=\"2\" style=\"text-align: center;\">SMA control plane node</th>\n<th colspan=\"3\" style=\"text-align: center;\">SMA worker node</th>\n<th style=\"text-align: center;\">EFS</th>\n</tr>\n<tr>\n<th style=\"text-align: center;\">Type</th>\n<th style=\"text-align: center;\">Volume</th>\n<th style=\"text-align: center;\">Quantity</th>\n<th style=\"text-align: center;\">Configuration</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>Owned by AWS</td>\n<td>m5.2xlarge</td>\n<td>Disk: 100 GB</td>\n<td>1</td>\n<td colspan=\"1\" rowspan=\"3\">No additional storage is required for SAM. SAM shares SMA storage resources.</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>Owned by AWS</td>\n<td>m5.2xlarge</td>\n<td>Disk: 100 GB</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Large</td>\n<td>Owned by AWS</td>\n<td>m5.2xlarge</td>\n<td>Disk: 100 GB</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Additional_hardware_requirements_for_CMS\">Additional hardware requirements for UCMDB</h2>\n<p>Native SACM and SAM require you to install UCMDB (UCMDB Server and UCMDB Gateway). You have two options:</p>\n<ul>\n<li>Classic UCMDB + Classic UCMDB Gateway</li>\n<li>Containerized UCMDB (which comes with the UCMDB Server and UCMDB Gateway)</li>\n</ul>\n<p>See <a href=\"/integration/Service_Management_Automation_X/all/Universal_Discovery_and_CMDB\" title=\"here\">here</a> for the supported UCMDB versions in this release.</p>\n<h3>Classic UCMDB</h3>\n<p>If you plan to use classic UCMDB, install UCMDB Gateway on the UCMDB Server, and add the following minimal resources for UCMDB Gateway (for classic UCMDB hardware resource requirements, see <a href=\"/doc/401/25.4/sysreqshardware\" title=\"Classic UD/UCMDB sizing recommendations\">Classic UCMDB sizing recommendations</a>):</p>\n<table>\n<tbody>\n<tr>\n<th><b>Deployment</b></th>\n<th><b>CPU</b></th>\n<th><b>RAM</b></th>\n<th><b>Disk Space</b></th>\n</tr>\n<tr>\n<td><b>All sizes</b></td>\n<td>4 cores</td>\n<td>2 GB (allocated for UCMDB Gateway only)</td>\n<td>100 GB</td>\n</tr>\n</tbody>\n</table>\n<h3>Containerized UCMDB </h3>\n<p>If you plan to install the containerized UCMDB (as the next deployment on the same OMT), use the following guidelines to determine the additional hardware resources required for UCMDB.</p>\n<p id=\"Determine_the_deployment_size\"><strong>1. Determine the deployment size</strong></p>\n<p>When installing UCMDB, you will need to select a deployment size: Small, Medium, or Large, so that the UCMDB deployment is automatically scaled according to the selected size. Use the following table to determine your deployment size.</p>\n<table>\n<thead>\n<tr>\n<th scope=\"col\">Deployment size </th>\n<th scope=\"col\">Small</th>\n<th scope=\"col\">Medium</th>\n<th scope=\"col\">Large</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Maximum number of CIs and Relationships in UCMDB</td>\n<td>10 million</td>\n<td>60 million</td>\n<td>200 million</td>\n</tr>\n</tbody>\n</table>\n<p id=\"Determine_the_hardware_resource_requirements\"><strong>2. Determine the hardware resource requirements</strong></p>\n<p>The following table lists the minimum resources needed for setting up a UCMDB environment in the cloud with high availability.<br/>\nA standard deployment includes control plane nodes (owned by AWS), worker nodes, EFS as storage, and RDS as databases.</p>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\"><b>Suite size (sizing profile)</b></th>\n<th rowspan=\"2\">UCMDB control plane node</th>\n<th colspan=\"3\">UCMDB worker node</th>\n<th colspan=\"2\">UCMDB EFS</th>\n<th colspan=\"3\">UCMDB Database</th>\n</tr>\n<tr>\n<th>Type</th>\n<th>Volume</th>\n<th>Quantity</th>\n<th>Type</th>\n<th>Volume</th>\n<th>Type</th>\n<th>Storage</th>\n<th>Quantity</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>Owned by AWS</td>\n<td>m5.2xlarge</td>\n<td>Disk: 100 GB</td>\n<td>2</td>\n<td>EFS</td>\n<td>200 GB    </td>\n<td>AWS RDS db.r6g.xlarge *</td>\n<td>200 GB</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>Owned by AWS</td>\n<td>m5.2xlarge</td>\n<td>Disk: 100 GB</td>\n<td>3</td>\n<td>EFS</td>\n<td>300 GB    </td>\n<td>AWS RDS db.r6g.2xlarge *</td>\n<td>400 GB</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Large</td>\n<td>Owned by AWS</td>\n<td>m5.2xlarge</td>\n<td>Disk: 100 GB</td>\n<td>4</td>\n<td>EFS</td>\n<td>400 GB</td>\n<td>AWS RDS db.r6g.4xlarge *</td>\n<td>800 GB (SSD)</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"> </div>\n<div class=\"admonition-content admonition-note-content\"> For EBS backend storage, including EBS backend storage used for OS storage and overlay2, and EBS backend RDS services, you may switch to IO1 instead of GP2 or GP3 for better performance. For your reference, the recommended size for IO1 in the Medium deployment ranges from 7000 to 12000 based on our testing.\n<p>For more details, see <a href=\"https://aws.amazon.com/ebs/details/\" title=\"EBS storage\">EBS storage</a> and <a href=\"https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html\" title=\"RDS Storage\">RDS Storage</a>.</p>\n</div>\n</div>\n<p><strong>Performance tuning for UCMDB RDS PostgreSQL</strong></p>\n<p>You can refer to the following values for parameters in RDS PostgreSQL:</p>\n<table resolved=\"\">\n<colgroup>\n<col/>\n<col/>\n</colgroup>\n<thead>\n<tr>\n<th scope=\"col\">Parameter</th>\n<th scope=\"col\">Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>shared_buffers</td>\n<td>40% of RDS memory</td>\n</tr>\n<tr>\n<td>work_mem</td>\n<td>0.1% of RDS memory</td>\n</tr>\n<tr>\n<td>maintenance_work_mem</td>\n<td>2GB</td>\n</tr>\n<tr>\n<td>temp_buffers</td>\n<td>32MB</td>\n</tr>\n<tr>\n<td>random_page_cost</td>\n<td>2</td>\n</tr>\n<tr>\n<td>max_wal_size</td>\n<td>6GB</td>\n</tr>\n<tr>\n<td>min_wal_size</td>\n<td>1GB</td>\n</tr>\n<tr>\n<td>autovacuum_vacuum_threshold</td>\n<td>2000</td>\n</tr>\n<tr>\n<td>autovacuum_vacuum_scale_factor</td>\n<td>0.01</td>\n</tr>\n<tr>\n<td>autovacuum_analyze_threshold</td>\n<td>2000</td>\n</tr>\n<tr>\n<td>autovacuum_analyze_scale_factor</td>\n<td>0.01</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Plan_resources_for_OData_integration \">Plan resources for OData integration </h2>\n<p>Before starting to use the OData integration, please read <a href=\"/doc/423/26.1/sizingforodata\" title=\"Sizing guide for OData integration\">Sizing guide for OData integration</a> to determine if you need additional Datahub pods, read-only platform pods, and additional worker nodes.</p>\n<h2>Related topics</h2>\n<ul>\n<li><a href=\"/doc/423/26.1/allinonesizingeks\" title=\"All-In-One sizing guide for EKS deployment\">All-In-One sizing guide for EKS deployment</a></li>\n</ul>\n</div>",
  "modifiedon": "2025-10-24 08:51:12"
}