{
  "title": "\"Warning FailedCreatePodSandBox\" message and pods do not start during upgrade",
  "content": "<div class=\"mw-parser-output\"><span class=\"snippet-start\" data-sname=\"423-25.4-423-25.3-FailedCreatePodSandBox||N\"></span><p class=\"mw-empty-elt\">\n</p><p>When you run the <code>upgrade -u</code> command to perform a manual upgrade on a node that has many network adapters, the pods on the node don't start. If you run the <code>kubectl describe pod &lt;pod_name&gt; -n &lt;namespace&gt;</code> command to check the pod status when this occurs, you see warning messages that resemble the following:  </p>\n<pre><code>Warning  FailedCreatePodSandBox  2m2s (x2438 over 91m)  kubelet  (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox                                                container \"c4b3b01869bbd664efc6c5b8e4f9a6b1a0fa2e2838fe947b07ccfb023316096c\" network for pod \"itom-cdf-upgrade-deployer-202105-9qvwn\": networkPlugin cni failed to set up pod \"itom-cdf-up                                               grade-deployer-202105-9qvwn_core\" network: failed to set bridge addr: \"cni0\" already has an IP address different from 172.16.60.1/24, failed to clean up sandbox container \"c4b3b01869bbd66                                               4efc6c5b8e4f9a6b1a0fa2e2838fe947b07ccfb023316096c\" network for pod \"itom-cdf-upgrade-deployer-202105-9qvwn\": networkPlugin cni failed to teardown pod \"itom-cdf-upgrade-deployer-202105-9qv                                               wn_core\" network: running [/usr/sbin/iptables -t nat -D POSTROUTING -s 172.16.60.171/24 -j CNI-22a8642b3b26382c0b66d1e8 -m comment --comment name: \"cbr0\" id: \"c4b3b01869bbd664efc6c5b8e4f9                                               a6b1a0fa2e2838fe947b07ccfb023316096c\" --wait]: exit status 2: iptables v1.4.21: Couldn't load target `CNI-22a8642b3b26382c0b66d1e8':No such file or directory</code></pre>\n<h2>Cause</h2>\n<p>This issue can occur when you don't run the <code>&lt;upgrade_package&gt;/k8s/scripts/cfgKubeViaMulNetAdapters.sh</code> script before you run the upgrade commands.</p>\n<p>To verify that you are experiencing this issue, log on to the first node where you ran the <code>upgrade.sh -i </code>command, and then run the following command: </p>\n<pre><code>cd ${CDF_HOME}/log/scripts ; grep -r \"Warning: Detected more Flannel node entries in ETCD than there are nodes in this cluster.\"</code></pre>\n<p>If the message is present on the node, follow the steps described in the Solution section.</p>\n<h2>Solution</h2>\n<p>Make sure you have already run the <code>upgrade.sh -i</code> on every node. If you don't, the following solutions may impact the OMT upgrade. </p>\n<ul>\n<li>If pods don't start on a worker node, use the <code>cdfctl</code> command to delete the worker node and then add it back.</li>\n<li>If pods don't start on a control plane node in a deployment that has multiple control plane nodes, use the <code>cdfctl</code> command to delete the worker node and then add it back.</li>\n<li>If pods don't start on a control plane node in a deployment that has only one control plane node, follow these steps to revert the Flannel iface to the original version:\n\t<ol>\n<li>On the first node where you ran the <code>upgrade.sh -i </code>command, run the following command to find the <code>nodePublicIPMapping</code> record in upgrade log:\n\t\t<pre><code>cd ${K8S_HOME}/log/scripts ; grep -rA 10  \"nodePublicIPMapping\"</code></pre>\n<p>The output contains text that resembles the following:</p>\n<pre><code>nodePublicIPMapping: {\n\"testvm05.mfswlab.net\": \"12.345.678.910\",\n\"testvm06.mfswlab.net\": \"12.345.678.911\"\n}</code></pre>\n</li>\n<li>Run the following command to find which node has pods that don't start:\n\t\t<pre><code>kubectl get pods -n &lt;OMT namespace&gt; -o wide</code></pre>\n</li>\n<li>Run the following command to edit the Flannel iface configuration:\n\t\t<pre><code>kubectl edit cm kube-flannel-cfg -n kube-system</code></pre>\n</li>\n<li>\n<p>Find the entry for the node that you identified in step 2. For example, if <code>testvm06.mfswlab.net</code> has the network issues, the corresponding entry in the <code>kube-flannel-cfg</code> ConfigMap resembles the following:</p>\n<pre><code>testvm05.mfswlab.net: |-\n    export KUBERNETES_SERVICE_HOST=testvm05.mfswlab.net\n    export FLANNELD_IFACE=12.345.678.910\n  testvm06.mfswlab.net: |-\n    export KUBERNETES_SERVICE_HOST=testvm05.mfswlab.net\n    export FLANNELD_IFACE=ens34</code></pre>\n</li>\n<li>Find the entry for the <code>FLANNELD_IFACE</code> parameter that belongs to the node that has network issues, and change the value back to the IP address you identified in the <code>nodePublicIPMapping</code> record above (in this example, \"12.345.678.911\") or to the name of the network adapter that owns this IP address in the <code>kube-flannel-cfg</code> ConfigMap. If you're unsure of the IP address, log on to the node, run the <code>ifconfig</code> command, and get the IP address from the right interface. If in doubt, ask your system administrator.</li>\n<li>Reboot the node that has the network issue. In the example above, reboot <code>testvm06.mfswlab.net</code>.</li>\n</ol>\n</li>\n</ul>\n<p><br/>\n</p>\n<p class=\"mw-empty-elt\"></p><span class=\"snippet-end\" data-sname=\"423-25.4-423-25.3-FailedCreatePodSandBox||N\"></span><p>  </p></div>",
  "modifiedon": "2025-10-24 08:51:12"
}