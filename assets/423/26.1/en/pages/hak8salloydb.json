{
  "title": "Install AlloyDB Omni on Kubernetes with high availability",
  "content": "<div class=\"mw-parser-output\"><p class=\"mw-empty-elt\">\n</p><p>You can use a Kubernetes-based AlloyDB Omni cluster as a database HA solution for an on-premises deployment. </p>\n<p>This document is a supplementary document to the official <a href=\"https://cloud.google.com/alloydb/docs/omni/deploy-kubernetes\" title=\"Install AlloyDB Omni on Kubernetes\">Install AlloyDB Omni on Kubernetes</a> published by Google. The procedure described below has the following assumptions:</p>\n<ol>\n<li>You have basic familiarity with Kubernetes operation.</li>\n<li>You want to deploy a database cluster on the embedded Kubernetes cluster in OpenText OPTIC Management Toolkit (OMT). </li>\n<li>You will include the database information in the OMT installation command. This means you need to use a Docker-based AlloyDB Omni as the OMT database server temporarily. After the Kubernetes AlloyDB is installed, you must migrate the OMT data from the Docker-based AlloyDB Omni to Kubernetes AlloyDB Omni.</li>\n</ol>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-important-icon\"></div>\n<div class=\"admonition-content admonition-important-content\">Practice guidance of any third-party products (for example, AlloyDB HA) provided here is for reference purposes only with an intention to help customers. However, customers will remain responsible for ensuring that the end-to-end solution works with the underlying infrastructure, hardware/software dependencies, and more. OpenText will extend its responsibility and support for OpenText products only per contractual agreements as applicable.</div>\n</div>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"></div>\n<div class=\"admonition-content admonition-note-content\">In this release, support for AlloyDB Omni is under controlled availability. Please contact our product management team before you begin the implementation in your production environment. </div>\n</div>\n<h2 id=\"alloydbha\"><span style=\"font-size: clamp(1.59375rem, 18.7895px + 1.39803vw, 2.125rem);\">Before you begin</span></h2>\n<p>The HA database solution described here utilizes two separate database servers to run database instances as Kubernetes pods. Each instance pod uses local host disks. </p>\n<h3>Hardware requirement</h3>\n<p style=\"margin-bottom:11px\"><span style=\"font-size:12pt\"><span style=\"line-height:115%\"><span style=\"font-family:Aptos,sans-serif\">Use the following table to plan the required hardware resources for installing AlloyDB Omni server in a HA deployment.</span></span></span><br/>\n </p>\n<table cellpadding=\"1\" cellspacing=\"1\" style=\"width: 500px;\">\n<tbody>\n<tr>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\"><b>SMAX deployment size</b></span></span></span></td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\"><b>Concurrent users</b></span></span></span></td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\"><b>Configuration</b></span></span></span></td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\"><b>Storage</b></span></span></span></td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\"><b>Quantity</b></span></span></span></td>\n</tr>\n<tr>\n<td>Small</td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\">100 ~ 400</span></span></span></td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\">8 CPU 32 GB RAM</span></span></span></td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\">200 GB </span></span></span></td>\n<td>2</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\">400 ~ 1000</span></span></span></td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\">12 CPU 48 GB RAM ~ 16 CPU 96 GB RAM</span></span></span></td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\">200 GB </span></span></span></td>\n<td>2</td>\n</tr>\n<tr>\n<td>Large</td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\">1000 ~ 3000</span></span></span></td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\">16 CPU 64 GB ~ 24 CPU 128 GB RAM</span></span></span></td>\n<td><span style=\"font-size:12pt\"><span style=\"line-height:normal\"><span style=\"font-family:Aptos,sans-serif\">400 GB </span></span></span></td>\n<td>2</td>\n</tr>\n</tbody>\n</table>\n<p><span style=\"color: rgb(22, 22, 22); font-size: clamp(1.3125rem, 15.4737px + 1.15132vw, 1.75rem);\">Prepare two VMs or physical hosts</span></p>\n<p>Prepare 2 virtual machines (VMs) or physical hosts. In this document, <strong>alloydb_node1</strong> and <strong>alloydb_node2</strong> refer to the two VMs. Read the Google AlloyDB Omni document <a href=\"https://cloud.google.com/alloydb/docs/omni/deploy-kubernetes#before\" title=\"Before you begin\">Before you begin</a> to prepare the two Linux servers. The VMs must meet the following requirements:</p>\n<table cellpadding=\"1\" cellspacing=\"1\" style=\"width: 500px;\">\n<thead>\n<tr>\n<th scope=\"col\">Item</th>\n<th scope=\"col\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Linux kernel version</td>\n<td>\n<p>See Kubernetes AlloyDB Omni's requirements for the Linux kernel. As an example, we have tested on Linux 9.4 (Rocky Linux 9.4 and Red Hat Enterprise Linux release 9.4).  You can check the Linux kernel version by running the \"<code>uname -r</code>\"command.</p>\n</td>\n</tr>\n<tr>\n<td>Control group v2 (cgroup v2) enabled</td>\n<td>\n<p>You can check it with the following command:</p>\n<p><code>cat /proc/filesystems |grep cgroup2</code></p>\n</td>\n</tr>\n<tr>\n<td>Hard disks/CPU/RAM</td>\n<td>For information about the capacity and IO throughput of disks, and CPU/RAM requirements, refer to the sizing guide.</td>\n</tr>\n<tr>\n<td>Network</td>\n<td>The two database hosts must be in the same subnet as other nodes.</td>\n</tr>\n<tr>\n<td>Firewall</td>\n<td>Make sure the service firewalld is running:<br/>\n<code>systemctl status firewalld</code></td>\n</tr>\n<tr>\n<td>Swapping enabled</td>\n<td>For more information, see <a href=\"https://cloud.google.com/alloydb/docs/omni/configure-omni\" title=\"Enable swapping\">Enable swapping</a>.</td>\n</tr>\n</tbody>\n</table>\n<h3>Install the Kubernetes cluster</h3>\n<p>You must have a Kubernetes cluster installed. For example, you have an OMT embedded Kubernetes cluster ready. </p>\n<p>Follow the steps in <a href=\"https://cloud.google.com/alloydb/docs/omni/deploy-kubernetes#before\" title=\"Before you begin\">Before you begin</a> to prepare the software on one of the Kubernetes control panel nodes. If you don't have access to the internet, ignore the Google Cloud CLI installation, which is used to download the AlloyDB Omni Operator package. </p>\n<h2 id=\"alloydbha\"><span style=\"font-size: clamp(1.59375rem, 18.7895px + 1.39803vw, 2.125rem);\">Install AlloyDB Omni on Kubernetes</span></h2>\n<p>Install the AlloyDB Omni Operator on the Kubernetes cluster, and complete the required configurations. </p>\n<h3>Install AlloyDB Omni Operator on the Kubernetes cluster</h3>\n<p>Perform these steps on the Kubernetes control panel node:</p>\n<ol>\n<li>Download the AlloyDB Omni Operator package (For example, <code>alloydbomni-operator-1.1.0.tgz</code>) by following the instructions in <a href=\"https://cloud.google.com/alloydb/docs/omni/deploy-kubernetes#install\" title=\"Install the AlloyDB Omni Operator\">Install the AlloyDB Omni Operator</a>. If you have no internet access on this node, download it from another host with internet access, and then copy it to this host.</li>\n<li>Install the AlloyDB Omni Operator by running the <strong>helm install</strong> command. For more information, see <a href=\"https://cloud.google.com/alloydb/docs/omni/deploy-kubernetes#install\" title=\"Install the AlloyDB Omni Operator\">Install the AlloyDB Omni Operator</a>.</li>\n<li>Run the following command to check if the pods are running with a status of <code>2/2</code>:\n\t<pre>kubectl get pods -n alloydb-omni-system</pre>\n\tThe expected result should list two pods:\n\n\t<pre>NAME                                        READY   STATUS    RESTARTS       AGE\nfleet-controller-manager-574586767f-4wmtg   2/2     Running   2 (4d2h ago)   4d5h\nlocal-controller-manager-7cbb8f5f7-scx6w    2/2     Running   2 (4d2h ago)   4d5h</pre>\n</li>\n<li>\n<p>If the pods failed to pull images from the Google <code>gcr.io</code> registry on the Kubernetes cluster node, you need to download the images and upload them to your private registry. </p>\n<p>Run the following commands to check which images are required:</p>\n<pre>kubectl get deployment fleet-controller-manager -n alloydb-omni-system -o yaml|grep image\nkubectl get deployment local-controller-manager -n alloydb-omni-system -o yaml|grep image\n</pre>\n<p>Download the required image from a host where you can access the <code>gcr.io </code>registry and upload them to your private registry with the repository.</p>\n<p>Run the following commands to modify the <strong>registry </strong>in values.yaml to the private registry in the <code>alloydbomni-operator-${OPERATOR_VERSION}.tgz</code> helm package:</p>\n<pre>tar zxvf alloydbomni-operator-1.1.0.tgz\nvi alloydbomni-operator/values.yaml #To change the registry\nmv alloydbomni-operator-1.1.0.tgz alloydbomni-operator-1.1.0.tgz.origin\ntar czvf alloydbomni-operator-1.1.0.tgz alloydbomni-operator   </pre>\n<p>Run the <strong>helm install </strong>command to install the AlloyDB Omni Operator again.</p>\n</li>\n</ol>\n<h3>Tune AlloyDB Omni server for HA</h3>\n<p style=\"margin-bottom:11px\"><span style=\"font-size:12pt\"><span style=\"line-height:115%\"><span style=\"font-family:Aptos,sans-serif\">For detail on how to tune AlloyDB Omni server for HA, see <a href=\"/doc/423/26.1/prepareexternaldbs#Configure_the_postgresql.conf_file\" title=\"Configure the postgresql.conf file\">Configure the postgresql.conf file</a>.</span></span></span><br/>\n </p>\n<h3>Add database nodes to the Kubernetes cluster</h3>\n<p>Follow these steps to add two database nodes to the Kubernetes cluster: </p>\n<ol>\n<li>Go to the $CDF_HOME directory on the OMT Kubernetes control panel node, and copy the command file <code>node_prereq</code> to each database node. Then, run the following command:\n\n\t<pre>./node_prereq -T worker --all</pre>\n</li>\n<li>Add the two database nodes to the Kubernetes cluster. To do so, log in to <code>https://&lt;external access url&gt;:5443</code> as admin, and then navigate to <strong>CLUSTER </strong>&gt; <strong>Nodes </strong>to add the nodes.</li>\n<li>Make sure that the database nodes are only used for the AlloyDB Omni cluster pods.\n\t<p>Run the following command to check the node label:</p>\n<pre>kubectl get nodes --show-labels</pre>\n<p>Make sure the node has no <code>\"group=autoscaling\"</code> label so that the suite pods don't autoscale on it.</p>\n<p>Plan the node tags and make sure the database nodes have different tags from other nodes. If you use the OMT Kubernetes, run the following command to remove the <strong>Worker </strong>tag from these two database nodes:</p>\n<pre>kubectl label nodes &lt;alloydb-node&gt;<alloydb node=\"\"> Worker-</alloydb></pre>\n</li>\n</ol>\n<h3>Prepare user and data folders on both database nodes</h3>\n<ol>\n<li>Create a group and a user with ID <strong>2345</strong> which will be used by the AlloyDB Instance pod as <code>\"runAsGroup\"</code> and <code>\"runAsUser\"</code> later. To do this, run the following commands as the root user:\n\n\t<div contenteditable=\"false\" tabindex=\"-1\">\n<pre><code>sudo groupadd -g 2345 alloygroup\nsudo useradd -u 2345 alloyuser -g alloygroup </code></pre>\n</div>\n</li>\n<li>Create four directories on the database host local disk with user ID <strong>2345</strong>. For example:\n\t<pre>mkdir -p /alloydb/data\nmkdir -p /alloydb/log\nmkdir -p /alloydb/obs\nmkdir -p /alloydb/backup\nchmod 777 /alloydb -R\nchown 2345:2345 /alloydb -R\n</pre>\n</li>\n</ol>\n<h3>Prepare persistent volumes</h3>\n<p>Perform the following steps on a control plane node in the Kubernetes cluster:</p>\n<ol>\n<li>Create a StorageClass for the data persistent volumes (PVs). Change &lt;itom-alloydb-data&gt; to a name you want to use. \n\t<pre>cat &lt;&lt; EOF | kubectl apply -f -\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: &lt;itom-alloydb-data&gt;\nprovisioner: kubernetes.io/no-provisioner\nreclaimPolicy: Retain\nallowVolumeExpansion: true\nEOF\n</pre>\n</li>\n<li>Create four PVs to mount to the four folders you created on the <strong>alloydb-node1 </strong>host.<br/>\n<strong>Note</strong>: Replace values in brackets (&lt;&gt;) with your own values, and replace &lt;db node host&gt; with the alloydb-node1 host name.<br/>\n<br/>\n\tCreate a PV for <strong>datadisk</strong>.<br/>\n<db host=\"\" node=\"\"></db>\n<pre>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: &lt;alloydbdata-node1&gt;\nspec:\n  capacity:\n    storage: &lt;20Gi&gt;\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: &lt;itom-alloydb-data&gt;\n  local:\n    path: &lt;/alloydb/data&gt;\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - &lt;db node host&gt;\nEOF\n</pre>\n<p>Create a PV for backup:</p>\n<pre>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: &lt;alloydbbackup-node1&gt;\nspec:\n  capacity:\n    storage: &lt;10Gi&gt;\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: standard\n  local:\n    path: &lt;/alloydb/backup&gt;\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - &lt;db node host&gt;\nEOF\n</pre>\n<p>Create a PV for <strong>log:</strong></p>\n<pre>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: &lt;alloydblog-node1&gt;\nspec:\n  capacity:\n    storage: &lt;2Gi&gt;\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: standard\n  local:\n    path: &lt;/alloydb/log&gt;\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - &lt;db node host&gt;\nEOF\n</pre>\n<p>Create a PV for <strong>obs:</strong></p>\n<pre>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: &lt;alloydbobs-node1&gt;\nspec:\n  capacity:\n    storage: &lt;3Gi&gt;\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: standard\n  local:\n    path: &lt;/alloydb/obs&gt;\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - &lt;db node host&gt;\nEOF\n</pre>\n<p>Run the following command to check the PVs:</p>\n<pre>kubectl get pv</pre>\n<p>The output should resemble the following:</p>\n<pre>NAME                                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                                                   STORAGECLASS        VOLUMEATTRIBUTESCLASS   REASON   AGE\nalloydbbackup-node1                    10Gi       RWO            Retain           Available                                                                                           standard            &lt;unset&gt;                          5s\nalloydbdata-node1                      20Gi       RWO            Retain           Available                                                                                           itom-alloydb-data   &lt;unset&gt;                          5s\nalloydblog-node1                       2Gi        RWO            Retain           Available                                                                                           standard            &lt;unset&gt;                          4s\nalloydbobs-node1                       3Gi        RWO            Retain           Available                                                                                           standard            &lt;unset&gt;                          4s\n</pre>\n</li>\n<li>If you have added the <strong>alloydb-node2</strong> to the Kubernetes cluster and prepared the user, group, and four folders, repeat the previous step to create another four PVs for the <strong>alloydb-node2 </strong>host.</li>\n</ol>\n<h3>Create a database cluster</h3>\n<p>Follow the guide <a href=\"https://cloud.google.com/alloydb/docs/omni/deploy-kubernetes#create\" title=\"Create a database cluster\">Create a database cluster</a> to create a database cluster. Here is an example.</p>\n<ol>\n<li>Run the following command to create a namespace for the AlloyDB cluster. In this document, it's named \"<strong>alloydb-ha</strong>\" (referred to as &lt;alloydb-ha&gt; in later steps).\n\n\t<pre>kubectl create namespace alloydb-ha  </pre>\n<p>This namespace must be different from the namespace you created for the AlloyDB Omni Operator (for example, <strong>alloydb-omni-system</strong>).</p>\n</li>\n<li>Create a Secret and a DBCluster. Throughout this document, &lt;<strong>mydbcluster&gt; </strong>refers to the dbcluster name.<br/>\n<strong>Note</strong>: Replace values in brackets (&lt;&gt;) with your own values.\n\t<pre>    \ncat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db-pw-&lt;mydbcluster&gt;\n  namespace: &lt;alloydb-ha&gt;\ntype: Opaque\ndata:\n  &lt;mydbcluster&gt;: \"&lt;base64-encoded-value-of-password&gt;\"\n---\napiVersion: alloydbomni.dbadmin.goog/v1\nkind: DBCluster\nmetadata:\n  name: &lt;mydbcluster&gt;\n  namespace: &lt;alloydb-ha&gt;\nspec:\n  databaseVersion: \"15.5.2\"\n  isDeleted: false\n  primarySpec:\n    adminUser:\n      passwordRef:\n        name: db-pw-&lt;mydbcluster&gt;\n    resources:\n      cpu: &lt;10&gt;\n      memory: &lt;56Gi&gt;\n      disks:\n      - name: DataDisk\n        size: &lt;20Gi&gt;\n        storageClass: &lt;itom-alloydb-data&gt;\n      - name: BackupDisk\n        size: &lt;10Gi&gt;\n        storageClass: standard\n      - name: LogDisk\n        size: &lt;2Gi&gt;\n        storageClass: standard\n      - name: ObsDisk\n        size: &lt;3Gi&gt;\n        storageClass: standard\nEOF\n</pre>\n</li>\n<li>Run the following command to check the DBCluster you created:\n\t<pre>kubectl get dbcluster -n &lt;alloydb-ha&gt;\n</pre>\n<p>The output should resemble the following (the PRIMARYPHASE and DBCLUSTERPHASE values are dynamic and can be different than the values below):</p>\n<pre>NAME     PRIMARYENDPOINT   PRIMARYPHASE   DBCLUSTERPHASE         HAREADYSTATUS   HAREADYREASON\nmydbcluster   172.17.174.98     InProgress     DBClusterReconciling</pre>\n</li>\n<li>Run the following command to check the statefulset:\n\t<pre>kubectl get statefulset -n &lt;alloydb-ha&gt;</pre>\n<p>You should see that the statefulset is created in the alloydb-ha namespace.</p>\n<pre>NAME             READY   AGE\nal-47d5-mydbcluster   0/1     40s\n</pre>\n</li>\n<li>Run the following command to check that the AlloyDB instance pod is up and running:\n\t<pre>kubectl get pods -n &lt;alloydb-ha&gt;</pre>\n<p>You should see the pod is running in the namespace.</p>\n<pre>NAME               READY   STATUS     RESTARTS   AGE\nal-47d5-mydbcluster-0   0/4     Init:0/1   0          47s\n</pre>\n</li>\n<li>Run the following command to check the PVC:\n\t<pre>kubectl get pvc -n &lt;alloydb-ha&gt;</pre>\n<p>Check that the status of each PVC changes to <code>Bound</code>.</p>\n<pre>NAME                          STATUS   VOLUME                CAPACITY   ACCESS MODES   STORAGECLASS        VOLUMEATTRIBUTESCLASS   AGE\nbackupdisk-al-47d5-mydbcluster-0   Bound    alloydbbackup-node1   10Gi       RWO            standard            <unset>                 9m10s\ndatadisk-al-47d5-mydbcluster-0     Bound    alloydbdata-node1     20Gi       RWO            itom-alloydb-data   <unset>                 9m10s\nlogdisk-al-47d5-mydbcluster-0      Bound    alloydblog-node1      2Gi        RWO            standard            <unset>                 9m10s\nobsdisk-al-47d5-mydbcluster-0      Bound    alloydbobs-node1      3Gi        RWO            standard            <unset>                 9m10s     </unset></unset></unset></unset></pre>\n</li>\n<li>Wait until the AlloyDB instance pod is up. Then, run the following command to check the instance:\n\t<pre>kubectl get instance -n &lt;alloydb-ha&gt;</pre>\n<p>You should see the instance <code>ROLE</code> is <code>primary</code> and the <code>PHASE</code> is <code>ready</code>.</p>\n<pre>NAME          ENDPOINT   URL   ROLE      PHASE   MESSAGE   HAREADYSTATUS   HAREADYREASON\n47d5-mydbcluster                    Primary   Ready</pre>\n</li>\n<li>Use <code>psql</code> to log in to the database as the user \"alloydbadmin\".\n\t<pre>kubectl get pods -n alloydb-ha\nkubectl exec -ti &lt;podname&gt; -c database -n &lt;alloydb-ha&gt; -- /bin/psql -h localhost -U alloydbadmin\n</pre>\n<p>Then, change the password for the postgres user. To do this, run the SQL below to make sure the postgres user's password is the one that you set in the \"<code>db-pw-&lt;mydbcluster&gt;\"</code> Kubernetes secret:</p>\n<pre>ALTER USER postgres with password '&lt;base64-decoded-value-of-password-in-secret&gt;';</pre>\n</li>\n<li>Make sure you can log in to the database as the postgres user.\n\t<pre>kubectl exec -ti &lt;podname&gt; -c database -n &lt;alloydb-ha&gt; -- /bin/psql -h localhost -U postgres</pre>\n</li>\n<li>Run the following command to check the services in the alloydb-ha namespace:\n\t<pre>kubectl get services -n &lt;alloydb-ha&gt;</pre>\n<p>The output should resemble the following (the suite will connect to the primary DB node through the <code>al-&lt;mydbcluster&gt;-rw-ilb</code> service):</p>\n<pre>NAME                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\nal-47d5-mydbcluster-dbd                 ClusterIP   172.17.52.115    &lt;none&gt;        3203/TCP            82m\nal-47d5-mydbcluster-ilb                 ClusterIP   172.17.180.78    &lt;none&gt;        5432/TCP            82m\nal-47d5-mydbcluster-monitoring-system   ClusterIP   172.17.212.110   &lt;none&gt;        9187/TCP            82m\nal-mydbcluster-backup-server            ClusterIP   172.17.175.97    &lt;none&gt;        8432/TCP,5432/TCP   82m\nal-mydbcluster-rw-ilb                   ClusterIP   172.17.174.98    &lt;none&gt;        5432/TCP            82m<none><none><none><none><none>\n</none></none></none></none></none></pre>\n</li>\n<li>Run the following command to check the CA certificate:\n\t<pre>kubectl get secret dbs-al-cert-&lt;mydbcluster&gt; -n &lt;alloydb-ha&gt; -o json|jq '.data'| jq -r '.[\"ca.crt\"]'|base64 -d|openssl x509 -text -noout</pre>\n<p>Run the following command to save the CA certificate to a file.</p>\n<pre>kubectl get secret dbs-al-cert-&lt;mydbcluster&gt; -n &lt;alloydb-ha&gt; -o json|jq '.data'| jq -r '.[\"ca.crt\"]'|base64 -d&gt;/tmp/ca.crt</pre>\n</li>\n<li>(Optional) Create a new service with a node port (for example, <code>35432</code>). The following uses \"<code>mydbcluster-ilb-svc</code>\" as an example name for the service.\n\t<pre>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: &lt;mydbcluster-ilb-svc&gt;\n  namespace:<strong> </strong>&lt;alloydb-ha&gt;\nspec:\n  ports:\n  - name: db\n    port: 5432\n    protocol: TCP\n    targetPort: 5432\n    nodePort: &lt;35432&gt;\n  selector:\n    alloydbomni.internal.dbadmin.goog/dbcluster: &lt;mydbcluster&gt;\n    alloydbomni.internal.dbadmin.goog/dbcluster-ns: &lt;alloydb-ha&gt;\n    alloydbomni.internal.dbadmin.goog/instance: &lt;instance-name&gt;\n    alloydbomni.internal.dbadmin.goog/task-type: database\n  type: NodePort\nEOF\n  </pre>\n</li>\n<li>Run the following command to connect to the Kubernetes internal AlloyDB from an external host through the node port using the CA certificate. You must first copy the ca.crt file to the location where you run the command.\n\t<pre>psql \"postgresql://postgres@&lt;k8s node host&gt;:&lt;node port&gt;/postgres?sslmode=verify-ca&amp;sslrootcert=/tmp/ca.crt\" -W\n</pre>\n</li>\n<li>Change the database parameters to meet the requirements described in <a href=\"/doc/423/26.1/prepareexternaldbs\" title=\"PrepareExternalDBs\">PrepareExternalDBs</a>. You must run the <code>ALTER SYSTEM</code> SQL command to change them. For example:\n\t<pre>ALTER SYSTEM SET effective_cache_size = '40GB';\nALTER SYSTEM SET max_connections = 2000;\nALTER SYSTEM SET shared_buffers = '24GB';\nALTER SYSTEM SET jit = 'off';\nALTER SYSTEM SET work_mem= '100MB';\n</pre>\n<p>Later, you can find these values in the <code>postgresql.auto.conf</code> file in the PGDATA directory.</p>\n<p>If the parameters you changed require a restart to take effect, restart the database instance. You can do this by setting the statefulset replicas to 0 and then back to 1. </p>\n<pre>kubectl scale statefulset &lt;statefulset-name&gt; -n &lt;alloydb-ha&gt; --replicas=0\nkubectl scale statefulset &lt;statefulset-name&gt; -n &lt;alloydb-ha&gt; --replicas=1</pre>\n</li>\n<li>Make sure the CPU and memory limits for the database instance pod are as expected. You can change them by running the following command:\n\t<pre>kubectl edit dbcluster &lt;dbcluster&gt; -n &lt;alloydb-ha&gt; -o yaml</pre>\n<p>Locate the following section, and then change the values:</p>\n<pre>    resources:\n      cpu: &lt;10&gt;\n      memory: &lt;56Gi&gt;</pre>\n</li>\n</ol>\n<h2><span style=\"font-size: clamp(1.59375rem, 18.7895px + 1.39803vw, 2.125rem);\">Enable High Availability</span></h2>\n<p>Before you begin, make sure you have met the following prerequisites:</p>\n<ul>\n<li>The primary AlloyDB instance is ready.</li>\n<li>On the <strong>alloydb-node2 </strong>host, the user <strong>alloyuser </strong>and the group <strong>alloygroup </strong>have been created, and the four folders are created with owner <strong>alloyuser:</strong><strong>alloydbgroup</strong>.</li>\n<li>You have added the <strong>alloydb-node2</strong> to this Kubernetes cluster successfully.</li>\n</ul>\n<h3>Enable High Availability</h3>\n<ol>\n<li>Create the Kubernetes PVs for the node2 (<strong>alloydb-node2</strong>) disk folders if they are not created.\n\n\t<p>Make sure the database host name is the node2 host name (<strong>alloydb-node2</strong>) in the PV creation yaml.</p>\n<pre>.......\n.......    \n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - &lt;db node 2 host&gt;</pre>\n<p>Make sure <code>PV STATUS</code> is <code>Available</code>.</p>\n<pre>kubectl get pv</pre>\n</li>\n<li>Edit the dbcluster by following the \"Enable HA\" section on the <a href=\"https://cloud.google.com/alloydb/docs/omni/kubernetes-ha\" title=\"Manage high availability in Kubernetes\">Manage high availability in Kubernetes</a>. \n\t<p>You can run the following command to do this:</p>\n<pre>kubectl edit dbcluster &lt;mydbcluster&gt;<db-cluster-name> -n &lt;alloydb-ha&gt;<namespace> -o yaml</namespace></db-cluster-name></pre>\n</li>\n<li>Check that a new statefulset has been created automatically in the namespace.</li>\n<li>Check the PVC and PV to see if they are bound as expected. The instance pod will begin to run the init container only after the PVC and PV are bound.</li>\n<li>Check the newly created pod status and wait for it to be <code>4/4</code>.</li>\n<li>After it enters the 4/4 status, it will do pg_basebackup to fetch the data from the primary database node. During this period, you can't connect to the standby database node through the psql client. You can check the pod log to see if the pg_basebackup is done and the standby database node is running.</li>\n<li>Check the database instances by running the following command:\n\t<pre>kubectl get instance -n &lt;alloydb-ha&gt;</pre>\n<p>The expected result should resemble the following:</p>\n<pre>NAME          ENDPOINT   URL   ROLE      PHASE   MESSAGE   HAREADYSTATUS   HAREADYREASON\n47d5-mydbcluster                    Primary   Ready             True            Ready\n4ec0-mydbcluster                    Standby   Ready             True            Ready      </pre>\n</li>\n<li>Check the dbcluster by running the following command:\n\t<pre>kubectl get dbcluster -n &lt;alloydb-ha&gt;</pre>\n<p>The expected result should resemble the following, and the <code>HAREADYSTATUS</code> value is <code>True</code>.</p>\n<pre>NAME     PRIMARYENDPOINT   PRIMARYPHASE   DBCLUSTERPHASE   HAREADYSTATUS   HAREADYREASON\nmydbcluster   172.17.174.98     Ready          DBClusterReady   True            Ready\n</pre>\n</li>\n<li>Check that the database instance pods are running on the node1 (<strong>alloydb-node1</strong>) host and the node2 (<strong>alloydb-node2</strong>) host separately.\n\t<pre>kubectl get pods -n &lt;alloydb-ha&gt; -o wide</pre>\n</li>\n</ol>\n<h2><span style=\"font-size: clamp(1.59375rem, 18.7895px + 1.39803vw, 2.125rem);\">How to recover the system to HA after a failover</span></h2>\n<o>\n<p>After failover, you can see:</p>\n<li>The statefulset of the failed database instance is removed from the Kubernetes system.</li>\n<li>A new statefulset is created, but the related pod is in <code>Pending</code> status.</li>\n<li>The status of the PV for that failed database instance is <code>Failed</code>.</li>\n</o>\n<p>Follow the steps to recover the system:</p>\n<ol>\n<li>Delete the failed PVs.</li>\n<li>Back up and clean up the data folders that have failures.</li>\n<li>Recreate the PVs.</li>\n<li>Wait for the dbcluster to enter the HA mode again.</li>\n</ol>\n<h2><span style=\"font-size: clamp(1.59375rem, 18.7895px + 1.39803vw, 2.125rem);\">How to migrate the DB data from an external database to the Kubernetes AlloyDB Omni</span></h2>\n<p>The procedure to migrate data from an external database to a Kubernetes-based AlloyDB Omni database cluster is described in <a href=\"/doc/423/26.1/migrateselfhostpg2alloydb\" title=\"Migrate Database\">Migrate from self-hosted PostgreSQL to AlloyDB</a>. The external database can be self-hosted PostgreSQL or docker-based AlloyDB Omni.</p>\n<p>Pay attention to the following.</p>\n<h3>Database restore</h3>\n<p>The \"mydbcluster-ilb-svc\" service described previously must be created before performing the data restore.</p>\n<p>You can use the k8s node host as the database host and the node port as the database port when you restore data to the k8s AlloyDB Omni.</p>\n<h3>Switching the database</h3>\n<ul>\n<li>The db host should be changed to the service name, for example \"<strong>al-mydbcluster-rw-ilb.alloydb-ha.svc</strong>\" here. The database port is the service port.</li>\n<li>As the k8s AlloyDB Omni uses the certificate in the secret 'dbs-al-cert-mydbcluster', the CN of the certificate is \"dbs-al-ca-<strong>mydbcluster</strong>\" by default. If you want to use sslmode 'verify-full', you need to replace the certificate in the secret. Otherwise, you need to make sure the hostname verification won't happen.</li>\n</ul>\n<p class=\"mw-empty-elt\"></p></div>",
  "modifiedon": "2025-10-24 08:51:12"
}