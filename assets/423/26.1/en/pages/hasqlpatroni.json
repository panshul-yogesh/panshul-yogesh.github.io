{
  "title": "High Availability PostgreSQL with Patroni",
  "content": "<div class=\"mw-parser-output\"><p class=\"mw-empty-elt\">\n</p><p>Patroni is a tool for deploying PostgreSQL servers in high-availability mode.</p>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-important-icon\"></div>\n<div class=\"admonition-content admonition-important-content\">Practice guidance of any third-party products (for example, PostgreSQL HA) provided here is for reference purposes only with an intention to help customers. However, customers will remain responsible for ensuring that the end-to-end solution works with the underlying infrastructure, hardware/software dependencies, and more. <code>OpenText</code> will extend its responsibility and support for <code>OpenText</code> products only per contractual agreements as applicable.</div>\n</div>\n<h2 id=\"Patroni_architecture\">Patroni architecture</h2>\n<p>The following diagram illustrates the architecture of a highly available PostgreSQL cluster using Patroni and HAProxy.</p>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"></div>\n<div class=\"admonition-content admonition-note-content\">You can deploy a hardware load balancer to replace HAProxy and Keepalived in the following diagram. In our lab environment, we've validated the configurations and deployments with an F5 load balancer.</div>\n</div>\n<p><a class=\"image\" href=\"/file/images/5/56/patroni20220816-1.PNG\" title=\"/file/images/5/56/patroni20220816-1.PNG\"> <img alt=\"/mediawiki/images/5/56/patroni20220816-1.PNG\" border=\"0\" file=\"/mediawiki/images/5/56/patroni20220816-1.PNG\" height=\"535\" hspace=\"0\" src=\"../../../images/patroni20220816-1_2b82b9f5.png\" style=\"width: 800px; height: 535px; margin: 0px; border: 0px solid black;\" vspace=\"0\" width=\"800\"/> </a></p>\n<p><br/>\n<span id=\"cke_bm_853C\" style=\"display: none;\"> </span></p>\n<p>Patroni is a cluster manager that can customize and automate the deployment and maintenance of PostgreSQL HA (High Availability) clusters. It supports database automatic failover and streaming replication. </p>\n<p>HAProxy or a load balancer offers load balancing and proxying for TCP and HTTP-based applications.</p>\n<p>Etcd is a strongly consistent, distributed key-value store that provides a reliable way to store data that a distributed system or cluster of machines needs to access. Use Etcd to store the state of the PostgreSQL cluster to keep the Postgres cluster up and running.</p>\n<p>To use this architecture, you must specify the HAProxy virtual IP address and port (5000 by default) as the database server host IP address and port.</p>\n<p>To connect to the read-only database server, you must specify the HAProxy virtual IP address and the read-only port (6000 by default) as the read-only database server host IP address and port.</p>\n<p>The following describes the procedure to install a highly available PostgreSQL cluster using Patroni, HAProxy and KeepAlived on RHEL8 in a lab environment. If you use a hardware load balancer, skip the installations of both HAProxy and Keepalived, but be aware you still need three etcd hosts here. For instructions on F5 hardware load balancer configurations, refer to the relevant section below. </p>\n<h2 id=\"Installation\">Preparation</h2>\n<p>Before you proceed, complete the following preparation steps.</p>\n<h3>Prepare VMs</h3>\n<p>Prepare 4 virtual machines (VMs) or physical hosts. In this example, we use 4 VMs prepared with Red Hat Enterprise Linux 8 /CentOS 8. </p>\n<ul>\n<li>2 VMs for Patroni and PostgreSQL<br/>\n\tThese two VMS need to meet the hardware requirements described in <a href=\"/doc/423/26.1/plandeployment\" title=\"Plan the deployment\">Plan the deployment</a>.</li>\n<li>2 VMs for HAProxy. Each VM must have a configuration of 4 vCPU, and 16GB RAM.</li>\n</ul>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"></div>\n<div class=\"admonition-content admonition-note-content\">Etcd requires three VMs. You don't need to use dedicated VMs for Etcd; instead, use shared VMs for Etcd: the two VMs for PostgreSQL, and one VM for HAProxy.<br/>\nAll of the VMs must have the same operating system and version. For example, if the two hosts for HAProxy are running on RHEL 8 and CentOS 8 respectively, the HA setup will fail. </div>\n</div>\n<p>In the following procedure, <strong>pg_node1 </strong>and <strong>pg_node2 </strong>refer to the two VMs for PostgreSQL, and <strong>etcd1</strong>, <strong>etcd2, </strong>and <strong>etcd3 </strong>refer to the Etcd hosts. </p>\n<ol>\n</ol>\n<h3>Prepare super user permission</h3>\n<p>You must have super user permission on these VMs. Run the installation commands as root.</p>\n<h3>Prepare a virtual IP address for HAproxy</h3>\n<p>Prepare an IP address as the virtual IP address of the HAproxy hosts.</p>\n<h3>Allow specific inbound ports for the VMs</h3>\n<p>Run the following commands for different VMs to allow the specific inbound ports.</p>\n<ol>\n<li>On the two Patroni &amp; Postgresql hosts, run the following commands:\n\t<pre><code>firewall-cmd --add-port={5432,8008}/tcp --permanent\nfirewall-cmd --reload</code></pre>\n</li>\n<li>On  the two HAProxy hosts, run the following commands:\n\t<pre><code>firewall-cmd --add-port={5000,6000,7000}/tcp --permanent\nfirewall-cmd --reload</code></pre>\n</li>\n<li>On the three Etcd cluster hosts, run the following commands:\n\t<pre><code>firewall-cmd --add-port={2379,2380}/tcp --permanent\nfirewall-cmd --reload</code></pre>\n</li>\n</ol>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"></div>\n<div class=\"admonition-content admonition-note-content\"><br/>\n You can change the ports if you use different ports on the Etcd service, Postgresql service, Patroni service, and HAProxy service (if used). The default ports for these services are as follows:\n<ul>\n<li>Service Etcd Ports: 2379, 2380</li>\n<li>Service Postgresql Port: 5432</li>\n<li>Service Patroni Port: 8008</li>\n<li>Service HAProxy Port: 5000, 6000, 7000</li>\n</ul>\n<p>The following example uses Red Hat Enterprise Linux release 8.2, Patroni 2.1.3, PG12, Etcd 3.3.11, Keepalived 2.1.5-6, and HAProxy1.8.27/F5 load balancer as example versions. <br/>\nThe parameters in the example are set for 2000 concurrent users for Service Management. Be sure to change the parameters according to your environment. </p>\n</div>\n</div>\n<h2 id=\"PGHAwithPatroni-Installetcd\">Install Etcd</h2>\n<p>Install Etcd on each of the three hosts.</p>\n<ol>\n<li>Download the Etcd rpm file, and then run the following command to install Etcd:\n\t<pre><code>rpm -ihv etcd-3.2.21-2.el8.x86_64.rpm</code></pre>\n</li>\n<li>Modify the <code>/etc/etcd/etcd.conf</code> file. Locate and uncomment the following parameters, and make sure you update the highlighted values to reflect yours.<br/>\n<strong>For the file on etcd1</strong>: \n\t<pre><code>[Member]\n\tETCD_LISTEN_PEER_URLS=\"http://&lt;etcd1 IP Address&gt;:2380\"\n\tETCD_LISTEN_CLIENT_URLS=\"http://&lt;etcd1 IP Address&gt;:2379,http://localhost:2379\"\n\tETCD_NAME=\"etcd1\"\n[Clustering]\n\tETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://&lt;etcd1 IP Address&gt;:2380\"\n\tETCD_ADVERTISE_CLIENT_URLS=\"http://&lt;etcd1 IP Address&gt;:2379\"\n\tETCD_INITIAL_CLUSTER=\"etcd1=http://&lt;etcd1 IP Address&gt;:2380,etcd2=http://&lt;etcd2 IP Address&gt;:2380,etcd3=http://&lt;etcd3 IP Address&gt;:2380\"\n\tETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"\n\tETCD_INITIAL_CLUSTER_STATE=\"new\"</code></pre>\n<br/>\n<strong>For the file on etcd2</strong>:\n\n\t<pre><code>[Member]\n\tETCD_LISTEN_PEER_URLS=\"http://&lt;etcd2 IP Address&gt;:2380\"\n\tETCD_LISTEN_CLIENT_URLS=\"http://&lt;etcd2 IP Address&gt;:2379,http://localhost:2379\"\n\tETCD_NAME=\"etcd2\"\n[Clustering]\n\tETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://&lt;etcd2 IP Address&gt;:2380\"\n\tETCD_ADVERTISE_CLIENT_URLS=\"http://&lt;etcd2 IP Address&gt;:2379\"\n\tETCD_INITIAL_CLUSTER=\"etcd1=http://&lt;etcd1 IP Address&gt;:2380,etcd2=http://&lt;etcd2 IP Address&gt;:2380,etcd3=http://&lt;etcd3 IP Address&gt;:2380\"\n\tETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"\n\tETCD_INITIAL_CLUSTER_STATE=\"new\"</code></pre>\n<br/>\n<strong>For the etcd3</strong>:\n\n\t<pre><code>[Member]\n\tETCD_LISTEN_PEER_URLS=\"http://&lt;etcd3 IP Address&gt;:2380\"\n\tETCD_LISTEN_CLIENT_URLS=\"http://&lt;etcd3 IP Address&gt;:2379,http://localhost:2379\"\n\tETCD_NAME=\"etcd3\"\n[Clustering]\n\tETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://&lt;etcd3 IP Address&gt;:2380\"\n\tETCD_ADVERTISE_CLIENT_URLS=\"http://&lt;etcd3 IP Address&gt;:2379\"\n\tETCD_INITIAL_CLUSTER=\"etcd1=http://&lt;etcd1 IP Address&gt;:2380,etcd2=http://&lt;etcd2 IP Address&gt;:2380,etcd3=http://&lt;etcd3 IP Address&gt;:2380\"\n\tETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"\n\tETCD_INITIAL_CLUSTER_STATE=\"new\"</code></pre>\n</li>\n<li>Make sure the owner of the folder \"/var/lib/etcd\" is user \"etcd\". You can change the folder ownership if it's not \"etcd\":\n\t<pre><code>chown -R etcd:etcd /var/lib/etcd</code></pre>\n</li>\n<li>Run the following command to enable the Etcd service to run at Linux startup:\n\t<pre><code>systemctl enable etcd</code></pre>\n</li>\n<li>Start the Etcd service:\n\t<pre><code>systemctl start etcd</code></pre>\n</li>\n<li>Check the Etcd status:\n\t<pre><code>systemctl status etcd</code></pre>\n</li>\n</ol>\n<p>After you have installed Etcd on the three hosts, check the cluster health by running the following commands on one of the Etcd hosts:</p>\n<ol>\n<li>Show the Etcd member list:\n\t<pre><code>etcdctl member list</code></pre>\n</li>\n<li>Check the Etcd cluster health:\n\t<pre><code>etcdctl cluster-health</code></pre>\n</li>\n</ol>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"></div>\n<div class=\"admonition-content admonition-note-content\">If you want to have a fresh start of the whole system, you need to delete the content under the Etcd data directory specified by the parameter \"ETCD_DATA_DIR\" in <code>/etc/etcd/etcd.conf</code>.<br/>\nThe default value for this parameter is <code>/var/lib/etcd/default.etcd</code>.For example, run the following command:<br/>\n<code>rm -rf /var/lib/etcd/default.etcd</code></div>\n</div>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"></div>\n<div class=\"admonition-content admonition-note-content\">Please don't use the domain name for the <strong>etcd </strong>IP Address here since the domain name is invalid for binding.</div>\n</div>\n<h2 id=\"PGHAwithPatroni-InstallPostgreSQL\">Install PostgreSQL</h2>\n<p>Install PostgreSQL on two hosts.</p>\n<ol>\n<li>Run the following command to add the PostgreSQL Yum Repository:\n\t<pre><code>yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm</code></pre>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"></div>\n<div class=\"admonition-content admonition-note-content\"> You can visit the <a href=\"https://www.postgresql.org/download/linux/redhat/\" title=\"PostgreSQL download site\">PostgreSQL download site</a> to obtain the noarch rpm package URL after selecting the version and the platform.</div>\n</div>\n</li>\n<li>Disable the built-in PostgreSQL module:\n\t<pre><code>dnf -qy module disable postgresql </code></pre>\n</li>\n<li>Install server packages:\n\t<pre><code>yum install -y postgresql12-server</code></pre>\n</li>\n<li>Check the installation result:\n\t<pre><code>rpm -qa|grep postgresql12</code></pre>\n\tThe result would be like the following:\n\n\t<pre><code>postgresql12-libs-12.9-1PGDG.rhel8.x86_64\npostgresql12-12.9-1PGDG.rhel8.x86_64\npostgresql12-server-12.9-1PGDG.rhel8.x86_64 </code></pre>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"></div>\n<div class=\"admonition-content admonition-note-content\"> You must <strong>not </strong>initialize the database here. Patroni will help initialize the database when it runs up. Otherwise, you will get failure when you try to start Patroni later.</div>\n</div>\n</li>\n</ol>\n<h2 id=\"PGHAwithPatroni-InstallPatroni\">Set up Patroni</h2>\n<p>Perform the following steps on two hosts to set up Patroni.</p>\n<h3>Install Patroni</h3>\n<p>Run the following commands to install Patroni.</p>\n<pre><code>curl https://bootstrap.pypa.io/pip/3.6/get-pip.py -o /tmp/get-pip.py -k\npython3 /tmp/get-pip.py\npip install psycopg2-binary\npip install patroni[etcd,consul]</code></pre>\n<h3>Prepare SSL certificates for PostgreSQL</h3>\n<ol start=\"1\">\n<li>\n<p>Create the following directory: </p>\n<pre><code>mkdir -p /usr/patroni/conf</code></pre>\n<p>You will copy the PostgreSQL SSL certificates to this directory. </p>\n</li>\n<li>Get or create a public/private key file pair, which you will use to turn on SSL for the PostgreSQL server. Here is an example of creating a self-signed certificate.\n\t<pre><code>openssl genrsa -out server.key 2048\nopenssl req -new -x509 -days 3650 -key server.key -out server.crt -subj \"/C=UK/ST=Massachusetts/L=Cambridge/O=Microfocus/OU=ITSMA/CN=*.domain.com\"\ncp server.crt /usr/patroni/conf/server.crt\ncp server.key /usr/patroni/conf/server.key</code></pre>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"></div>\n<div class=\"admonition-content admonition-note-content\"><br/>\n\tIn this example, the root CA certificate is server.crt itself. You will need to upload the root CA certificate to the suite during the suite installation. <br/>\n\tThe CN in the certificate must be the virtual IP address of the HAProxy cluster or the FQDN to which this IP address resolves</div>\n</div>\n</li>\n<li>Change the certificate file mode and owner.\n\t<pre><code>cd /usr/patroni/conf\nchmod 400 server.crt\nchmod 400 server.key\nchown postgres:postgres server.key\nchown postgres:postgres server.crt</code></pre>\n</li>\n</ol>\n<h3>Create a configuration YAML file for Patroni</h3>\n<ol start=\"1\">\n<li>Create the following directory if you haven't created it before:\n\t<pre><code>mkdir -p /usr/patroni/conf</code></pre>\n</li>\n<li>Run the following command to create the configuration file:\n\t<pre><code>touch /usr/patroni/conf/postgresql.yml</code></pre>\n</li>\n<li>Run the following command to edit the newly created file:\n\t<pre><code>vi /usr/patroni/conf/postgresql.yml</code></pre>\n</li>\n<li>Add the following configuration parameters to the file.<br/>\n\tMake sure you change the \"namespace,\" \"listen,\" and \"connect_address\" values to reflect your own setup.<br/>\n\tReplace &lt;pg_node1 IP Address&gt; and &lt;pg_node2 IP Address&gt; with your IP Addresses.<br/>\n\tReplace &lt;etcd1 IP Address&gt;, &lt;etcd2 IP Address&gt;, and &lt;etcd3 IP Address&gt; with your IP Addresses.<br/>\n\tReplace the server.crt and server.key filenames with your own values if your PostgreSQL SSL certificate files are different. <br/>\n<strong>On pg_nodde1, you can add the following:</strong>\n<pre><code>scope: postgres\nnamespace: /pg_cluster/\nname: pg_node1\nrestapi:\n  listen: &lt;pg_node1 IP Address&gt;:8008\n  connect_address: &lt;pg_node1 IP Address&gt;:8008\netcd:\n  hosts: &lt;etcd1 IP Address&gt;:2379, &lt;etcd2 IP Address&gt;:2379, &lt;etcd3 IP Address&gt;:2379\nbootstrap:\n  dcs:\n    ttl: 30\n    loop_wait: 10\n    retry_timeout: 10\n    maximum_lag_on_failover: 1048576\n    maximum_lag_on_syncnode: 15000000\n    synchronous_mode: false\n    postgresql:\n      use_pg_rewind: true\n      use_slots: true\n      parameters:\n        shared_buffers: 16GB\n        work_mem: 16MB\n        maintenance_work_mem: 2GB\n        max_worker_processes: 16\n        wal_buffers: 64MB\n        max_wal_size: 2GB\n        min_wal_size: 1GB\n        effective_cache_size: 64GB\n        fsync: on\n        checkpoint_completion_target: 0.9\n        log_rotation_size: 100MB\n        listen_addresses: \"*\"\n        max_connections: 2000\n        temp_buffers: 4MB\n        ssl: true\n        ssl_cert_file: /usr/patroni/conf/server.crt\n        ssl_key_file: /usr/patroni/conf/server.key\n  initdb:\n    - encoding: UTF8\n    - data-checksums\n  pg_hba:\n    - host replication replicator 127.0.0.1/32 md5\n    - host replication replicator &lt;pg_node1 IP Address&gt;/32 md5\n    - host replication replicator &lt;pg_node2 IP Address&gt;/32 md5\n    - host all all 0.0.0.0/0 md5\nusers:\n  admin:\n    password: admin\n    options:\n      - createrole\n      - createdb\n\npostgresql:\n  listen: &lt;pg_node1 IP Address&gt;:5432\n  connect_address: &lt;pg_node1 IP Address&gt;:5432\n  data_dir: /var/lib/pgsql/12/data\n  bin_dir: /usr/pgsql-12/bin\n  pgpass: /tmp/pgpass\n  authentication:\n    replication:\n      username: replicator\n      password: replicator\n    superuser:\n      username: postgres\n      password: postgres\n    rewind:\n      username: pgrewind\n      password: pgrewind\ntags:\n  nofailover: false\n  noloadbalance: false\n  clonefrom: false\n  nosync: true</code></pre>\n<strong>On pg_nodde2, you can add the following:</strong>\n<pre><code>scope: postgres\nnamespace: /pg_cluster/\nname: pg_node2\nrestapi:\n  listen: &lt;pg_node2 IP Address&gt;:8008\n  connect_address: &lt;pg_node2 IP Address&gt;:8008\netcd:\n  hosts: &lt;etcd1 IP Address&gt;:2379, &lt;etcd2 IP Address&gt;:2379, &lt;etcd3 IP Address&gt;:2379\nbootstrap:\n  dcs:\n    ttl: 30\n    loop_wait: 10\n    retry_timeout: 10\n    maximum_lag_on_failover: 1048576\n    maximum_lag_on_syncnode: 15000000\n    synchronous_mode: false\n    postgresql:\n      use_pg_rewind: true\n      use_slots: true\n      parameters:\n        shared_buffers: 16GB\n        work_mem: 16MB\n        maintenance_work_mem: 2GB\n        max_worker_processes: 16\n        wal_buffers: 64MB\n        max_wal_size: 2GB\n        min_wal_size: 1GB\n        effective_cache_size: 64GB\n        fsync: on\n        checkpoint_completion_target: 0.9\n        log_rotation_size: 100MB\n        listen_addresses: \"*\"\n        max_connections: 2000\n        temp_buffers: 4MB\n        ssl: true\n        ssl_cert_file: /usr/patroni/conf/server.crt\n        ssl_key_file: /usr/patroni/conf/server.key\n  initdb:\n    - encoding: UTF8\n    - data-checksums\n  pg_hba:\n    - host replication replicator 127.0.0.1/32 md5\n    - host replication replicator &lt;pg_node1 IP Address&gt;/32 md5\n    - host replication replicator &lt;pg_node2 IP Address&gt;/32 md5\n    - host all all 0.0.0.0/0 md5\nusers:\n  admin:\n    password: admin\n    options:\n      - createrole\n      - createdb\n\npostgresql:\n  listen: &lt;pg_node2 IP Address&gt;:5432\n  connect_address: &lt;pg_node2 IP Address&gt;:5432\n  data_dir: /var/lib/pgsql/12/data\n  bin_dir: /usr/pgsql-12/bin\n  pgpass: /tmp/pgpass\n  authentication:\n    replication:\n      username: replicator\n      password: replicator\n    superuser:\n      username: postgres\n      password: postgres\n    rewind:\n      username: pgrewind\n      password: pgrewind\ntags:\n  nofailover: false\n  noloadbalance: false\n  clonefrom: false\n  nosync: true</code></pre>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"></div>\n<div class=\"admonition-content admonition-note-content\"> Configure parameter values such as max_connections and listen_addresses according to the postgresql.conf configuration section in <a href=\"/doc/423/26.1/prepareexternaldbs\" title=\"Prepare external databases\">Prepare external databases</a>, and configure database connection settings according to the pg_hba.conf configuration section on the same page.</div>\n</div>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-tip-icon\"></div>\n<div class=\"admonition-content admonition-tip-content\"> You can run the following commands to change the configuration after Patroni has started:\n\t<pre><code>export LC_ALL=C.UTF-8\nexport LANG=C.UTF-8\npatronictl -c /usr/patroni/conf/postgresql.yml edit-config postgres </code></pre>\n</div>\n</div>\n</li>\n</ol>\n<h3>Create a Patroni service file</h3>\n<p>Run the following command to create a service file with the content below:</p>\n<pre><code>vi /usr/lib/systemd/system/patroni.service</code></pre>\n<pre><code>[Unit]\nDescription=patroni\nDocumentation=https://patroni.readthedocs.io/en/latest/index.html\nAfter=syslog.target network.target etcd.target\nWants=network-online.target\n\n[Service]\nType=simple\nUser=postgres\nGroup=postgres\nPermissionsStartOnly=true\nExecStart=/usr/local/bin/patroni /usr/patroni/conf/postgresql.yml\nExecReload=/bin/kill -HUP $MAINPID\nLimitNOFILE=65536\nKillMode=process\nKillSignal=SIGINT\nRestart=on-abnormal\nRestartSec=30s\nTimeoutSec=0\n\n[Install]\nWantedBy=multi-user.target\n      </code></pre>\n<h3>Start Patroni</h3>\n<p>Run the following commands to start Patroni:</p>\n<pre><code>systemctl daemon-reload\nsystemctl start patroni\nsystemctl enable patroni\nsystemctl status patroni</code></pre>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"></div>\n<div class=\"admonition-content admonition-note-content\">If you encounter a Patroni startup issue, run the \"<strong>journalctl -u patroni</strong>\"command to check the log. </div>\n</div>\n<h2 id=\"PGHAwithPatroni-InstallHAProxy\">Install HAProxy on two hosts</h2>\n<ol>\n<li>Run the following command to install HAProxy：\n\t<pre><code>yum -y install haproxy</code></pre>\n</li>\n<li>Back up the existing HAProxy configuration file of the HAProxy instance:\n\t<pre><code>cp -p /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.bkp</code></pre>\n</li>\n<li>Run the following command to update your HAProxy configuration file:\n\t<pre><code>vim /etc/haproxy/haproxy.cfg</code></pre>\n</li>\n<li>\n<p>Remove the content in the file and copy the following to the file. Then, update the values highlighted in <span style=\"color:#3498db;\">blue </span>according to your network setup:</p>\n<pre><code>global\n    log 127.0.0.1 local2\n    chroot /var/lib/haproxy\n    pidfile /var/run/haproxy.pid\n    maxconn <span style=\"color:#3498db;\">6000</span>\n    user haproxy\n    group haproxy\n    daemon\n    stats socket /var/lib/haproxy/stats\ndefaults\n    mode tcp\n    log global\n    retries 3\n    timeout queue 1m\n    timeout connect 10s\n    timeout client <span style=\"color:#3498db;\">31m</span>\n    timeout server <span style=\"color:#3498db;\">31m</span>\n    timeout check 10s\n    maxconn <span style=\"color:#3498db;\">3000</span>\nlisten stats\n    mode http\n    bind *:<span style=\"color:#3498db;\">7000</span>\n    stats enable\n    stats uri /\nlisten postgres\n    bind *:<span style=\"color:#3498db;\">5000</span>\n    option httpchk\n    http-check expect status 200\n    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions\n    server pg_node1 <span style=\"color:#3498db;\">&lt;pg_node1 IP address&gt;</span>:5432 maxconn 2000 check port 8008\n    server pg_node2 <span style=\"color:#3498db;\">&lt;pg_node2 IP address&gt;</span>:5432 maxconn 2000 check port 8008\nlisten postgres-readonly\n    bind *:<span style=\"color:#3498db;\">6000</span>\n    option httpchk GET /replica\n    http-check expect status 200\n    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions\n    server pg_node1 <span style=\"color:#3498db;\">&lt;pg_node1 IP address&gt;</span>:5432 maxconn 2000 check port 8008\n    server pg_node2 <span style=\"color:#3498db;\">&lt;pg_node2 IP address&gt;</span>:5432 maxconn 2000 check port 8008</code></pre>\n</li>\n<li>Run the following command to reload the system daemon:\n\t<pre><code>systemctl daemon-reload</code></pre>\n</li>\n<li>Make sure the SELinux boolean \"haproxy_connect_any\" is on. Run the following command:\n\t<pre><code>getsebool -a|grep haproxy_connect</code></pre>\n\tIf the result is like:\n\n\t<pre><code>haproxy_connect_any --&gt; off</code></pre>\n<br/>\n\tThen, run the following command:\n\t<pre><code>setsebool -P haproxy_connect_any=1</code></pre>\n</li>\n<li>Start the HAProxy service:\n\t<pre><code>systemctl start haproxy\nsystemctl status haproxy</code></pre>\n</li>\n<li>To enable HAProxy to run at startup:\n\t<pre><code>systemctl enable haproxy</code></pre>\n</li>\n<li>Make sure you can connect to PostgreSQL Server through HAProxy by running the following commands on a host where \"psql\" is available.\n\t<pre><code>psql -h &lt;haproxy1 ip address&gt; -p 5000 -d postgres -U postgres</code>\n<code>psql -h &lt;haproxy2 ip address&gt; -p 5000 -d postgres -U postgres</code></pre>\n<p>Use SSL connection to the database:</p>\n<pre><code>psql \"host=&lt;haproxy1 ip address&gt; port=5000 dbname=postgres user=postgres password=&lt;password&gt; sslmode=verify-ca sslrootcert=&lt;root ca certificate&gt;\"</code>\n<code>psql \"host=&lt;haproxy2 ip address&gt; port=5000 dbname=postgres user=postgres password=&lt;password&gt; sslmode=verify-ca sslrootcert=&lt;root ca certificate&gt;\"</code></pre>\n</li>\n<li>Check if the connections to the different ports are in readonly or write-read mode.\n\t<pre><code>psql -h &lt;haproxy1 ip address&gt; -U postgres -p <span style=\"color:#3498db;\">6000</span> -c \"SELECT pg_is_in_recovery();\"</code></pre>\n\tOutput for the readonly port:\n\n\t<pre>pg_is_in_recovery \n-------------------\nt\n(1 row)</pre>\n<pre><code>psql -h &lt;haproxy1 ip address&gt; -U postgres -p <span style=\"color:#3498db;\">5000</span> -c \"SELECT pg_is_in_recovery();\"</code></pre>\n\tOutput for the write-read port:\n\n\t<pre>pg_is_in_recovery \n-------------------\nf\n(1 row)</pre>\n</li>\n</ol>\n<h2 id=\"PGHAwithPatroni-TestPostgresCluster\">Test the PostgreSQL cluster </h2>\n<ol>\n<li>You can test and verify your PostgreSQL cluster by initiating a connection request to HAProxy:\n\t<pre><code>#psql -h &lt;haproxy host address&gt; -p 5000 -d postgres -U postgres</code></pre>\n</li>\n<li>Open a web browser and access http://&lt;Haproxy host ip&gt;:7000, you will see an HAProxy dashboard like below:<br/>\n<a class=\"image\" href=\"/file/images/3/3b/haproxy.PNG\" title=\"/file/images/3/3b/haproxy.PNG\"> <img alt=\"/mediawiki/images/3/3b/haproxy.PNG\" border=\"0\" file=\"/mediawiki/images/3/3b/haproxy.PNG\" height=\"280\" hspace=\"0\" src=\"../../../images/haproxy_39ab2eb2.png\" style=\"width: 800px; height: 280px; margin: 0px; border: 0px solid black;\" vspace=\"0\" width=\"800\"/> </a></li>\n<li>To list the nodes from the command line on DB hosts, run a database failover test:\n\t<pre><code>patronictl -c /opt/app/patroni/etc/postgresql.yml list\npatronictl -c /opt/app/patroni/etc/postgresql.yml failover</code></pre>\n<br/>\n<a class=\"image\" href=\"/file/images/7/7b/failoverofpatroni.PNG\" title=\"/file/images/7/7b/failoverofpatroni.PNG\"> <img alt=\"/mediawiki/images/7/7b/failoverofpatroni.PNG\" border=\"0\" file=\"/mediawiki/images/7/7b/failoverofpatroni.PNG\" height=\"720\" hspace=\"0\" src=\"../../../images/failoverofpatroni_539b0f47.png\" style=\"width:735px;height:720px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:0px solid black;\" vspace=\"0\" width=\"735\"/> </a></li>\n</ol>\n<h2 id=\"PGHAwithPatroni-InstallKeepalived\">Install and configure Keepalived on both HAProxy hosts</h2>\n<p>If you use a hardware load balancer instead of Keepalived, skip this step and perform the \"Configure a hardware load balancer\" step instead. </p>\n<ol>\n<li>Turn off SELinux.\n\t<ol start=\"1\">\n<li>Modify the file \"/etc/selinux/config\", by changing \"SELINUX=enforcing\" to \"SELINUX=disabled\".</li>\n<li>Run the following command:\n\t\t<pre><code>#setenforce 0</code></pre>\n</li>\n</ol>\n</li>\n<li>Get the network interface name. In this section, we will use \"<span style=\"color:#3498db;\">eth0</span>\" as an example. You need to replace it with yours in the next steps.\n\t<pre><code>ifconfig -a|grep \"UP,BROADCAST,RUNNING,MULTICAST\"|awk '{print $1}'</code></pre>\n</li>\n<li>Configure the firewall on both HAProxy hosts.\n\t<ol start=\"1\">\n<li>Back up the file \"/etc/firewalld/direct.xml\" if it exists.</li>\n<li>Add direct rules by running the following commands or by modifying <code>/etc/firewalld/direct.xml</code>:\n\t\t<pre><code>#firewall-cmd --direct --permanent --add-rule ipv4 filter INPUT 0 --in-interface <span style=\"color:#3498db;\">eth0</span> --destination 224.0.0.18 --protocol vrrp -j ACCEPT\n#firewall-cmd --direct --permanent --add-rule ipv4 filter OUTPUT 0 --out-interface <span style=\"color:#3498db;\">eth0</span> --destination 224.0.0.18 --protocol vrrp -j ACCEPT\n#firewall-cmd --reload</code></pre>\n<p><strong>Note:</strong> This step will write the rules to the /etc/firewalld/direct.xml file. If you receive an error at this step, please go to the /etc/firewalld/direct.xml file to modify it and run \"firewall-cmd --reload\" to make it take effect. Here is an example:</p>\n<pre>#cat /etc/firewalld/direct.xml\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;direct&gt;\n  &lt;rule ipv=\"ipv4\" table=\"filter\" chain=\"INPUT\" priority=\"0\"&gt;--in-interface <span style=\"color:#3498db;\">eth0</span> --destination 224.0.0.18 --protocol vrrp -j ACCEPT&lt;/rule&gt;\n  &lt;rule ipv=\"ipv4\" table=\"filter\" chain=\"OUTPUT\" priority=\"0\"&gt;--out-interface <span style=\"color:#3498db;\">eth0</span> --destination 224.0.0.18 --protocol vrrp -j ACCEPT&lt;/rule&gt;\n&lt;/direct&gt;</pre>\n</li>\n<li>\n<p>Run the following command to make sure the rules above are added:</p>\n<pre><code>#firewall-cmd --direct --get-rules ipv4 filter OUTPUT\n#firewall-cmd --direct --get-rules ipv4 filter INPUT</code></pre>\n</li>\n</ol>\n</li>\n<li>Install and configure Keepalived on both hosts.\n\t<ol start=\"1\">\n<li>On the haproxy1 host, run the following commands:\n\t\t<pre><code>#yum install keepalived -y\n#cat /etc/keepalived/keepalived.conf</code></pre>\n\t\tThe keepalived.conf file should look like:\n\n\t\t<pre><code>global_defs {\n  router_id gd_ha01\n  default_interface </code><span style=\"color:#3498db;\">eth0</span><code>\n}\nvrrp_script chk_haproxy {\n  script \"killall -0 haproxy\" \n  interval 2\n  fall 2\n  rise 2\n}\nvrrp_instance VI_1 {\n  interface </code><span style=\"color:#3498db;\">eth0</span><code>               # interface to monitor\n  nopreempt\n  state BACKUP                 # use nopreempt, so BACKUP on both of haproxy1 and haproxy2\n  virtual_router_id 91         # use a unique id shared between haproxy1 and haproxy2\n  priority 101                 # 101 on haproxy1, 100 on haproxy2\n  advert_int 1\t  \n\n  unicast_src_ip <span style=\"color:#9b59b6;\">&lt;haproxy1 ip address&gt;</span>\n  unicast_peer {\n    <span style=\"color:#9b59b6;\">&lt;haproxy2 ip address&gt;</span>\n  }\n  authentication {\n    auth_type PASS\n    auth_pass <span style=\"color:#9b59b6;\">&lt;specify a password&gt; </span># specify the same password for </code>haproxy1 and haproxy2\n<code>  }\n  virtual_ipaddress {\n    <span style=\"color:#9b59b6;\">&lt;virtual ip address&gt;</span>\n  }\n  track_script {\n    chk_haproxy\n  }\n}</code></pre>\n</li>\n<li>On the haproxy2 host, run the following commands:\n\t\t<pre><code>#yum install keepalived -y\n#cat /etc/keepalived/keepalived.conf</code></pre>\n\t\tThe keepalived.conf file should look like:\n\n\t\t<pre><code>global_defs {\n  router_id gd_ha02\n  default_interface </code><span style=\"color:#3498db;\">eth0</span><code>\n}\nvrrp_script chk_haproxy {\n  script \"killall -0 haproxy\" # check the haproxy process\n  interval 2                  # every 2 seconds\n  fall 2\n  rise 2\n}\nvrrp_instance VI_1 {\n  interface </code><span style=\"color:#3498db;\">eth0             </span><code># interface to monitor\n  nopreempt\n  state BACKUP                # use nopreempt, so BACKUP on both of haproxy1 and haproxy2\n  virtual_router_id 91        # use a unique id shared between haproxy1 and haproxy2\n  priority 100                # 101 on haproxy1, 100 on haproxy2\n  advert_int 1\n  authentication {\n    auth_type PASS\n    auth_pass <span style=\"color:#9b59b6;\">&lt;Specify a password&gt;  </span># specify the same password for </code>haproxy1 and haproxy2\n<code><font color=\"#000000\" face=\"Monaco, Consolas, Andale Mono, Ubuntu Mono, monospace\">  </font>}\n  unicast_src_ip <span style=\"color:#9b59b6;\">&lt;haproxy2 ip address&gt;</span> \n  unicast_peer {\n    <span style=\"color:#9b59b6;\">&lt;haproxy1 ip address&gt;</span>\n  }\n  virtual_ipaddress {\n    <span style=\"color:#9b59b6;\">&lt;virtual ip address&gt;</span>\n  }\n  track_script {\n    chk_haproxy\n  }\n}</code></pre>\n</li>\n</ol>\n</li>\n<li>On both hosts, configure IP forwarding and non-local binding.\n\t<ol start=\"1\">\n<li>Append the following lines to the end of the /etc/sysctl.conf file.\n\t\t<pre><code>net.ipv4.ip_forward = 1\nnet.ipv4.ip_nonlocal_bind = 1 </code></pre>\n</li>\n<li>Load the configuration:\n\t\t<pre><code>#sysctl -p</code></pre>\n</li>\n</ol>\n</li>\n<li>On both hosts, start the Keepalived service:\n\t<pre><code>#systemctl enable keepalived\n#systemctl start keepalived</code></pre>\n</li>\n<li>Check the virtual IP by running the following commands on both HAProxy hosts.\n\t<pre><code>#ip addr show dev eth0</code></pre>\n<p>The virtual IP address only appears on one of the two HAProxy hosts in the command output. </p>\n</li>\n<li>Change the bind address to the virtual IP for the HAProxy server.\n\t<ol start=\"1\">\n<li>Change the haproxy.cfg file.\n\t\t<pre><code>#vi /etc/haproxy/haproxy.cfg</code></pre>\n<p>Change the \"*\" to the &lt;virtual ip address&gt; in the following lines and save it:</p>\n<pre><code>...\nbind *:7000\n...\nbind *:5000\n...\nbind *:6000\n...</code></pre>\n</li>\n<li>Restart the HAProxy service:\n\t\t<pre><code>#systemctl stop haproxy\n#systemctl start haproxy </code></pre>\n</li>\n</ol>\n</li>\n<li>Make sure you can connect to the PostgreSQL service through the virtual IP by running the following commands on a host where \"psql\" is available:\n\t<pre><code>#psql -h &lt;virtual ip address&gt; -p 5000 -d postgres -U postgres</code></pre>\n<p>Use SSL to connect to PostgreSQL:</p>\n<pre><code>psql \"host=&lt;virtual ip address&gt; port=5000 dbname=postgres user=postgres password=&lt;password&gt; sslmode=verify-full sslrootcert=&lt;root ca certificate&gt;\"</code></pre>\n</li>\n</ol>\n<h2 id=\"PGHAwithPatroni-TestKeepalived\">Test the Keepalived service for HAProxy</h2>\n<p>In the following example,  haproxy1 has an IP address of 16.155.194.54, haproxy2 has an IP address of 16.155.198.152, and the virtual IP is 16.155.194.119.</p>\n<ol>\n<li>Verify the virtual IP as follows:\n\t<ol start=\"1\">\n<li>On haproxy1, run the following command:<br/>\n\t\t \n\t\t<pre><code>#ip addr show dev ens160</code></pre>\n\t\t.\n\n\t\t<p><a class=\"image\" href=\"/file/images/1/13/haproxy1.PNG\" title=\"/file/images/1/13/haproxy1.PNG\"> <img alt=\"/mediawiki/images/1/13/haproxy1.PNG\" border=\"0\" file=\"/mediawiki/images/1/13/haproxy1.PNG\" height=\"141\" hspace=\"0\" src=\"../../../images/haproxy1_3962891e.png\" style=\"width: 800px; height: 141px; margin: 0px; border: 0px solid black;\" vspace=\"0\" width=\"800\"/> </a></p>\n</li>\n<li>On haproxy2, run the following command:\n\t\t<pre><code>#ip addr show dev ens160</code></pre>\n<p><a class=\"image\" href=\"/file/images/6/61/haproxy2.PNG\" title=\"/file/images/6/61/haproxy2.PNG\"> <img alt=\"/mediawiki/images/6/61/haproxy2.PNG\" border=\"0\" file=\"/mediawiki/images/6/61/haproxy2.PNG\" height=\"103\" hspace=\"0\" src=\"../../../images/haproxy2_f9203357.png\" style=\"width: 800px; height: 103px; margin: 0px; border: 0px solid black;\" vspace=\"0\" width=\"800\"/> </a></p>\n</li>\n</ol>\n</li>\n<li>Check the Keepalived service log:\n\t<ol start=\"1\">\n<li>On haproxy1, run the following command:\n\t\t<pre><code>#journalctl -u keepalived -f </code></pre>\n<p><a class=\"image\" href=\"/file/images/5/5a/keepalive1.PNG\" title=\"/file/images/5/5a/keepalive1.PNG\"> <img alt=\"/mediawiki/images/5/5a/keepalive1.PNG\" border=\"0\" file=\"/mediawiki/images/5/5a/keepalive1.PNG\" height=\"292\" hspace=\"0\" src=\"../../../images/keepalive1_80e0b0c2.png\" style=\"width: 800px; height: 292px; margin: 0px; border: 0px solid black;\" vspace=\"0\" width=\"800\"/> </a></p>\n<p>The Keepalived service on this host is \"Entering MASTER STATE.\"<br/>\n\t\tThe gratuitous ARP is sent for the virtual IP (16.155.194.119).</p>\n</li>\n<li>On haproxy2, run the following command:\n\t\t<pre><code>#journalctl -u keepalived -f</code></pre>\n<p><a class=\"image\" href=\"/file/images/8/8e/keepalive2.PNG\" title=\"/file/images/8/8e/keepalive2.PNG\"> <img alt=\"/mediawiki/images/8/8e/keepalive2.PNG\" border=\"0\" file=\"/mediawiki/images/8/8e/keepalive2.PNG\" height=\"160\" hspace=\"0\" src=\"../../../images/keepalive2_68f89bcd.png\" style=\"width: 800px; height: 160px; margin: 0px; border: 0px solid black;\" vspace=\"0\" width=\"800\"/> </a></p>\n<p>The Keepalived service on this host is \"Entering BACKUP STATE.\"<br/>\n\t\tThe VIP is removed.</p>\n</li>\n</ol>\n</li>\n<li>Test Scenario 1: Simulate HAProxy Service problem.<br/>\n<pr><small><cite><strong>Test Purpose to check:</strong></cite></small></pr><br/>\n<pr><small><cite> -&gt;The virtual IP will switch to the BACKUP when the HAProxy on the MASTER is shut down.<br/>\n\t-&gt;The virtual IP will not switch back to haproxy1 when the HAProxy goes alive on it. Nopreempt will work. It will avoid the unnecessary connection switch.</cite></small></pr>\n<ol start=\"1\">\n<li>On the haproxy1, run the following command:\n\t\t<pre><code>#systemctl stop haproxy</code>\n<code>#journalctl -u keepalived -f </code></pre>\n<p><a class=\"image\" href=\"/file/images/a/a6/keepalived1-1.PNG\" title=\"/file/images/a/a6/keepalived1-1.PNG\"> <img alt=\"/mediawiki/images/a/a6/keepalived1-1.PNG\" border=\"0\" file=\"/mediawiki/images/a/a6/keepalived1-1.PNG\" height=\"225\" hspace=\"0\" src=\"../../../images/keepalived1-1_2a1c6215.png\" style=\"width:800px;height:225px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:0px solid black;\" vspace=\"0\" width=\"800\"/> </a></p>\n<p>The Keepalived on this host is \"Entering FAULT STATE.\"<br/>\n\t\tThe Virtual IP is removed.</p>\n</li>\n<li>On the haproxy2, run command:\n\t\t<pre><code>#journalctl -u keepalived -f</code></pre>\n<p><a class=\"image\" href=\"/file/images/a/aa/keepalived2-1.PNG\" title=\"/file/images/a/aa/keepalived2-1.PNG\"> <img alt=\"/mediawiki/images/a/aa/keepalived2-1.PNG\" border=\"0\" file=\"/mediawiki/images/a/aa/keepalived2-1.PNG\" height=\"193\" hspace=\"0\" src=\"../../../images/keepalived2-1_2cebaee6.png\" style=\"width: 800px; height: 193px; margin: 0px; border: 0px solid black;\" vspace=\"0\" width=\"800\"/> </a></p>\n<p>The Keepalived on this host is \"Entering MASTER STATE.\"<br/>\n\t\tThe gratuitous ARP is sent for the virtual IP (16.155.194.119).</p>\n</li>\n<li>On the haproxy1, run the following command:\n\t\t<pre><code>#systemctl start haproxy</code>\n<code>#journalctl -u keepalived -f </code></pre>\n<p><a class=\"image\" href=\"/file/images/a/a0/keepalived1-3.PNG\" title=\"/file/images/a/a0/keepalived1-3.PNG\"> <img alt=\"/mediawiki/images/a/a0/keepalived1-3.PNG\" border=\"0\" file=\"/mediawiki/images/a/a0/keepalived1-3.PNG\" height=\"111\" hspace=\"0\" src=\"../../../images/keepalived1-3_ec2c1d15.png\" style=\"width:762px;height:111px;margin-top:0px;margin-bottom:0px;margin-left:0px;margin-right:0px;border:0px solid black;\" vspace=\"0\" width=\"762\"/> </a></p>\n<p>The Keepalived on this host is \"Entering BACKUP STATE.\"<br/>\n\t\tThe gratuitous ARP is sent for the haproxy1 IP address (16.155.194.54) but not for the virtual IP (16.155.194.119).</p>\n<pre><code>#ip addr show dev ens160</code></pre>\n<p><a class=\"image\" href=\"/file/images/4/4e/ip-1.PNG\" title=\"/file/images/4/4e/ip-1.PNG\"> <img alt=\"/mediawiki/images/4/4e/ip-1.PNG\" border=\"0\" file=\"/mediawiki/images/4/4e/ip-1.PNG\" height=\"105\" hspace=\"0\" src=\"../../../images/ip-1_456c0681.png\" style=\"width: 800px; height: 105px; margin: 0px; border: 0px solid black;\" vspace=\"0\" width=\"800\"/> </a></p>\n</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"PGHAwithPatroni-ConfigF5\">Configure a hardware load balancer</h2>\n<p>Alternatively, you can deploy a hardware load balancer to replace HAProxy and Keepalived. The following screenshots are a typical example that demonstrates how to connect to the PostgreSQL cluster.</p>\n<p>Before you start, assign a virtual address to the physical address of the F5 network interface. The newly added virtual address will be used for the virtual server later on.</p>\n<ol>\n</ol>\n<ol>\n<li>Configure Secure Network Address Translation (SNAT) and create a new health monitor:<br/>\n<a class=\"image\" href=\"/file/images/c/ca/SMA_health_check_SNAT.png\" title=\"/file/images/c/ca/SMA_health_check_SNAT.png\"> <img alt=\"/mediawiki/images/c/ca/SMA_health_check_SNAT.png\" border=\"0\" file=\"/mediawiki/images/c/ca/SMA_health_check_SNAT.png\" height=\"403\" hspace=\"0\" src=\"../../../images/SMA_health_check_SNAT_1024efb8.png\" vspace=\"0\" width=\"700\"/> </a></li>\n<li>Create the node list based on the health monitor:<br/>\n<a class=\"image\" href=\"/file/images/3/36/ServiceManager_Create_node.png\" title=\"/file/images/3/36/ServiceManager_Create_node.png\"> <img alt=\"/mediawiki/images/3/36/ServiceManager_Create_node.png\" border=\"0\" file=\"/mediawiki/images/3/36/ServiceManager_Create_node.png\" height=\"397\" hspace=\"0\" src=\"../../../images/ServiceManager_Create_node_ca271b18.png\" vspace=\"0\" width=\"700\"/> </a></li>\n<li>Configure a pool member.</li>\n<li>Create a TCP profile from Local Traffic Virtual Servers Profiles Protocol TCP.\n\t<ul>\n<li>Set Idle Timeout to Indefinite.</li>\n<li>Set Keepalive Interval to Indefinite.</li>\n</ul>\n</li>\n<li>Create a new virtual server based on the pool member, the SNAT, and the TCP profile (Protocol Profile (Client) and Protocol Profile (Server)) created above.<br/>\n<a class=\"image\" href=\"/file/images/f/f1/Create_new_virtual_server1.png\" title=\"/file/images/f/f1/Create_new_virtual_server1.png\"> <img alt=\"/mediawiki/images/f/f1/Create_new_virtual_server1.png\" border=\"0\" file=\"/mediawiki/images/f/f1/Create_new_virtual_server1.png\" height=\"401\" hspace=\"0\" src=\"../../../images/Create_new_virtual_server1_15637e12.png\" vspace=\"0\" width=\"800\"/> </a><br/>\n<a class=\"image\" href=\"/file/images/8/8b/Create_new_virtual_server2.png\" title=\"/file/images/8/8b/Create_new_virtual_server2.png\"> <img alt=\"/mediawiki/images/8/8b/Create_new_virtual_server2.png\" border=\"0\" file=\"/mediawiki/images/8/8b/Create_new_virtual_server2.png\" height=\"459\" hspace=\"0\" src=\"../../../images/Create_new_virtual_server2_53201f95.png\" vspace=\"0\" width=\"800\"/> </a><br/>\n<a class=\"image\" href=\"/file/images/8/85/Create_new_virtual_server3.png\" title=\"/file/images/8/85/Create_new_virtual_server3.png\"> <img alt=\"/mediawiki/images/8/85/Create_new_virtual_server3.png\" border=\"0\" file=\"/mediawiki/images/8/85/Create_new_virtual_server3.png\" height=\"491\" hspace=\"0\" src=\"../../../images/Create_new_virtual_server3_3ab377c0.png\" vspace=\"0\" width=\"800\"/> </a><br/>\n<a class=\"image\" href=\"/file/images/8/8f/Create_new_virtual_server4.png\" title=\"/file/images/8/8f/Create_new_virtual_server4.png\"> <img alt=\"/mediawiki/images/8/8f/Create_new_virtual_server4.png\" border=\"0\" file=\"/mediawiki/images/8/8f/Create_new_virtual_server4.png\" height=\"459\" hspace=\"0\" src=\"../../../images/Create_new_virtual_server4_c4284243.png\" vspace=\"0\" width=\"800\"/> </a></li>\n</ol>\n<h2>Encrypt passwords used for Patroni and Keepalived</h2>\n<p>For security considerations, you need to encrypt the passwords in a Patroni configuration file and a Keepalived configuration file. Follow the instructions below to encrypt the passwords.</p>\n<h3>Download two scripts from the Marketplace</h3>\n<ol>\n<li>Download the <a href=\"https://marketplace.opentext.com/itom/content/service-management-automation-operation-toolkit\" title=\"SMA Operation Toolkit\">SMA Operation Toolkit</a> from the Marketplace.</li>\n<li>Extract the package and go to the <strong>db_patroni </strong>folder.</li>\n<li>Copy the <code>patroni_encrypt.sh</code> script to the two hosts on which you will install Patroni.</li>\n<li>(Only if you're using Keepalived) Copy the <code>keepalived_encrypt.sh</code> script to the two hosts on which you will install HAProxy.</li>\n</ol>\n<h3>Encrypt the passwords used for Patroni</h3>\n<ol>\n</ol>\n<p>To encrypt the passwords in Patroni configuration file (/usr/patroni/conf/postgresql.yml), you need to run this script on each Patroni host to start the service. </p>\n<p>The script help information is as follows:</p>\n<pre><code>Usage: patroni_encrypt.sh start         To start the Patroni service.\n       patroni_encrypt.sh stop          To stop the Patroni service.\n       patroni_encrypt.sh status        To check the Patroni service status.</code></pre>\n<p>On each Patroni host, perform the following steps.</p>\n<ol>\n<li>Run the <code>patroni_encrypt.sh start</code> command. When prompted, enter an encryption key to encrypt the passwords in the <code>postgresql.yml</code> file. </li>\n<li>Save this encryption key in a secure place. You will need it the next time you restart the service. </li>\n<li>Run the <code>patroni_encrypt.sh status</code> command to check that the service is started.</li>\n<li>Check that the line <code>password: &lt;plain text </code><code>password&gt;</code> has changed to <code>encrypass: &lt;encrypted code&gt;</code> in the file. The passwords have been encrypted successfully.</li>\n</ol>\n<p>Once the passwords of all users in the<code>postgresql.yml</code> are encrypted:</p>\n<ul>\n<li>If you want to change the password of only one user, change the \"<code>encrypass: &lt;encrypted code&gt;</code>\" under the <code>username: xxxxxx</code> to <code>password: &lt;new password in plain text&gt;</code> and then run the  <code>patroni_encrypt.sh start</code> command again to encrypt the new password. When prompted, you need to enter the original encryption key.</li>\n<li>If you want to update the passwords of all users in the postgresql.yml in one go, change the \"<code>encrypass: &lt;encrypted code&gt;</code>\" under the <code>username: xxxxxx</code> to <code>password: &lt;new password in plain text&gt;</code> for each user, and then run the  <code>patroni_encrypt.sh start</code> command again to encrypt the new passwords. When prompted, you can enter either the original encryption key or a new key. You can also use this method to change the encryption key only. </li>\n</ul>\n<h3>Encrypt the password used for Keepalived</h3>\n<p>This is required only if you're using Keepalived. </p>\n<p>Use the <code>keepalived_encrypt.sh</code> script to encrypt the password in the Keepalived configuration file (/etc/keepalived/keepalived.conf). </p>\n<p>The script help information is as follows:</p>\n<pre><code>Usage: keepalived_encrypt.sh start         To start the keepalived service.\n       keepalived_encrypt.sh stop          To stop the keepalived service.\n       keepalived_encrypt.sh status        To check the keepalived service status.</code></pre>\n<p>Follow these steps:</p>\n<ol>\n<li>Run the <code>keepalived_encrypt.sh start </code>command on each HAProxy host. When prompted, enter an encryption key to encrypt the password in the \"/etc/keepalived/keepalived.conf\" file. </li>\n<li>Save this encryption key in a secure place. You will need it the next time you restart the service. </li>\n<li>Run the <code>keepalived_encrypt.sh status</code> command to check that the service is started.</li>\n<li>Check that the line <code>auth_pass &lt;plain text password&gt; </code> under <code>auth_type PASS</code> in the file has changed to <code>encrypass &lt;encrytped code&gt;</code>. The password has been encrypted successfully. </li>\n</ol>\n<p>Once the password has been encrypted, if you want to change the password, you need to change the <code>encrypass &lt;password encrypted&gt;</code> under the <code>auth_type PASS</code> to <code>auth_pass &lt;new password in plain text&gt;</code> in the configuration file, and run <code>keepalived_encrypt.sh start</code>. The plain text password will be encrypted again. When prompted, enter the original encryption key or a new key. You can also use this method to change the encryption key only.</p>\n<p class=\"mw-empty-elt\"></p></div>",
  "modifiedon": "2025-10-24 08:51:12"
}