{
  "title": "Scenario 4: Kubernetes cluster has crashed",
  "content": "<div class=\"mw-parser-output\">\n \n\n<p>In this scenario, the current Kubernetes cluster has crashed, for example, the API server isn't available, or the worker node isn't ready. You need to rebuild a new cluster for the suite. Choose the restore solution according to your deployment platforms.</p>\n<h2 id=\"Prerequisite\">Prerequisites</h2>\n<p>Before you start, make sure:</p>\n<ul>\n<li>You have backed up the Kubernetes configurations.</li>\n<li>There's no data corruption on storage and in databases.</li>\n</ul>\n<h2>Restore EKS cluster on AWS</h2>\n<p>For AWS deployment, follow these steps to restore the EKS cluster.</p>\n<p><u><strong>Delete the older EKS worker nodes and cluster</strong></u></p>\n<ol>\n<li>Delete the old EKS worker nodes by navigating to the AWS Management Console and selecting <strong>CloudFormation</strong> &gt; the stack used to create your worker nodes &gt; <strong>Delete</strong>. </li>\n<li>Delete the old EKS cluster by navigating to the AWS Management Console and selecting <strong>CloudFormation</strong> &gt; the stack used to create your EKS cluster &gt; <strong>Delete</strong>. </li>\n</ol>\n<p><u><strong>Create a new EKS cluster</strong></u></p>\n<p>Create a new EKS cluster by following the steps at <a href=\"/doc/423/26.1/ekscluster\" title=\"Build EKS cluster\">Build EKS cluster</a>.</p>\n<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width: 500px;\">\n<thead>\n<tr>\n<th scope=\"col\">Field</th>\n<th scope=\"col\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><b>EKS Cluster Version</b></td>\n<td>Select the same version as the old EKS cluster.</td>\n</tr>\n<tr>\n<td><b>EKS Cluster Security Group</b></td>\n<td>Select the same security group as the old EKS cluster.</td>\n</tr>\n<tr>\n<td><strong>EKS Cluster Name</strong></td>\n<td>Use the same name as the old EKS cluster.</td>\n</tr>\n</tbody>\n</table>\n<p><u><strong>Configure and check the connectivity from kubectl to the new EKS cluster </strong></u></p>\n<p>In the following commands, <code>&lt;cluster_region&gt;</code> is the region you selected for the EKS you created, such as \"us-east-2\" and <code>&lt;eks_cluster_name&gt;</code> is the new cluster name you created in the previous step.</p>\n<ol>\n<li>Run the following command on the bastion to configure and check the connectivity from kubectl to the new EKS cluster:\n\t<pre>aws eks update-cluster-config --name &lt;eks_cluster_name&gt; --resources-vpc-config endpointPublicAccess=false,endpointPrivateAccess=true --region &lt;cluster_region&gt;\n</pre>\n</li>\n<li>Wait several minutes until the cluster status is <strong>Active</strong>, and then run the following command to check the status:\n\t<pre>aws eks describe-cluster --region=&lt;cluster_region&gt; --name &lt;eks_cluster_name&gt;\n</pre>\n</li>\n<li>Fetch the kubeconfig:\n\t<pre>aws eks --region &lt;cluster_region&gt; update-kubeconfig --name &lt;eks_cluster_name&gt;\n</pre>\n</li>\n<li>Check the bastion connectivity to the EKS cluster:\n\t<pre>kubectl get svc</pre>\n</li>\n</ol>\n<p><u><strong>Add labels to worker nodes</strong></u></p>\n<ol>\n<li>Run the following command on the bastion to test the status of the worker nodes:\n\t<pre>kubectl get nodes</pre>\n</li>\n<li>Add labels <code>Worker=label</code>, <code>role=loadbalancer</code>, <code>node.type=worker</code> to all your EKS nodes by running this command on the bastion:\n\t<pre>kubectl label node $(kubectl get nodes | awk 'NR&gt;1 {print $1}') Worker=label role=loadbalancer node.type=worker</pre>\n</li>\n</ol>\n<p><u><strong>Create core namespace</strong></u></p>\n<p>Run this command on the control plane node to create the <strong>core</strong> namespace. This will install Velero in the <strong>core</strong> namespace.</p>\n<pre>kubectl create ns core</pre>\n<p><u><strong>Reconfigure your Velero </strong></u></p>\n<p>For details, see <a href=\"/doc/423/26.1/installveleromanagedk8s#Set_up_Velero_on_AWS\" title=\"here\">here</a>.</p>\n<p><u><strong>Restore the Kubernetes configurations</strong></u></p>\n<ol>\n<li>Restore the Kubernetes configurations by running this command on the bastion. Replace <em>&lt;backup.all.example1&gt; </em>with the name of the backup you have to restore.\n\n\t<pre>velero restore create --from-backup &lt;backup.all.example1&gt; --wait</pre>\n</li>\n<li>Run this command on the bastion to verify the restoration:\n\t<pre>kubectl get pod --all-namespaces|grep -v 1/1|grep -v 2/2|grep -v 3/3|grep -v 4/4|grep -v Completed </pre>\n\tThis command returns a list of abnormal pods. If RabbitMQ isn't ready, see <a href=\"/doc/423/26.1/rabbitmqnotstart\" title=\"RabbitMQ isn't ready\">RabbitMQ isn't ready</a>. When it returns no result, all the suite pods are ready and you can perform quick tests to make sure the restoration is successful.</li>\n</ol>\n<p><u><strong>Make additional changes [AWS Load Balancer Controller Only]</strong></u></p>\n<p>If you are using AWS Load Balancer Controller, make the following additional changes.</p>\n<p><strong>Ensure you have IAM OIDC provider for your cluster</strong></p>\n<p>See <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html\" title=\"Create an IAM OIDC provider for your cluster\">Create an IAM OIDC provider for your cluster</a>.</p>\n<p><strong>Update your current OIDC in IAM role's trust relationship</strong></p>\n<ol>\n<li>Verify the service account name defined in your deployment:\n\t<pre>kubectl describe deploy aws-load-balancer-controller -n kube-system | grep -i \"Service Account\" </pre>\n</li>\n<li>Describe the service account:\n\t<pre>kubectl describe sa &lt;aws-load-balancer-controller&gt;  -n kube-system </pre>\n</li>\n<li>Verify the service account annotation for the IAM role:\n\t<pre>Annotations: eks.amazonaws.com/role-arn: arn:aws:iam::xxxxxxxxxx:role/&lt;AmazonEKSLoadBalancerControllerRole&gt; </pre>\n</li>\n<li>Update the OIDC in the role's trust relationship. For details, see <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/eks-load-balancer-webidentityerr/\" title=\"Example 1: IAM role or trust relationship isn't properly defined for the &quot;sts:AssumeRoleWithWebIdentity&quot; action\">Example 1: IAM role or trust relationship isn't properly defined for the \"sts:AssumeRoleWithWebIdentity\" action</a>.</li>\n<li>Restart the AWS load balancer controller deployment:\n\t<pre>kubectl rollout restart deployment  aws-load-balancer-controller  -n kube-system </pre>\n</li>\n<li>Check your ingress status with the following command.\n\t<pre>kubectl get ingress -n core |grep itom-mng-portal\n\n//Correct output should be like this:\n//k8s-xxxxxxvalb-8d81x60-159xxx727.us-xxxxt-2.elb.amazonaws.com\n</pre>\n</li>\n</ol>\n<h2>Restore AKS cluster on Azure</h2>\n<p>For Azure deployment, follow these steps to restore the AKS cluster:</p>\n<ol>\n<li>Delete the old AKS cluster by navigating to the Azure portal &gt; <strong>Kubernetes services</strong>&gt; your cluster, and then deleting it directly. </li>\n<li>Create a new AKS cluster by following the steps at <a href=\"/doc/423/26.1/akscluster#CreateanAKScluster\" title=\"Build AKS cluster\">Build AKS cluster</a>.\n\t<ul>\n<li>For <b>resource_group_name</b>, select the same resource group as the old AKS cluster.</li>\n<li>For <b>kubernetes_version</b>, select the same version as the old AKS cluster.</li>\n</ul>\n</li>\n<li>Log in to the Azure CLI by running the command below on the bastion and following the instructions in the command output:\n\t<pre>az login</pre>\n</li>\n<li>Test the connectivity from kubectl to AKS cluster by running these commands on the bastion:\n\t<pre># Get kubernetes cluster credentials for &lt;your username&gt;\n\naz aks get-credentials --resource-group &lt;resource_group_name&gt; --name &lt;kubernetes_cluster_name&gt; --subscription &lt;subscription_name_or_id&gt;  \n\n# If the above command is run successfully, a file named /home/&lt;your username&gt;/.kube/config will be created with cluster credentials \n\nkubectl get nodes \n\n# Your kubernetes nodes will be listed if everything works fine\n</pre>\n<p>Where:</p>\n<ul>\n<li><em>&lt;resource_group_name&gt;</em>: Enter the name of the resource group you created. Do <b>NOT</b> use your cluster's resource group name with the format <code>MC_&lt;resource_group&gt;_&lt;aks_cluster_name&gt;_&lt;location&gt;</code>.</li>\n<li><em>&lt;kubernetes_cluster_name&gt;</em>: Enter the name of the created Kubernetes cluster.</li>\n<li><em>&lt;subscription_name_or_id&gt;</em>: Enter your subscription name or ID. You can get it by navigating to the Azure portal <b>Home</b> page &gt; <b>Subscriptions</b>.</li>\n</ul>\n</li>\n<li>Your work nodes should have labels as such <code>Worker=label, role=loadbalancer, node.type=worker.</code> If any node is missing its appropriate label, run this command on the bastion to update labels for all work nodes:\n\t<pre>kubectl label node $(kubectl get nodes | awk 'NR&gt;1 {print $1}') Worker=label role=loadbalancer node.type=worker</pre>\n</li>\n<li>Run this command on the control plane node to create the <strong>core</strong> namespace. This will install Velero in the <strong>core</strong> namespace.\n\t<pre>kubectl create ns core</pre>\n</li>\n<li>Reconfigure Velero. For details, see the instructions for Azure in  <a href=\"/doc/423/26.1/installveleromanagedk8s#Set_up_Velero_on_Azure\" title=\"here\">Set up Velero for Managed Kubernetes</a>. Skip the steps to create a new container and use the same configuration as before.</li>\n<li>Restore the Kubernetes configurations by running this command on the bastion:\n\t<pre>velero restore create --from-backup  &lt;backup.all.example1&gt; --wait</pre>\n</li>\n<li>Update credentials in the <code>itom-vault</code> container with these steps:<br/>\n\tInstall <code>jq</code> on the bastion node:\n\t<pre>sudo yum -y install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\nsudo yum install jq -y</pre>\n<p>Retrieve the vault root token on the bastion node:</p>\n<pre>PASSPHRASE=$(kubectl get secret vault-passphrase -n core -o json 2&gt;/dev/null | jq -r '.data.passphrase')\n\nVAULT_CREDENTIAL_SECRET=$(kubectl get secret vault-credential -n core -o json 2&gt;/dev/null )\n \nENCRYPTED_ROOT_TOKEN=$(echo ${VAULT_CREDENTIAL_SECRET} | jq -r '.data.\"root.token\"')\n\nVAULT_TOKEN=$(echo ${ENCRYPTED_ROOT_TOKEN} | openssl aes-256-cbc -md sha256 -a -d -pass pass:\"${PASSPHRASE}\")\n</pre>\n\tEcho $VAULT_TOKEN and save the output as <strong>&lt;ROOT_TOKEN&gt;</strong>. You will need it later.<br/>\n\tThen go to the <code>itom-vault</code> container and run these commands:\n\n\t<pre>kubectl exec -it $(kubectl get pod -ncore -ocustom-columns=NAME:.metadata.name |grep itom-vault| head -1) -ncore -- bash\n\nexport VAULT_ADDR=https://itom-vault.core:8200\n \nexport VAULT_TOKEN=&lt;ROOT_TOKEN&gt;\n\nvault write -tls-skip-verify auth/kubernetes/config kubernetes_host=\"https://kubernetes.default\" kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt</pre>\n<p>Replace the <strong>&lt;ROOT_TOKEN&gt;</strong> with the actual value you got before.</p>\n</li>\n<li>Run this command on the bastion to verify the restoration:\n\t<pre>kubectl get pod --all-namespaces|grep -v 1/1|grep -v 2/2|grep -v 3/3|grep -v 4/4|grep -v Completed </pre>\n\tThis command returns a list of abnormal pods. If RabbitMQ isn't ready, see <a href=\"/doc/423/26.1/rabbitmqnotstart\" title=\"RabbitMQ isn't ready\">RabbitMQ isn't ready</a>. When it returns no result, all the suite pods are ready and you can perform quick tests to make sure the restoration is successful.</li>\n<li>After the AKS cluster is restored, you need to reconfigure the new <strong>IP address</strong> information in the <strong>Target</strong> of the AGW <b>Backend Pools</b> configurations. See <a href=\"/doc/423/24.3/configurelbazure#Step_2/_Configure_a_backend_pool\" title=\"Configure a load balancer on Azure\">Configure a load balancer on Azure</a> for details. </li>\n</ol>\n<h2>Restore GKE cluster on GCP</h2>\n<div>\n<p>For GCP deployment, complete the following steps to restore the GKE cluster:</p>\n<ol>\n<li>Delete the old GKE cluster. To delete, open Google Cloud Console, go to <strong>Kubernetes Engine</strong> &gt; <strong>Clusters delete your cluster.</strong></li>\n<li>Create a new cluster. Make sure you use the same GKE version. For details, see <a href=\"/doc/423/26.1/gcpcluster\" rel=\"nofollow\" title=\"Build Kubernetes cluster - Service Management\">Build Kubernetes cluster - Service Management</a>. After you create the cluster, login to bastion node and connect to the new cluster.</li>\n<li>Run the following command to create <strong>core </strong>name space:\n\t<pre><code>kubectl create ns core</code></pre>\n</li>\n<li>Reconfigure Velero. For details, see <a href=\"/doc/423/26.1/installveleromanagedk8s\" rel=\"nofollow\" title=\"https://docs.microfocus.com/doc/423/25.4/installveleromanagedk8s\">Set up Velero for Managed Kubernetes</a>.</li>\n<li>Run the following command to restore the instance. Replace <em>&lt;backup.all.example1&gt; </em>with the name of the backup you need to restore.\n\t<pre><code>velero restore create --from-backup &lt;backup.all.example1&gt; --wait\n</code></pre>\n</li>\n<li>Run the following command to verify the restored instance:\n\t<pre><code>kubectl get pod --all-namespaces|grep -v 1/1|grep -v 2/2|grep -v 3/3|grep -v 4/4|grep -v Completed </code></pre>\n</li>\n</ol>\n<p id=\"CloudInfraGCP-UpdatetheDNSpublicip\"><u><strong>Update the DNS public IP</strong></u></p>\n<p>The application load balancer public IP will change after you restore the cluster. To restore it, you must update the public IP of the load balancer in respective DNS mapping. To check the IP address, run the following command:</p>\n<pre data-bidi-marker=\"true\"><code>kubectl get ingress -n &lt;suite namespace&gt; sma-ingress\n\n</code></pre>\n<p> The ouput resembles the following. The public IP address is listed under the <strong>ADDRESS </strong>column.</p>\n<pre><code>NAME          CLASS    HOSTS                 ADDRESS      PORTS     AGE\nsma-ingress   &lt;none&gt;   example.com           34.8.32.28   80, 443   46m\n</code></pre>\n</div>\n<p>After the public IP is reserved successfully, you can map it to the external access hostname allocated for the suite by the DNS provider.</p>\n<h2>Restore K8s cluster on OpenShift</h2>\n<p>If the Kubernetes cluster has crashed, perform the following steps:</p>\n<ol>\n<li>Reinstall an OpenShift cluster. For details, see the OpenShift documentation.</li>\n<li>Run this command on the control plane node to create the <strong>core</strong> namespace. This will install Velero in the <strong>core</strong> namespace.\n\t<pre>kubectl create ns core</pre>\n</li>\n<li>Set up Velero (see <a href=\"/doc/423/26.1/installveleromanagedk8s#Set_up_Velero_on_OpenShift\" title=\"here\">here</a>).</li>\n<li>Restore the Kubernetes configurations by running this command on the bastion:\n\t<pre>velero restore create --from-backup  &lt;backup.all.example1&gt; --wait</pre>\n</li>\n<li>Restore the storage.</li>\n<li>Restore the databases.</li>\n</ol>\n<h2>Restore K8s cluster on embedded Kubernetes</h2>\n<p>There are two methods to restore a Kubernetes cluster:</p>\n<ul>\n<li>Using the original FQDN for the Kubernetes cluster.</li>\n<li>Using a new FQDN for the Kubernetes cluster</li>\n</ul>\n<p>Use the following steps to restore the Kubernetes cluster</p>\n<ol>\n<li>Install OMT. </li>\n<li>Update the ALLOW_WORKLOAD_ON_MASTER value in cdf-cluster-host configmap to <strong>false </strong>to prevent a control plane node from having the label of Worker. Run the below command on the control plane node to update the value.\n\t<pre>kubectl patch cm cdf-cluster-host -n core -p '{\"data\":{\"ALLOW_WORKLOAD_ON_MASTER\":\"false\"}}'</pre>\n</li>\n<li>Add worker nodes. For details, see <a href=\"/doc/406/25.4/addadditionalworker\" title=\"Add additional worker nodes manually\">Add or remove nodes from a cluster by using the CLI</a>. </li>\n<li>Run this command on the control plane node to delete the <strong>core</strong> namespace.\n\t<pre>kubectl delete ns core</pre>\n</li>\n<li>Run this command on the control plane node to create the <strong>core</strong> namespace. This will install Velero in the <strong>core</strong> namespace.\n\t<pre>kubectl create ns core</pre>\n</li>\n<li>Run this command on the control plane node to recreate persistent volumes.\n\t<pre>kubectl delete pv itom-vol\n# Replace &lt;nfs-server&gt; to your original nfs server address.\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: itom-vol\nspec:\n  accessModes:\n    - ReadWriteMany\n  capacity:\n    storage: 5Gi\n  claimRef:\n    apiVersion: v1\n    kind: PersistentVolumeClaim\n    name: itom-vol-claim\n    namespace: core\n  nfs:\n    path: /var/vols/itom/core\n    server: &lt;nfs-server&gt;\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: cdf-default\n  volumeMode: Filesystem\nEOF\n</pre>\n<p>Where: <code>&lt;nfs-server&gt;</code> is your NFS server hostname.</p>\n</li>\n<li>Run this command on the control plane node to create a persistent volume claim.\n\t<pre>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:<span id=\"cke_bm_1531C\" style=\"display: none;\"> </span>\n  name: itom-vol-claim\n  namespace: core\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n     storage: 5Gi\n  storageClassName: cdf-default\n  volumeMode: Filesystem\n  volumeName: itom-vol\nstatus:\n  accessModes:\n  - ReadWriteMany\n  capacity:\n    storage: 5Gi\nEOF\n</pre>\n</li>\n<li>Refer to <a href=\"/doc/423/26.1/installveleromanagedk8s#Set_up_Velero_on_Embedded_Kubernetes\" title=\"Set up Velero on embedded Kubernetes \">Set up Velero on embedded Kubernetes </a> to enable the kubernetes backup capability.</li>\n<li>Verify that you can get the Velero backup.\n\t<pre> velero get backup</pre>\n</li>\n<li>Restore the Kubernetes configurations for OMT and SMA by running this command on the control plane node:\n\t<pre>velero restore create --from-backup &lt;backup.all.example1&gt; --preserve-nodeports --wait\n</pre>\n</li>\n<li>Run this command on the control plane node to verify the restoration:\n\t<pre>kubectl get pod --all-namespaces|grep -v 1/1|grep -v 2/2|grep -v 3/3|grep -v 4/4|grep -v Completed</pre>\n<p>This command returns a list of abnormal pods. If RabbitMQ isn't ready, see <a href=\"/doc/423/26.1/rabbitmqnotstart\" title=\"RabbitMQ isn't ready\">RabbitMQ isn't ready</a>. When it returns no result, all the suite pods are ready and you can perform quick tests to make sure the restoration is successful.</p>\n</li>\n<li>\n<p>(Optional) If you want to use a new FQDN for the K8s cluster, see <a href=\"/doc/423/26.1/smaxchangefqdn\" title=\"Change the suite FQDN\">Change the suite FQDN</a>.</p>\n</li>\n</ol>\n</div>",
  "modifiedon": "2025-10-24 08:51:12"
}