{
  "title": "Migrate from self-hosted PostgreSQL to Azure Flexible Server",
  "content": "<div class=\"mw-parser-output\">\n<div></div>\n<p>If you currently have a classic deployment with self-hosted PostgreSQL on Azure, you need to follow the steps below to migrate to Azure Flexible Server before transforming to a Helm deployment.</p>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-caution-icon\"></div><div class=\"admonition-content admonition-caution-content\"><div>The steps below don't work if you have already transformed the suite to a Helm deployment.</div></div></div>\n<p>There are two major steps in the entire migration process:</p>\n<ol>\n<li>Data migration. Migrate the data including the database structure, business data, and database authentication data from the self-hosted PostgreSQL to Azure Flexible Server by using the script db_migrate.sh, which will be introduced later. In this script, the <strong>pg_dump</strong> and <strong>pg_restore </strong>commands are used to perform the data dump and restore. </li>\n<li>Apply the suite to connect to Azure Flexible Server. In this step, the production database connection configurations will be modified and applied.</li>\n</ol>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-important-icon\"></div><div class=\"admonition-content admonition-important-content\"><div>This database migration solution requires system downtime, which may vary depending on your data volume. You may want to run this migration in your test environment and plan a proper migration time window for production accordingly.  </div></div></div>\n<h2>Prerequisites</h2>\n<p>Before you proceed, make sure you meet the following prerequisites. </p>\n<h3>Create an Azure Flexible Server that meets the requirements for the database server</h3>\n<p>Follow the guide <a href=\"/doc/423/26.1/aksdatabase\" title=\"Create external databases for Azure deployment\">Create external databases for Azure deployment</a> to create an Azure Flexible Server. To minimize the time required to complete the data restore, make some database server tuning by referring to <a href=\"https://learn.microsoft.com/en-us/azure/postgresql/migrate/how-to-migrate-using-dump-and-restore#optimize-the-migration-process\" title=\"optimize-the-migration-process\">optimize-the-migration-process</a>.</p>\n<p>Note that the Azure Flexible Server must be clean. If you have used an Azure Flexible Server for testing before, to avoid potential conflicts during data import, make sure there are no databases and users in the Azure Flexible Server that are the same as those in the source PostgreSQL server.</p>\n<h3>Prepare a PostgreSQL client system</h3>\n<p>As mentioned above, you will need <strong>pg_dump</strong> and <strong>pg_restore </strong>commands, which are available once the PostgreSQL client is installed. We recommend that you use the higher database version. For example, if you're upgrading from PostgreSQL version 10 to 14, consider using a PostgreSQL version 14 client.</p>\n<p>Prepare a disk with enough space for the PostgreSQL client to store dump files according to your database size. When preparing this system, you need to consider sufficient network throughput and disk throughput which are important factors in reducing migration time. It's recommended to prepare this system in the same VLAN as the source database server and the target database server. The bastion node is used to install the PostgreSQL client in this article.</p>\n<h3>Use the same password encryption algorithm</h3>\n<p>Both the source and the target PostgreSQL Servers have the same password encryption algorithm. Otherwise, Service Management database users will encounter authentication failures when connecting to the target database after migration. You can check their password encryption algorithms by running the SQL command below:</p>\n<pre><code>show password_encryption;</code></pre>\n<p>If they don't use the same algorithm, update the target server's password encryption algorithm to align with the source database server. For details, see the <a href=\"https://www.postgresql.org/docs/current/auth-password.html\" title=\"PostgreSQL documentation\">PostgreSQL documentation</a> and Azure documentation. </p>\n<h3>Obtain the DBA credentials for the source and target database servers</h3>\n<p>The data migration includes both data export and data import, both of which require database administrator (DBA) credentials. </p>\n<p>Make sure you have the DBA credentials for the source and target database servers. </p>\n<p>Additionally, make sure you can access the <code>pg_shadow</code> view on the source database server as the DBA user.  For example, you can retrieve all the user records by running the following SQL:</p>\n<pre><code>SELECT * from pg_shadow;</code></pre>\n<h3><span style=\"color: rgb(0, 115, 231); font-size: 24px;\">Prepare for migration</span></h3>\n<p>Perform the following preparation steps.</p>\n<h3>Download the Azure Flexible Server CA certificate</h3>\n<p>Get the CA certificate of the Azure flexible server and upload it to any temporary folder on the bastion node.</p>\n<h3>Download the migration scripts and then install the scripts and the PostgreSQL CA certificate chain file</h3>\n<p>To complete the migration process, you need to run two scripts that are included in the <a href=\"https://marketplace.microfocus.com/itom/content/service-management-automation-operation-toolkit\" target=\"1\" title=\"SMA Operation Toolkit\">SMA Operation Toolkit</a> package. Download the package to the bastion node. </p>\n<ul>\n<li>For data migration, use the <strong>db_migrate.sh</strong> script in the <code>db_migrate</code> folder to migrate the source databases. Use the <code>database.json</code> file in the same folder to configure the databases that you'll migrate. Prepare the script to be ready for runing on the bastion node.</li>\n<li>To switch database connections, use the <strong>updateSMAExternalDBInfo.sh</strong> script in the <code>db_connection_management</code> folder to switch the database connection to the target Azure flexible server. The components are configured in the component.json in the same folder. You must run the <strong>updateSMAExternalDBInfo.sh</strong> script on the runtime platform provided by the <a href=\"https://marketplace.microfocus.com/itom/content/service-management-automation-support-assistant\" title=\"SMA Support Assistant\">SMA Support Assistant</a>. See <a href=\"/doc/423/26.1/tookitcontainer#Install_the_runtime_platform\" title=\"Install the runtime platform\">Install the runtime platform</a> to set up the toolkit in advance. Prepare the<strong> </strong>script and the CA certificate chain file by referring to <a href=\"/doc/423/26.1/tookitcontainer#Prepare_scripts_on_the_runtime_platform\" title=\"Prepare the scripts on the runtime platform\">Prepare the scripts on the runtime platform</a>.</li>\n<li>If CMP, CMP FinOps, and DND are installed, upload the PostgreSQL CA (with file extension .crt) to the NFS folder <code>&lt;global-volume&gt;/certificate/cmp/source</code> as well. Change the file ownership with the correct UID and GID. For example, run the following command: \n\t<pre><code> chown 1999:1999 *.crt</code></pre>\n</li>\n</ul>\n<h3>Grant the database role \"maas_admin\" to the DBA user</h3>\n<p>The purpose of this step is to give the DBA user the required permission to dump the data owned by \"maas_admin\". To do this, run the following SQL commands:</p>\n<pre><code>psql -U &lt;dba user&gt; -d postgres\ngrant maas_admin to &lt;dba user&gt;;</code></pre>\n<h3>Check the database.json file</h3>\n<p>Check the databases in your source PostgreSQL server and make sure they're all configured in <code>database.json</code>. Configure the OMT databases and the DB owners in <code>database.json</code> as shown in the example below. Replace the database user and the database name with your own.</p>\n<pre>   {\n      \"name\": \"CDFIDM\",                    -- Module name\n      \"userName\": \"cdfidmuser\",            -- OMT database user\n      \"userRole\": \" \",                     -- The role of the database user you will create in the target database\n      \"operate\": \"true\",                   -- If true, the database users and dbs will be dumped or restored. If false, skip this module.                                                                      \n      \"dbName\": [\n        {\n          \"name\": \"cdfidm\",                -- Database name\n          \"exist\": \"true\"                  -- If true, this db will be dumped or restored. If false, skip this database.\n        }\n      ]\n    },\n    {\n      \"name\": \"CDFApiServer\",\n      \"userName\": \"cdfapiserver\",\n      \"userRole\": \" \",\n      \"operate\": \"true\",\n      \"dbName\": [\n        {\n          \"name\": \"cdfapiserverdb\",\n          \"exist\": \"true\"\n        }\n      ]\n    }\n  </pre>\n<p>After changing the database.json file,  perform the following checking. </p>\n<p>Check the database users who will be dumped or restored:</p>\n<pre><code>cat database.json|jq \".Databases\" |jq '.[]|select(.operate == \"true\")'|jq -r \".userName\"\n</code></pre>\n<p>Check the databases that will be dumped or restored:</p>\n<pre><code>cat database.json|jq \".Databases\"|jq '.[]|select(.operate == \"true\")'|jq \".dbName\"|jq '.[]|select(.exist == \"true\")'|jq -r '.name'</code></pre>\n<h2 id=\"Stop_the_suite_pods\">Shut down the system to prevent any DB write operation during the database migration</h2>\n<p>To ensure data integrity during the migration, you must shut down the suite system. Stop the suite pods by performing the following steps on the bastion node:</p>\n<ol>\n<li>Navigate to the <code>$CDF_HOME/scripts</code> directory and run the following command to stop the suite pods in the suite namespace:\n\n\t<pre>./cdfctl.sh runlevel set -l DOWN -n &lt;namespace&gt;</pre>\n<p>Where: <code>&lt;namespace&gt;</code> is the suite namespace (for example: <code>itsma-pxh5s</code>), and the last character in <code>runlevel set -l</code> is a lowercase letter <b>L</b>.</p>\n</li>\n<li>\n<p>Wait 10 to 15 minutes to allow the suite pods to be stopped. You can run the following command to monitor the suite pod status:</p>\n<pre>watch kubectl get pods -n &lt;namespace&gt;</pre>\n<p>This command will refresh the display of the suite pod status every two seconds.<br/>\n<b>Note</b>: All of the suite pods must be stopped except the following ones (which must keep running): <code>itom-ingress-controller-xxxxx</code> and <code>itom-throttling-controller-xxxxx</code>.</p>\n</li>\n<li>Navigate to the <code>$CDF_HOME/scripts</code> directory, and run the following command to stop OMT pods in the core namespace:\n\t<pre>./cdfctl.sh runlevel set -l DOWN -n core</pre>\n<p>The last character in <code>runlevel set -l</code> is a lowercase letter <b>L</b>.</p>\n</li>\n<li>\n<p>Wait some time to allow the OMT pods to be stopped. You can run the following command to monitor the OMT pod status:</p>\n<pre>watch kubectl get pods -n core</pre>\n</li>\n</ol>\n<h2>Run database migration</h2>\n<p>On the bastion node, use <code>root</code> or <code>postgres</code> to run the following command to migrate SMAX/OMT data from the source database (self-hosted) to the target database (Azure Flexible Server).</p>\n<pre>Usage: ./db_migrate.sh dump|restore [-h|--host &lt;host/ip address&gt;] [-p|--port &lt;5432&gt;] [-d|--data &lt;path&gt;] [-u|--user &lt;dba user&gt;]\n       -h|--host        Database server host name or ip address.\n       -p|--port        Database server port.\n       -d|--data        The data path where to store the dump files.\n       -u|--user        The dba user name. For example, postgres.</pre>\n<p>To run the <code>db_migrate.sh</code> script, make sure the bastion node has commands such as <code>pg_dump</code>, <code>pg_restore</code>, <code>psql</code>, and <code>jq</code> installed.</p>\n<p>For examples:</p>\n<ol>\n<li>To dump the databases into files:\n\t<pre>./db_migrate.sh dump -h &lt;source_db_host&gt; -p &lt;source_db_port&gt; -d &lt;data folder&gt; -u &lt;dba user name&gt;</pre>\n</li>\n<li>To restore the databases to files:\n\t<pre>./db_migrate.sh restore -h &lt;target_db_host&gt; -p &lt;target_db_port&gt; -d &lt;data folder&gt; -u &lt;dba user name&gt;</pre>\n</li>\n</ol>\n<p>Notes:</p>\n<ol>\n<li>Enter the password for the dba user when the script prompts you to input that.</li>\n<li>Use the environment parameter \"PARALLEL_JOBS_NUMBER\" to set the number of jobs running in parallel. It will dramatically reduce the time to restore a large database to a server running on a multiprocessor machine. Please refer to <a href=\"https://www.postgresql.org/docs/current/app-pgrestore.html\" title=\"pg_restore\">pg_restore</a> to set this parameter to a specific value by running a command (For example, <code>export PARALLEL_JOBS_NUMBER=4</code>) before executing db_migrate.sh.</li>\n</ol>\n<h2 id=\"Enable_OMT_to_connect_to_external_PostgreSQL_through_SSL\" tabindex=\"-1\">Switch OMT to the new Azure flexible server with SSL</h2>\n<p>Update the OMT database connection settings to point to the new Azure flexible server and use SSL. </p>\n<ol>\n<li>Start the OMT Pods by running the following command:\n\t<pre><code>./cdfctl.sh runlevel set -l UP -n core</code></pre>\n\tWait until all the OMT pods are up and running.</li>\n<li>Run the <code>$CDF_HOME/bin/updateExternalDbInfo</code> command to modify the database host and the certificate for the two OMT databases. Here is an example:\n\t<pre><code>${CDF_HOME}/bin/updateExternalDbInfo -u cdfidmuser -U 'jdbc:postgresql://&lt;azure flexible server fqdn&gt;:5432/cdfidm' --cacert &lt;db-ca-chain.crt&gt; --component itom-idm\n${CDF_HOME}/bin/updateExternalDbInfo -u cdfapiserver -U 'jdbc:postgresql://&lt;azure flexible server fqdn&gt;:5432/cdfapiserverdb' --cacert &lt;db-ca-chain.crt&gt; --component itom-cdf-api</code></pre>\n<p>For more information, see <a href=\"/doc/423/26.1/modifyexternaldbconfig\" title=\"Modify the OMT external database configuration\">Modify the OMT external database configuration</a>.</p>\n</li>\n</ol>\n<h2>Switch the suite to the new Azure flexible server with SSL</h2>\n<p>In this step, you should have prepared the runtime platform and the <strong>updateSMAExternalDBInfo.sh</strong> script is ready for running.</p>\n<ol>\n<li>Enter the <strong><code>itom-toolkit</code></strong> container and go to \"/toolkit\" folder on the runtime platform by referring to <a href=\"/doc/423/26.1/tookitcontainer#Run_scripts_on_the_runtime_platform\" title=\"Run the scripts on the runtime platform\">Run the scripts on the runtime platform</a>, then run the script to apply the new database server host and port and the Azure Flexible server CA certificate chain<strong>:</strong>\n<p>Before running, you need to replace the values of the parameters  &lt;target_db_host&gt;, &lt;target_db_port&gt;, &lt;ca_file_with_path&gt;, and &lt;backup_directory&gt; with your own values in the next commands. If the suite components connected the self-hosted PosgreSQL through non-SSL before this migration, you can enable SSL connection by appending \"<code>--secure --ssl_mode &lt;ssl-mode&gt;</code>\" to the commands. The &lt;ssl-mode&gt; should be one of the supported values such as \"verify-ca\" and \"verify-full\".</p>\n<ul>\n<li>Run the following commands for the suite components:\n\t\t<pre><code>updateSMAExternalDBInfo.sh --component Autopass --host &lt;target_db_host&gt; --port &lt;target_db_port&gt; --cacert &lt;ca_file_with_path&gt; -b &lt;backup_directory&gt; \nupdateSMAExternalDBInfo.sh --component Idm --host &lt;target_db_host&gt; --port &lt;target_db_port&gt; --cacert &lt;ca_file_with_path&gt; -b &lt;backup_directory&gt; \nupdateSMAExternalDBInfo.sh --component SmartAnalytics --host &lt;target_db_host&gt; --port &lt;target_db_port&gt; --cacert &lt;ca_file_with_path&gt; -b &lt;backup_directory&gt;  \nupdateSMAExternalDBInfo.sh --component ServiceManagement --host &lt;target_db_host&gt; --port &lt;target_db_port&gt; --cacert &lt;ca_file_with_path&gt; -b &lt;backup_directory&gt;  \nupdateSMAExternalDBInfo.sh --component ServiceManagementReadOnly --host &lt;target_db_host&gt; --port &lt;target_db_port&gt; --cacert &lt;ca_file_with_path&gt; -b &lt;backup_directory&gt;  \nupdateSMAExternalDBInfo.sh --component SuiteAdministration --host &lt;target_db_host&gt; --port &lt;target_db_port&gt; --cacert &lt;ca_file_with_path&gt; -b &lt;backup_directory&gt;  </code></pre>\n</li>\n<li>Run the following commands if <strong>CMP</strong>,<strong> CMP FinOps</strong>, and <strong>DND </strong>components are installed:\n\t\t<pre><code>updateSMAExternalDBInfo.sh --component CMPFinOps --host &lt;target_db_host&gt; --port &lt;target_db_port&gt; -b &lt;backup_directory&gt; \nupdateSMAExternalDBInfo.sh --component CMP --host &lt;target_db_host&gt; --port &lt;target_db_port&gt; -b &lt;backup_directory&gt; \nupdateSMAExternalDBInfo.sh --component DND --host &lt;target_db_host&gt; --port &lt;target_db_port&gt; -b &lt;backup_directory&gt; </code></pre>\n<strong>Notes: </strong>\n<ul>\n<li>As mentioned in the section <strong>\"Prepare for migration\"</strong> above, the CA certificate chain file needs to be prepared in the target location during the preparation stage. So in this step, you have no need to add \"--cacert  &lt;ca_file_with_path&gt;\" to the commands.</li>\n<li>The CMP component is required only if you have DND or CMP FinOps installed; the DND component is required only if you have DND installed; the CMP FinOps component is required only if you have CMP FinOps installed.</li>\n</ul>\n</li>\n<li>Run the following command if <strong>SAM </strong>component is installed:\n\t\t<pre><code>updateSMAExternalDBInfo.sh --component SAM --host &lt;target_db_host&gt; --port &lt;target_db_port&gt; --cacert &lt;ca_file_with_path&gt; -b &lt;backup_directory&gt; </code></pre>\n</li>\n</ul>\n</li>\n<li>Start the suite on the bastion node.\n\t<p>Navigate to the <code>$CDF_HOME/scripts</code> directory and run the following command to start the suite pods in the suite namespace.</p>\n<pre>./cdfctl.sh runlevel set -l UP -n &lt;namespace&gt;</pre>\n<p>Where: <code>&lt;namespace&gt;</code> is the suite namespace (for example: <code>itsma-pxh5s</code>), note that the last character in <code>runlevel set -l</code> is a lowercase letter <b>l</b>.</p>\n</li>\n</ol>\n</div>",
  "modifiedon": "2025-10-24 08:51:12"
}