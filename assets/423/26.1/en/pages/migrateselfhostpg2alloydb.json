{
  "title": "Migrate from self-hosted PostgreSQL to AlloyDB Omni",
  "content": "<div class=\"mw-parser-output\">\n<p>You can use this document to migrate the suite from a standard PostgreSQL database to AlloyDB Omni.</p>\n<p>There are two main steps in the migration process:</p>\n<ol>\n<li>Data migration. Migrate the data including the database structure, business data, and database authentication data from the self-hosted PostgreSQL to AlloyDB Omni by using the script <code>db_migrate.sh</code>, which will be introduced later. This script uses the <strong>pg_dump</strong> and <strong>pg_restore </strong>commands to perform the data dump and restore. </li>\n<li>Connect the suite to the AlloyDB Omni database. In this step, the production database connection configurations will be modified and applied.</li>\n</ol>\n<p>This database migration solution requires service downtime, which may vary depending on your data volume. You may want to run this migration in your test environment and plan a proper migration time window for production accordingly.  </p>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><div>In this release, support for AlloyDB Omni is under controlled availability. Please contact our product management team before you begin the implementation in your production environment. </div></div></div>\n<h2>Prerequisites</h2>\n<p>Before you proceed, make sure you meet the following prerequisites. </p>\n<h3>Create an AlloyDB Omni database</h3>\n<p>Prepare an AlloyDB Omni database server by following the steps in <a href=\"/doc/423/26.1/prepareexternaldbs\" title=\"Prepare External Database\">Prepare external databases</a> to set up the database to meet the database requirements. You can also merge the parameter settings from the <code>postgresql.conf</code> and the <code>pg_hba.conf</code> of the original database server instance into the AlloyDB files. </p>\n<p>Note that the AlloyDB Omni database must be clean. If you have used an AlloyDB database for testing before, to avoid potential conflicts during data import, make sure there are no databases and users in the AlloyDB that are the same as those in the source PostgreSQL server.</p>\n<h3>Prepare a PostgreSQL client system</h3>\n<p>As mentioned above, you will need <strong>pg_dump</strong> and <strong>pg_restore </strong>commands, which are available once the PostgreSQL client is installed. We recommend that you use the latest database client version. For example, if you're migrating the data from PostgreSQL version 14 to PostgreSQL with version 15 in the AlloyDB Omni container, consider using a PostgreSQL version 15+ client. </p>\n<p>Prepare a disk with enough space for the PostgreSQL client to store dump files according to your database size. When preparing this system, you need to consider sufficient network throughput and disk throughput which are important factors in reducing migration time. We recommend that you prepare this system in the same VLAN as the source database server and the target database server. You can plan where to run the <code>db_migrate.sh</code> script to dump the data, where to store the backup contents, how to sync the backup contents, and then run the <code>db_migrate.sh</code> script to restore them to the target server according to your own situation.</p>\n<p>In this document, we use a control plane node to install the PostgreSQL client. </p>\n<h3>Use the same password encryption algorithm</h3>\n<p>Both the source and the target PostgreSQL servers must have the same password encryption algorithm. Otherwise, the database users will encounter authentication failures when connecting to the target database after migration. You can check their password encryption algorithms by running the SQL command below:</p>\n<pre><code>show password_encryption;</code></pre>\n<p>If they don't use the same algorithm, update the target server's password encryption algorithm to align with the source database server. For details, see the <a href=\"https://www.postgresql.org/docs/current/auth-password.html\" title=\"PostgreSQL documentation\">PostgreSQL documentation</a>. </p>\n<h3>Obtain the DBA credentials for the source and target database servers</h3>\n<p>The data migration includes both data export and data import, both of which require database administrator (DBA) credentials. </p>\n<p>Make sure you have the DBA credentials for the source and target database servers. </p>\n<p>Additionally, make sure you can access the <code>pg_shadow</code> view on the source database server as the DBA user.  For example, you can retrieve all the user records by running the following SQL:</p>\n<pre><code>SELECT * from pg_shadow;</code></pre>\n<h2>Prepare for migration</h2>\n<p>Perform the following preparation steps.</p>\n<h3>Download the target database CA certificate</h3>\n<p>Get the CA certificate of the target database and upload it to any temporary folder on the control plane node.</p>\n<h3>Download the migration scripts and then install the scripts and the PostgreSQL CA certificate chain file</h3>\n<p>To complete the migration process, you need to run two scripts that are included in the <a href=\"https://marketplace.microfocus.com/itom/content/service-management-automation-operation-toolkit\" target=\"1\" title=\"SMA Operation Toolkit\">SMA Operation Toolkit</a> package. Download the package to the control plane node. </p>\n<ul>\n<li>For data migration, use the <strong>db_migrate.sh</strong> script in the <code>db_migrate</code> folder to migrate the source databases. Use the <code>database.json</code> file in the same folder to configure the databases that you'll migrate. Prepare the script to be ready for runing on the control plane node.</li>\n<li>For classic deployment, use the <strong>updateSMAExternalDBInfo.sh</strong> script in the <code>db_connection_management</code> folder to switch the database connection to the target database server. The components are configured in the component.json in the same folder. You must run the <strong>updateSMAExternalDBInfo.sh</strong> script on the runtime platform provided by the <a href=\"https://marketplace.microfocus.com/itom/content/service-management-automation-support-assistant\" title=\"SMA Support Assistant\">SMA Support Assistant</a>. See <a href=\"/doc/423/26.1/tookitcontainer#Install_the_runtime_platform\" title=\"Install the runtime platform\">Install the runtime platform</a> to set up the toolkit in advance. Prepare the<strong> </strong>script and the CA certificate chain file by referring to <a href=\"/doc/423/26.1/tookitcontainer#Prepare_scripts_on_the_runtime_platform\" title=\"Prepare the scripts on the runtime platform\">Prepare the scripts on the runtime platform</a>.</li>\n</ul>\n<h3>Grant the \"maas_admin\" database role to the DBA user</h3>\n<p>The purpose of this step is to give the DBA user the required permission to dump the data owned by \"maas_admin\". To do this, run the following SQL commands:</p>\n<pre><code>psql -U &lt;dba user&gt; -d postgres\ngrant maas_admin to &lt;dba user&gt;;</code></pre>\n<h3>Check the database.json file</h3>\n<p>Check the databases in your source PostgreSQL server and make sure they're all configured in <code>database.json</code>. Configure the OMT databases and the database owners in <code>database.json</code> as shown in the example below. Replace the database user and the database name with your own.</p>\n<pre>   {\n      \"name\": \"CDFIDM\",                    -- Module name\n      \"userName\": \"cdfidmuser\",            -- OMT database user\n      \"userRole\": \" \",                     -- The role of the database user you will create in the target database\n      \"operate\": \"true\",                   -- If true, the database users and dbs will be dumped or restored. If false, skip this module.                                                                      \n      \"dbName\": [\n        {\n          \"name\": \"cdfidm\",                -- Database name\n          \"exist\": \"true\"                  -- If true, this db will be dumped or restored. If false, skip this database.\n        }\n      ]\n    },\n    {\n      \"name\": \"CDFApiServer\",\n      \"userName\": \"cdfapiserver\",\n      \"userRole\": \" \",\n      \"operate\": \"true\",\n      \"dbName\": [\n        {\n          \"name\": \"cdfapiserverdb\",\n          \"exist\": \"true\"\n        }\n      ]\n    }\n  </pre>\n<p>After changing the database.json file, perform the following checks:</p>\n<ul>\n<li>Check the database users who will be dumped or restored:\n\t<pre><code>cat database.json|jq \".Databases\" |jq '.[]|select(.operate == \"true\")'|jq -r \".userName\"\n</code></pre>\n</li>\n<li>Check the databases that will be dumped or restored:\n\t<pre><code>cat database.json|jq \".Databases\"|jq '.[]|select(.operate == \"true\")'|jq \".dbName\"|jq '.[]|select(.exist == \"true\")'|jq -r '.name'</code></pre>\n</li>\n</ul>\n<h2>Shut down the system to prevent any database write operation during the migration</h2>\n<p>To ensure data integrity during the migration, you must shut down the suite system. Stop the suite pods by performing the following steps on the control plane node:</p>\n<ol>\n<li>Navigate to the <code>$CDF_HOME/scripts</code> directory and run the following command to stop the suite pods in the suite namespace:\n\n\t<pre>./cdfctl.sh runlevel set -l DOWN -n &lt;namespace&gt;</pre>\n<p>Where: <code>&lt;namespace&gt;</code> is the suite namespace (for example: <code>itsma-pxh5s</code>), and the last character in <code>runlevel set -l</code> is a lowercase letter <b>L</b>.</p>\n</li>\n<li>\n<p>Wait 10 to 15 minutes to allow the suite pods to be stopped. You can run the following command to monitor the suite pod status:</p>\n<pre>watch kubectl get pods -n &lt;namespace&gt;</pre>\n<p>This command will refresh the display of the suite pod status every two seconds.</p>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><div>All of the suite pods must be stopped except the following ones (which must keep running): <code>itom-ingress-controller-xxxxx</code> and <code>itom-throttling-controller-xxxxx</code>.</div></div></div>\n</li>\n<li>Navigate to the <code>$CDF_HOME/scripts</code> directory, and run the following command to stop OMT pods in the core namespace:\n\t<pre>./cdfctl.sh runlevel set -l DOWN -n core</pre>\n<p>The last character in <code>runlevel set -l</code> is a lowercase letter <b>L</b>.</p>\n</li>\n<li>\n<p>Wait some time to allow the OMT pods to be stopped. You can run the following command to monitor the OMT pod status:</p>\n<pre>watch kubectl get pods -n core</pre>\n</li>\n</ol>\n<h2>Run database migration</h2>\n<p>On the control plane node, use <code>root</code> or <code>postgres</code> to run the following command to migrate data from the source database (self-hosted) to the target database.</p>\n<pre>Usage: ./db_migrate.sh dump|restore [-h|--host &lt;host/ip address&gt;] [-p|--port &lt;5432&gt;] [-d|--data &lt;path&gt;] [-u|--user &lt;dba user&gt;]\n       -h|--host        Database server host name or ip address.\n       -p|--port        Database server port.\n       -d|--data        The data path where to store the dump files.\n       -u|--user        The dba user name. For example, postgres.</pre>\n<p>To run the <code>db_migrate.sh</code> script, make sure the control plane node has commands such as <code>pg_dump</code>, <code>pg_restore</code>, <code>psql</code>, and <code>jq</code> installed.</p>\n<p>For examples:</p>\n<ol>\n<li>To dump the databases into files:\n\t<pre>./db_migrate.sh dump -h &lt;source_db_host&gt; -p &lt;source_db_port&gt; -d &lt;data folder&gt; -u &lt;dba user name&gt;</pre>\n</li>\n<li>To restore the databases to files:\n\t<pre>./db_migrate.sh restore -h &lt;target_db_host&gt; -p &lt;target_db_port&gt; -d &lt;data folder&gt; -u &lt;dba user name&gt;</pre>\n</li>\n</ol>\n<p>Notes:</p>\n<ol>\n<li>Enter the password for the dba user when the script prompts you to input that.</li>\n<li>Use the environment parameter \"PARALLEL_JOBS_NUMBER\" to set the number of jobs running in parallel. It will dramatically reduce the time to restore a large database to a server running on a multiprocessor machine. Please refer to <a href=\"https://www.postgresql.org/docs/current/app-pgrestore.html\" title=\"pg_restore\">pg_restore</a> to set this parameter to a specific value by running a command (For example, <code>export PARALLEL_JOBS_NUMBER=4</code>) before executing db_migrate.sh.</li>\n</ol>\n<h2>Switch OMT to the new database server with SSL</h2>\n<p>Update the OMT database connection settings to point to the new target database server and use SSL. </p>\n<ol>\n<li>Start the OMT Pods by running the following command:\n\t<pre><code>./cdfctl.sh runlevel set -l UP -n core</code></pre>\n\tWait until all the OMT pods are up and running.</li>\n<li>Run the <code>$CDF_HOME/bin/updateExternalDbInfo</code> command to modify the database host and the certificate for the two OMT databases. Here is an example:\n\t<pre><code>${CDF_HOME}/bin/updateExternalDbInfo -u cdfidmuser -U 'jdbc:postgresql://&lt;target database host&gt;:5432/cdfidm' --cacert &lt;db-ca-chain.crt&gt; --component itom-idm\n${CDF_HOME}/bin/updateExternalDbInfo -u cdfapiserver -U 'jdbc:postgresql://&lt;target database host&gt;:5432/cdfapiserverdb' --cacert &lt;db-ca-chain.crt&gt; --component itom-cdf-api</code></pre>\n<p>For more information about classic deployment, see <a href=\"/doc/423/24.3/modifyexternaldbconfig\" title=\"Modify the OMT external database configuration for classic deployment\">Modify the OMT external database configuration for classic deployment</a>.</p>\n<p>For more information about helm deployment, see <a href=\"/doc/406/25.4/modifyexternaldatabaseconfiguration\" title=\"Modify the OMT external database configuration for helm deployment\">Modify the OMT external database configuration for helm deployment</a>.</p>\n</li>\n</ol>\n<h2>Switch to the new database server with SSL</h2>\n<h3>For a classic deployment</h3>\n<p>In this step, you should have prepared the runtime platform, and the <strong>updateSMAExternalDBInfo.sh</strong> script is ready for running.</p>\n<ol>\n<li>Enter the <code>itom-toolkit</code> container and go to \"/toolkit\" folder on the runtime platform by referring to <a href=\"/doc/423/26.1/tookitcontainer#Run_scripts_on_the_runtime_platform\" title=\"Run the scripts on the runtime platform\">Run the scripts on the runtime platform</a>, then run the script to apply the new database server host and port and the target database server CA certificate chain<strong>:</strong>\n<p>Before running the commands, replace the values of the <code>&lt;target_db_host&gt;</code>, <code>&lt;target_db_port&gt;</code>, <code>&lt;ca_file_with_path&gt;</code>, and <code>&lt;backup_directory&gt;</code> parameters with your own values. If the suite components connected the self-hosted PostgreSQL through non-SSL before this migration, you can enable SSL connection by appending \"<code>--secure --ssl_mode &lt;ssl-mode&gt;</code>\" to the commands. The <code>&lt;ssl-mode&gt;</code> parameters should be one of the supported values, such as \"verify-ca\" and \"verify-full\".</p>\n\tFor instructions on how to use this script, see <a href=\"/SMAX/Main/ModifyExternalDBConfig\" title=\"Modify the configuration information of external databases\">Modify the configuration information of external databases</a>.</li>\n<li>Start the suite on the control plane node. To do this, navigate to the <code style=\"font-size: 14px;\">$CDF_HOME/scripts</code><span style=\"font-size: 14px;\"> directory and run the following command to start the suite pods in the suite namespace:</span>\n<pre>./cdfctl.sh runlevel set -l UP -n &lt;namespace&gt;</pre>\n<p>Where <code>&lt;namespace&gt;</code> is the suite namespace (for example: <code>itsma-pxh5s</code>). Note that the last character in <code>runlevel set -l</code> is a lowercase letter <b>l</b>.</p>\n</li>\n</ol>\n<h3>For a helm deployment</h3>\n<p>Use the following steps to modify the configuration information of external databases. These steps allows you to update the database host, port and SSL configuration.</p>\n<ol>\n<li>Run the following command to get your current helm values:\n\t<pre>helm get values &lt;ESM_RELEASE_NAME&gt; -n &lt;ESM_NAMESPACE&gt; &gt; my_values.yaml</pre>\n<p>Where:</p>\n<ul>\n<li><code>&lt;ESM_RELEASE_NAME&gt;</code> is the release name, which you can obtain by using the following command:\n\n\t\t<pre>helm list -n &lt;ESM_NAMESPACE&gt; | awk '{print $1}'</pre>\n</li>\n<li><code>&lt;ESM_NAMESPACE&gt;</code> is the unique namespace of the ESM chart deployment.</li>\n<li>my_values.yaml is the yaml file with your customized settings.</li>\n</ul>\n<p>Example:</p>\n<pre>helm get values sma -n itsma-xxx &gt; my_values.yaml</pre>\n</li>\n<li>\n<p>Modify the my_values.yaml file with the new database port, host, or SSL configurations you wish to update. </p>\n<pre>global:\n  database:\n    admin: postgres\n    host: &lt;DB host&gt;\n    port: &lt;DB port&gt;\n    tlsEnabled: true\n    tlsMode: verify-full    # or use verify-ca\n... ...\ndatabase:\n  caCertificates: \n    pg_ca.crt: &lt;cert-file-base64-encoded&gt; or PEM\n</pre>\n<p>To enable SSL, you need to configure the <code>tlsEnabled</code>, <code>tlsMode</code> and <code>caCertificates </code>values as above. See <a href=\"/doc/423/26.1/changecertforpostgresql\" title=\"Enable SSL connection\">Enable SSL connection</a>.</p>\n</li>\n<li>\n<p>Run the following helm command to update the database information:</p>\n<pre> helm upgrade  &lt;ESM_RELEASE_NAME&gt; -n &lt;ESM_NAMESPACE&gt; &lt;ESM_HELM_CHART_PATH&gt; -f my_values.yaml</pre>\n<p>Where <code>&lt;ESM_HELM_CHART_PATH&gt;</code> is the absolute path of the helm package.</p>\n<p>Example:</p>\n<pre> helm upgrade sma -n itsma-xxx esm-1.0.0+2x.x-xxx\\charts\\esm-1.0.0+2x.x-xxx.tgz -f my_values.yaml</pre>\n</li>\n<li>\n<p>Wait until the suite pods have restarted. You can run the following command to check the suite pod status:</p>\n<pre><code>watch kubectl get pods -n &lt;ESM_NAMESPACE&gt;</code></pre>\n</li>\n</ol>\n</div>",
  "modifiedon": "2025-10-24 08:51:12"
}