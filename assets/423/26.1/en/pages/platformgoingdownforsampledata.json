{
  "title": " Platform Pods going to 1/2 state when sample data deployment is triggered",
  "content": "<div class=\"mw-parser-output\">\n<h2><a id=\"Cause\" name=\"Cause\" title=\"Cause\"></a>Cause</h2>\n<p>When the sample data deployment is triggered, the existing load may affect connections to RabbitMQ. This can, in turn, cause the platform pods to enter a 1/2 state.</p>\n<h2><a id=\"Solution\" name=\"Solution\" title=\"Solution\"></a>Solution</h2>\n<div>\n<p>When RabbitMQ imposes restrictions on the connection, the platform pod goes to a 1/2 status and will return after a few minutes. Therefore, if this issue arises, wait for 15 minutes before checking if it resolves automatically.</p>\n<p>If the problem persists, check the system resources and network connectivity. If there's no resource or network issue, manually restart RabbitMQ. You can do either a hard or soft restart.  To do this, follow these steps:</p>\n<p><strong>Soft restart :</strong></p>\n<p>For a soft restart, follow the steps outlined in the hard restart instructions, but skip step 3 and do not delete any data.</p>\n<p><strong>Hard restart :</strong> </p>\n<ol style=\"padding-left: 0px;\"><li style=\"--number-width: 10px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following command on a master node (embedded Kubernetes) or the bastion node (managed Kubernetes) to stop RabbitMQ:\n\t<pre><code class=\"hljs\">kubectl scale statefulset infra-rabbitmq -n &lt;suite namespace&gt; --replicas=0</code></pre>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">\n<div>Wait until all RabbitMQ pods are terminated. </div>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">\n<div>Remove the <code>&lt;rabbitmq-infra-rabbitmq-n&gt;/data/xservices/rabbitmq-n&gt;/data/xservices/rabbitmq/x.x.x.xx/mnesia</code> folders on the NFS server. For example, remove the following folders:\n\t<pre><code class=\"hljs\">/var/vols/itom/itsma/rabbitmq-infra-rabbitmq-0/data/xservices/rabbitmq/x.x.x.xx/mnesia\n/var/vols/itom/itsma/rabbitmq-infra-rabbitmq-1/data/xservices/rabbitmq/x.x.x.xx/mnesia\n/var/vols/itom/itsma/rabbitmq-infra-rabbitmq-2/data/xservices/rabbitmq/x.x.x.xx/mnesia</code></pre>\n<br/>\n\tIf you use managed NFS, ask your cloud service administrator to delete the content in the RabbitMQ Persistent Volumes (PVs). The corresponding Persistent Volume Claims (PVCs) are <code>rabbitmq-infra-rabbitmq-0</code>, <code>rabbitmq-infra-rabbitmq-1</code>, and <code>rabbitmq-infra-rabbitmq-2</code>.</div>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following command on a master node (embedded Kubernetes) or the bastion node (managed Kubernetes) to restart RabbitMQ:\n\t<pre><code class=\"hljs\">kubectl scale statefulset infra-rabbitmq -n &lt;suite namespace&gt; --replicas=3</code></pre>\n</li></ol>\n</div>\n</div>",
  "modifiedon": "2025-10-24 08:51:12"
}