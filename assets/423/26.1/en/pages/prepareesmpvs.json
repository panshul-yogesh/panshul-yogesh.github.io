{
  "title": "Prepare persistent storage for ESM",
  "content": "<div class=\"mw-parser-output\"><p class=\"mw-empty-elt\"></p><div class=\"mw-parser-output\"><p class=\"mw-empty-elt\">\n</p><p>To install the application in the Helm mode, you must create Perisistent Volumes (PVs). You can create PVs either using static volume provisioning or dynamic volume provisioning. To learn more about the two methods, see <a href=\"/doc/423/26.1/install#Dynamic_vs_static_volume_provisioning\" title=\"Dynamic vs. static volume provisioning\">Dynamic vs. static volume provisioning</a>. </p>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-tip-icon\"></div>\n<div class=\"admonition-content admonition-tip-content\">If using Azure, see <a href=\"/doc/423/26.1/preparepvsaks\" title=\"Prepare persistent volumes for ESM on Azure\">Prepare persistent volumes for ESM on Azure</a> instead.\n\n<p>If using AWS, Ensure you have Amazon EFS CSI driver deployed to your Amazon EKS cluster. See <a href=\"https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html#efs-install-driver\" title=\"Installing the Amazon EFS CSI driver\">Installing the Amazon EFS CSI driver</a>.</p>\n</div>\n</div>\n<h2>Create PVs using dynamic volume provisioning</h2>\n<p>To create PVs for the suite using dynamic provisioning, you must update the deployment YAML file with the storage class name defined in the PVC. In the case of CLI installtion, add the storage class name to the<b> my-values.yaml </b>file.<b> </b>For AppHub installation, use the Yaml Editor to add the storage class name. PVC uses this storage class to create PVs required for the suite.</p>\n<p>The following sections provide instructions to create PVs in various deployment environments such as embedded Kubernetes, OpenShift, and AWS. Refer to the section best suited for your deployment. </p>\n<h3>Create PVs on embedded Kubernetes, OpenShift, or BYOK</h3>\n<p>OPTIC Management Toolkit (OMT) provides the NFS provisioner as part of its infrastructure.  When cluster entities such as Pods request for storage, the NFS provisioner can automatically provision PVs using PVCs from OMT's own storage class.</p>\n<p>If you choose to install OMT with the NFS provisioner capability enabled, you can use the OMT's storage class  named <code>cdf-nfs</code>, to create PVs for the suite. Add it to the deployment YAML file to use it.</p>\n<p>If you don't use OMT,  you must manually create a storage class. Complete the following steps to create the storage class:</p>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-recommendation-icon\"></div>\n<div class=\"admonition-content admonition-recommendation-content\">If your deployment uses OMT, use the same provisioning method that you used when you created PVs for OMT. </div>\n</div>\n<p>Complete the following steps to create PVs on embedded Kubernetes or OpenShift.</p>\n<ol>\n<li>Refer to your provisioner's official documentation for steps to create a storage class. Make sure the storage class parameters are configured as shown in the following section:\n\t<pre><code class=\"language-yaml\">allowVolumeExpansion: true\nreclaimPolicy: Retain</code></pre>\n\tFor example,\n\n\t<pre><code class=\"language-yaml\">allowVolumeExpansion: true\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: &lt;Storage_Class_Name&gt;\nparameters:\n  archiveOnDelete: \"true\"\nprovisioner: &lt;xxx-provisioner&gt;\nreclaimPolicy: Retain\nvolumeBindingMode: Immediate </code></pre>\n\tWhere:\n\n\t<ul>\n<li><strong>&lt;Storage_Class_Name&gt;:</strong> Is the name you wish to give your storage class.</li>\n<li><strong>&lt;xxx-provisioner&gt;:</strong> Is the name of your provisioner.</li>\n</ul>\n</li>\n<li>Take note of the name of your storage class, you will need to add it to your deployment. In case of CLI installation, include storage class name in the <strong>my-values.yaml</strong> file. For AppHub installation, add the storage class name using the YAML editor.\n\t<pre><code class=\"language-yaml\">global:\n  persistence:\n    storageClasses:\n      default-rwx: &lt;Storage_Class_Name&gt;\n      default-rwo: &lt;Storage_Class_Name&gt;</code></pre>\n</li>\n</ol>\n<h3>Create PVs on AWS</h3>\n<p>Complete the following steps to create PVs on AWS.</p>\n<table>\n<tbody>\n<tr>\n<th>Role</th>\n<th>Location</th>\n</tr>\n<tr>\n<td>Storage administrator</td>\n<td>Bastion node</td>\n</tr>\n</tbody>\n</table>\n<h4>(Optional) Mount EFS server</h4>\n<p>Run the following commands to mount the EFS server:</p>\n<pre>mount_point=/mnt/efs\nsudo mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 <code>&lt;efs_dns_name&gt;</code>:/ $mount_point              \nsudo sed -i '$a\\'\"<code>&lt;efs_dns_name&gt;</code>:/ $mount_point nfs4 nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,_netdev 0 0\" /etc/fstab\n</pre>\n<p>Where, <strong>&lt;efs_dns_name&gt;</strong> is the <code>FileServerDNSName</code> available on the <b>Outputs</b> tab at the time of creating the EFS stack. For details, see the last step in <a href=\"/doc/423/26.1/configureeksefshelm\" title=\"Launch EFS\">Launch EFS</a>.</p>\n<p>After the suite installation completes, you can find the automatically generated folders under <strong>$mount_point/{namespace}</strong>. For example, you may find your data-volume folders under the <strong>/mnt/efs/itsma-xxx/data-volume-faf2a790-0ecd-4d90-9c77-a71fcac2c4ca</strong> folder.</p>\n<h4 id=\"prepare-storageclass\">Prepare storageclass</h4>\n<ol>\n<li>Save the following content to <strong>storageclass.yaml</strong> file.\n\n\t<pre tabindex=\"0\"><code data-lang=\"fallback\">kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: efs-sc\nprovisioner: efs.csi.aws.com\nreclaimPolicy: Retain\nparameters:\n  uid: \"&lt;system_user_id&gt;\"\n  gid: \"&lt;system_group_id&gt;\"\n  provisioningMode: efs-ap\n  fileSystemId: &lt;file_system_id&gt;\n  directoryPerms: \"775\"\n  subPathPattern: \"${.PVC.namespace}/${.PVC.name}\" # optional\n  ensureUniqueDirectory: \"true\" # optional\n  reuseAccessPoint: \"false\" # optional\nmountOptions:\n  - noresvport\n</code></pre>\n\tWhere:\n\n\t<ul>\n<li><strong>&lt;file_system_id&gt;: </strong>Is the file system ID of the EFS server. For example,<em> </em>fs-004b5c35ebc21f966.</li>\n<li><strong>&lt;system_user_id&gt; </strong>and<strong> &lt;system_group_id&gt;</strong>: Are the operating system user ID (UID) and group ID (GID) used to run the process in the container. Both values must be 1999 or a number between 100000 and 2000000000 (for example, UID=100001 and GID=100002).</li>\n</ul>\n</li>\n<li>Run the following command to create the storageclass:\n\t<pre tabindex=\"0\"><code data-lang=\"fallback\">kubectl create -f storageclass.yaml</code></pre>\n\tMake a note of the storageclass name. You will need this in a later step for configuring the <code>global.persistence.storageClasses</code> parameter in the YAML file. In the case of CLI installation, update the storage class name in the <strong>my-values.yaml</strong> file. For AppHub installation, add the storage class name using the YAML editor.</li>\n</ol>\n<h2>Create PVs using static volume provisioning</h2>\n<p>You can create PVs using static provisioning in the following ways:</p>\n<ul>\n<li><strong>Using the createPV.sh script</strong>: The script contains predefined volume names to be created, hence you don't need to manually name each volume. </li>\n<li><strong>Manually create PVs</strong>: You can specify the volume names according to your organization.</li>\n</ul>\n<h3>Create PVs using script</h3>\n<p>After setting up the NFS server and volumes, complete the following steps to set up PVs using the <strong>createPV.sh</strong> script:</p>\n<ol>\n<li>Log on to the bastion or a control plane node.</li>\n<li>Navigate to the <strong>ESM_Helm_Chart-2x.x.x/scripts/createPV</strong> directory.</li>\n<li>Run the following commands to create PVs:\n\t<pre><code class=\"language-bash\">chmod u+x createPV.sh\n./createPV.sh -n &lt;NAME_SPACE&gt; -s &lt;NFS_SERVER&gt; -p &lt;BASE_DIRECTORY&gt; --size &lt;SUITE_SIZE&gt;</code></pre>\n\tWhere:\n\n\t<ul>\n<li><strong>&lt;NAME_SPACE&gt;</strong>: Is the namespace for the suite. Ensure that the <strong>&lt;NAME_SPACE&gt;</strong> is the same as the namespace you created for your deployment earlier.</li>\n<li><strong>&lt;NFS_SERVER&gt;</strong>: Is the hostname or IP address of the NFS or EFS server.</li>\n<li><strong>&lt;BASE_DIRECTORY&gt;</strong>: Is the base directory is the absolute path on the NFS/EFS server. This is different from your mount point.</li>\n<li><strong>&lt;SUITE_SIZE&gt;</strong>: Is the deployment size that matches your requirements: <code>small</code>, <code>medium</code>, or <code>large</code>.</li>\n</ul>\n\tFor example:\n\n\t<pre><code class=\"language-bash\">./createPV.sh  -n itsma -s test.example.net -p /var/vols/itom/itsma --size small</code></pre>\n</li>\n</ol>\n<h3>Create PVs manually</h3>\n<p>After configuring the NFS server, use the following table as a guide to create your PVs. The table lists all PVs required by the suite, the quantity, capacity, and their access mode. You can set the PV names according to your organization's policy; however, the <code>accessMode</code> and <code>capacity</code> should remain as stated in the table. The PVC and PV will be bound according to the <code>accessMode</code> and <code>capacity</code>.</p>\n<table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width: 500px;\">\n<tbody>\n<tr>\n<th rowspan=\"2\">Required PV</th>\n<th rowspan=\"2\">Count</th>\n<th rowspan=\"2\">accessMode</th>\n<th colspan=\"3\">Capacity Size</th>\n</tr>\n<tr>\n<th>Small</th>\n<th>Medium</th>\n<th>Large</th>\n</tr>\n<tr>\n<td>logging-volume</td>\n<td>1</td>\n<td>ReadWriteMany</td>\n<td>500Gi</td>\n<td>500Gi</td>\n<td>500Gi</td>\n</tr>\n<tr>\n<td>config-volume</td>\n<td>1</td>\n<td>ReadWriteMany</td>\n<td>100Gi</td>\n<td>100Gi</td>\n<td>100Gi</td>\n</tr>\n<tr>\n<td>data-volume</td>\n<td>1</td>\n<td>ReadWriteMany</td>\n<td>200Gi</td>\n<td>200Gi</td>\n<td>200Gi</td>\n</tr>\n<tr>\n<td>smarta-saw-con,smarta-saw-con-a,smarta-sawmeta-con,smarta-sawmeta-con-a</td>\n<td>16</td>\n<td>ReadWriteOnce</td>\n<td>15Gi</td>\n<td>30Gi</td>\n<td>60Gi</td>\n</tr>\n<tr>\n<td>smarta-sawarc-con,smarta-sawarc-con-a</td>\n<td>4</td>\n<td>ReadWriteOnce</td>\n<td>20Gi</td>\n<td>50Gi</td>\n<td>100Gi</td>\n</tr>\n<tr>\n<td>rabbitmq-infra-rabbitmq</td>\n<td>3</td>\n<td>ReadWriteOnce</td>\n<td>2Gi</td>\n<td>6Gi</td>\n<td>12Gi</td>\n</tr>\n</tbody>\n</table>\n<p class=\"mw-empty-elt\"></p></div>\n<p class=\"mw-empty-elt\"></p></div>",
  "modifiedon": "2025-10-24 08:51:12"
}