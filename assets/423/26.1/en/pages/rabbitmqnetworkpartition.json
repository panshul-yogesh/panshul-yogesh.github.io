{
  "title": "RabbitMQ network partition occurs",
  "content": "<div class=\"mw-parser-output\"><br/>\n<p>A RabbitMQ network partition occurs.</p>\n<h2>How to detect a RabbitMQ network partition in a living system</h2>\n<p>When all three RabbitMQ pods are running with 2/2, check if the cluster has a split-brain. To do this, perform the following steps on a control plane node or the bastion node.  </p>\n<div class=\"admonition\">\n<div class=\"admonition-icon admonition-note-icon\"> </div>\n<div class=\"admonition-content admonition-note-content\"> Your machine must have the <strong>jq</strong> command installed to run some of the commands below. Before you begin, make sure your machine has already this command installed.</div>\n</div>\n<ol>\n<li>Check the running status of RabbitMQ pods.\n\t<pre>nms=`kubectl get pods -A|grep rabbit|head -n 1|awk '{print $1}'`; if [[ `kubectl exec -it infra-rabbitmq-0 -c itom-xruntime-rabbitmq -n ${nms} -- rabbitmqctl cluster_status --formatter json| grep -v RABBITMQ_ERLANG_COOKIE |jq '.running_nodes'|jq 'length'` != \"3\" ]] &amp;&amp; [[ `kubectl exec -it infra-rabbitmq-1 -c itom-xruntime-rabbitmq -n ${nms} -- rabbitmqctl cluster_status --formatter json| grep -v RABBITMQ_ERLANG_COOKIE |jq '.running_nodes'|jq 'length'` != \"3\" ]] &amp;&amp; [[ `kubectl exec -it infra-rabbitmq-2 -c itom-xruntime-rabbitmq -n ${nms} -- rabbitmqctl cluster_status --formatter json| grep -v RABBITMQ_ERLANG_COOKIE |jq '.running_nodes'|jq 'length'` != \"3\" ]]; then echo \"--&gt; Network partition happens.\"; else echo \"--&gt; No network partition.\"; fi</pre>\n</li>\n<li>If all the RabbitMQ pods are running with 2/2, run the following command to see if the RabbitMQ nodes have a network partition:\n\t<pre>rabbitmqnetworkpartition</pre>\n</li>\n<li>If there is a network partition, perform the following steps:\n\t<ol start=\"1\">\n<li>Try to scale the Rabbitmq cluster to replica 1. \n\t\t<pre># kubectl scale statefulset infra-rabbitmq -n itsma-undnc --replicas=1</pre>\n</li>\n<li>Wait until the RabbitMQ cluster node becomes 1 and then scale the RabbitMQ to 3.\n\t\t<pre># kubectl scale statefulset infra-rabbitmq -n itsma-undnc --replicas=3</pre>\n</li>\n<li>Rerun step 2 to check the RabbitMQ cluster status.</li>\n<li>If scaling in and out doesn't help, you can scale them to 1 and prepare for a RabbitMQ refresh start. A RabbitMQ refresh start will cause a service interruption, wait for an acceptable time to do it. To do this, follow these steps:\n\t\t<ol start=\"1\">\n<li>\n<p>Run the following command on a control plane node (embedded Kubernetes) or the bastion node (managed Kubernetes) to stop RabbitMQ:</p>\n<pre>kubectl scale statefulset infra-rabbitmq -n &lt;suite namespace&gt; --replicas=0</pre>\n</li>\n<li>Wait until all RabbitMQ pods are terminated. </li>\n<li>Remove the <code>&lt;rabbitmq-infra-rabbitmq-n&gt;/data/xservices/rabbitmq/x.x.x.xx/mnesia</code> folders on the NFS server or the bastion node (if managed NFS is used, including EFS, Azure Files, Azure NetApp Files, and Filestore).<br/>\n\t\t\tFor example, remove the following folders:\n\t\t\t<pre>/var/vols/itom/itsma/rabbitmq-infra-rabbitmq-0/data/xservices/rabbitmq/x.x.x.xx/mnesia\n/var/vols/itom/itsma/rabbitmq-infra-rabbitmq-1/data/xservices/rabbitmq/x.x.x.xx/mnesia\n/var/vols/itom/itsma/rabbitmq-infra-rabbitmq-2/data/xservices/rabbitmq/x.x.x.xx/mnesia</pre>\n</li>\n<li>Run the following command on a control plane node (embedded Kubernetes) or the bastion node (managed Kubernetes) to restart RabbitMQ:\n\t\t\t<pre>kubectl scale statefulset infra-rabbitmq -n &lt;suite namespace&gt; --replicas=3</pre>\n</li>\n</ol>\n</li>\n<li>Perform a platform rollout restart:\n\t\t<pre><code>nms=`kubectl get pods -A|grep rabbit|head -n 1|awk '{print $1}'`;kubectl rollout restart deployment itom-xruntime-platform -n $nms</code></pre>\n</li>\n<li>Perform a gateway rollout restart:\n\t\t<pre><code>nms=`kubectl get pods -A|grep rabbit|head -n 1|awk '{print $1}'`;kubectl rollout restart deployment itom-xruntime-gateway -n $nms</code></pre>\n</li>\n</ol>\n</li>\n</ol>\n<h2>How to identify if a network partition has happened before</h2>\n<p>A split-brain may happen in the following scenarios:</p>\n<ul>\n<li>Scenario 1: When the RabbiMQ pods start abnormally. For example, all of the RabbiMQ pods start at the same time rather than in sequence.</li>\n<li>Scenario 2: After the RabbitMQ pods are up and running, one RabbitMQ pod can't communicate with the other two due to network issues.</li>\n</ul>\n<h3>Scenario 1</h3>\n<p>Check the startup log (<strong>rabbit@infra-rabbitmq-0.saw-rabbitmq.itsma-xxxxx.svc.cluster.local..log</strong>) of each pod for two lines that resemble the following, which indicate that the current pod and the other peers were in the same cluster (no split-brain):</p>\n<p><strong>rabbit on node 'rabbit@infra-rabbitmq-1.saw-rabbitmq.itsma-xxxxx.svc.cluster.local.' up</strong></p>\n<p><strong>rabbit on node 'rabbit@infra-rabbitmq-2.saw-rabbitmq.itsma-xxxxx.svc.cluster.local.' up</strong></p>\n<p>If you can find the two lines in each log, there was no split-brain. </p>\n<p>If you can't find the two lines or find only one line, a split-brain happened. In this case, perform a rollout start of the platform pods and gateway pods by running the following commands:</p>\n<pre>nms=`kubectl get pods -A|grep rabbit|head -n 1|awk '{print $1}'`;kubectl rollout restart deployment itom-xruntime-platform -n $nms \nnms=`kubectl get pods -A|grep rabbit|head -n 1|awk '{print $1}'`;kubectl rollout restart deployment itom-xruntime-gateway -n $nms</pre>\n<h3>Scenario 2</h3>\n<p>To check the log files to find if there was a split-brain:</p>\n<pre>#cd &lt;LOGGING_VOLUMN&gt;/xservices/rabbitmq \n\n#find . -name \"*.local..log\"|xargs grep -E \"net_tick_timeout|Autoheal request received|Autoheal decision|Autoheal finished\"</pre>\n<p>For example, you suspect a split-brain happened on 2022/3/28, you can run the following command:</p>\n<pre>#find . -name \"*.local..log\"|xargs grep -E \"net_tick_timeout|Autoheal request received|Autoheal decision|Autoheal finished\"|grep 20220328</pre>\n<p>If \"Autoheal finished\" is seen, the cluster had encountered a split-brain and it was healed automatically. In this case, perform a rollout start of the platform pods and gateway pods by running the following commands:</p>\n<pre>nms=`kubectl get pods -A|grep rabbit|head -n 1|awk '{print $1}'`;kubectl rollout restart deployment itom-xruntime-platform -n $nms \nnms=`kubectl get pods -A|grep rabbit|head -n 1|awk '{print $1}'`;kubectl rollout restart deployment itom-xruntime-gateway -n $nms\n</pre>\n<div> </div>\n</div>",
  "modifiedon": "2025-10-24 08:51:12"
}