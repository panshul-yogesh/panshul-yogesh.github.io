{
  "title": "Restore OMT manually",
  "content": "<div class=\"mw-parser-output\"><span class=\"snippet-start\" data-sname=\"423-25.4-423-25.3-RestoreCDF||N\"></span><p>Restore the OMT installation depending on the scenarios.</p>\n<h2>Restore files which are deleted accidentally</h2>\n<div class=\"transcontent stn-begin--file\"> </div>\n<p>When some files are deleted accidentally, you can restore them by copying them back.</p>\n<ul>\n<li>For the files in the directory bin, or the scripts, the tools, or the images in $CDF_HOME, restore them from the OMT installation package. For example: If the file $CDF_HOME/scripts/uploadimages.sh is deleted by accident, you can restore it from the OMT installation package.</li>\n<li>For other files, restore them from the backup directory. For example: If the file $CDF_HOME/ssl/ca.crt is deleted by accident, you can restore it from the cdf_backup/node/cdf_home.tar.gz backup file.</li>\n</ul>\n<div class=\"Admonition_Note\"><span class=\"autonumber\">Note:</span><br/>\nThe restored files must have the same owner and permission with the deleted files.</div>\n<div class=\"transcontent stn-end--file\"> </div>\n<div class=\"transcontent stn-begin--restorexternaldb\"> </div>\n<h2>Restore the external databases</h2>\n<p>If you used external databases (PostgreSQL or Oracle) to install OMT, you need to restore them. Refer to the related database manual for the detailed restore steps.</p>\n<div class=\"transcontent stn-end--restorexternaldb\"> </div>\n<h2>Restore the embedded PostgreSQL databases (cdfapiserver-db and cdfidm-db)</h2>\n<div class=\"transcontent stn-begin--restoreembeddb\"> </div>\n<p>If you have backed up the embedded PostgreSQL databases by the backup_recover.sh command, you can restore the embedded databases by running the following command:</p>\n<pre><code>cd $CDF_HOME/scripts\n./backup_recover.sh -m dbrestore -f /{backup file dir}/cdf-br-{IPv4 or FQDN}-YYYYMMDDHHMMSS.tar.gz</code></pre>\n<p>Your terminal resembles the following: </p>\n<pre><code>[root@control1 scripts]# ./backup_recover.sh -m dbrestore -f /opt/backup/cdf-br-control1.example.net-YYYYMMDDHHMMSS.tar.gz\n\nThis script will restore the embedded PostgreSQL databases. Please wait while Restoring backup...\n\nDuring restoring, this script will restart the components; this will cause the services to be unavailable during the restart.\n\nDo you want to start restore? (yY/nN):y\n\n[INFO] 2021-09-10 14:43:10 : Uncompressing backup data\n[INFO] 2021-09-10 14:43:11 : Restoring embedded databases\n[INFO] 2021-09-10 14:43:11 : - Waiting for itom-vault to be ready\n[INFO] 2021-09-10 14:48:12 : - Waiting for itom-pg-backup to be ready\n[INFO] 2021-09-10 14:48:12 : - Waiting for cdfapiserver-postgresql to be ready\n[INFO] 2021-09-10 14:48:13 : - Waiting for itom-postgresql to be ready\n[INFO] 2021-09-10 14:48:14 : - Stopping components before restoring\n[INFO] 2021-09-10 14:49:19 : - Triggering database restore\n[INFO] 2021-09-10 14:49:22 : - Waiting for restore to complete\n[INFO] 2021-09-10 14:49:36 : - Starting components after restore\n\nRestore complete.</code></pre>\n<div class=\"transcontent stn-end--restoreembeddb\"> </div>\n<h2>Restore etcd data</h2>\n<div class=\"transcontent stn-begin--RestoreEtcd\"> </div>\n<p>Restore etcd data according to your deployment.</p>\n<div class=\"transcontent stn-begin--RestoreEtcdIntro\"> </div>\n<div class=\"Admonition_Note\">\n<p><span class=\"autonumber\">Note</span><br/>\nThe parameters listed in the sections below can be found in the backup package. Run the following command to get the parameters:</p>\n<pre><code>tar zxvf cdf-br-control1.example.net-YYYYMMDDHHMMSS.tar.gz\nsource cdf_backup/cm/basic_config.env</code></pre>\n<p>Replace &lt;Control_Plane_Node1&gt;, &lt;Control_Plane_Node2&gt;, and &lt;Control_Plane_Node3&gt; with the full FQDN hostname of the three control plane nodes respectively.</p>\n</div>\n<div class=\"transcontent stn-end--RestoreEtcdIntro\"> </div>\n<h3>In a single-control plane node deployment</h3>\n<div class=\"transcontent stn-begin--RestoreEtcdSingle\"> </div>\n<p>Follow the steps below on the control plane node to restore etcd data in single-control plane node deployment environment.</p>\n<ol>\n<li>Run the following command to export parameters on the control plane node:<br/>\n<code>export THIS_NODE=&lt;Control_Plane_Node_FQDN&gt;</code></li>\n<li>Restore etcdv3 data with the following command:<br/>\n<code>ETCDCTL_API=3 etcdctl snapshot restore &lt;backup path&gt;/cdf_backup/etcd/snapshot.db --name ${THIS_NODE} --initial-cluster=${THIS_NODE}=https://${THIS_NODE}:2380 --initial-cluster-token etcd-cluster-1 --initial-advertise-peer-urls https://${THIS_NODE}:2380</code></li>\n<li>Run the following command to move the $CDF_HOME/runconf/etcd.yaml file to <code>/tmp/etcd.yaml</code>.<br/>\n<code>mv $CDF_HOME/runconf/etcd.yaml /tmp/etcd.yaml</code></li>\n<li>Run the following command to make sure the etcd container is stopped. If there is no return value, the pod is stopped. You need to wait till there is no return value of this command.<br/>\n<code>crictl ps | grep \"etcd\"</code></li>\n<li>Move etcdv3 data to <i>${CDF_HOME}/data/etcd/data</i> with the following commands:<br/>\n<code>rm -rf ${RUNTIME_CDFDATA_HOME}/etcd/data/member<br/>\n\t/bin/cp -r ${THIS_NODE}.etcd/member ${RUNTIME_CDFDATA_HOME}/etcd/data/member</code></li>\n<li>Run the following command to change permission of etcd data directory:<br/>\n<code>chown -R ${SYSTEM_USER_ID}:${SYSTEM_GROUP_ID} ${RUNTIME_CDFDATA_HOME}/etcd/data</code></li>\n<li>Move the <code>/tmp/etcd.yaml</code>to <code>$CDF_HOME/runconf/etcd.yaml</code><br/>\n<code>mv /tmp/etcd.yaml $CDF_HOME/runconf/etcd.yaml<br/>\n\t$CDF_HOME/bin/kube-restart.sh -y </code></li>\n</ol>\n<div class=\"transcontent stn-end--RestoreEtcdSingle\"> </div>\n<h3>In a multiple-control plane node deployment</h3>\n<div class=\"transcontent stn-begin--RestoreEtcdMultiple\"> </div>\n<p>Follow the steps on each control plane node below to restore the etcd data in multiple-control plane node deployment environment.</p>\n<ol>\n<li>Export parameters on the three control plane nodes with the following commands. Use the IPv4 address if this VM is installed with IPv4 address. Use the full FQDN if this VM is installed with FQDN.<br/>\n<code>export Control_Plane_Node1=&lt;Control_Plane_Node1&gt;</code><br/>\n<code>export Control_Plane_Node2=&lt;Control_Plane_Node2&gt;</code><br/>\n<code>export Control_Plane_Node3=&lt;Control_Plane_Node3&gt;</code></li>\n<li>Log on to the control plane node where you created the snapshot.db etcd backup file.</li>\n<li>Run the following command to get the etcd data directory permission:<br/>\n<code>ls -l ${RUNTIME_CDFDATA_HOME}/etcd/data</code></li>\n<li>Restore etcdv3 data with the following commands:\n\t<pre><code>ETCDCTL_API=3 etcdctl snapshot restore &lt;backup path&gt;/cdf_backup/etcd/snapshot.db --name ${Control_Plane_Node1} \\\n--initial-cluster=${Control_Plane_Node1}=https://${Control_Plane_Node1}:2380,${Control_Plane_Node2}=https://${Control_Plane_Node2}:2380,${Control_Plane_Node3}=https://${Control_Plane_Node3}:2380 \\\n--initial-cluster-token etcd-cluster-1 --initial-advertise-peer-urls https://${Control_Plane_Node1}:2380\nETCDCTL_API=3 etcdctl snapshot restore &lt;backup path&gt;/cdf_backup/etcd/snapshot.db --name ${Control_Plane_Node2} \\\n--initial-cluster=${Control_Plane_Node1}=https://${Control_Plane_Node1}:2380,${Control_Plane_Node2}=https://${Control_Plane_Node2}:2380,${Control_Plane_Node3}=https://${Control_Plane_Node3}:2380 \\\n--initial-cluster-token etcd-cluster-1 --initial-advertise-peer-urls https://${Control_Plane_Node2}:2380\nETCDCTL_API=3 etcdctl snapshot restore &lt;backup path&gt;/cdf_backup/etcd/snapshot.db --name ${Control_Plane_Node3} \\\n--initial-cluster=${Control_Plane_Node1}=https://${Control_Plane_Node1}:2380,${Control_Plane_Node2}=https://${Control_Plane_Node2}:2380,${Control_Plane_Node3}=https://${Control_Plane_Node3}:2380 \\\n--initial-cluster-token etcd-cluster-1 --initial-advertise-peer-urls https://${Control_Plane_Node3}:2380</code></pre>\n</li>\n<li>Run the following command to stop etcd container on all the control plane nodes :<br/>\n<code>mv $CDF_HOME/runconf/etcd.yaml /tmp/etcd.yaml</code></li>\n<li>Run the following command on each control plane node to check the status of the etcd container. Wait till there is no return value.<br/>\n<code>crictl ps | grep “etcd”</code></li>\n<li>Run the following command on all control plane nodes, one by one:\n\t<pre><code>rm -rf ${RUNTIME_CDFDATA_HOME}/etcd/data/member  </code></pre>\n</li>\n<li>Run the following commands on the control plane node where you created the snapshot.db etcd backup file to move etcdv3 data to ${CDF_HOME}/data/etcd/data:\n\t<pre><code>scp -r ${Control_Plane_Node1}.etcd/member root@${Control_Plane_Node1}:${RUNTIME_CDFDATA_HOME}/etcd/data/member\nscp -r ${Control_Plane_Node2}.etcd/member root@${Control_Plane_Node2}:${RUNTIME_CDFDATA_HOME}/etcd/data/member\nscp -r ${Control_Plane_Node3}.etcd/member root@${Control_Plane_Node3}:${RUNTIME_CDFDATA_HOME}/etcd/data/member</code></pre>\n</li>\n<li>Run the following command to change permission of etcd data directory in all the control plane nodes:<br/>\n<code>chown -R ${SYSTEM_USER_ID}:${SYSTEM_GROUP_ID} ${RUNTIME_CDFDATA_HOME}/etcd/data</code></li>\n<li>Run the following commands to start etcd container on all the control plane nodes concurrently:<br/>\n<code>mv /tmp/etcd.yaml $CDF_HOME/runconf/</code><br/>\n<code>$CDF_HOME/bin/kube-restart.sh -y</code></li>\n</ol>\n<div class=\"transcontent stn-end--RestoreEtcdMultiple\"> </div>\n<h3>Troubleshooting</h3>\n<div class=\"transcontent stn-begin--RestoreEtcdTroubleshoot\"> </div>\n<p>If you failed to start etcd container when trying to restore etcd, perform the following steps to restart etcd container.</p>\n<ol>\n<li>Run the following command to stop etcd container.<br/>\n<code>mv $CDF_HOME/runconf/etcd.yaml /tmp/</code></li>\n<li>Run the following command to make sure the etcd container is stopped. If there is no return value, the etcd pod is stopped.<br/>\n<code>crictl ps | grep \"etcd\"</code></li>\n<li>Run the following command to restart etcd container.<br/>\n<code>mv /tmp/etcd.yaml $CDF_HOME/runconf/</code><br/>\n<code>$CDF_HOME/bin/kube-restart.sh -y</code></li>\n</ol>\n<div class=\"transcontent stn-end--RestoreEtcdTroubleshoot\"> </div>\n<div class=\"transcontent stn-end--RestoreEtcd\"> </div>\n<h2>Restore NFS server</h2>\n<div class=\"transcontent stn-begin--NFS\"> </div>\n<p>There are two ways to restore NFS server when an NFS server crashed.</p>\n<ul>\n<li>Restore NFS server to the original NFS server and path.</li>\n<li>Restore NFS server to a new NFS server.</li>\n</ul>\n<h3 class=\"mw-headline\" id=\"Restore_NFS_server_to_the_orignal_NFS_server_and_path\">Restore NFS server to the original NFS server and path</h3>\n<p>Restore NFS server to the original NFS server and path with the following steps.</p>\n<ol>\n<li>Back up NFS server data regularly.</li>\n<li>Use the same hostname or IPv4 and directory to set a new NFS server on a new node to replace the old NFS server and directory.</li>\n<li>Restore the NFS data into the new NFS server path.</li>\n</ol>\n<h3 class=\"mw-headline\" id=\"Restore_old_NFS_to_a_new_NFS_server\">Restore old NFS to a new NFS server</h3>\n<p>Restore data into a new NFS server with the following steps.</p>\n<ol>\n<li>Back up NFS server data regularly.</li>\n<li>Set up new NFS paths and restore the NFS data from the old path to the new path. You need to change the persistent volume path with new NFS paths one by one with the steps listed below.\n\t<div class=\"Admonition_Note\">\n<p><span class=\"autonumber\">Note</span><br/>\n\tYou can only change the server and path for the persistent volume claims(PVC).<br/>\n\tAfter changing the persistent volume(PV) information, you must restart Kubernetes.</p>\n</div>\n</li>\n</ol>\n<p>Change the PVs after OMT has been installed successfully. See the \"Change the persistent volumes (PVs) after you install OMT\" topic under the <strong>Administer</strong> section for detailed steps.</p>\n<div class=\"transcontent stn-end--NFS\"> </div>\n<h2>Restore the vault-params-key Vault keys</h2>\n<div class=\"transcontent stn-begin--VaultKeys\"> </div>\n<ol>\n<li>Run the following commands in sequence on the first control plane node to obtain the ROLE_ID:\n\t<pre>source /etc/profile.d/itom-cdf.sh\nITOM_VAULT_IP=$(kubectl get svc -n $CDF_NAMESPACE itom-vault -o json|jq -r .spec.clusterIP)\nexport VAULT_ADDR=\"https://${ITOM_VAULT_IP}:8200\"\nCDF_APISERVER=$(kubectl get pods -n $CDF_NAMESPACE|grep cdf-apiserver|awk '{print $1}')\nVAULT_TOKEN=$(kubectl exec -it $CDF_APISERVER -c cdf-apiserver -n $CDF_NAMESPACE -- cat /run/secrets/boostport.com/vault-token)\necho $VAULT_TOKEN\nexport VAULT_TOKEN=\"&lt;VAULT_TOKEN&gt;\"\nROLE_ID=$(vault read -tls-skip-verify -format=json auth/approle/role/${CDF_NAMESPACE}-baseinfra/role-id|jq -r  .data.role_id)\n</pre>\n<p> <br/>\n</p>\n\tIn these commands, <code>&lt;VAULT_TOKEN&gt;</code> is a placeholder for the name of the Vault token. You can get it by running the command: <code>echo $VAULT_TOKEN</code></li>\n<li>Run the following commands in sequence to restore each <code>vault-params-key</code> Vault key that you backed up to Vault:\n\t<pre>VALUE=$(cat &lt;vault-params-key&gt;.json)\nVAULT_PARAMS_KEY=&lt;vault-params-key&gt;\nvault write -tls-skip-verify -field=value itom/suite/${ROLE_ID}/${VAULT_PARAMS_KEY}  value=\"${VALUE}\"\n</pre>\n<p> <br/>\n</p>\n\tIn these commands, <code>&lt;vault-params-key&gt;</code> is a placeholder for the name of the Vault key backup file. You can get it from the backup file in the cdf_backup/vault directory.</li>\n</ol>\n<div class=\"transcontent stn-end--VaultKeys\"> </div><span class=\"snippet-end\" data-sname=\"423-25.4-423-25.3-RestoreCDF||N\"></span><div class=\"transcontent stn-begin--relatedTopics\"></div><h2 class=\"mw-headline\" id=\"Related_topics\">Related topics</h2><p><a href=\"/doc/423/26.1/upgraderestorecdf\" title=\"SMAX:25.1/UpgradeRestoreCdf\">Restore OMT</a></p><div class=\"transcontent stn-end--relatedTopics\"></div></div>",
  "modifiedon": "2025-10-24 08:51:12"
}