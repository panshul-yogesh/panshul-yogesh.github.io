{
  "title": "Scenario 1: Kubernetes configurations are corrupted",
  "content": "<div class=\"mw-parser-output\"><p class=\"mw-empty-elt\"></p><div class=\"mw-parser-output\"><p>In this scenario, one or more Kubernetes configurations of the suite and OMT (for example, configmaps and deployments) were mistakenly changed. You need to restore them to the latest correct configuration. Choose the restore solution based on your managed Kubernetes deployment platforms. </p><h2>Prerequisites</h2><p>Before you start, make sure:</p><ul><li>You have backed up the Kubernetes configurations using Velero.</li><li>The Kubernetes cluster is functioning.</li><li>There is no data corruption in the storage and databases.</li></ul><h2>Restore on Kubernetes configurations Managed Kubernetes</h2><div class=\"admonition\"><div class=\"admonition-icon admonition-important-icon\"></div><div class=\"admonition-content admonition-important-content\">You can only use the restore process when the <code>PersistentVolumeReclaimPolicy</code> is <code>Retain</code>. Otherwise, data might be lost during restoring.</div></div><h3>Restore OMT deployment</h3><p>Perform the following steps to restore OMT deployment if you've removed the OMT namespace by accident.</p><ol><li>Run the following command to create the OMT namespace. In ns admin mode, ask the cluster administrator to create the OMT namespace.\n\t<div contenteditable=\"false\" tabindex=\"-1\"><div contenteditable=\"false\" tabindex=\"-1\"><pre><code>source $HOME/itom-cdf.sh\nkubectl create ns $CDF_NAMESPACE</code></pre></div></div></li><li>Refer to <a href=\"/doc/423/26.1/installveleromanagedk8s#Enable_the_capability_manually_after_installation\" title=\"Enable the capability manually after installation\">Enable the capability manually after installation</a> and enable the Kubernetes backup capability. You can skip the preparation steps.\n\t<div class=\"admonition\"><div class=\"admonition-icon admonition-important-icon\"></div><div class=\"admonition-content admonition-important-content\"><br/>\n\tMake sure the provided bucket information (including bucket name, bucket region, resource group name, and resource account name) and the credential file or the secret name are the same as the ones provided when installing. Otherwise, you can't get the available backup data. You can get the deploy commands of the <code>itom-velero</code> chart in the installation log file under <code>$CDF_HOME/log/install</code> on the bastion node.</div></div></li><li>After you've enabled the Kubernetes backup capability, run the following command to get the available backup:\n\t<div contenteditable=\"false\" tabindex=\"-1\"><div contenteditable=\"false\" tabindex=\"-1\"><pre><code>velero get backup</code></pre></div></div></li><li>Run the following command to delete all PVs used by OMT deployment:\n\t<div contenteditable=\"false\" tabindex=\"-1\"><div contenteditable=\"false\" tabindex=\"-1\"><pre><code>pvs=$(kubectl get pv -o custom-columns=NAME:.metadata.name,CLAIM:.spec.claimRef.namespace| grep $CDF_NAMESPACE | awk '{print $1}')\n\nfor pv in $pvs;do kubectl delete pv $pv;done</code></pre></div></div></li><li>Run the following command to restore deployment from backup:\n\t<div contenteditable=\"false\" tabindex=\"-1\"><div contenteditable=\"false\" tabindex=\"-1\"><pre><code>velero create restore &lt;restore name&gt; --from-backup &lt;backup name&gt; --include-namespaces $CDF_NAMESPACE --include-cluster-resources</code></pre></div></div></li><li>Run the following command to check the pod status and make sure all pods are running:\n\t<div contenteditable=\"false\" tabindex=\"-1\"><div contenteditable=\"false\" tabindex=\"-1\"><pre><code>kubectl get pods -n $CDF_NAMESPACE</code></pre></div></div></li><li>Run the following command to add labels or annotations on the namespace and the service account. Replace <code>&lt;backup name&gt;</code> with the real value and make sure the backup is the same as the one used for the restore.\n\t<div contenteditable=\"false\" tabindex=\"-1\"><div contenteditable=\"false\" tabindex=\"-1\"><pre><code>mkdir -p $TMP_FOLDER/k8s-backup-data &amp;&amp; cd $TMP_FOLDER/k8s-backup-data\nvelero backup download &lt;backup name&gt; -o ./k8s-backup-data.tar.gz\ntar -zxf k8s-backup-data.tar.gz\ndeploymentName=$(cat ./resources/namespaces/cluster/$CDF_NAMESPACE.json | jq -r '.metadata.labels.\"deployments.microfocus.com/deployment-name\"')\ndeploymentUuid=$(cat ./resources/namespaces/cluster/$CDF_NAMESPACE.json | jq -r '.metadata.labels.\"deployments.microfocus.com/deployment-uuid\"')\nkubectl label ns $CDF_NAMESPACE deployments.microfocus.com/deployment-uuid=$deploymentUuid\nkubectl label ns $CDF_NAMESPACE deployments.microfocus.com/deployment-name=$deploymentName\nrm -rf $TMP_FOLDER/k8s-backup-data</code></pre></div></div></li></ol><div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><br/>\nIf you've configured <code>aws-load-balancer-controller</code> on the cluster, bind FQDN with the updated Application Load Balancer (ALB) on AWS Route 53 console after restoring OMT deployment. Therefore, you can access the Management Portal through browsers.</div></div><h3>Restore suite deployment</h3><span class=\"snippet-start\" data-sname=\"423-25.4-423-25.3-VeleroBackup|restoresuite|N\"></span><p>Perform the following steps to restore application deployment if you've removed the application namespace by accident.</p>\n<ol>\n<li>Run the following command to check if there are available Velero backups before restoring. The Velero backups should be in <code>Completed</code> status and not expired:\n\n\t<div contenteditable=\"false\" tabindex=\"-1\">\n<div contenteditable=\"false\" tabindex=\"-1\">\n<pre><code>velero get backup</code></pre>\n</div>\n</div>\n</li>\n<li>Run the following command to delete all PVs used by application deployment. Replace <code>&lt;suite namespace&gt;</code> with the real value.\n\t<div contenteditable=\"false\" tabindex=\"-1\">\n<div contenteditable=\"false\" tabindex=\"-1\">\n<pre><code>export suite_ns=&lt;suite namespace&gt;\npvs=$(kubectl get pv -o custom-columns=NAME:.metadata.name,CLAIM:.spec.claimRef.namespace| grep $suite_ns | awk '{print $1}')\nfor pv in $pvs;do kubectl delete pv $pv;done</code></pre>\n</div>\n</div>\n</li>\n<li>Run the following command to restore application deployment from backup:\n\t<div contenteditable=\"false\" tabindex=\"-1\">\n<div contenteditable=\"false\" tabindex=\"-1\">\n<pre><code>velero create restore &lt;restore name&gt; --from-backup &lt;backup name&gt; --include-namespaces $suite_ns --include-cluster-resources</code></pre>\n</div>\n</div>\n</li>\n<li>Run the following command to check the pod status and make sure that all of the application pods are running:\n\t<div contenteditable=\"false\" tabindex=\"-1\">\n<div contenteditable=\"false\" tabindex=\"-1\">\n<pre><code>kubectl get pods -n $suite_ns</code></pre>\n</div>\n</div>\n</li>\n</ol>\n<p><br/>\n</p><span class=\"snippet-end\" data-sname=\"423-25.4-423-25.3-VeleroBackup|restoresuite|N\"></span><div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><ul><li>For AWS, after the Kubernetes configurations are restored, you need to bind the domain name with the updated Application Load Balancer (ALB) on the AWS Route 53 console. Go to <strong>Route 53</strong> &gt; <strong>Hosted zones</strong> , choose your domain name and choose your domain, click <strong>edit record</strong> and then change the load balancer to the new one. If your domain name isn't provided by Route 53, you may contact your domain name provider for this step.</li><li>For Azure, after the Kubernetes configurations are restored, you need to reconfigure the new <strong>IP address</strong> information in the <strong>Target</strong> of the AGW <b>Backend Pools</b> configurations. </li></ul></div></div><h2>Restore the Kubernetes configurations Embedded Kubernetes</h2><h3>Restore OMT deployment</h3><ol><li>Run these commands on the control plane node to delete the OMT namespaces and persistent volumes.\n\t<pre>kubectl delete ns core\npvs=$(kubectl get pv -o custom-columns=NAME:.metadata.name,CLAIM:.spec.claimRef.namespace| grep core | awk '{print $1}')\nfor pv in $pvs;do kubectl delete pv $pv;done</pre></li><li>Run this command on the control plane node to create the <strong>core</strong> namespace. This will install Velero in the <strong>core</strong> namespace.\n\t<pre> kubectl create ns core </pre></li><li>Run this command on the control plane node to create persistent volumes.\n\t<pre> # Replace &lt;nfs-server&gt; to your original nfs server address.\ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: itom-vol\nspec:\n  accessModes:\n   - ReadWriteMany\n  capacity:\n   storage: 5Gi\n  claimRef:\n   apiVersion: v1\n   kind: PersistentVolumeClaim\n   name: itom-vol-claim\n   namespace: core\n  nfs:\n   path: /var/vols/itom/core\n   server: &lt;nfs-server&gt;\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: cdf-default\n  volumeMode: Filesystem\nEOF</pre><p>Where: <code>&lt;nfs-server&gt;</code> Is your NFS server.</p></li><li>Run this command on the control plane node to create a persistent volume claim.\n\t<pre>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: itom-vol-claim\n  namespace: core\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 5Gi\n  storageClassName: cdf-default\n  volumeMode: Filesystem\n  volumeName: itom-vol\nstatus:\n  accessModes:\n   - ReadWriteMany\n  capacity:\n   storage: 5Gi\nEOF\n</pre></li><li>Refer to <a href=\"/doc/423/26.1/installveleromanagedk8s#Set_up_Velero_on_Embedded_Kubernetes\" title=\"Set up Velero on embedded Kubernetes \">Set up Velero on embedded Kubernetes </a> to enable the kubernetes  backup capability.</li><li>Verify you can get the Velero backup. The Velero backups should be in <code>Completed</code> status and not <code>expired.</code><pre>velero get backup</pre></li><li>Restore the Kubernetes configurations for OMT by running this command on the control plane node.\n\t<pre>velero create restore &lt;restore name&gt; --from-backup &lt;backup name&gt; --include-namespaces core --include-cluster-resources </pre></li><li>Run the following command to check the pod status and make sure that all the application pods are running.\n\t<pre>kubectl get pods -n core</pre><p>This command returns a list of abnormal pods. If RabbitMQ isn't ready, see <a href=\"/doc/423/26.1/rabbitmqnotstart\" title=\"RabbitMQ isn't ready\">RabbitMQ isn't ready</a>. When it returns no result, all the suite pods are ready and you can perform quick tests to make sure the restoration is successful.</p></li></ol><h3>Restore suite deployment</h3><p>Perform the following steps to restore the suite deployment.</p><ol><li>Run the following command to check if there are available Velero backups before restoring. The Velero backups should be in <code>Completed</code> status and not expired:\n\n\t<pre>velero get backup</pre></li><li>Run these commands on the control plane node to delete the SMAX namespaces and persistent volumes used by application deployment. Replace <code>&lt;suite namespace&gt;</code> with the real value.\n\t<pre>export suite_ns=&lt;suite namespace&gt;\nkubectl delete ns $suite_ns\npvs=$(kubectl get pv -o custom-columns=NAME:.metadata.name,CLAIM:.spec.claimRef.namespace| grep $suite_ns | awk '{print $1}') \nfor pv in $pvs;do kubectl delete pv $pv;done</pre></li><li>Run the following command to restore application deployment from backup:\n\t<pre>velero create restore &lt;restore name&gt; --from-backup &lt;backup name&gt; --include-namespaces $suite_ns --include-cluster-resources --preserve-nodeports </pre></li><li>Run the following command to check the pod status and make sure that all the application pods are running:\n\t<pre>kubectl get pods -n $suite_ns</pre></li></ol></div><p class=\"mw-empty-elt\"></p></div>",
  "modifiedon": "2025-10-24 08:51:12"
}