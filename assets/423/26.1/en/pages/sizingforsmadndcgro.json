{
  "title": "Hardware requirements for option 3: SMA + DND + OO + CMP FinOps",
  "content": "<div class=\"mw-parser-output\">\n<div></div>\n<p>If you plan to use deployment option 3 (SMA + DND + OO + CMP FinOps), plan your hardware resources as follows. </p>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><div>In this release, support for AlloyDB Omni is under <strong>controlled availability</strong>. Please contact our product management team before you begin the implementation in your production environment.</div></div></div>\n<p>To set up OO with a common database, you can choose to connect OO to the same database instance as the rest of the suite components. For hardware requirements, see the below table <strong>Standard hardware requirements with a common database.</strong></p>\n<ul style=\"padding-left: 0px;\"><li style=\"margin-left: 28px; position: relative;\">To deploy OO for a 5-tenant support, use the <strong>oo_size_values.yaml</strong> file provided <a href=\"/doc/423/26.1/399-oocpreparecreateyaml\" title=\"here\">here</a>. </li><li style=\"margin-left: 28px; position: relative;\">To deploy OO for a 20-tenant support, use the <strong>oo_size_values.yaml</strong> file provided <a href=\"/doc/423/26.1/399-oocpreparecreateyaml\" title=\"here\">here</a>.</li></ul>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\">Target size</th>\n<th rowspan=\"2\">Concurrent users</th>\n<th rowspan=\"2\">Number of OO tenants</th>\n<th colspan=\"3\">Control plane node</th>\n<th colspan=\"3\">Worker node</th>\n<th colspan=\"3\">NFS</th>\n<th colspan=\"3\">Suite Database - PostgreSQL</th>\n<th></th>\n<th colspan=\"4\" rowspan=\"1\">Suite Database - AlloyDB Omni</th>\n</tr>\n<tr>\n<th>Configuration</th>\n<th><strong>Storage</strong></th>\n<th>Quantity</th>\n<th>Configuration</th>\n<th><strong>Storage</strong></th>\n<th>Quantity</th>\n<th>Configuration</th>\n<th><strong>Storage*</strong></th>\n<th>Quantity</th>\n<th>Configuration</th>\n<th><strong>Storage</strong></th>\n<th>Quantity</th>\n<th>max_ connections</th>\n<th>Configuration</th>\n<th><strong>Storage</strong></th>\n<th>Quantity</th>\n<th>max_ connections</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>100~400</td>\n<td>1 to 5</td>\n<td>4 CPU 12 GB RAM</td>\n<td>200 GB</td>\n<td>3 (HA)</td>\n<td>8 CPU 32 GB RAM ~ 8 CPU 40 GB RAM</td>\n<td>400 ~ 500 GB</td>\n<td>7 ~ 8</td>\n<td>12 CPU 28 GB RAM</td>\n<td>1.7 TB</td>\n<td>1</td>\n<td>\n<p>24 CPU 128 GB ~ 24 CPU 144 GB RAM</p>\n</td>\n<td>1.2 TB ~ 5.4 TB</td>\n<td>1</td>\n<td>3400<sup>#</sup></td>\n<td>\n<p>24 CPU 128 GB ~ 24 CPU 144 GB RAM</p>\n</td>\n<td>1.2 TB ~ 5.4 TB</td>\n<td>1</td>\n<td>3400<sup>#</sup></td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>400~1000</td>\n<td>6 to 20</td>\n<td>4 CPU 16 GB RAM</td>\n<td>200 GB</td>\n<td>3 (HA)</td>\n<td>8 CPU 32 GB RAM</td>\n<td>400 ~ 500 GB</td>\n<td>14 ~ 15</td>\n<td>12 CPU 32 GB RAM</td>\n<td>2.3 TB</td>\n<td>1</td>\n<td>32 CPU 144 GB RAM ~ 32 CPU 208 GB RAM </td>\n<td>1.4 TB ~ 5.6 TB</td>\n<td>1</td>\n<td>7710<sup>##</sup></td>\n<td>32 CPU 144 GB RAM ~ 32 CPU 208 GB RAM </td>\n<td>1.4 TB ~ 5.6 TB</td>\n<td>1</td>\n<td>7710<sup>##</sup></td>\n</tr>\n</tbody>\n</table>\n<p>#The max_connections value for the combined database should be equal to or greater than 3400.</p>\n<p>##The <code>max_connections</code> value for the combined database should be equal to or greater than 7710.</p>\n<p>To determine standard hardware requirements with a dedicated database for OO, see<strong> </strong><a href=\"/doc/423/26.1/sizingforotheroptions#Additional_hardware_requirements_for_OO_Containerized\" title=\"Additional hardware requirements for OO containerized\">Additional hardware requirements for OO containerized</a>.</p>\n<p>For more information about NFS storage sizing, see <a href=\"/doc/423/26.1/managestoragesuite\" title=\"Manage persistent storage for the suite\">Manage persistent storage for the suite</a>.</p>\n<p>For more information on database settings, see <a href=\"/doc/423/26.1/prepareexternaldbs#Prepare_external_PostgreSQL_for_SMA\" title=\"Prepare external PostgreSQL\">Prepare external PostgreSQL</a>.</p>\n<p>For details on setting up AlloyDB Omni database for HA, see <a href=\"/doc/423/26.1/hak8salloydb\" title=\"Install AlloyDB Omni on Kubernetes for HA\">Prepare AlloyDB Omni for HA</a>.</p>\n<p>For Postgres and Vertica database HA configuration, follow recommendations provided in vendor-specific documentation. For example, for details on configuring Postgres HA, see <a href=\"/doc/423/26.1/hasqlpatroni\" title=\"High Availability PostgreSQL with Patroni\">High Availability PostgreSQL with Patroni</a>.</p>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><div><strong>Applicable only if you are using internal RAS </strong>Both OO Central and RAS containers use higher resources than other containers. To prevent OO Central and RAS pods from ending up on the same worker node, anti-affinity has been enabled between these two pods; however, if enough resources aren’t available on any node, the scheduler ignores anti-affinity and schedules both the pods on the same node. In such a situation, consider adding an additional worker node to prevent performance degradation.</div></div></div>\n<h2><a id=\"Additional_hardware_requirements_for_CMP_FinOps\" name=\"Additional_hardware_requirements_for_CMP_FinOps\" title=\"Additional hardware requirements for CMP FinOps\"></a>Additional hardware requirements for CMP FinOps</h2>\n<p>Use the table below to plan the additional hardware resources for CMP FinOps.</p>\n<table>\n<tbody>\n<tr>\n<th rowspan=\"2\"><b>Suite size (sizing profile)</b></th>\n<th rowspan=\"2\">\n<p>Number of Cloud Cost Provider Integrations</p>\n<p>(Total number of integrations across all CMP FinOps tenants)</p>\n</th>\n<th colspan=\"3\">Worker node</th>\n<th colspan=\"1\" rowspan=\"2\">Showback replica count</th>\n<th colspan=\"3\" rowspan=\"1\">Vertica EON database</th>\n</tr>\n<tr>\n<th>Instance type</th>\n<th>Volume</th>\n<th>Quantity</th>\n<th>Instance type</th>\n<th>Volume</th>\n<th>Vertica node count</th>\n</tr>\n<tr>\n<td>Small</td>\n<td>1 to 5</td>\n<td>\n<p>8 vCPU<br/>\n\t\t\t32 GB memory<br/>\n\t\t\t(m5.2xlarge)</p>\n</td>\n<td colspan=\"1\" rowspan=\"3\">Disk: 100 GB</td>\n<td>2-4</td>\n<td>2-4</td>\n<td>\n<p>16 vCPU<br/>\n\t\t\t128 GB memory<br/>\n\t\t\t(r5.4xlarge)</p>\n</td>\n<td colspan=\"1\" rowspan=\"3\">\n<p>Depot storage: 240 GB</p>\n<p>Catalog storage: 50 GB</p>\n<p>Temporary data storage: 100 GB</p>\n</td>\n<td>3</td>\n</tr>\n<tr>\n<td>Medium</td>\n<td>6 to 20</td>\n<td colspan=\"1\" rowspan=\"2\">\n<p>16 vCPU<br/>\n\t\t\t64 GB memory<br/>\n\t\t\t(m5.4xlarge)</p>\n</td>\n<td>4-6</td>\n<td>4-6</td>\n<td rowspan=\"2\">\n<p>32 vCPU<br/>\n\t\t\t256 GB memory<br/>\n\t\t\t(r5.8xlarge)</p>\n</td>\n<td>3-6</td>\n</tr>\n<tr>\n<td>Large</td>\n<td>21 to 30</td>\n<td>6-8</td>\n<td>6-8</td>\n<td>6-9</td>\n</tr>\n</tbody>\n</table>\n<p>Average load expected on CMP FinOps: 60,000,000 billing lines per month</p>\n<p><strong>NFS server sizing</strong></p>\n<table>\n<tbody>\n<tr>\n<th>Item</th>\n<th>Recommended specifications</th>\n</tr>\n<tr>\n<td colspan=\"2\"> CMP FinOps shares the same NFS server as the suite. You need to add the following additional resources to the NFS server.</td>\n</tr>\n<tr>\n<td>Free disk space</td>\n<td>200 GB (+500 GB/year for Azure or AWS governance reports)</td>\n</tr>\n</tbody>\n</table>\n<div class=\"admonition\"><div class=\"admonition-icon admonition-note-icon\"></div><div class=\"admonition-content admonition-note-content\"><div>  For NFS and Vertica database HA configuration, follow the recommendations provided in vendor-specific documentation.</div></div></div>\n<h3><a id=\"Performance_tuning_for_CMP_FinOps\" name=\"Performance_tuning_for_CMP_FinOps\" title=\"Performance tuning for CMP FinOps\"></a>Performance tuning for CMP FinOps</h3>\n<p>This section contains configuration changes required to support a higher number of Cloud Cost Provider integrations (small, medium, and large sizing profiles)</p>\n<h4><a id=\"Configure_node_group_on_your_OMT_portal\" name=\"Configure_node_group_on_your_OMT_portal\" title=\"Configure node group on your OMT portal\"></a>Configure node group on your OMT portal</h4>\n<p>On your OMT portal, configure the following parameters as part of node grouping based on your sizing profile.</p>\n<ul style=\"padding-left: 0px;\"><li style=\"margin-left: 28px; position: relative;\">Node group name. Example: <code>cgro</code></li><li style=\"margin-left: 28px; position: relative;\">Desired size (node count). Example: 8</li><li style=\"margin-left: 28px; position: relative;\">Add Kubernetes label. Example: Worker cgro.</li></ul>\n<h4><a id=\"Update_nodeSelector_parameter\" name=\"Update_nodeSelector_parameter\" title=\"Update 'nodeSelector' parameter\"></a>Update 'nodeSelector' parameter</h4>\n<ol style=\"padding-left: 0px;\"><li style=\"--number-width: 10px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Use the following steps to update the <code>nodeSelector</code> parameter.</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Locate the <code>itom-cgro-showback</code> sub-chart in the <b style=\"\">cgro-showback.yaml </b> file and copy the sub-chart details.\n\t<pre>itom-cgro-showback:\n    replicas: 1\n    global:\n      nodeSelector:\n</pre>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Save the copied content to a new YAML file (for example, <em>cgro_nodeSelector.yaml</em>), append the first line as the service chart name <code>cgro:</code>, and then then update the nodeSelector.\n\t<pre>cgro:\n  itom-cgro-showback:\n    replicas: 1\n    global:\n      nodeSelector:\n        Worker: \"cgro\"</pre>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following command to apply this update:\n\t<pre>helm get values $releaseName -n $NAMESPACE &gt; my-values.yaml\nhelm upgrade $releaseName &lt;ESM_HELM_CHART_PATH&gt; -n $NAMESPACE -f  my-values.yaml -f &lt;CUSTOM_REPLICAS.yaml&gt;\n</pre>\n<p>Where:</p>\n<ul style=\"padding-left: 0px;\"><li style=\"margin-left: 40px; position: relative;\"><em>&lt;ESM_HELM_CHART_PATH&gt;</em> is the absolute path of the helm package.</li><li style=\"margin-left: 40px; position: relative;\"><em>&lt;CUSTOM_REPLICAS.yaml&gt;</em> is your customized YAML file, which you prepared in the previous step. For example, <code>cgro_nodeSelector.yaml</code>.</li></ul>\n<p>Example:</p>\n<pre>helm upgrade sma esm-1.0.0+2x.x-xxx\\charts\\esm-1.0.0+2x.x-xxx.tgz -n itsma-xxx -f my-values.yaml -f cgro_nodeSelector.yaml</pre>\n<p>For more details, see <a href=\"/doc/423/26.1/ekssizing#Update_'nodeSelector'_parameter\" title=\"How to update nodeSelector\">How to update nodeSelector</a>.</p>\n</li><li style=\"--number-width: 12px; --number-spacing: 24px; margin-left: 36px; position: relative;\">To update the <code>CGRO_SHOWBACK_COLLECT_COPY_STREAM_RESOURCE_POOL_NAME</code> setting in the <strong>itom-cgro-showback-deployment</strong>:\n\t<ol start=\"1\" style=\"padding-left: 0px;\"><li style=\"--number-width: 21px; --number-spacing: 25px; margin-left: 49px; position: relative;\">Copy the following content to a new YAML file (for example, <em>cgro_copy_stream.yaml</em>):\n\t\t<pre><code>cgro:\n  itom-cgro-showback:\n    collect:\n      copy_stream_resource_pool_name: \"itom_finops_etl\"</code></pre>\n</li><li style=\"--number-width: 25px; --number-spacing: 29px; margin-left: 53px; position: relative;\">Change the value of the <code>copy_stream_resource_pool_name</code> property to the required value and save the file.</li></ol>\n</li><li style=\"--number-width: 12px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following command to apply this update:\n\t<pre><code>helm upgrade sma esm-1.0.0+2x.x-xxx\\charts\\esm-1.0.0+2x.x-xxx.tgz -n itsma-xxx -f my-values.yaml -f cgro_copy_stream.yaml</code></pre>\n</li></ol>\n<h4><a id=\"Separate_ETL_and_query_workloads\" name=\"Separate_ETL_and_query_workloads\" title=\"Separate ETL and query workloads\"></a>Separate ETL and query workloads</h4>\n<p>Perform the following tasks to divide ETL workloads and query workloads into separate resource pools.</p>\n<ol style=\"padding-left: 0px;\"><li style=\"--number-width: 10px; --number-spacing: 24px; margin-left: 36px; position: relative;\">On the Vertica database, as a database administrator, run the following command to create a custom ETL resource pool.\n\t<p>For small profile:</p>\n<pre> CREATE RESOURCE POOL itom_finops_etl MAXMEMORYSIZE '40%' PLANNEDCONCURRENCY 64 MAXCONCURRENCY 64;</pre>\n<p>For medium and large profiles:</p>\n<pre>CREATE RESOURCE POOL itom_finops_etl MAXMEMORYSIZE '40%' PLANNEDCONCURRENCY 128 MAXCONCURRENCY 128;</pre>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following commands to validate that the new resource pools are created:\n\t<pre>SELECT * from RESOURCE_POOLS\nSELECT * from RESOURCE_POOL_STATUS</pre>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Perform the following configuration changes on the Vertica database as a Vertica Database Administrator according to your profile size:\n\t<p>For small profile:</p>\n<pre>ALTER RESOURCE POOL general PLANNEDCONCURRENCY 32;\nALTER RESOURCE POOL general MAXCONCURRENCY 32;\n</pre>\n<p>For medium and large profiles:</p>\n<pre>ALTER RESOURCE POOL general PLANNEDCONCURRENCY 64;\nALTER RESOURCE POOL general MAXCONCURRENCY 64;</pre>\n</li><li style=\"--number-width: 13px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Run the following commands on the bastion host to restart the pods:\n\t<pre>kubectl scale deployment itom-cgro-costpolicy --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-costpolicy --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-insights --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-insights --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-insights-gateway --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-insights-gateway --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-policy-gateway --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-policy-gateway --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-showback --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-showback --replicas=1 -n &lt;itsma_namespace&gt;\n \nkubectl scale deployment itom-cgro-showback-gateway --replicas=0 -n &lt;itsma_namespace&gt;\nkubectl scale deployment itom-cgro-showback-gateway --replicas=1 -n &lt;itsma_namespace&gt;\n</pre>\n</li><li style=\"--number-width: 12px; --number-spacing: 24px; margin-left: 36px; position: relative;\">Scale up the showback pod as needed. For example, run the following command on the bastion host to scale up the replica to 2.\n\t<pre><code>kubectl scale deployment itom-cgro-showback --replicas=2 -n &lt;itsma_namespace&gt;</code></pre>\n\tSee the table above for specific replica counts based on workloads.</li></ol>\n<h2><a id=\"Related_topics\" name=\"Related_topics\" title=\"Related topics\"></a><span style=\"color: rgb(0, 115, 231); font-family: Roboto, sans-serif; font-size: 24px;\">Related topics</span></h2>\n<p><a href=\"/doc/423/26.1/plandeployment\" title=\"Plan the deployment\">Plan the deployment</a></p>\n</div>",
  "modifiedon": "2025-10-24 08:51:12"
}