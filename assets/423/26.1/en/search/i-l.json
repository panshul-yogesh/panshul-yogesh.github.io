[
  {
    "title": "Known issues",
    "content": "Category Issue ID Description Found in version Service Management OCTCL19XW2721453 Dev2Prod export or import fails when data records do not contain a UUID. See Workaround 25.3 Service Management OCTCR19XW2508184 When accessing the Related CIs tab for SACM records in Service Management, the tab fails to load and displays an error message similar to the following: Cannot read properties of undefined (reading 'map'). Note: You also need to upgrade UD/UCMDB 25.2.x to 25.4 or apply the hotfix mentioned in the 25.2 workaround. 25.2 Service Management OCTCR19XW2722859 Suite upgrade on AKS fails with Ingress should allow https only. error if the built-in Azure policy 'Kubernetes clusters should be accessible only over HTTPS' is enabled. See the workaround. 25.2 Service Management OCTCR19M2080015 When viewed in the Outlook web app, notification emails based on some notification templates contain awkward line breaks. See the workaround. 2023.05 Service Management N/A In SAM, the deployment infor",
    "url": "knownissues",
    "filename": "knownissues",
    "headings": [],
    "keywords": [
      "2020.11",
      "26.1",
      "2020.08",
      "2023.05",
      "2022.05",
      "2020.05",
      "25.2",
      "25.3",
      "25.4",
      "2.0",
      "2021.05",
      "known",
      "issues",
      "category",
      "issue",
      "id",
      "description",
      "found",
      "version",
      "service",
      "management",
      "octcl19xw2721453",
      "dev2prod",
      "export",
      "import",
      "fails",
      "data",
      "records",
      "contain",
      "uuid.",
      "see",
      "workaround",
      "octcr19xw2508184",
      "accessing",
      "related",
      "cis",
      "tab",
      "sacm",
      "load",
      "displays",
      "error",
      "message",
      "similar",
      "following",
      "cannot",
      "read",
      "properties",
      "undefined",
      "reading",
      "map",
      "note",
      "need",
      "upgrade",
      "ud",
      "ucmdb",
      "25.2.x",
      "apply",
      "hotfix",
      "mentioned",
      "workaround.",
      "octcr19xw2722859",
      "suite",
      "aks",
      "ingress",
      "allow",
      "https",
      "only.",
      "built-in",
      "azure",
      "policy",
      "kubernetes",
      "clusters",
      "accessible",
      "over",
      "enabled.",
      "octcr19m2080015",
      "viewed",
      "outlook",
      "web",
      "app",
      "notification",
      "emails",
      "based",
      "templates",
      "awkward",
      "line",
      "breaks.",
      "sam",
      "deployment",
      "information",
      "vmware",
      "vrealize",
      "operations",
      "oracle",
      "products",
      "isn",
      "visible.",
      "octcr19m1320762",
      "native",
      "enabled"
    ],
    "language": "en",
    "word_count": 109,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "known issues",
    "contentLower": "category issue id description found in version service management octcl19xw2721453 dev2prod export or import fails when data records do not contain a uuid. see workaround 25.3 service management octcr19xw2508184 when accessing the related cis tab for sacm records in service management, the tab fails to load and displays an error message similar to the following: cannot read properties of undefined (reading 'map'). note: you also need to upgrade ud/ucmdb 25.2.x to 25.4 or apply the hotfix mentioned in the 25.2 workaround. 25.2 service management octcr19xw2722859 suite upgrade on aks fails with ingress should allow https only. error if the built-in azure policy 'kubernetes clusters should be accessible only over https' is enabled. see the workaround. 25.2 service management octcr19m2080015 when viewed in the outlook web app, notification emails based on some notification templates contain awkward line breaks. see the workaround. 2023.05 service management n/a in sam, the deployment infor",
    "keywordsLower": [
      "2020.11",
      "26.1",
      "2020.08",
      "2023.05",
      "2022.05",
      "2020.05",
      "25.2",
      "25.3",
      "25.4",
      "2.0",
      "2021.05",
      "known",
      "issues",
      "category",
      "issue",
      "id",
      "description",
      "found",
      "version",
      "service",
      "management",
      "octcl19xw2721453",
      "dev2prod",
      "export",
      "import",
      "fails",
      "data",
      "records",
      "contain",
      "uuid.",
      "see",
      "workaround",
      "octcr19xw2508184",
      "accessing",
      "related",
      "cis",
      "tab",
      "sacm",
      "load",
      "displays",
      "error",
      "message",
      "similar",
      "following",
      "cannot",
      "read",
      "properties",
      "undefined",
      "reading",
      "map",
      "note",
      "need",
      "upgrade",
      "ud",
      "ucmdb",
      "25.2.x",
      "apply",
      "hotfix",
      "mentioned",
      "workaround.",
      "octcr19xw2722859",
      "suite",
      "aks",
      "ingress",
      "allow",
      "https",
      "only.",
      "built-in",
      "azure",
      "policy",
      "kubernetes",
      "clusters",
      "accessible",
      "over",
      "enabled.",
      "octcr19m2080015",
      "viewed",
      "outlook",
      "web",
      "app",
      "notification",
      "emails",
      "based",
      "templates",
      "awkward",
      "line",
      "breaks.",
      "sam",
      "deployment",
      "information",
      "vmware",
      "vrealize",
      "operations",
      "oracle",
      "products",
      "isn",
      "visible.",
      "octcr19m1320762",
      "native",
      "enabled"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Licensing",
    "content": "Service Management is offered in the license editions of Service Management Express and Service Management Premium. Asset Management is offered as an Asset Management edition. You need a Cloud Management license if you would like to include the Cloud Management capability in Service Management. Service Management Express and Service Management Premium: provide access to IT Service Management and Extended Service Management solutions.Asset Management: provides access to a consolidated Asset Management solution. A Service Management Premium license is required if you would like to include the Asset Management capability in Service Management. Asset Management can also be deployed independently using an Asset Management license if you intend to use Asset Management capabilities only.Cloud Management: provides access to a consolidated Cloud Management solution. A Cloud Management license is required if you would like to include the Cloud Management capability in Service Management. Cloud M",
    "url": "licensing",
    "filename": "licensing",
    "headings": [],
    "keywords": [
      "licensing",
      "service",
      "management",
      "offered",
      "license",
      "editions",
      "express",
      "premium.",
      "asset",
      "edition.",
      "need",
      "cloud",
      "like",
      "include",
      "capability",
      "management.",
      "premium",
      "provide",
      "access",
      "extended",
      "solutions.asset",
      "provides",
      "consolidated",
      "solution.",
      "required",
      "deployed",
      "independently",
      "intend",
      "capabilities",
      "only.cloud",
      "only.",
      "following",
      "tables",
      "list",
      "features",
      "products",
      "available",
      "license.",
      "module",
      "note",
      "portfolio",
      "catalog",
      "limited",
      "cases.",
      "time",
      "period",
      "level",
      "company",
      "basic",
      "companies",
      "brands",
      "tabs.",
      "csm-extended",
      "product",
      "models",
      "instances",
      "versions",
      "modules",
      "environments",
      "licenses",
      "entitlements",
      "change",
      "release",
      "knowledge",
      "configuration",
      "finance",
      "tab",
      "device",
      "infrastructure",
      "peripheral",
      "subscription",
      "records",
      "enterprise",
      "native",
      "sacm",
      "impact",
      "analysis",
      "model",
      "ucmdb.",
      "manually",
      "created",
      "imported",
      "discovered",
      "discovery",
      "licenses.",
      "survey",
      "design",
      "supports",
      "aws",
      "microsoft",
      "azure",
      "gcp.",
      "request",
      "incident",
      "problem",
      "on-call",
      "schedule",
      "deployment",
      "operations",
      "automated"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "licensing",
    "contentLower": "service management is offered in the license editions of service management express and service management premium. asset management is offered as an asset management edition. you need a cloud management license if you would like to include the cloud management capability in service management. service management express and service management premium: provide access to it service management and extended service management solutions.asset management: provides access to a consolidated asset management solution. a service management premium license is required if you would like to include the asset management capability in service management. asset management can also be deployed independently using an asset management license if you intend to use asset management capabilities only.cloud management: provides access to a consolidated cloud management solution. a cloud management license is required if you would like to include the cloud management capability in service management. cloud m",
    "keywordsLower": [
      "licensing",
      "service",
      "management",
      "offered",
      "license",
      "editions",
      "express",
      "premium.",
      "asset",
      "edition.",
      "need",
      "cloud",
      "like",
      "include",
      "capability",
      "management.",
      "premium",
      "provide",
      "access",
      "extended",
      "solutions.asset",
      "provides",
      "consolidated",
      "solution.",
      "required",
      "deployed",
      "independently",
      "intend",
      "capabilities",
      "only.cloud",
      "only.",
      "following",
      "tables",
      "list",
      "features",
      "products",
      "available",
      "license.",
      "module",
      "note",
      "portfolio",
      "catalog",
      "limited",
      "cases.",
      "time",
      "period",
      "level",
      "company",
      "basic",
      "companies",
      "brands",
      "tabs.",
      "csm-extended",
      "product",
      "models",
      "instances",
      "versions",
      "modules",
      "environments",
      "licenses",
      "entitlements",
      "change",
      "release",
      "knowledge",
      "configuration",
      "finance",
      "tab",
      "device",
      "infrastructure",
      "peripheral",
      "subscription",
      "records",
      "enterprise",
      "native",
      "sacm",
      "impact",
      "analysis",
      "model",
      "ucmdb.",
      "manually",
      "created",
      "imported",
      "discovered",
      "discovery",
      "licenses.",
      "survey",
      "design",
      "supports",
      "aws",
      "microsoft",
      "azure",
      "gcp.",
      "request",
      "incident",
      "problem",
      "on-call",
      "schedule",
      "deployment",
      "operations",
      "automated"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Key concepts",
    "content": "This guide encompasses the Service Management applications inside of Service Management,including Service Request, Incident, Change, and many others. Join implementer Glenn Walker as he learns what functionality is available in Service Management, investigating the \"what\" and \"why\" aspects of the product rather than the \"how\". Meet Glenn. Glenn is the newly appointed administrator for a new implementation of the OpenText Service Management product at his company, ACME Products Inc. ACME provides widgets for many other companies and handles shipping them worldwide from a number of locations. ACME has grown over the years, and the number of systems it needs to ensure the delivery of its widgets has grown as well. Additionally, the company has invested in internal products to help it run smoothly. This growth has resulted in the need for IT to run in an agile and streamlined manner, which was one of the biggest reasons for ACME to go with Service Management. Join us as we follow the imple",
    "url": "keyconceptssmax",
    "filename": "keyconceptssmax",
    "headings": [],
    "keywords": [
      "key",
      "concepts",
      "guide",
      "encompasses",
      "service",
      "management",
      "applications",
      "inside",
      "including",
      "request",
      "incident",
      "change",
      "many",
      "others.",
      "join",
      "implementer",
      "glenn",
      "walker",
      "learns",
      "what",
      "functionality",
      "available",
      "investigating",
      "aspects",
      "product",
      "rather",
      "meet",
      "glenn.",
      "newly",
      "appointed",
      "administrator",
      "new",
      "implementation",
      "opentext",
      "company",
      "acme",
      "products",
      "inc.",
      "provides",
      "widgets",
      "companies",
      "handles",
      "shipping",
      "worldwide",
      "number",
      "locations.",
      "grown",
      "over",
      "years",
      "systems",
      "needs",
      "ensure",
      "delivery",
      "well.",
      "additionally",
      "invested",
      "internal",
      "help",
      "run",
      "smoothly.",
      "growth",
      "resulted",
      "need",
      "agile",
      "streamlined",
      "manner",
      "one",
      "biggest",
      "reasons",
      "go",
      "management.",
      "follow",
      "process",
      "uses",
      "get",
      "automation",
      "running.",
      "document",
      "system",
      "designed",
      "continuing",
      "demands",
      "offer",
      "best",
      "customers.",
      "focusing",
      "decisions",
      "makes",
      "result",
      "quick",
      "efficient",
      "implementation.",
      "involves",
      "iterative",
      "approach",
      "problem",
      "solving",
      "delivering",
      "value",
      "quickly"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "key concepts",
    "contentLower": "this guide encompasses the service management applications inside of service management,including service request, incident, change, and many others. join implementer glenn walker as he learns what functionality is available in service management, investigating the \"what\" and \"why\" aspects of the product rather than the \"how\". meet glenn. glenn is the newly appointed administrator for a new implementation of the opentext service management product at his company, acme products inc. acme provides widgets for many other companies and handles shipping them worldwide from a number of locations. acme has grown over the years, and the number of systems it needs to ensure the delivery of its widgets has grown as well. additionally, the company has invested in internal products to help it run smoothly. this growth has resulted in the need for it to run in an agile and streamlined manner, which was one of the biggest reasons for acme to go with service management. join us as we follow the imple",
    "keywordsLower": [
      "key",
      "concepts",
      "guide",
      "encompasses",
      "service",
      "management",
      "applications",
      "inside",
      "including",
      "request",
      "incident",
      "change",
      "many",
      "others.",
      "join",
      "implementer",
      "glenn",
      "walker",
      "learns",
      "what",
      "functionality",
      "available",
      "investigating",
      "aspects",
      "product",
      "rather",
      "meet",
      "glenn.",
      "newly",
      "appointed",
      "administrator",
      "new",
      "implementation",
      "opentext",
      "company",
      "acme",
      "products",
      "inc.",
      "provides",
      "widgets",
      "companies",
      "handles",
      "shipping",
      "worldwide",
      "number",
      "locations.",
      "grown",
      "over",
      "years",
      "systems",
      "needs",
      "ensure",
      "delivery",
      "well.",
      "additionally",
      "invested",
      "internal",
      "help",
      "run",
      "smoothly.",
      "growth",
      "resulted",
      "need",
      "agile",
      "streamlined",
      "manner",
      "one",
      "biggest",
      "reasons",
      "go",
      "management.",
      "follow",
      "process",
      "uses",
      "get",
      "automation",
      "running.",
      "document",
      "system",
      "designed",
      "continuing",
      "demands",
      "offer",
      "best",
      "customers.",
      "focusing",
      "decisions",
      "makes",
      "result",
      "quick",
      "efficient",
      "implementation.",
      "involves",
      "iterative",
      "approach",
      "problem",
      "solving",
      "delivering",
      "value",
      "quickly"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Implementation Checklist",
    "content": "This section contains an example implementation checklist you can use to guide you through the minimal steps to prepare a system for use by end users and agents. While not every area in the system is covered, the necessary steps are included for the initial setup and the most popular modules. Links to relevant documentation are also provided. Setup Core Data & Customization The core data (such as location and group definitions) is widely used in different modules. This makes it quite important to implement early in the implementation process while ensuring the data is correctly created (or imported). For high-level information on the types of core data needed, see the Getting Started guide here. For information about how to import data, see Import Data Format. ☐ Review out-of-the-box Roles and modify or add as necessary Review the roles documentation, which includes a list of out of the box roles. ☐ Create or import Location data For more details, please review the Locations section in",
    "url": "implementationchecklist",
    "filename": "implementationchecklist",
    "headings": [
      "Setup Core Data & Customization",
      "☐ Review out-of-the-box Roles and modify or add as necessary",
      "☐ Create or import Location data",
      "☐ Create or import Organizational Groups",
      "☐ Create or import Functional Groups",
      "☐ Import any Users that were not part of the SSO integration",
      "☐ Create or import Contacts (non-users) into the system",
      "☐ Assign Users to Roles, Locations, and Groups",
      "☐ Customize the notification header & footer",
      "☐ Update Agent training material with Location and Group information if needed",
      "Define your primary services",
      "☐ Create Service Definitions for a few of your most important/used Services",
      "☐ Create the Actual Service entries that represent these Services in your environment",
      "☐ Where possible, define the Service Component/System Element hierarchy under these Services",
      "☐ (optional) Set the primary Functional Groups responsible for supporting these Services either in the Actual Service or an associated Routing Definition",
      "☐ Update Agent training material with Service information",
      "Setting up Request Management",
      "☐ Review the default Record Categories, adding or removing where necessary",
      "☐ Import existing Knowledge Articles",
      "☐ Review Request Roles and ensure Agents have correct Roles"
    ],
    "keywords": [
      "implementation",
      "checklist",
      "setup",
      "core",
      "data",
      "customization",
      "review",
      "out-of-the-box",
      "roles",
      "modify",
      "add",
      "necessary",
      "create",
      "import",
      "location",
      "organizational",
      "groups",
      "functional",
      "any",
      "users",
      "part",
      "sso",
      "integration",
      "contacts",
      "non-users",
      "system",
      "assign",
      "locations",
      "customize",
      "notification",
      "header",
      "footer",
      "update",
      "agent",
      "training",
      "material",
      "group",
      "information",
      "needed",
      "define",
      "primary",
      "services",
      "service",
      "definitions",
      "few",
      "most",
      "important",
      "actual",
      "entries",
      "represent",
      "environment",
      "possible",
      "component",
      "element",
      "hierarchy",
      "under",
      "optional",
      "set",
      "responsible",
      "supporting",
      "either",
      "associated",
      "routing",
      "definition",
      "setting",
      "request",
      "management",
      "default",
      "record",
      "categories",
      "adding",
      "removing",
      "existing",
      "knowledge",
      "articles",
      "ensure",
      "agents",
      "correct",
      "views",
      "reports",
      "editing",
      "implement",
      "satisfaction",
      "survey",
      "desired",
      "level",
      "agreement",
      "targets",
      "support",
      "requests",
      "materials",
      "process",
      "flows",
      "catalog",
      "high-level",
      "category",
      "structure",
      "entitlement",
      "records",
      "attach"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "implementation checklist",
    "contentLower": "this section contains an example implementation checklist you can use to guide you through the minimal steps to prepare a system for use by end users and agents. while not every area in the system is covered, the necessary steps are included for the initial setup and the most popular modules. links to relevant documentation are also provided. setup core data & customization the core data (such as location and group definitions) is widely used in different modules. this makes it quite important to implement early in the implementation process while ensuring the data is correctly created (or imported). for high-level information on the types of core data needed, see the getting started guide here. for information about how to import data, see import data format. ☐ review out-of-the-box roles and modify or add as necessary review the roles documentation, which includes a list of out of the box roles. ☐ create or import location data for more details, please review the locations section in",
    "keywordsLower": [
      "implementation",
      "checklist",
      "setup",
      "core",
      "data",
      "customization",
      "review",
      "out-of-the-box",
      "roles",
      "modify",
      "add",
      "necessary",
      "create",
      "import",
      "location",
      "organizational",
      "groups",
      "functional",
      "any",
      "users",
      "part",
      "sso",
      "integration",
      "contacts",
      "non-users",
      "system",
      "assign",
      "locations",
      "customize",
      "notification",
      "header",
      "footer",
      "update",
      "agent",
      "training",
      "material",
      "group",
      "information",
      "needed",
      "define",
      "primary",
      "services",
      "service",
      "definitions",
      "few",
      "most",
      "important",
      "actual",
      "entries",
      "represent",
      "environment",
      "possible",
      "component",
      "element",
      "hierarchy",
      "under",
      "optional",
      "set",
      "responsible",
      "supporting",
      "either",
      "associated",
      "routing",
      "definition",
      "setting",
      "request",
      "management",
      "default",
      "record",
      "categories",
      "adding",
      "removing",
      "existing",
      "knowledge",
      "articles",
      "ensure",
      "agents",
      "correct",
      "views",
      "reports",
      "editing",
      "implement",
      "satisfaction",
      "survey",
      "desired",
      "level",
      "agreement",
      "targets",
      "support",
      "requests",
      "materials",
      "process",
      "flows",
      "catalog",
      "high-level",
      "category",
      "structure",
      "entitlement",
      "records",
      "attach"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident Management",
    "content": "Incident Management An Incident encompasses any disruption or degradation of a service, whether the service is internal or external. Issues with redundant hardware (or software) are also handled as an incident, even if the degradation isn't apparent to an end user. The goal of the incident management process is to resolve the disruption or degradation as quickly as possible. This may often involve enacting a workaround (such as rebooting a server or restarting a specific service) rather than a permanent fix. When a similar issue occurs multiple times, each instance should be tracked as a separate Incident. This allows other processes (such as problem management), as well as reporting, to discover trends that will help to find the root cause of the recurring incidents. Setting up Incident Management Previously we set up our request management process to handle everything coming from the consumers of our services. Now we’ll set up the processes we use to support how we provide those serv",
    "url": "incidentmanagement",
    "filename": "incidentmanagement",
    "headings": [
      "Setting up Incident Management",
      "Incident Models"
    ],
    "keywords": [
      "incident",
      "management",
      "setting",
      "models",
      "encompasses",
      "any",
      "disruption",
      "degradation",
      "service",
      "whether",
      "internal",
      "external.",
      "issues",
      "redundant",
      "hardware",
      "software",
      "handled",
      "even",
      "isn",
      "apparent",
      "end",
      "user.",
      "goal",
      "process",
      "resolve",
      "quickly",
      "possible.",
      "often",
      "involve",
      "enacting",
      "workaround",
      "such",
      "rebooting",
      "server",
      "restarting",
      "specific",
      "rather",
      "permanent",
      "fix.",
      "similar",
      "issue",
      "occurs",
      "multiple",
      "times",
      "instance",
      "tracked",
      "separate",
      "incident.",
      "allows",
      "processes",
      "problem",
      "well",
      "reporting",
      "discover",
      "trends",
      "help",
      "find",
      "root",
      "cause",
      "recurring",
      "incidents.",
      "previously",
      "set",
      "request",
      "handle",
      "everything",
      "coming",
      "consumers",
      "services.",
      "now",
      "ll",
      "support",
      "provide",
      "services",
      "users.",
      "first",
      "back-office",
      "process.",
      "triggered",
      "there",
      "outage",
      "detected",
      "one",
      "detect",
      "monitoring",
      "found",
      "technicians",
      "reported",
      "user",
      "through",
      "request.",
      "incidents",
      "happen",
      "managers",
      "get",
      "very",
      "grumpy",
      "need",
      "terms",
      "happy"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident management",
    "contentLower": "incident management an incident encompasses any disruption or degradation of a service, whether the service is internal or external. issues with redundant hardware (or software) are also handled as an incident, even if the degradation isn't apparent to an end user. the goal of the incident management process is to resolve the disruption or degradation as quickly as possible. this may often involve enacting a workaround (such as rebooting a server or restarting a specific service) rather than a permanent fix. when a similar issue occurs multiple times, each instance should be tracked as a separate incident. this allows other processes (such as problem management), as well as reporting, to discover trends that will help to find the root cause of the recurring incidents. setting up incident management previously we set up our request management process to handle everything coming from the consumers of our services. now we’ll set up the processes we use to support how we provide those serv",
    "keywordsLower": [
      "incident",
      "management",
      "setting",
      "models",
      "encompasses",
      "any",
      "disruption",
      "degradation",
      "service",
      "whether",
      "internal",
      "external.",
      "issues",
      "redundant",
      "hardware",
      "software",
      "handled",
      "even",
      "isn",
      "apparent",
      "end",
      "user.",
      "goal",
      "process",
      "resolve",
      "quickly",
      "possible.",
      "often",
      "involve",
      "enacting",
      "workaround",
      "such",
      "rebooting",
      "server",
      "restarting",
      "specific",
      "rather",
      "permanent",
      "fix.",
      "similar",
      "issue",
      "occurs",
      "multiple",
      "times",
      "instance",
      "tracked",
      "separate",
      "incident.",
      "allows",
      "processes",
      "problem",
      "well",
      "reporting",
      "discover",
      "trends",
      "help",
      "find",
      "root",
      "cause",
      "recurring",
      "incidents.",
      "previously",
      "set",
      "request",
      "handle",
      "everything",
      "coming",
      "consumers",
      "services.",
      "now",
      "ll",
      "support",
      "provide",
      "services",
      "users.",
      "first",
      "back-office",
      "process.",
      "triggered",
      "there",
      "outage",
      "detected",
      "one",
      "detect",
      "monitoring",
      "found",
      "technicians",
      "reported",
      "user",
      "through",
      "request.",
      "incidents",
      "happen",
      "managers",
      "get",
      "very",
      "grumpy",
      "need",
      "terms",
      "happy"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident Flows",
    "content": "Incident Flows Now that we have set up a good portion of what we need for incident management, we can look at the incident workflow. Along the way, we will investigate the similarities and differences between incident and request management and how they work together. Request and Incident… Better Together You’ve probably noticed that there is quite a bit of similarity between the Request and Incident processes. The real difference between the two is that Request handles requests and issues coming from the consumers of our Services while Incident handles issues on the provider side. I’ve come up with two tenants for our Service Desk to help define what each of these processes is used for: Service Request management encompasses the consumer-facing processes that make up the expected, day to day activities involved in providing a service to an individual or group. Incident Management encompasses the processes used by the service provider to track and resolve any issue that impacts the abi",
    "url": "incidentflows",
    "filename": "incidentflows",
    "headings": [
      "Request and Incident… Better Together",
      "Incident Workflow",
      "Classification",
      "Resolution",
      "Validation",
      "Incident Task Plans",
      "Service Level Targets",
      "Request/Incident Workflow"
    ],
    "keywords": [
      "incident",
      "flows",
      "request",
      "better",
      "together",
      "workflow",
      "classification",
      "resolution",
      "validation",
      "task",
      "plans",
      "service",
      "level",
      "targets",
      "now",
      "set",
      "good",
      "portion",
      "what",
      "need",
      "management",
      "look",
      "workflow.",
      "along",
      "way",
      "investigate",
      "similarities",
      "differences",
      "between",
      "work",
      "together.",
      "ve",
      "probably",
      "noticed",
      "there",
      "quite",
      "bit",
      "similarity",
      "processes.",
      "real",
      "difference",
      "two",
      "handles",
      "requests",
      "issues",
      "coming",
      "consumers",
      "services",
      "while",
      "provider",
      "side.",
      "come",
      "tenants",
      "desk",
      "help",
      "define",
      "processes",
      "encompasses",
      "consumer-facing",
      "make",
      "expected",
      "day",
      "activities",
      "involved",
      "providing",
      "individual",
      "group.",
      "track",
      "resolve",
      "any",
      "issue",
      "impacts",
      "ability",
      "user",
      "consume",
      "service.",
      "crossed",
      "mind",
      "situations",
      "actually",
      "both",
      "let",
      "take",
      "example",
      "detail.",
      "say",
      "one",
      "end",
      "users",
      "called",
      "because",
      "access",
      "company",
      "hr",
      "web",
      "page.",
      "call",
      "agents",
      "creates",
      "support"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident flows",
    "contentLower": "incident flows now that we have set up a good portion of what we need for incident management, we can look at the incident workflow. along the way, we will investigate the similarities and differences between incident and request management and how they work together. request and incident… better together you’ve probably noticed that there is quite a bit of similarity between the request and incident processes. the real difference between the two is that request handles requests and issues coming from the consumers of our services while incident handles issues on the provider side. i’ve come up with two tenants for our service desk to help define what each of these processes is used for: service request management encompasses the consumer-facing processes that make up the expected, day to day activities involved in providing a service to an individual or group. incident management encompasses the processes used by the service provider to track and resolve any issue that impacts the abi",
    "keywordsLower": [
      "incident",
      "flows",
      "request",
      "better",
      "together",
      "workflow",
      "classification",
      "resolution",
      "validation",
      "task",
      "plans",
      "service",
      "level",
      "targets",
      "now",
      "set",
      "good",
      "portion",
      "what",
      "need",
      "management",
      "look",
      "workflow.",
      "along",
      "way",
      "investigate",
      "similarities",
      "differences",
      "between",
      "work",
      "together.",
      "ve",
      "probably",
      "noticed",
      "there",
      "quite",
      "bit",
      "similarity",
      "processes.",
      "real",
      "difference",
      "two",
      "handles",
      "requests",
      "issues",
      "coming",
      "consumers",
      "services",
      "while",
      "provider",
      "side.",
      "come",
      "tenants",
      "desk",
      "help",
      "define",
      "processes",
      "encompasses",
      "consumer-facing",
      "make",
      "expected",
      "day",
      "activities",
      "involved",
      "providing",
      "individual",
      "group.",
      "track",
      "resolve",
      "any",
      "issue",
      "impacts",
      "ability",
      "user",
      "consume",
      "service.",
      "crossed",
      "mind",
      "situations",
      "actually",
      "both",
      "let",
      "take",
      "example",
      "detail.",
      "say",
      "one",
      "end",
      "users",
      "called",
      "because",
      "access",
      "company",
      "hr",
      "web",
      "page.",
      "call",
      "agents",
      "creates",
      "support"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Knowledge Management",
    "content": "In this chapter, we will look at the knowledge management process. Knowledge management allows the IT department to gather, analyze, store and share information about its provided services. The primary purpose of the process is to improve efficiency by reducing the need to rediscover information that's already known in some part of the organization. Knowledge articles may be related to many of the other primary IT processes. Articles can be created from these other processes, as well as used to assist in the resolution of these processes as well. Knowledge Articles and News In a perfect world, a service desk agent would never have to figure out the solution to a specific issue or answer a question more than once. While we don’t live in a perfect world, we can do our best to capture all of our knowledge for use by both our agents and end users. While everything in Service Management is knowledge of some form or another, knowledge management allows us to create and maintain a curated dat",
    "url": "knowledgemanagement",
    "filename": "knowledgemanagement",
    "headings": [
      "Knowledge Articles and News",
      "Creating New Knowledge",
      "Models",
      "Localization",
      "Knowledge Workflow",
      "Creation",
      "Publication",
      "Consumption",
      "Retirement",
      "Using Knowledge in Service Management",
      "News Articles in the Service Portal",
      "Searching in the Service Portal",
      "Using Live Support",
      "Working a Request or Incident",
      "Global Search",
      "Discovering Needed Knowledge",
      "Hot Topic Analytics",
      "Configuring the “Stop list”"
    ],
    "keywords": [
      "knowledge",
      "management",
      "articles",
      "news",
      "creating",
      "new",
      "models",
      "localization",
      "workflow",
      "creation",
      "publication",
      "consumption",
      "retirement",
      "service",
      "portal",
      "searching",
      "live",
      "support",
      "working",
      "request",
      "incident",
      "global",
      "search",
      "discovering",
      "needed",
      "hot",
      "topic",
      "analytics",
      "configuring",
      "stop",
      "list",
      "chapter",
      "look",
      "process.",
      "allows",
      "department",
      "gather",
      "analyze",
      "store",
      "share",
      "information",
      "about",
      "provided",
      "services.",
      "primary",
      "purpose",
      "process",
      "improve",
      "efficiency",
      "reducing",
      "need",
      "rediscover",
      "already",
      "known",
      "part",
      "organization.",
      "related",
      "many",
      "processes.",
      "created",
      "processes",
      "well",
      "assist",
      "resolution",
      "well.",
      "perfect",
      "world",
      "desk",
      "agent",
      "never",
      "figure",
      "out",
      "solution",
      "specific",
      "issue",
      "answer",
      "question",
      "once.",
      "while",
      "don",
      "best",
      "capture",
      "all",
      "both",
      "agents",
      "end",
      "users.",
      "everything",
      "form",
      "another",
      "create",
      "maintain",
      "curated",
      "database",
      "way",
      "ensure",
      "integrity",
      "quality",
      "most",
      "pieces"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "knowledge management",
    "contentLower": "in this chapter, we will look at the knowledge management process. knowledge management allows the it department to gather, analyze, store and share information about its provided services. the primary purpose of the process is to improve efficiency by reducing the need to rediscover information that's already known in some part of the organization. knowledge articles may be related to many of the other primary it processes. articles can be created from these other processes, as well as used to assist in the resolution of these processes as well. knowledge articles and news in a perfect world, a service desk agent would never have to figure out the solution to a specific issue or answer a question more than once. while we don’t live in a perfect world, we can do our best to capture all of our knowledge for use by both our agents and end users. while everything in service management is knowledge of some form or another, knowledge management allows us to create and maintain a curated dat",
    "keywordsLower": [
      "knowledge",
      "management",
      "articles",
      "news",
      "creating",
      "new",
      "models",
      "localization",
      "workflow",
      "creation",
      "publication",
      "consumption",
      "retirement",
      "service",
      "portal",
      "searching",
      "live",
      "support",
      "working",
      "request",
      "incident",
      "global",
      "search",
      "discovering",
      "needed",
      "hot",
      "topic",
      "analytics",
      "configuring",
      "stop",
      "list",
      "chapter",
      "look",
      "process.",
      "allows",
      "department",
      "gather",
      "analyze",
      "store",
      "share",
      "information",
      "about",
      "provided",
      "services.",
      "primary",
      "purpose",
      "process",
      "improve",
      "efficiency",
      "reducing",
      "need",
      "rediscover",
      "already",
      "known",
      "part",
      "organization.",
      "related",
      "many",
      "processes.",
      "created",
      "processes",
      "well",
      "assist",
      "resolution",
      "well.",
      "perfect",
      "world",
      "desk",
      "agent",
      "never",
      "figure",
      "out",
      "solution",
      "specific",
      "issue",
      "answer",
      "question",
      "once.",
      "while",
      "don",
      "best",
      "capture",
      "all",
      "both",
      "agents",
      "end",
      "users.",
      "everything",
      "form",
      "another",
      "create",
      "maintain",
      "curated",
      "database",
      "way",
      "ensure",
      "integrity",
      "quality",
      "most",
      "pieces"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Knowledge",
    "content": "Knowledge Articles Knowledge is more centrally featured in Service Management than other service management tools. For end users, Knowledge Articles are an integral part of the Service Catalog, giving answers and direction without the need to directly address a service desk agent. For the agents, the Articles are integrated directly into their UI, showing possible solutions to a user's issue without having to leave the screen they are working on. The existing knowledge in Service Manager is a critical piece of the Evolving to Service Management process. Bringing this asset into The Service Management system is extremely important, and tools have been created to assist in this process. Before this process is performed, it is suggested to review the existing Knowledge Documents to determine if any do not belong in the new system. In order to move the documents from Service Manager to Service Management, follow the two steps listed below: ☐ Export knowledge documents from Service Manager ",
    "url": "smaev_knowledge",
    "filename": "smaev_knowledge",
    "headings": [
      "Knowledge Articles",
      "☐ Export knowledge documents from Service Manager",
      "☐ Import knowledge documents into Service Management",
      "Creating Knowledge from transactional records"
    ],
    "keywords": [
      "knowledge",
      "articles",
      "export",
      "documents",
      "service",
      "manager",
      "import",
      "management",
      "creating",
      "transactional",
      "records",
      "centrally",
      "featured",
      "tools.",
      "end",
      "users",
      "integral",
      "part",
      "catalog",
      "giving",
      "answers",
      "direction",
      "need",
      "directly",
      "address",
      "desk",
      "agent.",
      "agents",
      "integrated",
      "ui",
      "showing",
      "possible",
      "solutions",
      "user",
      "issue",
      "having",
      "leave",
      "screen",
      "working",
      "on.",
      "existing",
      "critical",
      "piece",
      "evolving",
      "process.",
      "bringing",
      "asset",
      "system",
      "extremely",
      "important",
      "tools",
      "created",
      "assist",
      "before",
      "process",
      "performed",
      "suggested",
      "review",
      "determine",
      "any",
      "belong",
      "new",
      "system.",
      "order",
      "move",
      "follow",
      "two",
      "steps",
      "listed",
      "below",
      "exported",
      "packaged",
      "prepare",
      "importing",
      "utilities",
      "purpose.",
      "instructions",
      "found",
      "here",
      "phase",
      "please",
      "see",
      "article",
      "packages",
      "strongly",
      "recommended",
      "incidents",
      "changes",
      "etc.",
      "moved",
      "keep",
      "information",
      "help",
      "solve",
      "speed",
      "resolution",
      "future",
      "transactions",
      "subject",
      "matter"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "knowledge",
    "contentLower": "knowledge articles knowledge is more centrally featured in service management than other service management tools. for end users, knowledge articles are an integral part of the service catalog, giving answers and direction without the need to directly address a service desk agent. for the agents, the articles are integrated directly into their ui, showing possible solutions to a user's issue without having to leave the screen they are working on. the existing knowledge in service manager is a critical piece of the evolving to service management process. bringing this asset into the service management system is extremely important, and tools have been created to assist in this process. before this process is performed, it is suggested to review the existing knowledge documents to determine if any do not belong in the new system. in order to move the documents from service manager to service management, follow the two steps listed below: ☐ export knowledge documents from service manager ",
    "keywordsLower": [
      "knowledge",
      "articles",
      "export",
      "documents",
      "service",
      "manager",
      "import",
      "management",
      "creating",
      "transactional",
      "records",
      "centrally",
      "featured",
      "tools.",
      "end",
      "users",
      "integral",
      "part",
      "catalog",
      "giving",
      "answers",
      "direction",
      "need",
      "directly",
      "address",
      "desk",
      "agent.",
      "agents",
      "integrated",
      "ui",
      "showing",
      "possible",
      "solutions",
      "user",
      "issue",
      "having",
      "leave",
      "screen",
      "working",
      "on.",
      "existing",
      "critical",
      "piece",
      "evolving",
      "process.",
      "bringing",
      "asset",
      "system",
      "extremely",
      "important",
      "tools",
      "created",
      "assist",
      "before",
      "process",
      "performed",
      "suggested",
      "review",
      "determine",
      "any",
      "belong",
      "new",
      "system.",
      "order",
      "move",
      "follow",
      "two",
      "steps",
      "listed",
      "below",
      "exported",
      "packaged",
      "prepare",
      "importing",
      "utilities",
      "purpose.",
      "instructions",
      "found",
      "here",
      "phase",
      "please",
      "see",
      "article",
      "packages",
      "strongly",
      "recommended",
      "incidents",
      "changes",
      "etc.",
      "moved",
      "keep",
      "information",
      "help",
      "solve",
      "speed",
      "resolution",
      "future",
      "transactions",
      "subject",
      "matter"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrations",
    "content": "",
    "url": "wow_integrations",
    "filename": "wow_integrations",
    "headings": [],
    "keywords": [
      "integrations"
    ],
    "language": "en",
    "word_count": 1,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrations",
    "contentLower": "",
    "keywordsLower": [
      "integrations"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration with Operations Orchestration",
    "content": "Introduction Service Management provides an Operations Orchestration (OO) integration that allows you to automate tasks or integrate with other products. The integration uses the On-Premises Bridge to connect SMAX to an Operations Orchestration installation. This document describes how to configure the OO integration with Service Management and provides an example of how to use SMAX with OO. This chapter will include instructions to: Create the OPB Integration User Install the OPB Agent Configure SSL for OPB Create the Endpoint Configure an OO Endpoint Use SMAX with OO Troubleshoot the integration Create the OPB Integration User A suite admin user needs to create an Integration User within the Suite Administration before it can be used within the Tenant. To do this, access the Suite Administration URL. The URL resembles the following example: https://smademovm07.swinfra.net/BO Once logged in go to the Users section. Click New to add an Integration user. Enter all relevant user details,",
    "url": "wow_oo_integration",
    "filename": "wow_oo_integration",
    "headings": [
      "Introduction",
      "Create the OPB Integration User",
      "Install the OPB Agent",
      "Configure SSL for OPB",
      "Export OO Server Certificate",
      "Import SSL Certificates into OPB",
      "Create the Endpoint",
      "Configure the OO Endpoint",
      "Using SMAX with OO",
      "Create OO Flow",
      "Create Content Pack",
      "Log in to OO Central",
      "Create SMAX Offering",
      "Prerequisite work",
      "Now create the Offering",
      "Incident Model",
      "Service Request",
      "Additional Tasks if Required",
      "Run through the Password Reset Scenario",
      "Run through the same scenario – This time to show Incident Creation"
    ],
    "keywords": [
      "https://<YOURSERVER_FQDN>/auth/authentication-endpoint/authenticate/login",
      "Maas_oo_integration.log",
      "https://<YOURSERVER_FQDN",
      "https://FQDN_servername/auth/authentication-endpoint/authenticate/login",
      "https://marketplace.microfocus.com/itom/category/operations-orchestration",
      "error.log",
      "Maas_error.log",
      "1.16.0",
      "OOserver.cert",
      "output.log",
      "swinfra.net",
      "4.70",
      "Maas_workflow.log",
      "0.log",
      "1.0",
      "3.2.0",
      "https://FQDN_servername",
      "ISO-8859",
      "https://smademovm07.swinfra.net/BO",
      "keytool.exe",
      "entity.id",
      "entity.Id",
      "microfocus.com",
      "Maas_unhandled_exceptions.log",
      "1.16",
      "integration",
      "operations",
      "orchestration",
      "introduction",
      "create",
      "opb",
      "user",
      "install",
      "agent",
      "configure",
      "ssl",
      "export",
      "oo",
      "server",
      "certificate",
      "import",
      "certificates",
      "endpoint",
      "smax",
      "flow",
      "content",
      "pack",
      "log",
      "central",
      "offering",
      "prerequisite",
      "work",
      "now",
      "incident",
      "model",
      "service",
      "request",
      "additional",
      "tasks",
      "required",
      "run",
      "through",
      "password",
      "reset",
      "scenario",
      "same",
      "time",
      "show",
      "creation",
      "troubleshooting",
      "management",
      "provides",
      "allows",
      "automate",
      "integrate",
      "products.",
      "uses",
      "on-premises",
      "bridge",
      "connect",
      "installation.",
      "document",
      "describes",
      "example",
      "oo.",
      "chapter",
      "include",
      "instructions",
      "troubleshoot",
      "suite",
      "admin",
      "needs",
      "administration",
      "before",
      "tenant.",
      "access",
      "url.",
      "url",
      "resembles",
      "following"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration with operations orchestration",
    "contentLower": "introduction service management provides an operations orchestration (oo) integration that allows you to automate tasks or integrate with other products. the integration uses the on-premises bridge to connect smax to an operations orchestration installation. this document describes how to configure the oo integration with service management and provides an example of how to use smax with oo. this chapter will include instructions to: create the opb integration user install the opb agent configure ssl for opb create the endpoint configure an oo endpoint use smax with oo troubleshoot the integration create the opb integration user a suite admin user needs to create an integration user within the suite administration before it can be used within the tenant. to do this, access the suite administration url. the url resembles the following example: https://smademovm07.swinfra.net/bo once logged in go to the users section. click new to add an integration user. enter all relevant user details,",
    "keywordsLower": [
      "https://<yourserver_fqdn>/auth/authentication-endpoint/authenticate/login",
      "maas_oo_integration.log",
      "https://<yourserver_fqdn",
      "https://fqdn_servername/auth/authentication-endpoint/authenticate/login",
      "https://marketplace.microfocus.com/itom/category/operations-orchestration",
      "error.log",
      "maas_error.log",
      "1.16.0",
      "ooserver.cert",
      "output.log",
      "swinfra.net",
      "4.70",
      "maas_workflow.log",
      "0.log",
      "1.0",
      "3.2.0",
      "https://fqdn_servername",
      "iso-8859",
      "https://smademovm07.swinfra.net/bo",
      "keytool.exe",
      "entity.id",
      "entity.id",
      "microfocus.com",
      "maas_unhandled_exceptions.log",
      "1.16",
      "integration",
      "operations",
      "orchestration",
      "introduction",
      "create",
      "opb",
      "user",
      "install",
      "agent",
      "configure",
      "ssl",
      "export",
      "oo",
      "server",
      "certificate",
      "import",
      "certificates",
      "endpoint",
      "smax",
      "flow",
      "content",
      "pack",
      "log",
      "central",
      "offering",
      "prerequisite",
      "work",
      "now",
      "incident",
      "model",
      "service",
      "request",
      "additional",
      "tasks",
      "required",
      "run",
      "through",
      "password",
      "reset",
      "scenario",
      "same",
      "time",
      "show",
      "creation",
      "troubleshooting",
      "management",
      "provides",
      "allows",
      "automate",
      "integrate",
      "products.",
      "uses",
      "on-premises",
      "bridge",
      "connect",
      "installation.",
      "document",
      "describes",
      "example",
      "oo.",
      "chapter",
      "include",
      "instructions",
      "troubleshoot",
      "suite",
      "admin",
      "needs",
      "administration",
      "before",
      "tenant.",
      "access",
      "url.",
      "url",
      "resembles",
      "following"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install",
    "content": "This topic provides an overview of the installation options and considerations and a To-Do list of tasks before, during, and after the installation of OpenText Service Management. Before you begin, review the information on this page to get an overall picture of your installation journey. Always follow the installation instructions and install the latest version of OpenText Service Management. If you have a reason to install an older release, please contact OpenText Service Management Product Management. It is the customer's responsibility to implement appropriate network security controls in their system. By not implementing appropriate network security controls you may be exposing the system to increased security risks. You understand and agree to assume all associated risks and hold OpenText harmless for the same. It remains at all times the Customer's sole responsibility to assess its own regulatory and business requirements. OpenText does not represent or warrant that its products",
    "url": "install",
    "filename": "install",
    "headings": [
      "OpenText SaaS, embedded K8s, managed K8s, or external K8s",
      "Supported deployment platforms",
      "OpenText OPTIC Management Toolkit (OMT)",
      "Suite Helm chart",
      "Dynamic vs static volume provisioning",
      "Download and upload of container images",
      "Suite installation",
      "Add-on installation",
      "Native SACM or traditional UD/UCMDB integration",
      "Native SACM",
      "Traditional OPB-based integration (deprecated)",
      "Classic OO or OO Containerized",
      "To-Do list",
      "Pre-installation phase",
      "Installation phase",
      "Post-installation phase"
    ],
    "keywords": [
      "K8s.zip",
      "suite.Copy",
      "Containerized.If",
      "installation.On",
      "UCMDB.For",
      "file.The",
      "suite.Run",
      "xxx-15001",
      "values.yaml",
      "install",
      "opentext",
      "saas",
      "embedded",
      "k8s",
      "managed",
      "external",
      "supported",
      "deployment",
      "platforms",
      "optic",
      "management",
      "toolkit",
      "omt",
      "suite",
      "helm",
      "chart",
      "dynamic",
      "vs",
      "static",
      "volume",
      "provisioning",
      "download",
      "upload",
      "container",
      "images",
      "installation",
      "add-on",
      "native",
      "sacm",
      "traditional",
      "ud",
      "ucmdb",
      "integration",
      "opb-based",
      "deprecated",
      "classic",
      "oo",
      "containerized",
      "to-do",
      "list",
      "pre-installation",
      "phase",
      "post-installation",
      "topic",
      "provides",
      "overview",
      "options",
      "considerations",
      "tasks",
      "before",
      "during",
      "after",
      "service",
      "management.",
      "begin",
      "review",
      "information",
      "page",
      "get",
      "overall",
      "picture",
      "journey.",
      "always",
      "follow",
      "instructions",
      "latest",
      "version",
      "reason",
      "older",
      "release",
      "please",
      "contact",
      "product",
      "customer",
      "responsibility",
      "implement",
      "appropriate",
      "network",
      "security",
      "controls",
      "system.",
      "implementing",
      "exposing",
      "system",
      "increased",
      "risks.",
      "understand",
      "agree",
      "assume",
      "all"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install",
    "contentLower": "this topic provides an overview of the installation options and considerations and a to-do list of tasks before, during, and after the installation of opentext service management. before you begin, review the information on this page to get an overall picture of your installation journey. always follow the installation instructions and install the latest version of opentext service management. if you have a reason to install an older release, please contact opentext service management product management. it is the customer's responsibility to implement appropriate network security controls in their system. by not implementing appropriate network security controls you may be exposing the system to increased security risks. you understand and agree to assume all associated risks and hold opentext harmless for the same. it remains at all times the customer's sole responsibility to assess its own regulatory and business requirements. opentext does not represent or warrant that its products",
    "keywordsLower": [
      "k8s.zip",
      "suite.copy",
      "containerized.if",
      "installation.on",
      "ucmdb.for",
      "file.the",
      "suite.run",
      "xxx-15001",
      "values.yaml",
      "install",
      "opentext",
      "saas",
      "embedded",
      "k8s",
      "managed",
      "external",
      "supported",
      "deployment",
      "platforms",
      "optic",
      "management",
      "toolkit",
      "omt",
      "suite",
      "helm",
      "chart",
      "dynamic",
      "vs",
      "static",
      "volume",
      "provisioning",
      "download",
      "upload",
      "container",
      "images",
      "installation",
      "add-on",
      "native",
      "sacm",
      "traditional",
      "ud",
      "ucmdb",
      "integration",
      "opb-based",
      "deprecated",
      "classic",
      "oo",
      "containerized",
      "to-do",
      "list",
      "pre-installation",
      "phase",
      "post-installation",
      "topic",
      "provides",
      "overview",
      "options",
      "considerations",
      "tasks",
      "before",
      "during",
      "after",
      "service",
      "management.",
      "begin",
      "review",
      "information",
      "page",
      "get",
      "overall",
      "picture",
      "journey.",
      "always",
      "follow",
      "instructions",
      "latest",
      "version",
      "reason",
      "older",
      "release",
      "please",
      "contact",
      "product",
      "customer",
      "responsibility",
      "implement",
      "appropriate",
      "network",
      "security",
      "controls",
      "system.",
      "implementing",
      "exposing",
      "system",
      "increased",
      "risks.",
      "understand",
      "agree",
      "assume",
      "all"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install on Embedded K8s",
    "content": "This section describes how to install the suite in an embedded K8s setup. Before you begin, ensure that you've reviewed the following planning topics for the embedded K8s deployment: Choose a deployment option and calculate the deployment sizes. System requirements. Check the file system requirements. Persona The installation tasks involve more than one role. The primary role is of the suite administrator. The suite administrator coordinates with the rest of the roles to complete the installation. The following table describes for what each role is responsible. Role Responsibility Suite administrator A suite administrator must successfully execute the entire installation including verification and post-installation tasks. This person must have a good understanding of the entire installation process, request support from other appropriate roles as needed, and complete the installation once the environment is ready for installation. System administrator A system administrator prepares ph",
    "url": "installsmaonpremises",
    "filename": "installsmaonpremises",
    "headings": [
      "Persona",
      "Deployment tasks",
      "Related topics"
    ],
    "keywords": [
      "install",
      "embedded",
      "k8s",
      "persona",
      "deployment",
      "tasks",
      "related",
      "topics",
      "section",
      "describes",
      "suite",
      "setup.",
      "before",
      "begin",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "choose",
      "option",
      "calculate",
      "sizes.",
      "system",
      "requirements.",
      "check",
      "file",
      "installation",
      "involve",
      "one",
      "role.",
      "primary",
      "role",
      "administrator.",
      "administrator",
      "coordinates",
      "rest",
      "roles",
      "complete",
      "installation.",
      "table",
      "what",
      "responsible.",
      "responsibility",
      "successfully",
      "execute",
      "entire",
      "including",
      "verification",
      "post-installation",
      "tasks.",
      "person",
      "good",
      "understanding",
      "process",
      "request",
      "support",
      "appropriate",
      "needed",
      "once",
      "environment",
      "ready",
      "prepares",
      "physical",
      "virtual",
      "machines",
      "requested",
      "network",
      "manages",
      "configurations",
      "organization.",
      "needs",
      "perform",
      "configuration",
      "storage",
      "plans",
      "deploys",
      "all",
      "types",
      "set",
      "nfs",
      "servers",
      "required",
      "sma.",
      "database",
      "databases",
      "prepare",
      "databases.",
      "refer",
      "deploy",
      "preparation",
      "setup",
      "uninstall",
      "omt"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install on embedded k8s",
    "contentLower": "this section describes how to install the suite in an embedded k8s setup. before you begin, ensure that you've reviewed the following planning topics for the embedded k8s deployment: choose a deployment option and calculate the deployment sizes. system requirements. check the file system requirements. persona the installation tasks involve more than one role. the primary role is of the suite administrator. the suite administrator coordinates with the rest of the roles to complete the installation. the following table describes for what each role is responsible. role responsibility suite administrator a suite administrator must successfully execute the entire installation including verification and post-installation tasks. this person must have a good understanding of the entire installation process, request support from other appropriate roles as needed, and complete the installation once the environment is ready for installation. system administrator a system administrator prepares ph",
    "keywordsLower": [
      "install",
      "embedded",
      "k8s",
      "persona",
      "deployment",
      "tasks",
      "related",
      "topics",
      "section",
      "describes",
      "suite",
      "setup.",
      "before",
      "begin",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "choose",
      "option",
      "calculate",
      "sizes.",
      "system",
      "requirements.",
      "check",
      "file",
      "installation",
      "involve",
      "one",
      "role.",
      "primary",
      "role",
      "administrator.",
      "administrator",
      "coordinates",
      "rest",
      "roles",
      "complete",
      "installation.",
      "table",
      "what",
      "responsible.",
      "responsibility",
      "successfully",
      "execute",
      "entire",
      "including",
      "verification",
      "post-installation",
      "tasks.",
      "person",
      "good",
      "understanding",
      "process",
      "request",
      "support",
      "appropriate",
      "needed",
      "once",
      "environment",
      "ready",
      "prepares",
      "physical",
      "virtual",
      "machines",
      "requested",
      "network",
      "manages",
      "configurations",
      "organization.",
      "needs",
      "perform",
      "configuration",
      "storage",
      "plans",
      "deploys",
      "all",
      "types",
      "set",
      "nfs",
      "servers",
      "required",
      "sma.",
      "database",
      "databases",
      "prepare",
      "databases.",
      "refer",
      "deploy",
      "preparation",
      "setup",
      "uninstall",
      "omt"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Installation tasks (on-premises)",
    "content": "This section contains some of the mandatory and optional tasks to install the suite in an on-premises environment. Prepare external databases Install and configure Vertica database Configure NFS volumes Upload images to a registry Configure a load balancer (reverse proxy server) for applications The rest of the installation tasks are listed on the Helm installation common tasks page.",
    "url": "tasksonpremises",
    "filename": "tasksonpremises",
    "headings": [],
    "keywords": [
      "installation",
      "tasks",
      "on-premises",
      "section",
      "contains",
      "mandatory",
      "optional",
      "install",
      "suite",
      "environment.",
      "prepare",
      "external",
      "databases",
      "configure",
      "vertica",
      "database",
      "nfs",
      "volumes",
      "upload",
      "images",
      "registry",
      "load",
      "balancer",
      "reverse",
      "proxy",
      "server",
      "applications",
      "rest",
      "listed",
      "helm",
      "common",
      "page."
    ],
    "language": "en",
    "word_count": 41,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "installation tasks (on-premises)",
    "contentLower": "this section contains some of the mandatory and optional tasks to install the suite in an on-premises environment. prepare external databases install and configure vertica database configure nfs volumes upload images to a registry configure a load balancer (reverse proxy server) for applications the rest of the installation tasks are listed on the helm installation common tasks page.",
    "keywordsLower": [
      "installation",
      "tasks",
      "on-premises",
      "section",
      "contains",
      "mandatory",
      "optional",
      "install",
      "suite",
      "environment.",
      "prepare",
      "external",
      "databases",
      "configure",
      "vertica",
      "database",
      "nfs",
      "volumes",
      "upload",
      "images",
      "registry",
      "load",
      "balancer",
      "reverse",
      "proxy",
      "server",
      "applications",
      "rest",
      "listed",
      "helm",
      "common",
      "page."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install AlloyDB Omni on Kubernetes with high availability",
    "content": "You can use a Kubernetes-based AlloyDB Omni cluster as a database HA solution for an on-premises deployment. This document is a supplementary document to the official Install AlloyDB Omni on Kubernetes published by Google. The procedure described below has the following assumptions: You have basic familiarity with Kubernetes operation. You want to deploy a database cluster on the embedded Kubernetes cluster in OpenText OPTIC Management Toolkit (OMT). You will include the database information in the OMT installation command. This means you need to use a Docker-based AlloyDB Omni as the OMT database server temporarily. After the Kubernetes AlloyDB is installed, you must migrate the OMT data from the Docker-based AlloyDB Omni to Kubernetes AlloyDB Omni. Practice guidance of any third-party products (for example, AlloyDB HA) provided here is for reference purposes only with an intention to help customers. However, customers will remain responsible for ensuring that the end-to-end solution ",
    "url": "hak8salloydb",
    "filename": "hak8salloydb",
    "headings": [
      "Before you begin",
      "Hardware requirement",
      "Install the Kubernetes cluster",
      "Install AlloyDB Omni on Kubernetes",
      "Install AlloyDB Omni Operator on the Kubernetes cluster",
      "Tune AlloyDB Omni server for HA",
      "Add database nodes to the Kubernetes cluster",
      "Prepare user and data folders on both database nodes",
      "Prepare persistent volumes",
      "Create a database cluster",
      "Enable High Availability",
      "Enable High Availability",
      "How to recover the system to HA after a failover",
      "How to migrate the DB data from an external database to the Kubernetes AlloyDB Omni",
      "Database restore",
      "Switching the database"
    ],
    "keywords": [
      "ca.crt",
      "172.17.180",
      "9.4",
      "172.17.52.115",
      "175.97",
      "postgresql.auto",
      "dbadmin.goog",
      "172.17.180.78",
      "postgresql.conf",
      "https://<external",
      "ha.svc",
      "postgresql://postgres@<k8s",
      "1.1.0",
      "172.17.212",
      "gcr.io",
      "values.yaml",
      "172.17.52",
      "172.17.174",
      "172.17.175",
      "kubernetes.io",
      "52.115",
      "172.17.174.98",
      "172.17.175.97",
      "180.78",
      "212.110",
      "172.17.212.110",
      "174.98",
      "172.17",
      "0.tgz",
      "storage.k8s",
      "15.5.2",
      "install",
      "alloydb",
      "omni",
      "kubernetes",
      "high",
      "availability",
      "before",
      "begin",
      "hardware",
      "requirement",
      "cluster",
      "operator",
      "tune",
      "server",
      "ha",
      "add",
      "database",
      "nodes",
      "prepare",
      "user",
      "data",
      "folders",
      "both",
      "persistent",
      "volumes",
      "create",
      "enable",
      "recover",
      "system",
      "after",
      "failover",
      "migrate",
      "db",
      "external",
      "restore",
      "switching",
      "kubernetes-based",
      "solution",
      "on-premises",
      "deployment.",
      "document",
      "supplementary",
      "official",
      "published",
      "google.",
      "procedure",
      "described",
      "below",
      "following",
      "assumptions",
      "basic",
      "familiarity",
      "operation.",
      "want",
      "deploy",
      "embedded",
      "opentext",
      "optic",
      "management",
      "toolkit",
      "omt",
      "include",
      "information",
      "installation",
      "command.",
      "means",
      "need",
      "docker-based",
      "temporarily."
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install alloydb omni on kubernetes with high availability",
    "contentLower": "you can use a kubernetes-based alloydb omni cluster as a database ha solution for an on-premises deployment. this document is a supplementary document to the official install alloydb omni on kubernetes published by google. the procedure described below has the following assumptions: you have basic familiarity with kubernetes operation. you want to deploy a database cluster on the embedded kubernetes cluster in opentext optic management toolkit (omt). you will include the database information in the omt installation command. this means you need to use a docker-based alloydb omni as the omt database server temporarily. after the kubernetes alloydb is installed, you must migrate the omt data from the docker-based alloydb omni to kubernetes alloydb omni. practice guidance of any third-party products (for example, alloydb ha) provided here is for reference purposes only with an intention to help customers. however, customers will remain responsible for ensuring that the end-to-end solution ",
    "keywordsLower": [
      "ca.crt",
      "172.17.180",
      "9.4",
      "172.17.52.115",
      "175.97",
      "postgresql.auto",
      "dbadmin.goog",
      "172.17.180.78",
      "postgresql.conf",
      "https://<external",
      "ha.svc",
      "postgresql://postgres@<k8s",
      "1.1.0",
      "172.17.212",
      "gcr.io",
      "values.yaml",
      "172.17.52",
      "172.17.174",
      "172.17.175",
      "kubernetes.io",
      "52.115",
      "172.17.174.98",
      "172.17.175.97",
      "180.78",
      "212.110",
      "172.17.212.110",
      "174.98",
      "172.17",
      "0.tgz",
      "storage.k8s",
      "15.5.2",
      "install",
      "alloydb",
      "omni",
      "kubernetes",
      "high",
      "availability",
      "before",
      "begin",
      "hardware",
      "requirement",
      "cluster",
      "operator",
      "tune",
      "server",
      "ha",
      "add",
      "database",
      "nodes",
      "prepare",
      "user",
      "data",
      "folders",
      "both",
      "persistent",
      "volumes",
      "create",
      "enable",
      "recover",
      "system",
      "after",
      "failover",
      "migrate",
      "db",
      "external",
      "restore",
      "switching",
      "kubernetes-based",
      "solution",
      "on-premises",
      "deployment.",
      "document",
      "supplementary",
      "official",
      "published",
      "google.",
      "procedure",
      "described",
      "below",
      "following",
      "assumptions",
      "basic",
      "familiarity",
      "operation.",
      "want",
      "deploy",
      "embedded",
      "opentext",
      "optic",
      "management",
      "toolkit",
      "omt",
      "include",
      "information",
      "installation",
      "command.",
      "means",
      "need",
      "docker-based",
      "temporarily."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install UD/UCMDB on OMT",
    "content": "To install UD/UCMDB on OMT, follow the instructions in this topic.Deploy UD/UCMDB RoleLocationSuite adminControl plane nodeTo install UD/UCMDB, run the following command:helm install <UD/UCMDB RELEASE NAME> <UD/UCMDB CHART FILE> --namespace <UD/UCMDB NAMESPACE> -f <VALUES YAML FILE> -f <SECRETS YAML FILE>Where,<UD/UCMDB RELEASE NAME> is the unique release name that you need to specify for the UD/UCMDB deployment. You need to use the same release name as specified here when you upgrade UD/UCMDB in the future.<UD/UCMDB CHART FILE> is the tar file containing the UD/UCMDB installation charts.<CMS NAMESPACE> is the unique namespace of the UD/UCMDB deployment. Use the namespace that you specified in the \"Create a deployment for UD/UCMDB\" topic.<VALUES YAML FILE> is your custom my-values.yaml file with properties configured for the UD/UCMDB deployment.<SECRETS YAML FILE> is the YAML file that stores the UD/UCMDB secrets. This file is created when you run gen_secrets.sh to generate vault secre",
    "url": "installcmswithsmaxinstallcms",
    "filename": "installcmswithsmaxinstallcms",
    "headings": [
      "Deploy UD/UCMDB",
      "(Optional) Create custom certificates for UD/UCMDB external access",
      "(Optional) Install a classic Data Flow Probe"
    ],
    "keywords": [
      "uducmdb",
      "ca.crt",
      "server.crt",
      "root.crt",
      "Probe.You",
      "server.key",
      "x_Windows.zip",
      "Probe.For",
      "x.tgz",
      "secrets.yaml",
      "base-64",
      "x_Linux.zip",
      "website.For",
      "1.xx",
      "values.yaml",
      "example.com",
      "gen_secrets.sh",
      "step.For",
      "install",
      "ud",
      "ucmdb",
      "omt",
      "deploy",
      "optional",
      "create",
      "custom",
      "certificates",
      "external",
      "access",
      "classic",
      "data",
      "flow",
      "probe",
      "follow",
      "instructions",
      "topic.deploy",
      "rolelocationsuite",
      "admincontrol",
      "plane",
      "nodeto",
      "run",
      "following",
      "command",
      "helm",
      "--namespace",
      "-f",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "tar",
      "file",
      "containing",
      "installation",
      "charts.",
      "namespace",
      "deployment",
      "topic.",
      "my-values.yaml",
      "properties",
      "configured",
      "yaml",
      "stores",
      "secrets.",
      "created",
      "generate",
      "vault",
      "secrets",
      "previous",
      "example",
      "ucmdb-prod",
      "ucmdb-helm-charts",
      "charts",
      "ucmdb-1.xx.x-xxx",
      "2x.x.x.tgz",
      "cms-secrets.yamlyou",
      "above",
      "--dry-run",
      "argument",
      "simulate",
      "installation.",
      "useful",
      "validate",
      "content",
      "before",
      "real",
      "installation.after",
      "delete",
      "cms-secrets.yaml",
      "contains",
      "sensitive",
      "data.",
      "log"
    ],
    "language": "en",
    "word_count": 86,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install ud/ucmdb on omt",
    "contentLower": "to install ud/ucmdb on omt, follow the instructions in this topic.deploy ud/ucmdb rolelocationsuite admincontrol plane nodeto install ud/ucmdb, run the following command:helm install <ud/ucmdb release name> <ud/ucmdb chart file> --namespace <ud/ucmdb namespace> -f <values yaml file> -f <secrets yaml file>where,<ud/ucmdb release name> is the unique release name that you need to specify for the ud/ucmdb deployment. you need to use the same release name as specified here when you upgrade ud/ucmdb in the future.<ud/ucmdb chart file> is the tar file containing the ud/ucmdb installation charts.<cms namespace> is the unique namespace of the ud/ucmdb deployment. use the namespace that you specified in the \"create a deployment for ud/ucmdb\" topic.<values yaml file> is your custom my-values.yaml file with properties configured for the ud/ucmdb deployment.<secrets yaml file> is the yaml file that stores the ud/ucmdb secrets. this file is created when you run gen_secrets.sh to generate vault secre",
    "keywordsLower": [
      "uducmdb",
      "ca.crt",
      "server.crt",
      "root.crt",
      "probe.you",
      "server.key",
      "x_windows.zip",
      "probe.for",
      "x.tgz",
      "secrets.yaml",
      "base-64",
      "x_linux.zip",
      "website.for",
      "1.xx",
      "values.yaml",
      "example.com",
      "gen_secrets.sh",
      "step.for",
      "install",
      "ud",
      "ucmdb",
      "omt",
      "deploy",
      "optional",
      "create",
      "custom",
      "certificates",
      "external",
      "access",
      "classic",
      "data",
      "flow",
      "probe",
      "follow",
      "instructions",
      "topic.deploy",
      "rolelocationsuite",
      "admincontrol",
      "plane",
      "nodeto",
      "run",
      "following",
      "command",
      "helm",
      "--namespace",
      "-f",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "tar",
      "file",
      "containing",
      "installation",
      "charts.",
      "namespace",
      "deployment",
      "topic.",
      "my-values.yaml",
      "properties",
      "configured",
      "yaml",
      "stores",
      "secrets.",
      "created",
      "generate",
      "vault",
      "secrets",
      "previous",
      "example",
      "ucmdb-prod",
      "ucmdb-helm-charts",
      "charts",
      "ucmdb-1.xx.x-xxx",
      "2x.x.x.tgz",
      "cms-secrets.yamlyou",
      "above",
      "--dry-run",
      "argument",
      "simulate",
      "installation.",
      "useful",
      "validate",
      "content",
      "before",
      "real",
      "installation.after",
      "delete",
      "cms-secrets.yaml",
      "contains",
      "sensitive",
      "data.",
      "log"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install Audit collector",
    "content": "Services that are integrated with the Audit service store the audits on the NFS file system when the audit service isn't reachable. Audit collector is a microservice that collects the audits from the NFS file system and passes them to the Audit service via REST. You can deploy the Audit collector using Helm charts. Install and configure Audit collector in each of the namespaces that contain the Audit producer services, for example, Service Management and UCMDB. Audit collector requires the Audit producer file system and it will use the Audit producer PVC and subpath. This section describes the steps to install the Audit collector.",
    "url": "installauditcollectorhelm",
    "filename": "installauditcollectorhelm",
    "headings": [],
    "keywords": [
      "install",
      "audit",
      "collector",
      "services",
      "integrated",
      "service",
      "store",
      "audits",
      "nfs",
      "file",
      "system",
      "isn",
      "reachable.",
      "microservice",
      "collects",
      "passes",
      "via",
      "rest.",
      "deploy",
      "helm",
      "charts.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "example",
      "management",
      "ucmdb.",
      "requires",
      "pvc",
      "subpath.",
      "section",
      "describes",
      "steps",
      "collector."
    ],
    "language": "en",
    "word_count": 65,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install audit collector",
    "contentLower": "services that are integrated with the audit service store the audits on the nfs file system when the audit service isn't reachable. audit collector is a microservice that collects the audits from the nfs file system and passes them to the audit service via rest. you can deploy the audit collector using helm charts. install and configure audit collector in each of the namespaces that contain the audit producer services, for example, service management and ucmdb. audit collector requires the audit producer file system and it will use the audit producer pvc and subpath. this section describes the steps to install the audit collector.",
    "keywordsLower": [
      "install",
      "audit",
      "collector",
      "services",
      "integrated",
      "service",
      "store",
      "audits",
      "nfs",
      "file",
      "system",
      "isn",
      "reachable.",
      "microservice",
      "collects",
      "passes",
      "via",
      "rest.",
      "deploy",
      "helm",
      "charts.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "example",
      "management",
      "ucmdb.",
      "requires",
      "pvc",
      "subpath.",
      "section",
      "describes",
      "steps",
      "collector."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install Audit collector on-premises",
    "content": "This section provides the steps required to install the Audit collector. Install and configure Audit collector in each of the namespaces that contain the Audit producer services, for example, Service Management and UCMDB. Audit collector requires the Audit producer file system and it will use the Audit producer PVC and subpath. Generate vault secrets To generate vault secrets for the Audit collector chart, perform the following steps on the control plane node: Run the following command: $CDF_HOME/scripts/gen_secrets.sh -n <Audit Producer Namespace> -c <Audit Collector Chart File> -o <Secrets YAML File> Where, <Audit Audit Producer Namespace> is the namespace in which the Audit Producer (for example, Service Management and UCMDB) is running. <Audit Collector Chart File> is the tar file containing the Audit collector installation charts (audit-collector-<version>.tgz). <Secrets YAML File> is the YAML file that stores the Audit collector secret. Specify the file name (for example, audit-c",
    "url": "installauditcollector",
    "filename": "installauditcollector",
    "headings": [
      "Generate vault secrets",
      "Install Audit collector"
    ],
    "keywords": [
      "cert_two.crt",
      "idm.crt",
      "cert_one.crt",
      "ase.crt",
      "caCertificates.idm",
      "secrets.yaml",
      "secret.yaml",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "audit",
      "collector",
      "on-premises",
      "generate",
      "vault",
      "secrets",
      "section",
      "provides",
      "steps",
      "required",
      "collector.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "services",
      "example",
      "service",
      "management",
      "ucmdb.",
      "requires",
      "file",
      "system",
      "pvc",
      "subpath.",
      "chart",
      "perform",
      "following",
      "control",
      "plane",
      "node",
      "run",
      "command",
      "scripts",
      "-n",
      "-c",
      "-o",
      "namespace",
      "ucmdb",
      "running.",
      "tar",
      "containing",
      "installation",
      "charts",
      "audit-collector-.tgz",
      "yaml",
      "stores",
      "secret.",
      "specify",
      "name",
      "audit-collector-secret.yaml",
      "script",
      "after",
      "script.",
      "need",
      "opt",
      "cdf",
      "itsma-cfpni",
      "secret",
      "utility",
      "prompts",
      "integration",
      "user",
      "password",
      "complete",
      "deployment",
      "helm",
      "-f",
      "audit-collector-secrets.yaml",
      "--set-file",
      ".crt",
      "path",
      "unique",
      "release",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "charts.",
      "properties",
      "configured",
      "generates",
      "holds",
      "secrets.",
      "pass",
      "certificates",
      "option.",
      "see"
    ],
    "language": "en",
    "word_count": 87,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install audit collector on-premises",
    "contentLower": "this section provides the steps required to install the audit collector. install and configure audit collector in each of the namespaces that contain the audit producer services, for example, service management and ucmdb. audit collector requires the audit producer file system and it will use the audit producer pvc and subpath. generate vault secrets to generate vault secrets for the audit collector chart, perform the following steps on the control plane node: run the following command: $cdf_home/scripts/gen_secrets.sh -n <audit producer namespace> -c <audit collector chart file> -o <secrets yaml file> where, <audit audit producer namespace> is the namespace in which the audit producer (for example, service management and ucmdb) is running. <audit collector chart file> is the tar file containing the audit collector installation charts (audit-collector-<version>.tgz). <secrets yaml file> is the yaml file that stores the audit collector secret. specify the file name (for example, audit-c",
    "keywordsLower": [
      "cert_two.crt",
      "idm.crt",
      "cert_one.crt",
      "ase.crt",
      "cacertificates.idm",
      "secrets.yaml",
      "secret.yaml",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "audit",
      "collector",
      "on-premises",
      "generate",
      "vault",
      "secrets",
      "section",
      "provides",
      "steps",
      "required",
      "collector.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "services",
      "example",
      "service",
      "management",
      "ucmdb.",
      "requires",
      "file",
      "system",
      "pvc",
      "subpath.",
      "chart",
      "perform",
      "following",
      "control",
      "plane",
      "node",
      "run",
      "command",
      "scripts",
      "-n",
      "-c",
      "-o",
      "namespace",
      "ucmdb",
      "running.",
      "tar",
      "containing",
      "installation",
      "charts",
      "audit-collector-.tgz",
      "yaml",
      "stores",
      "secret.",
      "specify",
      "name",
      "audit-collector-secret.yaml",
      "script",
      "after",
      "script.",
      "need",
      "opt",
      "cdf",
      "itsma-cfpni",
      "secret",
      "utility",
      "prompts",
      "integration",
      "user",
      "password",
      "complete",
      "deployment",
      "helm",
      "-f",
      "audit-collector-secrets.yaml",
      "--set-file",
      ".crt",
      "path",
      "unique",
      "release",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "charts.",
      "properties",
      "configured",
      "generates",
      "holds",
      "secrets.",
      "pass",
      "certificates",
      "option.",
      "see"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Containerized (on-premises)",
    "content": "Operations Orchestration (OO) embedded in Design and Deploy (DND) has been removed. You must migrate from OO embedded in DND to OO Containerized before you can use the DND capability. OO Containerized is a capability that enables seamless automation and orchestration of repeatable IT operations across teams, tools, and environments. When installing OO Containerized you will get OO Central with an Internal OO Remote Action Service (RAS). The internal OO RAS is deployed automatically for each OO Central tenant in a tight coupling framework during the tenant deployment process. Additionally, you can install the following components depending on your requirements. External RAS - To achieve high availability of RASes, you can install an external RAS apart from the internal RAS available upon installation of OO Containerized. OO Workflow Designer - OO Workflow Designer is a web-based environment for authoring flows, which is available as a standalone product that can be deployed either on a ",
    "url": "installoocontainerized",
    "filename": "installoocontainerized",
    "headings": [],
    "keywords": [
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "on-premises",
      "operations",
      "orchestration",
      "embedded",
      "design",
      "deploy",
      "dnd",
      "removed.",
      "migrate",
      "before",
      "capability.",
      "capability",
      "enables",
      "seamless",
      "automation",
      "repeatable",
      "across",
      "teams",
      "tools",
      "environments.",
      "installing",
      "get",
      "central",
      "internal",
      "remote",
      "action",
      "service",
      "ras",
      "deployed",
      "automatically",
      "tenant",
      "tight",
      "coupling",
      "framework",
      "during",
      "deployment",
      "process.",
      "additionally",
      "following",
      "components",
      "depending",
      "requirements.",
      "external",
      "achieve",
      "high",
      "availability",
      "rases",
      "apart",
      "available",
      "upon",
      "installation",
      "containerized.",
      "workflow",
      "designer",
      "web-based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "either",
      "windows",
      "linux",
      "server.",
      "authored",
      "cloudslang",
      "language",
      "run",
      "central.",
      "script",
      "opb",
      "download",
      "center",
      "management",
      "ui",
      "displays",
      "direct",
      "links",
      "designer.",
      "see",
      "enable",
      "ui.",
      "information",
      "together",
      "manage",
      "various",
      "services",
      "devices",
      "organization",
      "lifecycle.",
      "mandatory",
      "table",
      "contains",
      "summary",
      "instructions",
      "topic"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo containerized (on-premises)",
    "contentLower": "operations orchestration (oo) embedded in design and deploy (dnd) has been removed. you must migrate from oo embedded in dnd to oo containerized before you can use the dnd capability. oo containerized is a capability that enables seamless automation and orchestration of repeatable it operations across teams, tools, and environments. when installing oo containerized you will get oo central with an internal oo remote action service (ras). the internal oo ras is deployed automatically for each oo central tenant in a tight coupling framework during the tenant deployment process. additionally, you can install the following components depending on your requirements. external ras - to achieve high availability of rases, you can install an external ras apart from the internal ras available upon installation of oo containerized. oo workflow designer - oo workflow designer is a web-based environment for authoring flows, which is available as a standalone product that can be deployed either on a ",
    "keywordsLower": [
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "on-premises",
      "operations",
      "orchestration",
      "embedded",
      "design",
      "deploy",
      "dnd",
      "removed.",
      "migrate",
      "before",
      "capability.",
      "capability",
      "enables",
      "seamless",
      "automation",
      "repeatable",
      "across",
      "teams",
      "tools",
      "environments.",
      "installing",
      "get",
      "central",
      "internal",
      "remote",
      "action",
      "service",
      "ras",
      "deployed",
      "automatically",
      "tenant",
      "tight",
      "coupling",
      "framework",
      "during",
      "deployment",
      "process.",
      "additionally",
      "following",
      "components",
      "depending",
      "requirements.",
      "external",
      "achieve",
      "high",
      "availability",
      "rases",
      "apart",
      "available",
      "upon",
      "installation",
      "containerized.",
      "workflow",
      "designer",
      "web-based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "either",
      "windows",
      "linux",
      "server.",
      "authored",
      "cloudslang",
      "language",
      "run",
      "central.",
      "script",
      "opb",
      "download",
      "center",
      "management",
      "ui",
      "displays",
      "direct",
      "links",
      "designer.",
      "see",
      "enable",
      "ui.",
      "information",
      "together",
      "manage",
      "various",
      "services",
      "devices",
      "organization",
      "lifecycle.",
      "mandatory",
      "table",
      "contains",
      "summary",
      "instructions",
      "topic"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Containerized on-premises",
    "content": "To deploy OO Containerized, run the following command on the control plane node. If you use custom values for resources in the values.yaml file, the global.oo.size parameter gets disabled. If you're adding certificates at Helm install time using CLI, ensure to remove the caCertificates parameter from the values.yaml file. helm install <OO INSTALL NAME> <OO CHART FILE> --namespace <OO NAMESPACE> -f <values.yaml> --set global.oo.size=xx --set-file \"caCertificates.cert_one\\.crt\"=/path/to/cert_one.crt Where: <OO INSTALL NAME> = the unique release name you need to specify for the OO deployment. You need to use the same release name as specified here when you upgrade OO in the future. <OO CHART FILE> = the absolute path of the OO chart file <OO NAMESPACE> = unique namespace of the OO deployment. Use the namespace you specified in the Create a new deployment for OO topic. <values.yaml> = the path to OO values.yaml file configured as part of the preparation step Create and configure values.yam",
    "url": "399-installooonpremises",
    "filename": "399-installooonpremises",
    "headings": [],
    "keywords": [
      "cert_one.crt",
      "x.tgz",
      "values.yaml",
      "global.oo",
      "install",
      "oo",
      "containerized",
      "on-premises",
      "deploy",
      "run",
      "following",
      "command",
      "control",
      "plane",
      "node.",
      "custom",
      "values",
      "resources",
      "file",
      "global.oo.size",
      "parameter",
      "gets",
      "disabled.",
      "re",
      "adding",
      "certificates",
      "helm",
      "time",
      "cli",
      "ensure",
      "remove",
      "cacertificates",
      "file.",
      "--namespace",
      "-f",
      "--set",
      "xx",
      "--set-file",
      ".crt",
      "path",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "absolute",
      "chart",
      "namespace",
      "create",
      "new",
      "deployment",
      "topic.",
      "configured",
      "part",
      "preparation",
      "step",
      "configure",
      "number",
      "tenants",
      "value",
      "between",
      "20.",
      "default",
      "don",
      "parameter.",
      "optional",
      "arguments",
      "because",
      "address",
      "one",
      "way",
      "having",
      "trust-specific",
      "certificates.",
      "depending",
      "choice",
      "during",
      "see",
      "service",
      "management",
      "integration",
      "parameters",
      "opted",
      "providing",
      "required",
      "provided",
      "directly",
      "command.",
      "example",
      "oo-helm-charts-1.x.x",
      "26.x.x",
      "oo-helm-charts",
      "charts",
      "oo-1.x.x"
    ],
    "language": "en",
    "word_count": 90,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo containerized on-premises",
    "contentLower": "to deploy oo containerized, run the following command on the control plane node. if you use custom values for resources in the values.yaml file, the global.oo.size parameter gets disabled. if you're adding certificates at helm install time using cli, ensure to remove the cacertificates parameter from the values.yaml file. helm install <oo install name> <oo chart file> --namespace <oo namespace> -f <values.yaml> --set global.oo.size=xx --set-file \"cacertificates.cert_one\\.crt\"=/path/to/cert_one.crt where: <oo install name> = the unique release name you need to specify for the oo deployment. you need to use the same release name as specified here when you upgrade oo in the future. <oo chart file> = the absolute path of the oo chart file <oo namespace> = unique namespace of the oo deployment. use the namespace you specified in the create a new deployment for oo topic. <values.yaml> = the path to oo values.yaml file configured as part of the preparation step create and configure values.yam",
    "keywordsLower": [
      "cert_one.crt",
      "x.tgz",
      "values.yaml",
      "global.oo",
      "install",
      "oo",
      "containerized",
      "on-premises",
      "deploy",
      "run",
      "following",
      "command",
      "control",
      "plane",
      "node.",
      "custom",
      "values",
      "resources",
      "file",
      "global.oo.size",
      "parameter",
      "gets",
      "disabled.",
      "re",
      "adding",
      "certificates",
      "helm",
      "time",
      "cli",
      "ensure",
      "remove",
      "cacertificates",
      "file.",
      "--namespace",
      "-f",
      "--set",
      "xx",
      "--set-file",
      ".crt",
      "path",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "absolute",
      "chart",
      "namespace",
      "create",
      "new",
      "deployment",
      "topic.",
      "configured",
      "part",
      "preparation",
      "step",
      "configure",
      "number",
      "tenants",
      "value",
      "between",
      "20.",
      "default",
      "don",
      "parameter.",
      "optional",
      "arguments",
      "because",
      "address",
      "one",
      "way",
      "having",
      "trust-specific",
      "certificates.",
      "depending",
      "choice",
      "during",
      "see",
      "service",
      "management",
      "integration",
      "parameters",
      "opted",
      "providing",
      "required",
      "provided",
      "directly",
      "command.",
      "example",
      "oo-helm-charts-1.x.x",
      "26.x.x",
      "oo-helm-charts",
      "charts",
      "oo-1.x.x"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import OO CA certificate into the suite",
    "content": "After completing the OO installation, you must import the OO CA certificate into the suite before you can deploy the OO capability in Suite Administration. Refer to the section applicable to your deployment. Classic deployment To import the OO CA certificate into the suite on a classic deployment, follow these steps: Get the OO CA certificate by following the document Get OO CA certificate. Rename the .CER file to .crt file using the command line. Change the ownership of the file OO certificate by using the command chown <SYSTEM_USER_ID>:<SYSTEM_GROUP_ID> <path-to-global-volume>/certificate/source/. For <system_user_id> and <system_group_id>: specify an operating system user ID and group ID respectively that are used to run the process in the container. Both values must be 1999 or a number between 100000 and 2000000000 (for example, UID=100001 and GID=100002). Copy the saved .crt file into <path-to-global-volume>/certificate/source/. Example of <path-to-global-volume>: /var/vols/itom/g",
    "url": "importoocertificateintosuite",
    "filename": "importoocertificateintosuite",
    "headings": [
      "Classic deployment",
      "Helm deployment"
    ],
    "keywords": [
      "import",
      "oo",
      "ca",
      "certificate",
      "suite",
      "classic",
      "deployment",
      "helm",
      "after",
      "completing",
      "installation",
      "before",
      "deploy",
      "capability",
      "administration.",
      "refer",
      "section",
      "applicable",
      "deployment.",
      "follow",
      "steps",
      "get",
      "following",
      "document",
      "certificate.",
      "rename",
      ".cer",
      "file",
      ".crt",
      "command",
      "line.",
      "change",
      "ownership",
      "chown",
      "source",
      "specify",
      "operating",
      "system",
      "user",
      "id",
      "group",
      "respectively",
      "run",
      "process",
      "container.",
      "both",
      "values",
      "1999",
      "number",
      "between",
      "100000",
      "2000000000",
      "example",
      "uid",
      "100001",
      "gid",
      "100002",
      "copy",
      "saved",
      "var",
      "vols",
      "itom",
      "global-volume",
      "restart",
      "platform",
      "pods",
      "running",
      "commands",
      "kubectl",
      "rollout",
      "-n",
      "itom-xruntime-platform",
      "itom-xruntime-platform-offline",
      "suite.",
      "unique",
      "namespace"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import oo ca certificate into the suite",
    "contentLower": "after completing the oo installation, you must import the oo ca certificate into the suite before you can deploy the oo capability in suite administration. refer to the section applicable to your deployment. classic deployment to import the oo ca certificate into the suite on a classic deployment, follow these steps: get the oo ca certificate by following the document get oo ca certificate. rename the .cer file to .crt file using the command line. change the ownership of the file oo certificate by using the command chown <system_user_id>:<system_group_id> <path-to-global-volume>/certificate/source/. for <system_user_id> and <system_group_id>: specify an operating system user id and group id respectively that are used to run the process in the container. both values must be 1999 or a number between 100000 and 2000000000 (for example, uid=100001 and gid=100002). copy the saved .crt file into <path-to-global-volume>/certificate/source/. example of <path-to-global-volume>: /var/vols/itom/g",
    "keywordsLower": [
      "import",
      "oo",
      "ca",
      "certificate",
      "suite",
      "classic",
      "deployment",
      "helm",
      "after",
      "completing",
      "installation",
      "before",
      "deploy",
      "capability",
      "administration.",
      "refer",
      "section",
      "applicable",
      "deployment.",
      "follow",
      "steps",
      "get",
      "following",
      "document",
      "certificate.",
      "rename",
      ".cer",
      "file",
      ".crt",
      "command",
      "line.",
      "change",
      "ownership",
      "chown",
      "source",
      "specify",
      "operating",
      "system",
      "user",
      "id",
      "group",
      "respectively",
      "run",
      "process",
      "container.",
      "both",
      "values",
      "1999",
      "number",
      "between",
      "100000",
      "2000000000",
      "example",
      "uid",
      "100001",
      "gid",
      "100002",
      "copy",
      "saved",
      "var",
      "vols",
      "itom",
      "global-volume",
      "restart",
      "platform",
      "pods",
      "running",
      "commands",
      "kubectl",
      "rollout",
      "-n",
      "itom-xruntime-platform",
      "itom-xruntime-platform-offline",
      "suite.",
      "unique",
      "namespace"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install external OO Containerized components",
    "content": "Operations Orchestration (OO) supports the following components: OO Remote Action Server (RAS): OO RAS enables execution in remote data centers and networks. The RAS interacts with Central and polls it for operations to execute. Installation of external OO RAS is optional and depends on whether you need additional RASes apart from that's already available when OO Central installs. OO Workflow Designer: OO Workflow Designer is a web based environment for authoring flows, which is available as a standalone product that you can deploy either on a Windows or Linux server. Installation of OO Workflow Designer is optional and depends on your requirement to author flows. You need another installation for the OO Workflow Designer as it's not installed along with OO Central. For information about installing these external components, see Install additional OO components.",
    "url": "installoocomponents",
    "filename": "installoocomponents",
    "headings": [],
    "keywords": [
      "install",
      "external",
      "oo",
      "containerized",
      "components",
      "operations",
      "orchestration",
      "supports",
      "following",
      "remote",
      "action",
      "server",
      "ras",
      "enables",
      "execution",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "installation",
      "optional",
      "depends",
      "whether",
      "need",
      "additional",
      "rases",
      "apart",
      "already",
      "available",
      "installs.",
      "workflow",
      "designer",
      "web",
      "based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deploy",
      "either",
      "windows",
      "linux",
      "server.",
      "requirement",
      "author",
      "flows.",
      "another",
      "installed",
      "along",
      "central.",
      "information",
      "about",
      "installing",
      "see",
      "components."
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install external oo containerized components",
    "contentLower": "operations orchestration (oo) supports the following components: oo remote action server (ras): oo ras enables execution in remote data centers and networks. the ras interacts with central and polls it for operations to execute. installation of external oo ras is optional and depends on whether you need additional rases apart from that's already available when oo central installs. oo workflow designer: oo workflow designer is a web based environment for authoring flows, which is available as a standalone product that you can deploy either on a windows or linux server. installation of oo workflow designer is optional and depends on your requirement to author flows. you need another installation for the oo workflow designer as it's not installed along with oo central. for information about installing these external components, see install additional oo components.",
    "keywordsLower": [
      "install",
      "external",
      "oo",
      "containerized",
      "components",
      "operations",
      "orchestration",
      "supports",
      "following",
      "remote",
      "action",
      "server",
      "ras",
      "enables",
      "execution",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "installation",
      "optional",
      "depends",
      "whether",
      "need",
      "additional",
      "rases",
      "apart",
      "already",
      "available",
      "installs.",
      "workflow",
      "designer",
      "web",
      "based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deploy",
      "either",
      "windows",
      "linux",
      "server.",
      "requirement",
      "author",
      "flows.",
      "another",
      "installed",
      "along",
      "central.",
      "information",
      "about",
      "installing",
      "see",
      "components."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install Audit on-premises",
    "content": "This section gives the steps required to install the Audit service. Prepare the certificates You need to copy the root certificate (base64 encoded) of the Audit database instance and the root suite certificate (base64 encoded) in the same folder where you have downloaded the Audit Helm chart (for example, audit-2x.x). Generate vault secrets A secret is an object that has sensitive data such as a password, a token, or a key. Use the gen_secrets.sh script to create secrets. This script creates a YAML file that contains all the Audit secrets. You must pass this YAML file in the command line during Audit installation. To generate vault secrets for the Audit charts, perform the following steps on the control plane node: Run the following command: $CDF_HOME/scripts/gen_secrets.sh -n <Audit Namespace> -c <Audit Chart File> -o <Secrets YAML File> Where, <Audit Namespace> is the unique namespace of the Audit deployment. <Audit Chart File> is the tar file containing the Audit installation charts",
    "url": "installauditonpremises",
    "filename": "installauditonpremises",
    "headings": [
      "Prepare the certificates",
      "Generate vault secrets",
      "IdM transport user password",
      "Install Audit service",
      "Verify the installation",
      "(Optional) Enable Audit menu for existing active tenants",
      "Enable Audit menu for all active tenants",
      "Enable Audit menu for a specific active tenant",
      "Related topic"
    ],
    "keywords": [
      "cert_two.crt",
      "EnableAuditMegaMenu.bin",
      "RE_ca_db.crt",
      "cert_one.crt",
      "https://<AuditService",
      "https://audit.example.com:443",
      "EnableAuditMegaMenu.sh",
      "https://audit.example.com:9889",
      "RE_ca_idm.crt",
      "secrets.yaml",
      "rootCA.crt",
      "customCA.crt",
      "values.yaml",
      "example.com",
      "gen_secrets.sh",
      "install",
      "audit",
      "on-premises",
      "prepare",
      "certificates",
      "generate",
      "vault",
      "secrets",
      "idm",
      "transport",
      "user",
      "password",
      "service",
      "verify",
      "installation",
      "optional",
      "enable",
      "menu",
      "existing",
      "active",
      "tenants",
      "all",
      "specific",
      "tenant",
      "related",
      "topic",
      "section",
      "gives",
      "steps",
      "required",
      "service.",
      "need",
      "copy",
      "root",
      "certificate",
      "base64",
      "encoded",
      "database",
      "instance",
      "suite",
      "same",
      "folder",
      "downloaded",
      "helm",
      "chart",
      "example",
      "audit-2x.x",
      "secret",
      "object",
      "sensitive",
      "data",
      "such",
      "token",
      "key.",
      "script",
      "create",
      "secrets.",
      "creates",
      "yaml",
      "file",
      "contains",
      "pass",
      "command",
      "line",
      "during",
      "installation.",
      "charts",
      "perform",
      "following",
      "control",
      "plane",
      "node",
      "run",
      "scripts",
      "-n",
      "-c",
      "-o",
      "unique",
      "namespace",
      "deployment.",
      "tar",
      "containing",
      "audit-.tgz",
      "stores",
      "specify"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install audit on-premises",
    "contentLower": "this section gives the steps required to install the audit service. prepare the certificates you need to copy the root certificate (base64 encoded) of the audit database instance and the root suite certificate (base64 encoded) in the same folder where you have downloaded the audit helm chart (for example, audit-2x.x). generate vault secrets a secret is an object that has sensitive data such as a password, a token, or a key. use the gen_secrets.sh script to create secrets. this script creates a yaml file that contains all the audit secrets. you must pass this yaml file in the command line during audit installation. to generate vault secrets for the audit charts, perform the following steps on the control plane node: run the following command: $cdf_home/scripts/gen_secrets.sh -n <audit namespace> -c <audit chart file> -o <secrets yaml file> where, <audit namespace> is the unique namespace of the audit deployment. <audit chart file> is the tar file containing the audit installation charts",
    "keywordsLower": [
      "cert_two.crt",
      "enableauditmegamenu.bin",
      "re_ca_db.crt",
      "cert_one.crt",
      "https://<auditservice",
      "https://audit.example.com:443",
      "enableauditmegamenu.sh",
      "https://audit.example.com:9889",
      "re_ca_idm.crt",
      "secrets.yaml",
      "rootca.crt",
      "customca.crt",
      "values.yaml",
      "example.com",
      "gen_secrets.sh",
      "install",
      "audit",
      "on-premises",
      "prepare",
      "certificates",
      "generate",
      "vault",
      "secrets",
      "idm",
      "transport",
      "user",
      "password",
      "service",
      "verify",
      "installation",
      "optional",
      "enable",
      "menu",
      "existing",
      "active",
      "tenants",
      "all",
      "specific",
      "tenant",
      "related",
      "topic",
      "section",
      "gives",
      "steps",
      "required",
      "service.",
      "need",
      "copy",
      "root",
      "certificate",
      "base64",
      "encoded",
      "database",
      "instance",
      "suite",
      "same",
      "folder",
      "downloaded",
      "helm",
      "chart",
      "example",
      "audit-2x.x",
      "secret",
      "object",
      "sensitive",
      "data",
      "such",
      "token",
      "key.",
      "script",
      "create",
      "secrets.",
      "creates",
      "yaml",
      "file",
      "contains",
      "pass",
      "command",
      "line",
      "during",
      "installation.",
      "charts",
      "perform",
      "following",
      "control",
      "plane",
      "node",
      "run",
      "scripts",
      "-n",
      "-c",
      "-o",
      "unique",
      "namespace",
      "deployment.",
      "tar",
      "containing",
      "audit-.tgz",
      "stores",
      "specify"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install on AWS (EKS)",
    "content": "This section describes how to install the suite on Amazon Web Services (AWS) using Amazon Elastic Kubernetes Service (EKS). Before you begin Before you begin, ensure that you've reviewed the following planning topics for the EKS deployment: The overall reference architecture. The support matrix. The suite size definitions and the required hardware resources. The EKS deployment FAQs. Deployment tasks Refer to the Deploy the suite on AWS for all the preparation, installation, and setup tasks to deploy the suite on EKS.",
    "url": "eks",
    "filename": "eks",
    "headings": [
      "Before you begin",
      "Deployment tasks"
    ],
    "keywords": [
      "install",
      "aws",
      "eks",
      "before",
      "begin",
      "deployment",
      "tasks",
      "section",
      "describes",
      "suite",
      "amazon",
      "web",
      "services",
      "elastic",
      "kubernetes",
      "service",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "topics",
      "overall",
      "reference",
      "architecture.",
      "support",
      "matrix.",
      "size",
      "definitions",
      "required",
      "hardware",
      "resources.",
      "faqs.",
      "refer",
      "deploy",
      "all",
      "preparation",
      "installation",
      "setup",
      "eks."
    ],
    "language": "en",
    "word_count": 56,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install on aws (eks)",
    "contentLower": "this section describes how to install the suite on amazon web services (aws) using amazon elastic kubernetes service (eks). before you begin before you begin, ensure that you've reviewed the following planning topics for the eks deployment: the overall reference architecture. the support matrix. the suite size definitions and the required hardware resources. the eks deployment faqs. deployment tasks refer to the deploy the suite on aws for all the preparation, installation, and setup tasks to deploy the suite on eks.",
    "keywordsLower": [
      "install",
      "aws",
      "eks",
      "before",
      "begin",
      "deployment",
      "tasks",
      "section",
      "describes",
      "suite",
      "amazon",
      "web",
      "services",
      "elastic",
      "kubernetes",
      "service",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "topics",
      "overall",
      "reference",
      "architecture.",
      "support",
      "matrix.",
      "size",
      "definitions",
      "required",
      "hardware",
      "resources.",
      "faqs.",
      "refer",
      "deploy",
      "all",
      "preparation",
      "installation",
      "setup",
      "eks."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Installation tasks (EKS)",
    "content": "This section contains some of the mandatory and optional tasks to install the suite in an on-premises environment. Activate the Docker Hub account Download the cloud deployment package Create AWS user policy and key pair Prepare VPC Build EKS cluster Configure bastion Configure custom networking Configure EFS Launch RDS Transfer images beforehand Configure an Application Load Balancer Configure an ingress for the suite Configure an Application Load Balancer The rest of the installation tasks are listed on the Helm installation common tasks page.",
    "url": "tasksonaws",
    "filename": "tasksonaws",
    "headings": [],
    "keywords": [
      "installation",
      "tasks",
      "eks",
      "section",
      "contains",
      "mandatory",
      "optional",
      "install",
      "suite",
      "on-premises",
      "environment.",
      "activate",
      "docker",
      "hub",
      "account",
      "download",
      "cloud",
      "deployment",
      "package",
      "create",
      "aws",
      "user",
      "policy",
      "key",
      "pair",
      "prepare",
      "vpc",
      "build",
      "cluster",
      "configure",
      "bastion",
      "custom",
      "networking",
      "efs",
      "launch",
      "rds",
      "transfer",
      "images",
      "beforehand",
      "application",
      "load",
      "balancer",
      "ingress",
      "rest",
      "listed",
      "helm",
      "common",
      "page."
    ],
    "language": "en",
    "word_count": 63,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "installation tasks (eks)",
    "contentLower": "this section contains some of the mandatory and optional tasks to install the suite in an on-premises environment. activate the docker hub account download the cloud deployment package create aws user policy and key pair prepare vpc build eks cluster configure bastion configure custom networking configure efs launch rds transfer images beforehand configure an application load balancer configure an ingress for the suite configure an application load balancer the rest of the installation tasks are listed on the helm installation common tasks page.",
    "keywordsLower": [
      "installation",
      "tasks",
      "eks",
      "section",
      "contains",
      "mandatory",
      "optional",
      "install",
      "suite",
      "on-premises",
      "environment.",
      "activate",
      "docker",
      "hub",
      "account",
      "download",
      "cloud",
      "deployment",
      "package",
      "create",
      "aws",
      "user",
      "policy",
      "key",
      "pair",
      "prepare",
      "vpc",
      "build",
      "cluster",
      "configure",
      "bastion",
      "custom",
      "networking",
      "efs",
      "launch",
      "rds",
      "transfer",
      "images",
      "beforehand",
      "application",
      "load",
      "balancer",
      "ingress",
      "rest",
      "listed",
      "helm",
      "common",
      "page."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Launch RDS",
    "content": "The suite contains an internal (embedded) PostgreSQL database instance, which should be used for demonstration purposes only. You must use external databases in a production environment. For information about supported external databases, see Support matrix for EKS. When deployed on AWS, the suite uses RDS as the database engine. This RDS instance will be used as the external databases for the suite in a production environment. Create an RDS instance Role Location Database administrator AWS Management Console We recommend that you use RDS as the database engine on AWS. Follow these steps to launch RDS. Log in to the AWS Management Console and switch to the selected region. Go to All services > Management & Governance > CloudFormation. Click Create stack > With new resources(standard), click Template is ready > Upload a template file > Choose file and select the smax-rds.template file under ../Cloud Deployment x.x.x/../byok/smax-cloudformation-aws. Click Next and populate the following ",
    "url": "eksdatabase",
    "filename": "eksdatabase",
    "headings": [
      "Create an RDS instance"
    ],
    "keywords": [
      "launch",
      "rds",
      "create",
      "instance",
      "suite",
      "contains",
      "internal",
      "embedded",
      "postgresql",
      "database",
      "demonstration",
      "purposes",
      "only.",
      "external",
      "databases",
      "production",
      "environment.",
      "information",
      "about",
      "supported",
      "see",
      "support",
      "matrix",
      "eks.",
      "deployed",
      "aws",
      "uses",
      "engine.",
      "role",
      "location",
      "administrator",
      "management",
      "console",
      "recommend",
      "engine",
      "aws.",
      "follow",
      "steps",
      "rds.",
      "log",
      "switch",
      "selected",
      "region.",
      "go",
      "all",
      "services",
      "governance",
      "cloudformation.",
      "click",
      "stack",
      "new",
      "resources",
      "standard",
      "template",
      "ready",
      "upload",
      "file",
      "choose",
      "select",
      "smax-rds.template",
      "under",
      "cloud",
      "deployment",
      "x.x.x",
      "byok",
      "smax-cloudformation-aws.",
      "next",
      "populate",
      "following",
      "fields",
      "name",
      "give",
      "such",
      "sma-rds.",
      "vpc",
      "id",
      "created",
      "drop-down",
      "list.",
      "first",
      "second",
      "subnet",
      "two",
      "subnets",
      "type",
      "sizing",
      "considerations.",
      "identifier",
      "enter",
      "name.",
      "user",
      "define",
      "username",
      "admin",
      "account",
      "postgres.",
      "password",
      "account.",
      "storage",
      "size"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "launch rds",
    "contentLower": "the suite contains an internal (embedded) postgresql database instance, which should be used for demonstration purposes only. you must use external databases in a production environment. for information about supported external databases, see support matrix for eks. when deployed on aws, the suite uses rds as the database engine. this rds instance will be used as the external databases for the suite in a production environment. create an rds instance role location database administrator aws management console we recommend that you use rds as the database engine on aws. follow these steps to launch rds. log in to the aws management console and switch to the selected region. go to all services > management & governance > cloudformation. click create stack > with new resources(standard), click template is ready > upload a template file > choose file and select the smax-rds.template file under ../cloud deployment x.x.x/../byok/smax-cloudformation-aws. click next and populate the following ",
    "keywordsLower": [
      "launch",
      "rds",
      "create",
      "instance",
      "suite",
      "contains",
      "internal",
      "embedded",
      "postgresql",
      "database",
      "demonstration",
      "purposes",
      "only.",
      "external",
      "databases",
      "production",
      "environment.",
      "information",
      "about",
      "supported",
      "see",
      "support",
      "matrix",
      "eks.",
      "deployed",
      "aws",
      "uses",
      "engine.",
      "role",
      "location",
      "administrator",
      "management",
      "console",
      "recommend",
      "engine",
      "aws.",
      "follow",
      "steps",
      "rds.",
      "log",
      "switch",
      "selected",
      "region.",
      "go",
      "all",
      "services",
      "governance",
      "cloudformation.",
      "click",
      "stack",
      "new",
      "resources",
      "standard",
      "template",
      "ready",
      "upload",
      "file",
      "choose",
      "select",
      "smax-rds.template",
      "under",
      "cloud",
      "deployment",
      "x.x.x",
      "byok",
      "smax-cloudformation-aws.",
      "next",
      "populate",
      "following",
      "fields",
      "name",
      "give",
      "such",
      "sma-rds.",
      "vpc",
      "id",
      "created",
      "drop-down",
      "list.",
      "first",
      "second",
      "subnet",
      "two",
      "subnets",
      "type",
      "sizing",
      "considerations.",
      "identifier",
      "enter",
      "name.",
      "user",
      "define",
      "username",
      "admin",
      "account",
      "postgres.",
      "password",
      "account.",
      "storage",
      "size"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install and configure Vertica database",
    "content": "If you plan to install Cloud Management Platform (CMP) FinOps, you must install and configure the Vertica database as a prerequisite. Create a Vertica cluster You can install by creating a Vertica cluster using a CloudFormation template and step-by-step wizards in MC, or manually deploy it using an Amazon Machine Image (AMI) that has Vertica installed when you create your instances. Eon Mode databases are currently only supported on AWS resources. For the AWS-specific installation procedure, see Deploy AWS Instances for your Vertica Database Cluster. For information on configuring a Vertica database, see Generating TLS Certificates and Keys. Configure Vertica Set up an AWS network load balancer (NLB) for the Vertica cluster Once you have set up a Vertica cluster, Vertica recommends putting an AWS NLB in front of your Vertica cluster. This section describes how to configure an NLB. Create a target group In the AWS Management Console, navigate to the AWS EC2 for the region where you have",
    "url": "installconfverticaeks",
    "filename": "installconfverticaeks",
    "headings": [
      "Create a Vertica cluster",
      "Configure Vertica",
      "Create a target group",
      "Create a load balancer",
      "Configure connection settings"
    ],
    "keywords": [
      "install",
      "configure",
      "vertica",
      "database",
      "create",
      "cluster",
      "target",
      "group",
      "load",
      "balancer",
      "connection",
      "settings",
      "plan",
      "cloud",
      "management",
      "platform",
      "cmp",
      "finops",
      "prerequisite.",
      "creating",
      "cloudformation",
      "template",
      "step-by-step",
      "wizards",
      "mc",
      "manually",
      "deploy",
      "amazon",
      "machine",
      "image",
      "ami",
      "installed",
      "instances.",
      "eon",
      "mode",
      "databases",
      "currently",
      "supported",
      "aws",
      "resources.",
      "aws-specific",
      "installation",
      "procedure",
      "see",
      "instances",
      "cluster.",
      "information",
      "configuring",
      "generating",
      "tls",
      "certificates",
      "keys.",
      "set",
      "network",
      "nlb",
      "once",
      "recommends",
      "putting",
      "front",
      "section",
      "describes",
      "nlb.",
      "console",
      "navigate",
      "ec2",
      "region",
      "deployed",
      "under",
      "balancing",
      "click",
      "groups.",
      "group.",
      "select",
      "type.",
      "enter",
      "value",
      "name.",
      "example",
      "vertica-target-group.",
      "protocol",
      "tcp",
      "port",
      "5433.",
      "same",
      "vpc",
      "health",
      "check.",
      "leave",
      "advanced",
      "default.",
      "next.",
      "instance",
      "list",
      "ones",
      "key-pair",
      "them.",
      "don",
      "console.",
      "ports",
      "selected"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install and configure vertica database",
    "contentLower": "if you plan to install cloud management platform (cmp) finops, you must install and configure the vertica database as a prerequisite. create a vertica cluster you can install by creating a vertica cluster using a cloudformation template and step-by-step wizards in mc, or manually deploy it using an amazon machine image (ami) that has vertica installed when you create your instances. eon mode databases are currently only supported on aws resources. for the aws-specific installation procedure, see deploy aws instances for your vertica database cluster. for information on configuring a vertica database, see generating tls certificates and keys. configure vertica set up an aws network load balancer (nlb) for the vertica cluster once you have set up a vertica cluster, vertica recommends putting an aws nlb in front of your vertica cluster. this section describes how to configure an nlb. create a target group in the aws management console, navigate to the aws ec2 for the region where you have",
    "keywordsLower": [
      "install",
      "configure",
      "vertica",
      "database",
      "create",
      "cluster",
      "target",
      "group",
      "load",
      "balancer",
      "connection",
      "settings",
      "plan",
      "cloud",
      "management",
      "platform",
      "cmp",
      "finops",
      "prerequisite.",
      "creating",
      "cloudformation",
      "template",
      "step-by-step",
      "wizards",
      "mc",
      "manually",
      "deploy",
      "amazon",
      "machine",
      "image",
      "ami",
      "installed",
      "instances.",
      "eon",
      "mode",
      "databases",
      "currently",
      "supported",
      "aws",
      "resources.",
      "aws-specific",
      "installation",
      "procedure",
      "see",
      "instances",
      "cluster.",
      "information",
      "configuring",
      "generating",
      "tls",
      "certificates",
      "keys.",
      "set",
      "network",
      "nlb",
      "once",
      "recommends",
      "putting",
      "front",
      "section",
      "describes",
      "nlb.",
      "console",
      "navigate",
      "ec2",
      "region",
      "deployed",
      "under",
      "balancing",
      "click",
      "groups.",
      "group.",
      "select",
      "type.",
      "enter",
      "value",
      "name.",
      "example",
      "vertica-target-group.",
      "protocol",
      "tcp",
      "port",
      "5433.",
      "same",
      "vpc",
      "health",
      "check.",
      "leave",
      "advanced",
      "default.",
      "next.",
      "instance",
      "list",
      "ones",
      "key-pair",
      "them.",
      "don",
      "console.",
      "ports",
      "selected"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Launch EKS cluster worker nodes for UD/UCMDB",
    "content": "This task is required only when you need to install UD/UCMDB. See Sizing guides (EKS) for information about the additional EKS cluster worker nodes required to deploy UD/UCMDB. Launch EKS cluster worker nodes for UD/UCMDB Navigate to the smax-managed-workernode.template file under ../Cloud Deployment x.x.x/byok/smax-cloudformation-aws in the cloud deployment package. Create a copy of the smax-managed-workernode.template file and rename the new file to cms-managed-workernode.template. Open the renamed file, replace smax-cluster and smax-workernodes to cms-cluster and cms-workernodes, and then save the file. Use the updated cms-managed-workernode.template file to create additional EKS cluster worker nodes for UD/UCMDB based on the EKS cluster that you have created for the suite, and specify a dedicated Worker Node Group name for UD/UCMDB. For reference, see the steps that you performed for the suite as described in Launch EKS cluster worker nodes. Specify a label for the UD/UCMDB node gr",
    "url": "launcheksclusterworkernodes4cms",
    "filename": "launcheksclusterworkernodes4cms",
    "headings": [
      "Launch EKS cluster worker nodes for UD/UCMDB",
      "Specify a label for the UD/UCMDB node group",
      "Verify"
    ],
    "keywords": [
      "uducmdb",
      "node.type",
      "launch",
      "eks",
      "cluster",
      "worker",
      "nodes",
      "ud",
      "ucmdb",
      "specify",
      "label",
      "node",
      "group",
      "verify",
      "task",
      "required",
      "need",
      "install",
      "ucmdb.",
      "see",
      "sizing",
      "guides",
      "information",
      "about",
      "additional",
      "deploy",
      "navigate",
      "smax-managed-workernode.template",
      "file",
      "under",
      "cloud",
      "deployment",
      "x.x.x",
      "byok",
      "smax-cloudformation-aws",
      "package.",
      "create",
      "copy",
      "rename",
      "new",
      "cms-managed-workernode.template.",
      "open",
      "renamed",
      "replace",
      "smax-cluster",
      "smax-workernodes",
      "cms-cluster",
      "cms-workernodes",
      "save",
      "file.",
      "updated",
      "cms-managed-workernode.template",
      "based",
      "created",
      "suite",
      "dedicated",
      "name",
      "reference",
      "steps",
      "performed",
      "described",
      "nodes.",
      "perform",
      "following",
      "go",
      "locate",
      "created.",
      "click",
      "compute",
      "find",
      "group.",
      "edit.",
      "edit",
      "kubernetes",
      "labels",
      "follows.",
      "key",
      "value",
      "role",
      "loadbalancer",
      "cms",
      "changes.",
      "after",
      "run",
      "command",
      "bastion",
      "check",
      "successfully",
      "assigned",
      "kubectl",
      "get",
      "--show-labels",
      "review",
      "output",
      "returned",
      "above",
      "checking"
    ],
    "language": "en",
    "word_count": 107,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "launch eks cluster worker nodes for ud/ucmdb",
    "contentLower": "this task is required only when you need to install ud/ucmdb. see sizing guides (eks) for information about the additional eks cluster worker nodes required to deploy ud/ucmdb. launch eks cluster worker nodes for ud/ucmdb navigate to the smax-managed-workernode.template file under ../cloud deployment x.x.x/byok/smax-cloudformation-aws in the cloud deployment package. create a copy of the smax-managed-workernode.template file and rename the new file to cms-managed-workernode.template. open the renamed file, replace smax-cluster and smax-workernodes to cms-cluster and cms-workernodes, and then save the file. use the updated cms-managed-workernode.template file to create additional eks cluster worker nodes for ud/ucmdb based on the eks cluster that you have created for the suite, and specify a dedicated worker node group name for ud/ucmdb. for reference, see the steps that you performed for the suite as described in launch eks cluster worker nodes. specify a label for the ud/ucmdb node gr",
    "keywordsLower": [
      "uducmdb",
      "node.type",
      "launch",
      "eks",
      "cluster",
      "worker",
      "nodes",
      "ud",
      "ucmdb",
      "specify",
      "label",
      "node",
      "group",
      "verify",
      "task",
      "required",
      "need",
      "install",
      "ucmdb.",
      "see",
      "sizing",
      "guides",
      "information",
      "about",
      "additional",
      "deploy",
      "navigate",
      "smax-managed-workernode.template",
      "file",
      "under",
      "cloud",
      "deployment",
      "x.x.x",
      "byok",
      "smax-cloudformation-aws",
      "package.",
      "create",
      "copy",
      "rename",
      "new",
      "cms-managed-workernode.template.",
      "open",
      "renamed",
      "replace",
      "smax-cluster",
      "smax-workernodes",
      "cms-cluster",
      "cms-workernodes",
      "save",
      "file.",
      "updated",
      "cms-managed-workernode.template",
      "based",
      "created",
      "suite",
      "dedicated",
      "name",
      "reference",
      "steps",
      "performed",
      "described",
      "nodes.",
      "perform",
      "following",
      "go",
      "locate",
      "created.",
      "click",
      "compute",
      "find",
      "group.",
      "edit.",
      "edit",
      "kubernetes",
      "labels",
      "follows.",
      "key",
      "value",
      "role",
      "loadbalancer",
      "cms",
      "changes.",
      "after",
      "run",
      "command",
      "bastion",
      "check",
      "successfully",
      "assigned",
      "kubectl",
      "get",
      "--show-labels",
      "review",
      "output",
      "returned",
      "above",
      "checking"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Launch RDS for UD/UCMDB",
    "content": "UD/UCMDB contains an internal (embedded) PostgreSQL database instance, which should be used for demonstration purposes only. You must use external databases in a production environment. When deployed on AWS, the suite uses RDS as the database engine. This RDS instance will be used as the external databases for the following UD/UCMDB components: UCMDB Server AutoPass License Service (APLS) Prepare RDS template for UD/UCMDB Navigate to the smax-rds.template file under ../Cloud Deployment x.x.x/byok/smax-cloudformation-aws in the cloud deployment package. Create a copy of the smax-rds.template file and rename the new file to cms-rds.template. Replace the two SMAX-RDS-Security-Group keys (under RDSSecurityGroup in cms-rds.template) with CMS-RDS-Security-Group, and then save the file. Create an RDS instance for UD/UCMDB We recommend that you use RDS as the database engine on AWS. Follow these steps to launch RDS for UD/UCMDB. Log in to the AWS Management Console and switch to the selected r",
    "url": "launchrdsekscms",
    "filename": "launchrdsekscms",
    "headings": [
      "Prepare RDS template for UD/UCMDB",
      "Create an RDS instance for UD/UCMDB",
      "Set up PostgreSQL databases for UD/UCMDB on the RDS instance"
    ],
    "keywords": [
      "uducmdb",
      "values.yaml",
      "launch",
      "rds",
      "ud",
      "ucmdb",
      "prepare",
      "template",
      "create",
      "instance",
      "set",
      "postgresql",
      "databases",
      "contains",
      "internal",
      "embedded",
      "database",
      "demonstration",
      "purposes",
      "only.",
      "external",
      "production",
      "environment.",
      "deployed",
      "aws",
      "suite",
      "uses",
      "engine.",
      "following",
      "components",
      "server",
      "autopass",
      "license",
      "service",
      "apls",
      "navigate",
      "smax-rds.template",
      "file",
      "under",
      "cloud",
      "deployment",
      "x.x.x",
      "byok",
      "smax-cloudformation-aws",
      "package.",
      "copy",
      "rename",
      "new",
      "cms-rds.template.",
      "replace",
      "two",
      "smax-rds-security-group",
      "keys",
      "rdssecuritygroup",
      "cms-rds.template",
      "cms-rds-security-group",
      "save",
      "file.",
      "recommend",
      "engine",
      "aws.",
      "follow",
      "steps",
      "ucmdb.",
      "log",
      "management",
      "console",
      "switch",
      "selected",
      "region.",
      "go",
      "all",
      "services",
      "governance",
      "cloudformation.",
      "click",
      "stack",
      "resources",
      "standard",
      "ready",
      "upload",
      "choose",
      "select",
      "smax-cloudformation-aws.",
      "next",
      "populate",
      "fields",
      "name",
      "give",
      "such",
      "cms-rds.",
      "vpc",
      "id",
      "created",
      "drop-down",
      "list.",
      "first",
      "second",
      "subnet",
      "rds."
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "launch rds for ud/ucmdb",
    "contentLower": "ud/ucmdb contains an internal (embedded) postgresql database instance, which should be used for demonstration purposes only. you must use external databases in a production environment. when deployed on aws, the suite uses rds as the database engine. this rds instance will be used as the external databases for the following ud/ucmdb components: ucmdb server autopass license service (apls) prepare rds template for ud/ucmdb navigate to the smax-rds.template file under ../cloud deployment x.x.x/byok/smax-cloudformation-aws in the cloud deployment package. create a copy of the smax-rds.template file and rename the new file to cms-rds.template. replace the two smax-rds-security-group keys (under rdssecuritygroup in cms-rds.template) with cms-rds-security-group, and then save the file. create an rds instance for ud/ucmdb we recommend that you use rds as the database engine on aws. follow these steps to launch rds for ud/ucmdb. log in to the aws management console and switch to the selected r",
    "keywordsLower": [
      "uducmdb",
      "values.yaml",
      "launch",
      "rds",
      "ud",
      "ucmdb",
      "prepare",
      "template",
      "create",
      "instance",
      "set",
      "postgresql",
      "databases",
      "contains",
      "internal",
      "embedded",
      "database",
      "demonstration",
      "purposes",
      "only.",
      "external",
      "production",
      "environment.",
      "deployed",
      "aws",
      "suite",
      "uses",
      "engine.",
      "following",
      "components",
      "server",
      "autopass",
      "license",
      "service",
      "apls",
      "navigate",
      "smax-rds.template",
      "file",
      "under",
      "cloud",
      "deployment",
      "x.x.x",
      "byok",
      "smax-cloudformation-aws",
      "package.",
      "copy",
      "rename",
      "new",
      "cms-rds.template.",
      "replace",
      "two",
      "smax-rds-security-group",
      "keys",
      "rdssecuritygroup",
      "cms-rds.template",
      "cms-rds-security-group",
      "save",
      "file.",
      "recommend",
      "engine",
      "aws.",
      "follow",
      "steps",
      "ucmdb.",
      "log",
      "management",
      "console",
      "switch",
      "selected",
      "region.",
      "go",
      "all",
      "services",
      "governance",
      "cloudformation.",
      "click",
      "stack",
      "resources",
      "standard",
      "ready",
      "upload",
      "choose",
      "select",
      "smax-cloudformation-aws.",
      "next",
      "populate",
      "fields",
      "name",
      "give",
      "such",
      "cms-rds.",
      "vpc",
      "id",
      "created",
      "drop-down",
      "list.",
      "first",
      "second",
      "subnet",
      "rds."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Launch RDS for UD/UCMDB (FIPS mode)",
    "content": "UD/UCMDB contains an internal (embedded) PostgreSQL database instance, which should be used for demonstration purposes only. You must use external databases in a production environment. When deployed on AWS, the suite uses RDS as the database engine. This RDS instance will be used as the external databases for the following UD/UCMDB components: UCMDB Server AutoPass License Service (APLS) Prepare RDS template for UD/UCMDB Navigate to the smax-rds.template file under ../Cloud Deployment x.x.x/byok/smax-cloudformation-aws in the cloud deployment package. Create a copy of the smax-rds.template file and rename the new file to cms-rds.template. Replace the two SMAX-RDS-Security-Group keys (under RDSSecurityGroup in cms-rds.template) with CMS-RDS-Security-Group, and then save the file. Create an RDS instance for UD/UCMDB We recommend that you use RDS as the database engine on AWS. Follow these steps to launch RDS for UD/UCMDB. Log in to the AWS Management Console and switch to the selected r",
    "url": "launchrdsekscmsfips",
    "filename": "launchrdsekscmsfips",
    "headings": [
      "Prepare RDS template for UD/UCMDB",
      "Create an RDS instance for UD/UCMDB",
      "Set up PostgreSQL databases for UD/UCMDB on the RDS instance"
    ],
    "keywords": [
      "uducmdb",
      "values.yaml",
      "launch",
      "rds",
      "ud",
      "ucmdb",
      "fips",
      "mode",
      "prepare",
      "template",
      "create",
      "instance",
      "set",
      "postgresql",
      "databases",
      "contains",
      "internal",
      "embedded",
      "database",
      "demonstration",
      "purposes",
      "only.",
      "external",
      "production",
      "environment.",
      "deployed",
      "aws",
      "suite",
      "uses",
      "engine.",
      "following",
      "components",
      "server",
      "autopass",
      "license",
      "service",
      "apls",
      "navigate",
      "smax-rds.template",
      "file",
      "under",
      "cloud",
      "deployment",
      "x.x.x",
      "byok",
      "smax-cloudformation-aws",
      "package.",
      "copy",
      "rename",
      "new",
      "cms-rds.template.",
      "replace",
      "two",
      "smax-rds-security-group",
      "keys",
      "rdssecuritygroup",
      "cms-rds.template",
      "cms-rds-security-group",
      "save",
      "file.",
      "recommend",
      "engine",
      "aws.",
      "follow",
      "steps",
      "ucmdb.",
      "log",
      "management",
      "console",
      "switch",
      "selected",
      "region.",
      "go",
      "all",
      "services",
      "governance",
      "cloudformation.",
      "click",
      "stack",
      "resources",
      "standard",
      "ready",
      "upload",
      "choose",
      "select",
      "smax-cloudformation-aws.",
      "next",
      "populate",
      "fields",
      "name",
      "give",
      "such",
      "cms-rds.",
      "vpc",
      "id",
      "created",
      "drop-down",
      "list.",
      "first",
      "second"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "launch rds for ud/ucmdb (fips mode)",
    "contentLower": "ud/ucmdb contains an internal (embedded) postgresql database instance, which should be used for demonstration purposes only. you must use external databases in a production environment. when deployed on aws, the suite uses rds as the database engine. this rds instance will be used as the external databases for the following ud/ucmdb components: ucmdb server autopass license service (apls) prepare rds template for ud/ucmdb navigate to the smax-rds.template file under ../cloud deployment x.x.x/byok/smax-cloudformation-aws in the cloud deployment package. create a copy of the smax-rds.template file and rename the new file to cms-rds.template. replace the two smax-rds-security-group keys (under rdssecuritygroup in cms-rds.template) with cms-rds-security-group, and then save the file. create an rds instance for ud/ucmdb we recommend that you use rds as the database engine on aws. follow these steps to launch rds for ud/ucmdb. log in to the aws management console and switch to the selected r",
    "keywordsLower": [
      "uducmdb",
      "values.yaml",
      "launch",
      "rds",
      "ud",
      "ucmdb",
      "fips",
      "mode",
      "prepare",
      "template",
      "create",
      "instance",
      "set",
      "postgresql",
      "databases",
      "contains",
      "internal",
      "embedded",
      "database",
      "demonstration",
      "purposes",
      "only.",
      "external",
      "production",
      "environment.",
      "deployed",
      "aws",
      "suite",
      "uses",
      "engine.",
      "following",
      "components",
      "server",
      "autopass",
      "license",
      "service",
      "apls",
      "navigate",
      "smax-rds.template",
      "file",
      "under",
      "cloud",
      "deployment",
      "x.x.x",
      "byok",
      "smax-cloudformation-aws",
      "package.",
      "copy",
      "rename",
      "new",
      "cms-rds.template.",
      "replace",
      "two",
      "smax-rds-security-group",
      "keys",
      "rdssecuritygroup",
      "cms-rds.template",
      "cms-rds-security-group",
      "save",
      "file.",
      "recommend",
      "engine",
      "aws.",
      "follow",
      "steps",
      "ucmdb.",
      "log",
      "management",
      "console",
      "switch",
      "selected",
      "region.",
      "go",
      "all",
      "services",
      "governance",
      "cloudformation.",
      "click",
      "stack",
      "resources",
      "standard",
      "ready",
      "upload",
      "choose",
      "select",
      "smax-cloudformation-aws.",
      "next",
      "populate",
      "fields",
      "name",
      "give",
      "such",
      "cms-rds.",
      "vpc",
      "id",
      "created",
      "drop-down",
      "list.",
      "first",
      "second"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install UD/UCMDB on AWS",
    "content": "Role Location Suite admin Bastion node To install UD/UCMDB on AWS, follow these steps: On the bastion node, navigate to the OMT installation directory: cd OMT_External_K8s_2x.x-xxx/bin Run the following command: ./helm install <UD/UCMDB RELEASE NAME> <UD/UCMDB CHART FILE> --namespace <UD/UCMDB NAMESPACE> -f <VALUES YAML FILE> -f <SECRETS YAML FILE> Where, <UD/UCMDB RELEASE NAME> is the unique release name that you need to specify for the UD/UCMDB deployment. You need to use the same release name as specified here when you upgrade UD/UCMDB in the future. <UD/UCMDB CHART FILE> is the tar file containing the UD/UCMDB installation charts. <UD/UCMDB NAMESPACE> is the unique namespace of the UD/UCMDB deployment. Use the namespace that you specified in the \"Create a deployment for UD/UCMDB\" topic. <VALUES YAML FILE> is your custom my-values.yaml file with properties configured for the UD/UCMDB deployment. <SECRETS YAML FILE> is the YAML file that stores the UD/UCMDB secrets. This file is crea",
    "url": "installcmsoneks",
    "filename": "installcmsoneks",
    "headings": [],
    "keywords": [
      "uducmdb",
      "x.tgz",
      "secrets.yaml",
      "1.xx",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "ud",
      "ucmdb",
      "aws",
      "role",
      "location",
      "suite",
      "admin",
      "bastion",
      "node",
      "follow",
      "steps",
      "navigate",
      "omt",
      "installation",
      "directory",
      "cd",
      "bin",
      "run",
      "following",
      "command",
      "helm",
      "--namespace",
      "-f",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "tar",
      "file",
      "containing",
      "charts.",
      "namespace",
      "create",
      "deployment",
      "topic.",
      "custom",
      "my-values.yaml",
      "properties",
      "configured",
      "yaml",
      "stores",
      "secrets.",
      "created",
      "generate",
      "vault",
      "secrets",
      "previous",
      "step.",
      "example",
      "ucmdb-prod",
      "ucmdb-helm-charts",
      "charts",
      "ucmdb-1.xx.x-xxx",
      "2x.x.x.tgz",
      "cms-secrets.yaml",
      "above",
      "--dry-run",
      "argument",
      "simulate",
      "installation.",
      "useful",
      "validate",
      "content",
      "before",
      "real",
      "after",
      "delete",
      "contains",
      "sensitive",
      "data."
    ],
    "language": "en",
    "word_count": 86,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install ud/ucmdb on aws",
    "contentLower": "role location suite admin bastion node to install ud/ucmdb on aws, follow these steps: on the bastion node, navigate to the omt installation directory: cd omt_external_k8s_2x.x-xxx/bin run the following command: ./helm install <ud/ucmdb release name> <ud/ucmdb chart file> --namespace <ud/ucmdb namespace> -f <values yaml file> -f <secrets yaml file> where, <ud/ucmdb release name> is the unique release name that you need to specify for the ud/ucmdb deployment. you need to use the same release name as specified here when you upgrade ud/ucmdb in the future. <ud/ucmdb chart file> is the tar file containing the ud/ucmdb installation charts. <ud/ucmdb namespace> is the unique namespace of the ud/ucmdb deployment. use the namespace that you specified in the \"create a deployment for ud/ucmdb\" topic. <values yaml file> is your custom my-values.yaml file with properties configured for the ud/ucmdb deployment. <secrets yaml file> is the yaml file that stores the ud/ucmdb secrets. this file is crea",
    "keywordsLower": [
      "uducmdb",
      "x.tgz",
      "secrets.yaml",
      "1.xx",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "ud",
      "ucmdb",
      "aws",
      "role",
      "location",
      "suite",
      "admin",
      "bastion",
      "node",
      "follow",
      "steps",
      "navigate",
      "omt",
      "installation",
      "directory",
      "cd",
      "bin",
      "run",
      "following",
      "command",
      "helm",
      "--namespace",
      "-f",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "tar",
      "file",
      "containing",
      "charts.",
      "namespace",
      "create",
      "deployment",
      "topic.",
      "custom",
      "my-values.yaml",
      "properties",
      "configured",
      "yaml",
      "stores",
      "secrets.",
      "created",
      "generate",
      "vault",
      "secrets",
      "previous",
      "step.",
      "example",
      "ucmdb-prod",
      "ucmdb-helm-charts",
      "charts",
      "ucmdb-1.xx.x-xxx",
      "2x.x.x.tgz",
      "cms-secrets.yaml",
      "above",
      "--dry-run",
      "argument",
      "simulate",
      "installation.",
      "useful",
      "validate",
      "content",
      "before",
      "real",
      "after",
      "delete",
      "contains",
      "sensitive",
      "data."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Containerized on AWS (EKS)",
    "content": "This section contains high-level installation instructions for OO Containerized on AWS EKS alongside Service Management. To enable Service Management to work with OO Containerized, you must configure a load balancer between Service Management and OO. Perform the following steps to install OO Containerized: Topic Description Plan Review the required resources and create a deployment plan for the installation of OO with EKS: Review support matrix Plan the deployment Prepare Prepare the required resources for the installation of OO with EKS: Download OO charts package Download and upload container images for OO Configure EFS for OO Create a new deployment for OO Prepare persistent volumes and claims for OO Launch Amazon RDS for OO Get connection details for connecting to the Service Management IdM Create OO integration admin user in the Service Management IdM Generate sensitive data for OO Create values.yaml for OO Configure load balancers for OO Post-install task Install OO on AWS Verify",
    "url": "ooceks",
    "filename": "ooceks",
    "headings": [],
    "keywords": [
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "aws",
      "eks",
      "section",
      "contains",
      "high-level",
      "installation",
      "instructions",
      "alongside",
      "service",
      "management.",
      "enable",
      "management",
      "work",
      "configure",
      "load",
      "balancer",
      "between",
      "oo.",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "plan",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "support",
      "matrix",
      "prepare",
      "download",
      "charts",
      "package",
      "upload",
      "container",
      "images",
      "efs",
      "new",
      "persistent",
      "volumes",
      "claims",
      "launch",
      "amazon",
      "rds",
      "get",
      "connection",
      "details",
      "connecting",
      "idm",
      "integration",
      "admin",
      "user",
      "generate",
      "sensitive",
      "data",
      "balancers",
      "post-install",
      "task",
      "verify",
      "deploy",
      "external",
      "components",
      "uninstall",
      "same",
      "optic",
      "toolkit",
      "omt",
      "installed.",
      "separate",
      "instances"
    ],
    "language": "en",
    "word_count": 117,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo containerized on aws (eks)",
    "contentLower": "this section contains high-level installation instructions for oo containerized on aws eks alongside service management. to enable service management to work with oo containerized, you must configure a load balancer between service management and oo. perform the following steps to install oo containerized: topic description plan review the required resources and create a deployment plan for the installation of oo with eks: review support matrix plan the deployment prepare prepare the required resources for the installation of oo with eks: download oo charts package download and upload container images for oo configure efs for oo create a new deployment for oo prepare persistent volumes and claims for oo launch amazon rds for oo get connection details for connecting to the service management idm create oo integration admin user in the service management idm generate sensitive data for oo create values.yaml for oo configure load balancers for oo post-install task install oo on aws verify",
    "keywordsLower": [
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "aws",
      "eks",
      "section",
      "contains",
      "high-level",
      "installation",
      "instructions",
      "alongside",
      "service",
      "management.",
      "enable",
      "management",
      "work",
      "configure",
      "load",
      "balancer",
      "between",
      "oo.",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "plan",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "support",
      "matrix",
      "prepare",
      "download",
      "charts",
      "package",
      "upload",
      "container",
      "images",
      "efs",
      "new",
      "persistent",
      "volumes",
      "claims",
      "launch",
      "amazon",
      "rds",
      "get",
      "connection",
      "details",
      "connecting",
      "idm",
      "integration",
      "admin",
      "user",
      "generate",
      "sensitive",
      "data",
      "balancers",
      "post-install",
      "task",
      "verify",
      "deploy",
      "external",
      "components",
      "uninstall",
      "same",
      "optic",
      "toolkit",
      "omt",
      "installed.",
      "separate",
      "instances"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Launch Amazon RDS for OO",
    "content": "OO Containerized uses Amazon Relational Database Service (Amazon RDS) to store its data. There are two options for setting up Amazon RDS for OO: (Recommended) Use a dedicated Amazon RDS instance for OO to ensure a smooth and fluent experience. Use the same Amazon RDS instance as the rest of the suite components. Irrespective of the choice, review the sizing considerations to check the Amazon RDS requirements. For option #1: Perform all the following steps: Prepare Amazon RDS template for OO Create an Amazon RDS instance for OO Create databases for OO on the Amazon RDS instance For option #2: You must resize the shared Amazon RDS instance to an instance type capable of supporting the added requirements of both suite and OO. Perform the step Create databases for OO on the Amazon RDS instance. Prepare Amazon RDS template for OO Copy and save the following as oo-rds.template. Upload it later in the AWS Console: { \"AWSTemplateFormatVersion\": \"2010-09-09\", \"Description\": \"Amazon EKS Control ",
    "url": "launchrdseksooc",
    "filename": "launchrdseksooc",
    "headings": [
      "Prepare Amazon RDS template for OO",
      "Create an Amazon RDS instance for OO",
      "Verify creating Amazon RDS",
      "Create databases for OO on the Amazon RDS instance"
    ],
    "keywords": [
      "db.r5",
      "db.m6g",
      "12.9",
      "Endpoint.Port",
      "db.m5",
      "launch",
      "amazon",
      "rds",
      "oo",
      "prepare",
      "template",
      "create",
      "instance",
      "verify",
      "creating",
      "databases",
      "containerized",
      "uses",
      "relational",
      "database",
      "service",
      "store",
      "data.",
      "there",
      "two",
      "options",
      "setting",
      "recommended",
      "dedicated",
      "ensure",
      "smooth",
      "fluent",
      "experience.",
      "same",
      "rest",
      "suite",
      "components.",
      "irrespective",
      "choice",
      "review",
      "sizing",
      "considerations",
      "check",
      "requirements.",
      "option",
      "perform",
      "all",
      "following",
      "steps",
      "resize",
      "shared",
      "type",
      "capable",
      "supporting",
      "added",
      "requirements",
      "both",
      "oo.",
      "step",
      "instance.",
      "copy",
      "save",
      "oo-rds.template.",
      "upload",
      "later",
      "aws",
      "console",
      "awstemplateformatversion",
      "2010-09-09",
      "description",
      "eks",
      "control",
      "plane",
      "parameters",
      "dbsubnet1",
      "first",
      "subnet",
      "rds.",
      "ec2",
      "id",
      "dbsubnet2",
      "second",
      "dbinstanceidentifier",
      "default",
      "oo-postgres",
      "identifier",
      "string",
      "minlength",
      "maxlength",
      "64",
      "allowedpattern",
      "a-za-z",
      "constraintdescription",
      "begin",
      "letter",
      "contain",
      "alphanumeric",
      "characters.",
      "dbuser",
      "postgres"
    ],
    "language": "en",
    "word_count": 113,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "launch amazon rds for oo",
    "contentLower": "oo containerized uses amazon relational database service (amazon rds) to store its data. there are two options for setting up amazon rds for oo: (recommended) use a dedicated amazon rds instance for oo to ensure a smooth and fluent experience. use the same amazon rds instance as the rest of the suite components. irrespective of the choice, review the sizing considerations to check the amazon rds requirements. for option #1: perform all the following steps: prepare amazon rds template for oo create an amazon rds instance for oo create databases for oo on the amazon rds instance for option #2: you must resize the shared amazon rds instance to an instance type capable of supporting the added requirements of both suite and oo. perform the step create databases for oo on the amazon rds instance. prepare amazon rds template for oo copy and save the following as oo-rds.template. upload it later in the aws console: { \"awstemplateformatversion\": \"2010-09-09\", \"description\": \"amazon eks control ",
    "keywordsLower": [
      "db.r5",
      "db.m6g",
      "12.9",
      "endpoint.port",
      "db.m5",
      "launch",
      "amazon",
      "rds",
      "oo",
      "prepare",
      "template",
      "create",
      "instance",
      "verify",
      "creating",
      "databases",
      "containerized",
      "uses",
      "relational",
      "database",
      "service",
      "store",
      "data.",
      "there",
      "two",
      "options",
      "setting",
      "recommended",
      "dedicated",
      "ensure",
      "smooth",
      "fluent",
      "experience.",
      "same",
      "rest",
      "suite",
      "components.",
      "irrespective",
      "choice",
      "review",
      "sizing",
      "considerations",
      "check",
      "requirements.",
      "option",
      "perform",
      "all",
      "following",
      "steps",
      "resize",
      "shared",
      "type",
      "capable",
      "supporting",
      "added",
      "requirements",
      "both",
      "oo.",
      "step",
      "instance.",
      "copy",
      "save",
      "oo-rds.template.",
      "upload",
      "later",
      "aws",
      "console",
      "awstemplateformatversion",
      "2010-09-09",
      "description",
      "eks",
      "control",
      "plane",
      "parameters",
      "dbsubnet1",
      "first",
      "subnet",
      "rds.",
      "ec2",
      "id",
      "dbsubnet2",
      "second",
      "dbinstanceidentifier",
      "default",
      "oo-postgres",
      "identifier",
      "string",
      "minlength",
      "maxlength",
      "64",
      "allowedpattern",
      "a-za-z",
      "constraintdescription",
      "begin",
      "letter",
      "contain",
      "alphanumeric",
      "characters.",
      "dbuser",
      "postgres"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Containerized on AWS",
    "content": "To install OO Containerized, run the following command on the bastion node. If you use custom values for resources in the values.yaml file, the global.oo.size parameter gets disabled. If you're adding certificates at Helm install time using CLI, ensure to remove the caCertificates parameter from the values.yaml file. <OMT_External_K8s_2x.x-xxx>/bin/helm install <OO INSTALL NAME> <OO CHART FILE> --namespace <OO NAMESPACE> -f <values.yaml> --set global.oo.size=xx --set-file \"caCertificates.cert_one\\.crt\"=/path/to/cert_one.crt Where, <OMT_External_K8s_2x.x-xxx> =  installation directory of OMT.<OO INSTALL NAME> = the unique release name that you need to specify for the OO deployment. <OO CHART FILE> = the absolute path of the OO chart file.<OO NAMESPACE> = the unique namespace of the OO deployment. Use the namespace that you specified in the Create a new deployment for OO in EKS topic. <values.yaml> = the path to OO values.yaml file configured as part of the preparation step Create values",
    "url": "installooconeks",
    "filename": "installooconeks",
    "headings": [],
    "keywords": [
      "oo_db.crt",
      "1.5",
      "smax_lb.crt",
      "cert_one.crt",
      "global.oo",
      "24.4",
      "x.tgz",
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "aws",
      "run",
      "following",
      "command",
      "bastion",
      "node.",
      "custom",
      "values",
      "resources",
      "file",
      "global.oo.size",
      "parameter",
      "gets",
      "disabled.",
      "re",
      "adding",
      "certificates",
      "helm",
      "time",
      "cli",
      "ensure",
      "remove",
      "cacertificates",
      "file.",
      "bin",
      "--namespace",
      "-f",
      "--set",
      "xx",
      "--set-file",
      ".crt",
      "path",
      "installation",
      "directory",
      "omt.",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "absolute",
      "chart",
      "namespace",
      "specified",
      "create",
      "new",
      "deployment",
      "eks",
      "topic.",
      "configured",
      "part",
      "preparation",
      "step",
      "number",
      "tenants",
      "value",
      "between",
      "20.",
      "default",
      "don",
      "configure",
      "parameter.",
      "optional",
      "arguments",
      "because",
      "address",
      "one",
      "way",
      "having",
      "trust-specific",
      "certificates.",
      "depending",
      "choice",
      "during",
      "see",
      "service",
      "management",
      "integration",
      "parameters",
      "opted",
      "providing",
      "required",
      "provided",
      "directly",
      "command.",
      "example",
      "oo-helm-charts-1.5.x",
      "24.4.x"
    ],
    "language": "en",
    "word_count": 84,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo containerized on aws",
    "contentLower": "to install oo containerized, run the following command on the bastion node. if you use custom values for resources in the values.yaml file, the global.oo.size parameter gets disabled. if you're adding certificates at helm install time using cli, ensure to remove the cacertificates parameter from the values.yaml file. <omt_external_k8s_2x.x-xxx>/bin/helm install <oo install name> <oo chart file> --namespace <oo namespace> -f <values.yaml> --set global.oo.size=xx --set-file \"cacertificates.cert_one\\.crt\"=/path/to/cert_one.crt where, <omt_external_k8s_2x.x-xxx> =  installation directory of omt.<oo install name> = the unique release name that you need to specify for the oo deployment. <oo chart file> = the absolute path of the oo chart file.<oo namespace> = the unique namespace of the oo deployment. use the namespace that you specified in the create a new deployment for oo in eks topic. <values.yaml> = the path to oo values.yaml file configured as part of the preparation step create values",
    "keywordsLower": [
      "oo_db.crt",
      "1.5",
      "smax_lb.crt",
      "cert_one.crt",
      "global.oo",
      "24.4",
      "x.tgz",
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "aws",
      "run",
      "following",
      "command",
      "bastion",
      "node.",
      "custom",
      "values",
      "resources",
      "file",
      "global.oo.size",
      "parameter",
      "gets",
      "disabled.",
      "re",
      "adding",
      "certificates",
      "helm",
      "time",
      "cli",
      "ensure",
      "remove",
      "cacertificates",
      "file.",
      "bin",
      "--namespace",
      "-f",
      "--set",
      "xx",
      "--set-file",
      ".crt",
      "path",
      "installation",
      "directory",
      "omt.",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "absolute",
      "chart",
      "namespace",
      "specified",
      "create",
      "new",
      "deployment",
      "eks",
      "topic.",
      "configured",
      "part",
      "preparation",
      "step",
      "number",
      "tenants",
      "value",
      "between",
      "20.",
      "default",
      "don",
      "configure",
      "parameter.",
      "optional",
      "arguments",
      "because",
      "address",
      "one",
      "way",
      "having",
      "trust-specific",
      "certificates.",
      "depending",
      "choice",
      "during",
      "see",
      "service",
      "management",
      "integration",
      "parameters",
      "opted",
      "providing",
      "required",
      "provided",
      "directly",
      "command.",
      "example",
      "oo-helm-charts-1.5.x",
      "24.4.x"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install external OO components (EKS)",
    "content": "Operations Orchestration (OO) supports the following components: OO External RAS OO RAS enables execution in remote data centers and networks. The RAS interacts with Central and polls it for operations to execute. Installation of external OO RAS is optional and depends on whether you need additional RASes apart from what's already available when you install OO Central. OO Workflow Designer OO Workflow Designer is a web based environment for authoring flows, which is available as a standalone product that you can deploy either on a Windows or Linux server. Installing OO Workflow Designer is optional and depends on your requirement to author flows. You need to install the OO Workflow Designer as it's not installed along with OO Central. For information about installing these external components, see Install additional OO components.",
    "url": "installoocomponentseks",
    "filename": "installoocomponentseks",
    "headings": [],
    "keywords": [
      "install",
      "external",
      "oo",
      "components",
      "eks",
      "operations",
      "orchestration",
      "supports",
      "following",
      "ras",
      "enables",
      "execution",
      "remote",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "installation",
      "optional",
      "depends",
      "whether",
      "need",
      "additional",
      "rases",
      "apart",
      "what",
      "already",
      "available",
      "central.",
      "workflow",
      "designer",
      "web",
      "based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deploy",
      "either",
      "windows",
      "linux",
      "server.",
      "installing",
      "requirement",
      "author",
      "flows.",
      "installed",
      "along",
      "information",
      "about",
      "see",
      "components."
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install external oo components (eks)",
    "contentLower": "operations orchestration (oo) supports the following components: oo external ras oo ras enables execution in remote data centers and networks. the ras interacts with central and polls it for operations to execute. installation of external oo ras is optional and depends on whether you need additional rases apart from what's already available when you install oo central. oo workflow designer oo workflow designer is a web based environment for authoring flows, which is available as a standalone product that you can deploy either on a windows or linux server. installing oo workflow designer is optional and depends on your requirement to author flows. you need to install the oo workflow designer as it's not installed along with oo central. for information about installing these external components, see install additional oo components.",
    "keywordsLower": [
      "install",
      "external",
      "oo",
      "components",
      "eks",
      "operations",
      "orchestration",
      "supports",
      "following",
      "ras",
      "enables",
      "execution",
      "remote",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "installation",
      "optional",
      "depends",
      "whether",
      "need",
      "additional",
      "rases",
      "apart",
      "what",
      "already",
      "available",
      "central.",
      "workflow",
      "designer",
      "web",
      "based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deploy",
      "either",
      "windows",
      "linux",
      "server.",
      "installing",
      "requirement",
      "author",
      "flows.",
      "installed",
      "along",
      "information",
      "about",
      "see",
      "components."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Launch RDS for Audit service",
    "content": "Use the Relational Database Service (RDS) instance for the suite and create a database for the Audit service. Set up PostgreSQL database for Audit service on the RDS instance Make sure you enable SSL on PostgreSQL because Audit uses SSL to communicate with PostgreSQL. Below are sample SQL commands for creating a user and a database for a service: Log in to the bastion node and then run the following command to connect to the PostgreSQL instance: psql -h <RDSEndpointAddress> -p <RDSEndpointPort> -U <postgres admin> -d postgres Run the following commands to create the databases and the users used by the Audit service: CREATE USER <USERNAME> PASSWORD '<PASSWORD>'; grant <USERNAME> to <PostgresSQL admin>; CREATE DATABASE <DBNAME> OWNER <USERNAME>; <PASSWORD> for the database user should be at least 16 characters long and have alphanumeric characters, at least one lower case letter, at least one upper case letter, and at least one digit. For example, CREATE USER auditdbuser PASSWORD 'Passwo",
    "url": "launchrdseksaudit",
    "filename": "launchrdseksaudit",
    "headings": [],
    "keywords": [
      "launch",
      "rds",
      "audit",
      "service",
      "relational",
      "database",
      "instance",
      "suite",
      "create",
      "service.",
      "set",
      "postgresql",
      "make",
      "sure",
      "enable",
      "ssl",
      "because",
      "uses",
      "communicate",
      "postgresql.",
      "below",
      "sample",
      "sql",
      "commands",
      "creating",
      "user",
      "log",
      "bastion",
      "node",
      "run",
      "following",
      "command",
      "connect",
      "psql",
      "-h",
      "-p",
      "-u",
      "-d",
      "postgres",
      "databases",
      "users",
      "password",
      "grant",
      "owner",
      "least",
      "16",
      "characters",
      "long",
      "alphanumeric",
      "one",
      "lower",
      "case",
      "letter",
      "upper",
      "digit.",
      "example",
      "auditdbuser",
      "audit1234",
      "auditdb"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "launch rds for audit service",
    "contentLower": "use the relational database service (rds) instance for the suite and create a database for the audit service. set up postgresql database for audit service on the rds instance make sure you enable ssl on postgresql because audit uses ssl to communicate with postgresql. below are sample sql commands for creating a user and a database for a service: log in to the bastion node and then run the following command to connect to the postgresql instance: psql -h <rdsendpointaddress> -p <rdsendpointport> -u <postgres admin> -d postgres run the following commands to create the databases and the users used by the audit service: create user <username> password '<password>'; grant <username> to <postgressql admin>; create database <dbname> owner <username>; <password> for the database user should be at least 16 characters long and have alphanumeric characters, at least one lower case letter, at least one upper case letter, and at least one digit. for example, create user auditdbuser password 'passwo",
    "keywordsLower": [
      "launch",
      "rds",
      "audit",
      "service",
      "relational",
      "database",
      "instance",
      "suite",
      "create",
      "service.",
      "set",
      "postgresql",
      "make",
      "sure",
      "enable",
      "ssl",
      "because",
      "uses",
      "communicate",
      "postgresql.",
      "below",
      "sample",
      "sql",
      "commands",
      "creating",
      "user",
      "log",
      "bastion",
      "node",
      "run",
      "following",
      "command",
      "connect",
      "psql",
      "-h",
      "-p",
      "-u",
      "-d",
      "postgres",
      "databases",
      "users",
      "password",
      "grant",
      "owner",
      "least",
      "16",
      "characters",
      "long",
      "alphanumeric",
      "one",
      "lower",
      "case",
      "letter",
      "upper",
      "digit.",
      "example",
      "auditdbuser",
      "audit1234",
      "auditdb"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install Audit Collector on AWS (EKS)",
    "content": "Services that are integrated with the Audit service store the audits on the NFS file system when the audit service isn't reachable. Audit collector is a microservice that collects the audits from the NFS file system and passes them to the Audit service via REST. You can deploy the Audit collector using Helm charts. Install and configure Audit collector in each of the namespaces that contain the Audit producer services, for example, Service Management and UCMDB. Audit collector requires the Audit producer file system and it will use the Audit producer PVC and subpath. This section describes the steps to install the Audit collector.",
    "url": "installauditcollectoreks",
    "filename": "installauditcollectoreks",
    "headings": [],
    "keywords": [
      "install",
      "audit",
      "collector",
      "aws",
      "eks",
      "services",
      "integrated",
      "service",
      "store",
      "audits",
      "nfs",
      "file",
      "system",
      "isn",
      "reachable.",
      "microservice",
      "collects",
      "passes",
      "via",
      "rest.",
      "deploy",
      "helm",
      "charts.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "example",
      "management",
      "ucmdb.",
      "requires",
      "pvc",
      "subpath.",
      "section",
      "describes",
      "steps",
      "collector."
    ],
    "language": "en",
    "word_count": 67,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install audit collector on aws (eks)",
    "contentLower": "services that are integrated with the audit service store the audits on the nfs file system when the audit service isn't reachable. audit collector is a microservice that collects the audits from the nfs file system and passes them to the audit service via rest. you can deploy the audit collector using helm charts. install and configure audit collector in each of the namespaces that contain the audit producer services, for example, service management and ucmdb. audit collector requires the audit producer file system and it will use the audit producer pvc and subpath. this section describes the steps to install the audit collector.",
    "keywordsLower": [
      "install",
      "audit",
      "collector",
      "aws",
      "eks",
      "services",
      "integrated",
      "service",
      "store",
      "audits",
      "nfs",
      "file",
      "system",
      "isn",
      "reachable.",
      "microservice",
      "collects",
      "passes",
      "via",
      "rest.",
      "deploy",
      "helm",
      "charts.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "example",
      "management",
      "ucmdb.",
      "requires",
      "pvc",
      "subpath.",
      "section",
      "describes",
      "steps",
      "collector."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install on Azure (AKS)",
    "content": "This section describes how to install the suite on Azure using Azure Kubernetes Service (AKS). Before you begin Before you begin, ensure that you've reviewed the following planning topics for the AKS deployment: The overall reference architecture. The support matrix. The definition of suite size and the required hardware resources. The most frequently asked questions. Deployment tasks See Deploy the suite on Azure for all the preparation, installation, and setup tasks to deploy the suite on Azure.",
    "url": "aks",
    "filename": "aks",
    "headings": [
      "Before you begin",
      "Deployment tasks"
    ],
    "keywords": [
      "install",
      "azure",
      "aks",
      "before",
      "begin",
      "deployment",
      "tasks",
      "section",
      "describes",
      "suite",
      "kubernetes",
      "service",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "topics",
      "overall",
      "reference",
      "architecture.",
      "support",
      "matrix.",
      "definition",
      "size",
      "required",
      "hardware",
      "resources.",
      "most",
      "frequently",
      "asked",
      "questions.",
      "see",
      "deploy",
      "all",
      "preparation",
      "installation",
      "setup",
      "azure."
    ],
    "language": "en",
    "word_count": 53,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install on azure (aks)",
    "contentLower": "this section describes how to install the suite on azure using azure kubernetes service (aks). before you begin before you begin, ensure that you've reviewed the following planning topics for the aks deployment: the overall reference architecture. the support matrix. the definition of suite size and the required hardware resources. the most frequently asked questions. deployment tasks see deploy the suite on azure for all the preparation, installation, and setup tasks to deploy the suite on azure.",
    "keywordsLower": [
      "install",
      "azure",
      "aks",
      "before",
      "begin",
      "deployment",
      "tasks",
      "section",
      "describes",
      "suite",
      "kubernetes",
      "service",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "topics",
      "overall",
      "reference",
      "architecture.",
      "support",
      "matrix.",
      "definition",
      "size",
      "required",
      "hardware",
      "resources.",
      "most",
      "frequently",
      "asked",
      "questions.",
      "see",
      "deploy",
      "all",
      "preparation",
      "installation",
      "setup",
      "azure."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Installation tasks (AKS)",
    "content": "This section contains part of the mandatory and optional tasks to install the suite in an AKS environment. Activate Docker Hub account Build AKS cluster Configure bastion Prepare File Storage - Subscribe NetApp Files or Prepare File Storage - Subscribe premium Azure Files and Azure Disks Prepare persistent storage for ESM on Azure Create external databases Transfer images beforehand Configure a load balancer on Azure Configure the Suite to use the same load balancer as OMT The rest of the installation tasks are listed on the Helm installation common tasks page.",
    "url": "aksprepare",
    "filename": "aksprepare",
    "headings": [],
    "keywords": [
      "installation",
      "tasks",
      "aks",
      "section",
      "contains",
      "part",
      "mandatory",
      "optional",
      "install",
      "suite",
      "environment.",
      "activate",
      "docker",
      "hub",
      "account",
      "build",
      "cluster",
      "configure",
      "bastion",
      "prepare",
      "file",
      "storage",
      "subscribe",
      "netapp",
      "files",
      "premium",
      "azure",
      "disks",
      "persistent",
      "esm",
      "create",
      "external",
      "databases",
      "transfer",
      "images",
      "beforehand",
      "load",
      "balancer",
      "same",
      "omt",
      "rest",
      "listed",
      "helm",
      "common",
      "page."
    ],
    "language": "en",
    "word_count": 67,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "installation tasks (aks)",
    "contentLower": "this section contains part of the mandatory and optional tasks to install the suite in an aks environment. activate docker hub account build aks cluster configure bastion prepare file storage - subscribe netapp files or prepare file storage - subscribe premium azure files and azure disks prepare persistent storage for esm on azure create external databases transfer images beforehand configure a load balancer on azure configure the suite to use the same load balancer as omt the rest of the installation tasks are listed on the helm installation common tasks page.",
    "keywordsLower": [
      "installation",
      "tasks",
      "aks",
      "section",
      "contains",
      "part",
      "mandatory",
      "optional",
      "install",
      "suite",
      "environment.",
      "activate",
      "docker",
      "hub",
      "account",
      "build",
      "cluster",
      "configure",
      "bastion",
      "prepare",
      "file",
      "storage",
      "subscribe",
      "netapp",
      "files",
      "premium",
      "azure",
      "disks",
      "persistent",
      "esm",
      "create",
      "external",
      "databases",
      "transfer",
      "images",
      "beforehand",
      "load",
      "balancer",
      "same",
      "omt",
      "rest",
      "listed",
      "helm",
      "common",
      "page."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install and configure Vertica database",
    "content": "If you plan to install CMP FinOps, you must install and configure the Vertica database as a prerequisite. For information, refer to Vertica on Microsoft Azure.",
    "url": "installconfverticaaks",
    "filename": "installconfverticaaks",
    "headings": [],
    "keywords": [
      "install",
      "configure",
      "vertica",
      "database",
      "plan",
      "cmp",
      "finops",
      "prerequisite.",
      "information",
      "refer",
      "microsoft",
      "azure."
    ],
    "language": "en",
    "word_count": 18,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install and configure vertica database",
    "contentLower": "if you plan to install cmp finops, you must install and configure the vertica database as a prerequisite. for information, refer to vertica on microsoft azure.",
    "keywordsLower": [
      "install",
      "configure",
      "vertica",
      "database",
      "plan",
      "cmp",
      "finops",
      "prerequisite.",
      "information",
      "refer",
      "microsoft",
      "azure."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Launch AKS cluster worker nodes for UD/UCMDB",
    "content": "See Sizing guides (AKS) for information about the additional AKS cluster worker nodes required to deploy UD/UCMDB. To create worker nodes for the UD/UCMDB installation, make sure you have the Owner role and run this command in the Azure Cloud Shell: az aks nodepool add -g <resource_group_name> -n <node_pool> --cluster-name <kubernetes_cluster_name> --node-vm-size <node-vm-size> --node-count <node-count> --mode System --labels Worker=cms role=loadbalancer where: <resource_group_name>: Enter the name of the resource group created in the prerequisite tasks. For example, myResourceGroup. <node_pool>: Specify the node pool name. The name should be different from the node pool name of the suite. <kubernetes_cluster_name>: Enter the name of the cluster created for the suite in the previous task. <node_vm_size>: Specify the size of virtual machines to create as Kubernetes nodes by referring to Sizing guides (AKS). For example, Standard_D8s_v3. <node-count>: Specify the number of nodes in the K",
    "url": "launchworkernodesakscms",
    "filename": "launchworkernodesakscms",
    "headings": [
      "See Sizing guides (AKS) for information about the additional AKS cluster worker nodes required to deploy UD/UCMDB."
    ],
    "keywords": [
      "uducmdb",
      "launch",
      "aks",
      "cluster",
      "worker",
      "nodes",
      "ud",
      "ucmdb",
      "see",
      "sizing",
      "guides",
      "information",
      "about",
      "additional",
      "required",
      "deploy",
      "ucmdb.",
      "create",
      "installation",
      "make",
      "sure",
      "owner",
      "role",
      "run",
      "command",
      "azure",
      "cloud",
      "shell",
      "az",
      "nodepool",
      "add",
      "-g",
      "-n",
      "--cluster-name",
      "--node-vm-size",
      "--node-count",
      "--mode",
      "system",
      "--labels",
      "cms",
      "loadbalancer",
      "enter",
      "name",
      "resource",
      "group",
      "created",
      "prerequisite",
      "tasks.",
      "example",
      "myresourcegroup.",
      "specify",
      "node",
      "pool",
      "name.",
      "different",
      "suite.",
      "suite",
      "previous",
      "task.",
      "size",
      "virtual",
      "machines",
      "kubernetes",
      "referring",
      "number",
      "pool.",
      "after",
      "following",
      "check",
      "whether",
      "label",
      "successfully",
      "assigned",
      "kubectl",
      "get",
      "--show-labels",
      "review",
      "output",
      "nodes."
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "launch aks cluster worker nodes for ud/ucmdb",
    "contentLower": "see sizing guides (aks) for information about the additional aks cluster worker nodes required to deploy ud/ucmdb. to create worker nodes for the ud/ucmdb installation, make sure you have the owner role and run this command in the azure cloud shell: az aks nodepool add -g <resource_group_name> -n <node_pool> --cluster-name <kubernetes_cluster_name> --node-vm-size <node-vm-size> --node-count <node-count> --mode system --labels worker=cms role=loadbalancer where: <resource_group_name>: enter the name of the resource group created in the prerequisite tasks. for example, myresourcegroup. <node_pool>: specify the node pool name. the name should be different from the node pool name of the suite. <kubernetes_cluster_name>: enter the name of the cluster created for the suite in the previous task. <node_vm_size>: specify the size of virtual machines to create as kubernetes nodes by referring to sizing guides (aks). for example, standard_d8s_v3. <node-count>: specify the number of nodes in the k",
    "keywordsLower": [
      "uducmdb",
      "launch",
      "aks",
      "cluster",
      "worker",
      "nodes",
      "ud",
      "ucmdb",
      "see",
      "sizing",
      "guides",
      "information",
      "about",
      "additional",
      "required",
      "deploy",
      "ucmdb.",
      "create",
      "installation",
      "make",
      "sure",
      "owner",
      "role",
      "run",
      "command",
      "azure",
      "cloud",
      "shell",
      "az",
      "nodepool",
      "add",
      "-g",
      "-n",
      "--cluster-name",
      "--node-vm-size",
      "--node-count",
      "--mode",
      "system",
      "--labels",
      "cms",
      "loadbalancer",
      "enter",
      "name",
      "resource",
      "group",
      "created",
      "prerequisite",
      "tasks.",
      "example",
      "myresourcegroup.",
      "specify",
      "node",
      "pool",
      "name.",
      "different",
      "suite.",
      "suite",
      "previous",
      "task.",
      "size",
      "virtual",
      "machines",
      "kubernetes",
      "referring",
      "number",
      "pool.",
      "after",
      "following",
      "check",
      "whether",
      "label",
      "successfully",
      "assigned",
      "kubectl",
      "get",
      "--show-labels",
      "review",
      "output",
      "nodes."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install UD/UCMDB on Azure",
    "content": "Role Location Suite admin Bastion node To install UD/UCMDB on AWS, follow these steps: On the bastion node, navigate to the OMT installation directory: cd OMT_External_K8s_2x.x-xxx/bin Run the following command: ./helm install <UD/UCMDB RELEASE NAME> <UD/UCMDB CHART FILE> --namespace <UD/UCMDB NAMESPACE > -f <VALUES YAML FILE> -f <SECRETS YAML FILE> Where, <UD/UCMDB RELEASE NAME> is the unique release name that you need to specify for the UD/UCMDB deployment. You need to use the same release name as specified here when you upgrade UD/UCMDB in the future. <UD/UCMDB CHART FILE> is the tar file containing the UD/UCMDB installation charts. <UD/UCMDB NAMESPACE> is the unique namespace of the UD/UCMDB deployment. Use the namespace that you specified in the \"Create a deployment for UD/UCMDB\" topic. <VALUES YAML FILE> is your custom my-values.yaml file with properties configured for the UD/UCMDB deployment. <SECRETS YAML FILE> is the YAML file that stores the UD/UCMDB secrets. This file is cre",
    "url": "installcmsonaks",
    "filename": "installcmsonaks",
    "headings": [],
    "keywords": [
      "uducmdb",
      "values.yaml",
      "secrets.yaml",
      "gen_secrets.sh",
      "xxx.tgz",
      "install",
      "ud",
      "ucmdb",
      "azure",
      "role",
      "location",
      "suite",
      "admin",
      "bastion",
      "node",
      "aws",
      "follow",
      "steps",
      "navigate",
      "omt",
      "installation",
      "directory",
      "cd",
      "bin",
      "run",
      "following",
      "command",
      "helm",
      "--namespace",
      "-f",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "tar",
      "file",
      "containing",
      "charts.",
      "namespace",
      "create",
      "deployment",
      "topic.",
      "custom",
      "my-values.yaml",
      "properties",
      "configured",
      "yaml",
      "stores",
      "secrets.",
      "created",
      "generate",
      "vault",
      "secrets",
      "previous",
      "step.",
      "example",
      "ucmdb-prod",
      "ucmdb-helm-charts",
      "charts",
      "ucmdb-1.x.0",
      "2x.x.x-xxx.tgz",
      "cms-secrets.yaml",
      "above",
      "--dry-run",
      "argument",
      "simulate",
      "installation.",
      "useful",
      "validate",
      "content",
      "before",
      "real",
      "after",
      "delete",
      "contains",
      "sensitive",
      "data."
    ],
    "language": "en",
    "word_count": 86,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install ud/ucmdb on azure",
    "contentLower": "role location suite admin bastion node to install ud/ucmdb on aws, follow these steps: on the bastion node, navigate to the omt installation directory: cd omt_external_k8s_2x.x-xxx/bin run the following command: ./helm install <ud/ucmdb release name> <ud/ucmdb chart file> --namespace <ud/ucmdb namespace > -f <values yaml file> -f <secrets yaml file> where, <ud/ucmdb release name> is the unique release name that you need to specify for the ud/ucmdb deployment. you need to use the same release name as specified here when you upgrade ud/ucmdb in the future. <ud/ucmdb chart file> is the tar file containing the ud/ucmdb installation charts. <ud/ucmdb namespace> is the unique namespace of the ud/ucmdb deployment. use the namespace that you specified in the \"create a deployment for ud/ucmdb\" topic. <values yaml file> is your custom my-values.yaml file with properties configured for the ud/ucmdb deployment. <secrets yaml file> is the yaml file that stores the ud/ucmdb secrets. this file is cre",
    "keywordsLower": [
      "uducmdb",
      "values.yaml",
      "secrets.yaml",
      "gen_secrets.sh",
      "xxx.tgz",
      "install",
      "ud",
      "ucmdb",
      "azure",
      "role",
      "location",
      "suite",
      "admin",
      "bastion",
      "node",
      "aws",
      "follow",
      "steps",
      "navigate",
      "omt",
      "installation",
      "directory",
      "cd",
      "bin",
      "run",
      "following",
      "command",
      "helm",
      "--namespace",
      "-f",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "tar",
      "file",
      "containing",
      "charts.",
      "namespace",
      "create",
      "deployment",
      "topic.",
      "custom",
      "my-values.yaml",
      "properties",
      "configured",
      "yaml",
      "stores",
      "secrets.",
      "created",
      "generate",
      "vault",
      "secrets",
      "previous",
      "step.",
      "example",
      "ucmdb-prod",
      "ucmdb-helm-charts",
      "charts",
      "ucmdb-1.x.0",
      "2x.x.x-xxx.tgz",
      "cms-secrets.yaml",
      "above",
      "--dry-run",
      "argument",
      "simulate",
      "installation.",
      "useful",
      "validate",
      "content",
      "before",
      "real",
      "after",
      "delete",
      "contains",
      "sensitive",
      "data."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Containerized on Azure (AKS)",
    "content": "This topic contains high level installation instructions for Operations Orchestration Containerized (OO Containerized) on AKS. Perform the following steps to install OO Containerized: Topic Description Plan Review the required resources and create a deployment plan for OO installation on AKS: Review support matrixPlan the OO deployment Prepare Prepare the required resources for OO installation on AKS: Download the OO charts package (AKS)Download and upload images for OO (AKS)Create a new deployment for OO (AKS)Subscribe to Azure Premium FilesPrepare persistent volumes and claims for OO (AKS)Prepare external databases for OO (AKS)Get connection details for connecting to the IdM service (AKS)Create an OO integration admin user in the IdM portal (AKS)Generate vault secrets (AKS)Create values.yaml for OO (AKS) Deploy Install OO on AKSConfigure the application gateway for OOVerify the OO installation Install external OO components Install external OO components Uninstall Uninstall OO on AKS",
    "url": "installooaks",
    "filename": "installooaks",
    "headings": [
      "Perform the following steps to install OO Containerized:"
    ],
    "keywords": [
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "azure",
      "aks",
      "perform",
      "following",
      "steps",
      "topic",
      "contains",
      "high",
      "level",
      "installation",
      "instructions",
      "operations",
      "orchestration",
      "aks.",
      "description",
      "plan",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "support",
      "matrixplan",
      "prepare",
      "download",
      "charts",
      "package",
      "upload",
      "images",
      "new",
      "subscribe",
      "premium",
      "filesprepare",
      "persistent",
      "volumes",
      "claims",
      "external",
      "databases",
      "get",
      "connection",
      "details",
      "connecting",
      "idm",
      "service",
      "integration",
      "admin",
      "user",
      "portal",
      "generate",
      "vault",
      "secrets",
      "deploy",
      "aksconfigure",
      "application",
      "gateway",
      "ooverify",
      "components",
      "uninstall",
      "same",
      "optic",
      "management",
      "toolkit",
      "omt",
      "server",
      "installed",
      "management."
    ],
    "language": "en",
    "word_count": 120,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo containerized on azure (aks)",
    "contentLower": "this topic contains high level installation instructions for operations orchestration containerized (oo containerized) on aks. perform the following steps to install oo containerized: topic description plan review the required resources and create a deployment plan for oo installation on aks: review support matrixplan the oo deployment prepare prepare the required resources for oo installation on aks: download the oo charts package (aks)download and upload images for oo (aks)create a new deployment for oo (aks)subscribe to azure premium filesprepare persistent volumes and claims for oo (aks)prepare external databases for oo (aks)get connection details for connecting to the idm service (aks)create an oo integration admin user in the idm portal (aks)generate vault secrets (aks)create values.yaml for oo (aks) deploy install oo on aksconfigure the application gateway for ooverify the oo installation install external oo components install external oo components uninstall uninstall oo on aks",
    "keywordsLower": [
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "azure",
      "aks",
      "perform",
      "following",
      "steps",
      "topic",
      "contains",
      "high",
      "level",
      "installation",
      "instructions",
      "operations",
      "orchestration",
      "aks.",
      "description",
      "plan",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "support",
      "matrixplan",
      "prepare",
      "download",
      "charts",
      "package",
      "upload",
      "images",
      "new",
      "subscribe",
      "premium",
      "filesprepare",
      "persistent",
      "volumes",
      "claims",
      "external",
      "databases",
      "get",
      "connection",
      "details",
      "connecting",
      "idm",
      "service",
      "integration",
      "admin",
      "user",
      "portal",
      "generate",
      "vault",
      "secrets",
      "deploy",
      "aksconfigure",
      "application",
      "gateway",
      "ooverify",
      "components",
      "uninstall",
      "same",
      "optic",
      "management",
      "toolkit",
      "omt",
      "server",
      "installed",
      "management."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Containerized on AKS",
    "content": "To install Operations Orchestration (OO Containerized), run the following command on the bastion node. If you use custom values for resources in the values.yaml file, the global.oo.size parameter gets disabled. If you're adding certificates at Helm install time using CLI, ensure to remove the caCertificates parameter from the values.yaml file. <OMT_External_K8s_2x.x-xxx>/bin/helm install <OO INSTALL NAME> <OO CHART FILE> --namespace <OO NAMESPACE> -f <values.yaml> --set global.oo.size=xx --set-file \"caCertificates.cert_one\\.crt\"=/path/to/cert_one.crt Where: <OMT_External_K8s_2x.x-xxx> is the installation directory of OMT. <OO INSTALL NAME> = the unique release name you need to specify for the OO deployment. <OO CHART FILE> = the absolute path of the OO chart file. <OO NAMESPACE> = the unique namespace of the OO deployment. Use the namespace you specified in the Create a new deployment for OO in AKS topic. <values.yaml> = the path to OO values.yaml file which you configured as part of t",
    "url": "399-installooonaks",
    "filename": "399-installooonaks",
    "headings": [],
    "keywords": [
      "oo_db.crt",
      "smax_lb.crt",
      "cert_one.crt",
      "global.oo",
      "x.tgz",
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "aks",
      "operations",
      "orchestration",
      "run",
      "following",
      "command",
      "bastion",
      "node.",
      "custom",
      "values",
      "resources",
      "file",
      "global.oo.size",
      "parameter",
      "gets",
      "disabled.",
      "re",
      "adding",
      "certificates",
      "helm",
      "time",
      "cli",
      "ensure",
      "remove",
      "cacertificates",
      "file.",
      "bin",
      "--namespace",
      "-f",
      "--set",
      "xx",
      "--set-file",
      ".crt",
      "path",
      "installation",
      "directory",
      "omt.",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "absolute",
      "chart",
      "namespace",
      "specified",
      "create",
      "new",
      "deployment",
      "topic.",
      "configured",
      "part",
      "preparation",
      "step",
      "aks.",
      "number",
      "tenants",
      "value",
      "between",
      "20.",
      "default",
      "don",
      "configure",
      "parameter.",
      "optional",
      "arguments",
      "because",
      "address",
      "one",
      "way",
      "having",
      "trust-specific",
      "certificates.",
      "depending",
      "choice",
      "during",
      "see",
      "service",
      "management",
      "integration",
      "parameters",
      "opted",
      "provide",
      "required",
      "provided",
      "directly",
      "command.",
      "example",
      "oo-helm-charts-1.x.x",
      "26.x.x"
    ],
    "language": "en",
    "word_count": 83,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo containerized on aks",
    "contentLower": "to install operations orchestration (oo containerized), run the following command on the bastion node. if you use custom values for resources in the values.yaml file, the global.oo.size parameter gets disabled. if you're adding certificates at helm install time using cli, ensure to remove the cacertificates parameter from the values.yaml file. <omt_external_k8s_2x.x-xxx>/bin/helm install <oo install name> <oo chart file> --namespace <oo namespace> -f <values.yaml> --set global.oo.size=xx --set-file \"cacertificates.cert_one\\.crt\"=/path/to/cert_one.crt where: <omt_external_k8s_2x.x-xxx> is the installation directory of omt. <oo install name> = the unique release name you need to specify for the oo deployment. <oo chart file> = the absolute path of the oo chart file. <oo namespace> = the unique namespace of the oo deployment. use the namespace you specified in the create a new deployment for oo in aks topic. <values.yaml> = the path to oo values.yaml file which you configured as part of t",
    "keywordsLower": [
      "oo_db.crt",
      "smax_lb.crt",
      "cert_one.crt",
      "global.oo",
      "x.tgz",
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "aks",
      "operations",
      "orchestration",
      "run",
      "following",
      "command",
      "bastion",
      "node.",
      "custom",
      "values",
      "resources",
      "file",
      "global.oo.size",
      "parameter",
      "gets",
      "disabled.",
      "re",
      "adding",
      "certificates",
      "helm",
      "time",
      "cli",
      "ensure",
      "remove",
      "cacertificates",
      "file.",
      "bin",
      "--namespace",
      "-f",
      "--set",
      "xx",
      "--set-file",
      ".crt",
      "path",
      "installation",
      "directory",
      "omt.",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "absolute",
      "chart",
      "namespace",
      "specified",
      "create",
      "new",
      "deployment",
      "topic.",
      "configured",
      "part",
      "preparation",
      "step",
      "aks.",
      "number",
      "tenants",
      "value",
      "between",
      "20.",
      "default",
      "don",
      "configure",
      "parameter.",
      "optional",
      "arguments",
      "because",
      "address",
      "one",
      "way",
      "having",
      "trust-specific",
      "certificates.",
      "depending",
      "choice",
      "during",
      "see",
      "service",
      "management",
      "integration",
      "parameters",
      "opted",
      "provide",
      "required",
      "provided",
      "directly",
      "command.",
      "example",
      "oo-helm-charts-1.x.x",
      "26.x.x"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install external OO components (AKS)",
    "content": "Operations Orchestration (OO) supports the following components: OO External RAS OO RAS enables execution in remote data centers and networks. The RAS interacts with Central and polls it for operations to execute. Installation of external OO RAS is optional and depends on whether you need additional RASes apart from what's already available when you install OO Central. OO Workflow Designer OO Workflow Designer is a web based environment for authoring flows, which is available as a standalone product that you can deploy either on a Windows or Linux server. Installing OO Workflow Designer is optional and depends on your requirement to author flows. You need to install the OO Workflow Designer as it's not installed along with OO Central. For information about installing these external components, see Install additional OO components.",
    "url": "installoocomponentsaks",
    "filename": "installoocomponentsaks",
    "headings": [],
    "keywords": [
      "install",
      "external",
      "oo",
      "components",
      "aks",
      "operations",
      "orchestration",
      "supports",
      "following",
      "ras",
      "enables",
      "execution",
      "remote",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "installation",
      "optional",
      "depends",
      "whether",
      "need",
      "additional",
      "rases",
      "apart",
      "what",
      "already",
      "available",
      "central.",
      "workflow",
      "designer",
      "web",
      "based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deploy",
      "either",
      "windows",
      "linux",
      "server.",
      "installing",
      "requirement",
      "author",
      "flows.",
      "installed",
      "along",
      "information",
      "about",
      "see",
      "components."
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install external oo components (aks)",
    "contentLower": "operations orchestration (oo) supports the following components: oo external ras oo ras enables execution in remote data centers and networks. the ras interacts with central and polls it for operations to execute. installation of external oo ras is optional and depends on whether you need additional rases apart from what's already available when you install oo central. oo workflow designer oo workflow designer is a web based environment for authoring flows, which is available as a standalone product that you can deploy either on a windows or linux server. installing oo workflow designer is optional and depends on your requirement to author flows. you need to install the oo workflow designer as it's not installed along with oo central. for information about installing these external components, see install additional oo components.",
    "keywordsLower": [
      "install",
      "external",
      "oo",
      "components",
      "aks",
      "operations",
      "orchestration",
      "supports",
      "following",
      "ras",
      "enables",
      "execution",
      "remote",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "installation",
      "optional",
      "depends",
      "whether",
      "need",
      "additional",
      "rases",
      "apart",
      "what",
      "already",
      "available",
      "central.",
      "workflow",
      "designer",
      "web",
      "based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deploy",
      "either",
      "windows",
      "linux",
      "server.",
      "installing",
      "requirement",
      "author",
      "flows.",
      "installed",
      "along",
      "information",
      "about",
      "see",
      "components."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install on GCP",
    "content": "This section describes how to install the suite on Google Cloud Platform (GCP) using Google Kubernetes Engine (GKE). Before you begin, ensure that you've reviewed the following planning topics for the GCP deployment: The overall reference architecture. The support matrix. The definition of suite size and the required hardware resources. The most frequently asked questions. For information on preparation, installation, and setup tasks to deploy the suite, see Deploy the suite on GCP",
    "url": "gcpinstall",
    "filename": "gcpinstall",
    "headings": [],
    "keywords": [
      "install",
      "gcp",
      "section",
      "describes",
      "suite",
      "google",
      "cloud",
      "platform",
      "kubernetes",
      "engine",
      "gke",
      "before",
      "begin",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "topics",
      "deployment",
      "overall",
      "reference",
      "architecture.",
      "support",
      "matrix.",
      "definition",
      "size",
      "required",
      "hardware",
      "resources.",
      "most",
      "frequently",
      "asked",
      "questions.",
      "information",
      "preparation",
      "installation",
      "setup",
      "tasks",
      "deploy",
      "see"
    ],
    "language": "en",
    "word_count": 50,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install on gcp",
    "contentLower": "this section describes how to install the suite on google cloud platform (gcp) using google kubernetes engine (gke). before you begin, ensure that you've reviewed the following planning topics for the gcp deployment: the overall reference architecture. the support matrix. the definition of suite size and the required hardware resources. the most frequently asked questions. for information on preparation, installation, and setup tasks to deploy the suite, see deploy the suite on gcp",
    "keywordsLower": [
      "install",
      "gcp",
      "section",
      "describes",
      "suite",
      "google",
      "cloud",
      "platform",
      "kubernetes",
      "engine",
      "gke",
      "before",
      "begin",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "topics",
      "deployment",
      "overall",
      "reference",
      "architecture.",
      "support",
      "matrix.",
      "definition",
      "size",
      "required",
      "hardware",
      "resources.",
      "most",
      "frequently",
      "asked",
      "questions.",
      "information",
      "preparation",
      "installation",
      "setup",
      "tasks",
      "deploy",
      "see"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Installation tasks (GCP)",
    "content": "This section contains all the mandatory and optional tasks to install the suite in a GCP environment. Activate Docker Hub account Prepare GCP project and roles Create Virtual Private Cloud Build Kubernetes cluster Prepare persistent volumes for ESM on GCP Configure bastion node Download installation packages Subscribe to Cloud Filestore Create external databases (Optional) Install and configure Vertica database Download and upload container images to a registry Deploy bootstrap Select the deployment configuration Check and download the container images Deploy the infrastructure services Configure GCP external application load balancer (Optional) Configure GCP internal application load balancer",
    "url": "gcpprepare",
    "filename": "gcpprepare",
    "headings": [],
    "keywords": [
      "installation",
      "tasks",
      "gcp",
      "section",
      "contains",
      "all",
      "mandatory",
      "optional",
      "install",
      "suite",
      "environment.",
      "activate",
      "docker",
      "hub",
      "account",
      "prepare",
      "project",
      "roles",
      "create",
      "virtual",
      "private",
      "cloud",
      "build",
      "kubernetes",
      "cluster",
      "persistent",
      "volumes",
      "esm",
      "configure",
      "bastion",
      "node",
      "download",
      "packages",
      "subscribe",
      "filestore",
      "external",
      "databases",
      "vertica",
      "database",
      "upload",
      "container",
      "images",
      "registry",
      "deploy",
      "bootstrap",
      "select",
      "deployment",
      "configuration",
      "check",
      "infrastructure",
      "services",
      "application",
      "load",
      "balancer",
      "internal"
    ],
    "language": "en",
    "word_count": 80,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "installation tasks (gcp)",
    "contentLower": "this section contains all the mandatory and optional tasks to install the suite in a gcp environment. activate docker hub account prepare gcp project and roles create virtual private cloud build kubernetes cluster prepare persistent volumes for esm on gcp configure bastion node download installation packages subscribe to cloud filestore create external databases (optional) install and configure vertica database download and upload container images to a registry deploy bootstrap select the deployment configuration check and download the container images deploy the infrastructure services configure gcp external application load balancer (optional) configure gcp internal application load balancer",
    "keywordsLower": [
      "installation",
      "tasks",
      "gcp",
      "section",
      "contains",
      "all",
      "mandatory",
      "optional",
      "install",
      "suite",
      "environment.",
      "activate",
      "docker",
      "hub",
      "account",
      "prepare",
      "project",
      "roles",
      "create",
      "virtual",
      "private",
      "cloud",
      "build",
      "kubernetes",
      "cluster",
      "persistent",
      "volumes",
      "esm",
      "configure",
      "bastion",
      "node",
      "download",
      "packages",
      "subscribe",
      "filestore",
      "external",
      "databases",
      "vertica",
      "database",
      "upload",
      "container",
      "images",
      "registry",
      "deploy",
      "bootstrap",
      "select",
      "deployment",
      "configuration",
      "check",
      "infrastructure",
      "services",
      "application",
      "load",
      "balancer",
      "internal"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install and configure Vertica database",
    "content": "If you plan to install CMP FinOps, you must install and configure the Vertica database as a prerequisite. For information, refer to Install and configure Vertica database.",
    "url": "installconfverticagcp",
    "filename": "installconfverticagcp",
    "headings": [],
    "keywords": [
      "install",
      "configure",
      "vertica",
      "database",
      "plan",
      "cmp",
      "finops",
      "prerequisite.",
      "information",
      "refer",
      "database."
    ],
    "language": "en",
    "word_count": 19,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install and configure vertica database",
    "contentLower": "if you plan to install cmp finops, you must install and configure the vertica database as a prerequisite. for information, refer to install and configure vertica database.",
    "keywordsLower": [
      "install",
      "configure",
      "vertica",
      "database",
      "plan",
      "cmp",
      "finops",
      "prerequisite.",
      "information",
      "refer",
      "database."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Containerized on GCP (GKE)",
    "content": "This section describes how to install OO Containerized on Google Cloud Platform (GCP) using Google Kubernetes Engine (GKE). Topic Description Plan Review the required resources and create a deployment plan for the installation of OO with GKE: Review support matrixPlan the deployment Prepare Prepare the required resources for the installation of OO with GKE: Prepare GCP project and rolesCreate Virtual Private CloudSet up IP and external access hostnameDownload OO Containerized chart package (GKE)Download and upload container images for OOConfigure Google Cloud Filestore for OOCreate a new deployment for OO on GKEPrepare persistent volumes and claims for OO (GKE)Create OO integration admin user in the Service Management IdM (GKE)Subscribe NFS FilestoreCreate external databasesConfigure load balancers for OO (GKE)Create values.yaml for OO (GKE)Launch Google Cloud SQL for OO Deploy Install OO Containerized on GCPVerify the OO installation on GKE Install external OO components Install exter",
    "url": "399-installoocgcp",
    "filename": "399-installoocgcp",
    "headings": [],
    "keywords": [
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "gcp",
      "gke",
      "section",
      "describes",
      "google",
      "cloud",
      "platform",
      "kubernetes",
      "engine",
      "topic",
      "description",
      "plan",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "installation",
      "support",
      "matrixplan",
      "prepare",
      "project",
      "rolescreate",
      "virtual",
      "private",
      "cloudset",
      "ip",
      "external",
      "access",
      "hostnamedownload",
      "chart",
      "package",
      "download",
      "upload",
      "container",
      "images",
      "ooconfigure",
      "filestore",
      "oocreate",
      "new",
      "gkeprepare",
      "persistent",
      "volumes",
      "claims",
      "integration",
      "admin",
      "user",
      "service",
      "management",
      "idm",
      "subscribe",
      "nfs",
      "filestorecreate",
      "databasesconfigure",
      "load",
      "balancers",
      "launch",
      "sql",
      "deploy",
      "gcpverify",
      "components",
      "uninstall",
      "same",
      "optic",
      "toolkit",
      "omt",
      "installed."
    ],
    "language": "en",
    "word_count": 116,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo containerized on gcp (gke)",
    "contentLower": "this section describes how to install oo containerized on google cloud platform (gcp) using google kubernetes engine (gke). topic description plan review the required resources and create a deployment plan for the installation of oo with gke: review support matrixplan the deployment prepare prepare the required resources for the installation of oo with gke: prepare gcp project and rolescreate virtual private cloudset up ip and external access hostnamedownload oo containerized chart package (gke)download and upload container images for ooconfigure google cloud filestore for oocreate a new deployment for oo on gkeprepare persistent volumes and claims for oo (gke)create oo integration admin user in the service management idm (gke)subscribe nfs filestorecreate external databasesconfigure load balancers for oo (gke)create values.yaml for oo (gke)launch google cloud sql for oo deploy install oo containerized on gcpverify the oo installation on gke install external oo components install exter",
    "keywordsLower": [
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "gcp",
      "gke",
      "section",
      "describes",
      "google",
      "cloud",
      "platform",
      "kubernetes",
      "engine",
      "topic",
      "description",
      "plan",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "installation",
      "support",
      "matrixplan",
      "prepare",
      "project",
      "rolescreate",
      "virtual",
      "private",
      "cloudset",
      "ip",
      "external",
      "access",
      "hostnamedownload",
      "chart",
      "package",
      "download",
      "upload",
      "container",
      "images",
      "ooconfigure",
      "filestore",
      "oocreate",
      "new",
      "gkeprepare",
      "persistent",
      "volumes",
      "claims",
      "integration",
      "admin",
      "user",
      "service",
      "management",
      "idm",
      "subscribe",
      "nfs",
      "filestorecreate",
      "databasesconfigure",
      "load",
      "balancers",
      "launch",
      "sql",
      "deploy",
      "gcpverify",
      "components",
      "uninstall",
      "same",
      "optic",
      "toolkit",
      "omt",
      "installed."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Launch Google Cloud SQL for OO",
    "content": "OO Containerized uses Google Cloud SQL to store its data. There are two options for setting up Google Cloud SQL for OO: (Recommended) Use a dedicated Google Cloud SQL instance for OO to ensure a smooth and fluent experience. Use the same Google Cloud SQL instance as the rest of the suite components. Irrespective of the choice, review the sizing considerations to check the Google Cloud SQL requirements. For option 1: Perform the following steps: Verify creating Google Cloud SQL Create databases for OO on the Google Cloud SQL instance For option 2: You must resize the shared Google Cloud SQL instance to an instance type capable of supporting the added requirements of both suite and OO. Perform the Create databases for OO on the Google Cloud SQL instance step. Verify creating Google Cloud SQL After creating the Google Cloud SQL instance, you must test its connectivity by running the command below on the bastion node. Make sure to install the PostgreSQL client on the bastion node before ru",
    "url": "399-gcplaunchcsqlooc",
    "filename": "399-gcplaunchcsqlooc",
    "headings": [
      "Verify creating Google Cloud SQL",
      "Create databases for OO on the Google Cloud SQL instance"
    ],
    "keywords": [
      "launch",
      "google",
      "cloud",
      "sql",
      "oo",
      "verify",
      "creating",
      "create",
      "databases",
      "instance",
      "containerized",
      "uses",
      "store",
      "data.",
      "there",
      "two",
      "options",
      "setting",
      "recommended",
      "dedicated",
      "ensure",
      "smooth",
      "fluent",
      "experience.",
      "same",
      "rest",
      "suite",
      "components.",
      "irrespective",
      "choice",
      "review",
      "sizing",
      "considerations",
      "check",
      "requirements.",
      "option",
      "perform",
      "following",
      "steps",
      "resize",
      "shared",
      "type",
      "capable",
      "supporting",
      "added",
      "requirements",
      "both",
      "oo.",
      "step.",
      "after",
      "test",
      "connectivity",
      "running",
      "command",
      "below",
      "bastion",
      "node.",
      "make",
      "sure",
      "install",
      "postgresql",
      "client",
      "node",
      "before",
      "command.",
      "psql",
      "-h",
      "-p",
      "5432",
      "-u",
      "-d",
      "postgres",
      "database",
      "hostname",
      "specified",
      "launching",
      "instance.",
      "replace",
      "actual",
      "value.",
      "admin",
      "user",
      "empty",
      "initialize",
      "during",
      "installation.",
      "table",
      "shows",
      "example",
      "information",
      "prepared",
      "component",
      "name",
      "schema",
      "default",
      "central",
      "oocentraldbuser",
      "oocentraldb",
      "session",
      "manager"
    ],
    "language": "en",
    "word_count": 120,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "launch google cloud sql for oo",
    "contentLower": "oo containerized uses google cloud sql to store its data. there are two options for setting up google cloud sql for oo: (recommended) use a dedicated google cloud sql instance for oo to ensure a smooth and fluent experience. use the same google cloud sql instance as the rest of the suite components. irrespective of the choice, review the sizing considerations to check the google cloud sql requirements. for option 1: perform the following steps: verify creating google cloud sql create databases for oo on the google cloud sql instance for option 2: you must resize the shared google cloud sql instance to an instance type capable of supporting the added requirements of both suite and oo. perform the create databases for oo on the google cloud sql instance step. verify creating google cloud sql after creating the google cloud sql instance, you must test its connectivity by running the command below on the bastion node. make sure to install the postgresql client on the bastion node before ru",
    "keywordsLower": [
      "launch",
      "google",
      "cloud",
      "sql",
      "oo",
      "verify",
      "creating",
      "create",
      "databases",
      "instance",
      "containerized",
      "uses",
      "store",
      "data.",
      "there",
      "two",
      "options",
      "setting",
      "recommended",
      "dedicated",
      "ensure",
      "smooth",
      "fluent",
      "experience.",
      "same",
      "rest",
      "suite",
      "components.",
      "irrespective",
      "choice",
      "review",
      "sizing",
      "considerations",
      "check",
      "requirements.",
      "option",
      "perform",
      "following",
      "steps",
      "resize",
      "shared",
      "type",
      "capable",
      "supporting",
      "added",
      "requirements",
      "both",
      "oo.",
      "step.",
      "after",
      "test",
      "connectivity",
      "running",
      "command",
      "below",
      "bastion",
      "node.",
      "make",
      "sure",
      "install",
      "postgresql",
      "client",
      "node",
      "before",
      "command.",
      "psql",
      "-h",
      "-p",
      "5432",
      "-u",
      "-d",
      "postgres",
      "database",
      "hostname",
      "specified",
      "launching",
      "instance.",
      "replace",
      "actual",
      "value.",
      "admin",
      "user",
      "empty",
      "initialize",
      "during",
      "installation.",
      "table",
      "shows",
      "example",
      "information",
      "prepared",
      "component",
      "name",
      "schema",
      "default",
      "central",
      "oocentraldbuser",
      "oocentraldb",
      "session",
      "manager"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Containerized on GCP",
    "content": "To install OO Containerized on GCP run the following command on the bastion node. If you use custom values for resources in the values.yaml file, the global.oo.size parameter gets disabled. If you're adding certificates at Helm install time using CLI, ensure to remove the caCertificates parameter from the values.yaml file. <OMT_External_K8s_2x.x-xxx>/bin/helm install <OO INSTALL NAME> <OO CHART FILE> --namespace <OO NAMESPACE> -f <values.yaml> --set global.oo.size=xx --set-file \"caCertificates.cert_one\\.crt\"=/path/to/cert_one.crt Where, <OMT_External_K8s_2x.x-xxx> = installation directory of OMT. <OO INSTALL NAME> = the unique release name you need to specify for the OO deployment. <OO CHART FILE> = the absolute path of the OO chart file. <OO NAMESPACE> = the unique namespace of the OO deployment. Use the namespace you specified in the Create a new deployment for OO in GKE topic. <values.yaml> = the path to OO values.yaml file configured as part of the preparation step Create values.ya",
    "url": "399-installoocongcp",
    "filename": "399-installoocongcp",
    "headings": [],
    "keywords": [
      "oo_db.crt",
      "smax_lb.crt",
      "cert_one.crt",
      "global.oo",
      "x.tgz",
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "gcp",
      "run",
      "following",
      "command",
      "bastion",
      "node.",
      "custom",
      "values",
      "resources",
      "file",
      "global.oo.size",
      "parameter",
      "gets",
      "disabled.",
      "re",
      "adding",
      "certificates",
      "helm",
      "time",
      "cli",
      "ensure",
      "remove",
      "cacertificates",
      "file.",
      "bin",
      "--namespace",
      "-f",
      "--set",
      "xx",
      "--set-file",
      ".crt",
      "path",
      "installation",
      "directory",
      "omt.",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "absolute",
      "chart",
      "namespace",
      "specified",
      "create",
      "new",
      "deployment",
      "gke",
      "topic.",
      "configured",
      "part",
      "preparation",
      "step",
      "gke.",
      "number",
      "tenants",
      "value",
      "between",
      "20.",
      "default",
      "don",
      "configure",
      "parameter.",
      "optional",
      "arguments",
      "because",
      "address",
      "one",
      "way",
      "having",
      "trust-specific",
      "certificates.",
      "depending",
      "choice",
      "during",
      "see",
      "service",
      "management",
      "integration",
      "parameters",
      "opted",
      "provide",
      "required",
      "provided",
      "directly",
      "command.",
      "example",
      "oo-helm-charts-1.x.x",
      "26.x.x",
      "oo-helm-charts"
    ],
    "language": "en",
    "word_count": 85,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo containerized on gcp",
    "contentLower": "to install oo containerized on gcp run the following command on the bastion node. if you use custom values for resources in the values.yaml file, the global.oo.size parameter gets disabled. if you're adding certificates at helm install time using cli, ensure to remove the cacertificates parameter from the values.yaml file. <omt_external_k8s_2x.x-xxx>/bin/helm install <oo install name> <oo chart file> --namespace <oo namespace> -f <values.yaml> --set global.oo.size=xx --set-file \"cacertificates.cert_one\\.crt\"=/path/to/cert_one.crt where, <omt_external_k8s_2x.x-xxx> = installation directory of omt. <oo install name> = the unique release name you need to specify for the oo deployment. <oo chart file> = the absolute path of the oo chart file. <oo namespace> = the unique namespace of the oo deployment. use the namespace you specified in the create a new deployment for oo in gke topic. <values.yaml> = the path to oo values.yaml file configured as part of the preparation step create values.ya",
    "keywordsLower": [
      "oo_db.crt",
      "smax_lb.crt",
      "cert_one.crt",
      "global.oo",
      "x.tgz",
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "gcp",
      "run",
      "following",
      "command",
      "bastion",
      "node.",
      "custom",
      "values",
      "resources",
      "file",
      "global.oo.size",
      "parameter",
      "gets",
      "disabled.",
      "re",
      "adding",
      "certificates",
      "helm",
      "time",
      "cli",
      "ensure",
      "remove",
      "cacertificates",
      "file.",
      "bin",
      "--namespace",
      "-f",
      "--set",
      "xx",
      "--set-file",
      ".crt",
      "path",
      "installation",
      "directory",
      "omt.",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "absolute",
      "chart",
      "namespace",
      "specified",
      "create",
      "new",
      "deployment",
      "gke",
      "topic.",
      "configured",
      "part",
      "preparation",
      "step",
      "gke.",
      "number",
      "tenants",
      "value",
      "between",
      "20.",
      "default",
      "don",
      "configure",
      "parameter.",
      "optional",
      "arguments",
      "because",
      "address",
      "one",
      "way",
      "having",
      "trust-specific",
      "certificates.",
      "depending",
      "choice",
      "during",
      "see",
      "service",
      "management",
      "integration",
      "parameters",
      "opted",
      "provide",
      "required",
      "provided",
      "directly",
      "command.",
      "example",
      "oo-helm-charts-1.x.x",
      "26.x.x",
      "oo-helm-charts"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install external OO components (GKE)",
    "content": "Operations Orchestration (OO) supports the following components: OO External RAS OO RAS enables execution in remote data centers and networks. The RAS interacts with Central and polls it for operations to execute. Installation of external OO RAS is optional and depends on whether you need additional RASes apart from what's already available when you install OO Central. OO Workflow Designer OO Workflow Designer is a web-based environment for authoring flows, which is available as a standalone product that you can deploy on a Windows or a Linux server. Installing OO Workflow Designer is optional and depends on your requirement to author flows. You need to install the OO Workflow Designer as it's not installed along with OO Central. For information about installing these external components, see Install additional OO components.",
    "url": "399-gcpinstallooccomponents",
    "filename": "399-gcpinstallooccomponents",
    "headings": [],
    "keywords": [
      "install",
      "external",
      "oo",
      "components",
      "gke",
      "operations",
      "orchestration",
      "supports",
      "following",
      "ras",
      "enables",
      "execution",
      "remote",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "installation",
      "optional",
      "depends",
      "whether",
      "need",
      "additional",
      "rases",
      "apart",
      "what",
      "already",
      "available",
      "central.",
      "workflow",
      "designer",
      "web-based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deploy",
      "windows",
      "linux",
      "server.",
      "installing",
      "requirement",
      "author",
      "flows.",
      "installed",
      "along",
      "information",
      "about",
      "see",
      "components."
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install external oo components (gke)",
    "contentLower": "operations orchestration (oo) supports the following components: oo external ras oo ras enables execution in remote data centers and networks. the ras interacts with central and polls it for operations to execute. installation of external oo ras is optional and depends on whether you need additional rases apart from what's already available when you install oo central. oo workflow designer oo workflow designer is a web-based environment for authoring flows, which is available as a standalone product that you can deploy on a windows or a linux server. installing oo workflow designer is optional and depends on your requirement to author flows. you need to install the oo workflow designer as it's not installed along with oo central. for information about installing these external components, see install additional oo components.",
    "keywordsLower": [
      "install",
      "external",
      "oo",
      "components",
      "gke",
      "operations",
      "orchestration",
      "supports",
      "following",
      "ras",
      "enables",
      "execution",
      "remote",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "installation",
      "optional",
      "depends",
      "whether",
      "need",
      "additional",
      "rases",
      "apart",
      "what",
      "already",
      "available",
      "central.",
      "workflow",
      "designer",
      "web-based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deploy",
      "windows",
      "linux",
      "server.",
      "installing",
      "requirement",
      "author",
      "flows.",
      "installed",
      "along",
      "information",
      "about",
      "see",
      "components."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install UD/UCMDB on GCP",
    "content": "Role Location UD/UCMDB administrator Bastion node To install UD/UCMDB on GCP, follow these steps: On the bastion node, navigate to the OMT installation directory: cd OMT_External_K8s_2x.x-xxx/bin Run the following command: ./helm install <UD/UCMDB RELEASE NAME> <UD/UCMDB CHART FILE> --namespace <UD/UCMDB NAMESPACE> -f <VALUES YAML FILE> -f <SECRETS YAML FILE> Where, <UD/UCMDB RELEASE NAME> is the unique release name that you need to specify for the UD/UCMDB deployment. You need to use the same release name as specified here when you upgrade UD/UCMDB in the future. <UD/UCMDB CHART FILE> is the tar file containing the UD/UCMDB installation charts. <UD/UCMDB NAMESPACE> is the unique namespace of the UD/UCMDB deployment. Use the namespace that you specified in the \"Create a deployment for UD/UCMDB\" topic. <VALUES YAML FILE> is your custom my-values.yaml file with properties configured for the UD/UCMDB deployment. <SECRETS YAML FILE> is the YAML file that stores the UD/UCMDB secrets. This f",
    "url": "403-installucmdbongcp",
    "filename": "403-installucmdbongcp",
    "headings": [],
    "keywords": [
      "uducmdb",
      "x.tgz",
      "secrets.yaml",
      "1.xx",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "ud",
      "ucmdb",
      "gcp",
      "role",
      "location",
      "administrator",
      "bastion",
      "node",
      "follow",
      "steps",
      "navigate",
      "omt",
      "installation",
      "directory",
      "cd",
      "bin",
      "run",
      "following",
      "command",
      "helm",
      "--namespace",
      "-f",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "tar",
      "file",
      "containing",
      "charts.",
      "namespace",
      "create",
      "deployment",
      "topic.",
      "custom",
      "my-values.yaml",
      "properties",
      "configured",
      "yaml",
      "stores",
      "secrets.",
      "created",
      "generate",
      "vault",
      "secrets",
      "previous",
      "step.",
      "example",
      "ucmdb-prod",
      "ucmdb-helm-charts",
      "charts",
      "ucmdb-1.xx.x-xxx",
      "2x.x.x.tgz",
      "uducmdb-secrets.yaml",
      "above",
      "--dry-run",
      "argument",
      "simulate",
      "installation.",
      "useful",
      "validate",
      "content",
      "before",
      "real",
      "after",
      "delete",
      "contains",
      "sensitive",
      "data."
    ],
    "language": "en",
    "word_count": 86,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install ud/ucmdb on gcp",
    "contentLower": "role location ud/ucmdb administrator bastion node to install ud/ucmdb on gcp, follow these steps: on the bastion node, navigate to the omt installation directory: cd omt_external_k8s_2x.x-xxx/bin run the following command: ./helm install <ud/ucmdb release name> <ud/ucmdb chart file> --namespace <ud/ucmdb namespace> -f <values yaml file> -f <secrets yaml file> where, <ud/ucmdb release name> is the unique release name that you need to specify for the ud/ucmdb deployment. you need to use the same release name as specified here when you upgrade ud/ucmdb in the future. <ud/ucmdb chart file> is the tar file containing the ud/ucmdb installation charts. <ud/ucmdb namespace> is the unique namespace of the ud/ucmdb deployment. use the namespace that you specified in the \"create a deployment for ud/ucmdb\" topic. <values yaml file> is your custom my-values.yaml file with properties configured for the ud/ucmdb deployment. <secrets yaml file> is the yaml file that stores the ud/ucmdb secrets. this f",
    "keywordsLower": [
      "uducmdb",
      "x.tgz",
      "secrets.yaml",
      "1.xx",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "ud",
      "ucmdb",
      "gcp",
      "role",
      "location",
      "administrator",
      "bastion",
      "node",
      "follow",
      "steps",
      "navigate",
      "omt",
      "installation",
      "directory",
      "cd",
      "bin",
      "run",
      "following",
      "command",
      "helm",
      "--namespace",
      "-f",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "tar",
      "file",
      "containing",
      "charts.",
      "namespace",
      "create",
      "deployment",
      "topic.",
      "custom",
      "my-values.yaml",
      "properties",
      "configured",
      "yaml",
      "stores",
      "secrets.",
      "created",
      "generate",
      "vault",
      "secrets",
      "previous",
      "step.",
      "example",
      "ucmdb-prod",
      "ucmdb-helm-charts",
      "charts",
      "ucmdb-1.xx.x-xxx",
      "2x.x.x.tgz",
      "uducmdb-secrets.yaml",
      "above",
      "--dry-run",
      "argument",
      "simulate",
      "installation.",
      "useful",
      "validate",
      "content",
      "before",
      "real",
      "after",
      "delete",
      "contains",
      "sensitive",
      "data."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Launch RDS for Audit service (GCP)",
    "content": "Use the Relational Database Service (RDS) instance for the suite and create a database for the Audit service. Set up PostgreSQL database for Audit service on RDS instance Make sure you enable SSL on PostgreSQL because Audit uses SSL to communicate with PostgreSQL. The following are sample SQL commands for creating a user and a database for a service: Log in to the bastion node and then run the following command to connect to the PostgreSQL instance: psql -h <RDSEndpointAddress> -p <RDSEndpointPort> -U <postgres admin> -d postgres Run the following commands to create the databases and the users used by the Audit service: CREATE USER <USERNAME> PASSWORD '<PASSWORD>'; grant <USERNAME> to <PostgresSQL admin>; CREATE DATABASE <DBNAME> OWNER <USERNAME>; <PASSWORD> for the database user should be at least 16 characters long and have alphanumeric characters, at least one lower case letter, at least one upper case letter, and at least one digit. For example, CREATE USER auditdbuser PASSWORD 'Pa",
    "url": "131-launchrdsgcpaudit",
    "filename": "131-launchrdsgcpaudit",
    "headings": [],
    "keywords": [
      "launch",
      "rds",
      "audit",
      "service",
      "gcp",
      "relational",
      "database",
      "instance",
      "suite",
      "create",
      "service.",
      "set",
      "postgresql",
      "make",
      "sure",
      "enable",
      "ssl",
      "because",
      "uses",
      "communicate",
      "postgresql.",
      "following",
      "sample",
      "sql",
      "commands",
      "creating",
      "user",
      "log",
      "bastion",
      "node",
      "run",
      "command",
      "connect",
      "psql",
      "-h",
      "-p",
      "-u",
      "-d",
      "postgres",
      "databases",
      "users",
      "password",
      "grant",
      "owner",
      "least",
      "16",
      "characters",
      "long",
      "alphanumeric",
      "one",
      "lower",
      "case",
      "letter",
      "upper",
      "digit.",
      "example",
      "auditdbuser",
      "audit1234",
      "auditdb"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "launch rds for audit service (gcp)",
    "contentLower": "use the relational database service (rds) instance for the suite and create a database for the audit service. set up postgresql database for audit service on rds instance make sure you enable ssl on postgresql because audit uses ssl to communicate with postgresql. the following are sample sql commands for creating a user and a database for a service: log in to the bastion node and then run the following command to connect to the postgresql instance: psql -h <rdsendpointaddress> -p <rdsendpointport> -u <postgres admin> -d postgres run the following commands to create the databases and the users used by the audit service: create user <username> password '<password>'; grant <username> to <postgressql admin>; create database <dbname> owner <username>; <password> for the database user should be at least 16 characters long and have alphanumeric characters, at least one lower case letter, at least one upper case letter, and at least one digit. for example, create user auditdbuser password 'pa",
    "keywordsLower": [
      "launch",
      "rds",
      "audit",
      "service",
      "gcp",
      "relational",
      "database",
      "instance",
      "suite",
      "create",
      "service.",
      "set",
      "postgresql",
      "make",
      "sure",
      "enable",
      "ssl",
      "because",
      "uses",
      "communicate",
      "postgresql.",
      "following",
      "sample",
      "sql",
      "commands",
      "creating",
      "user",
      "log",
      "bastion",
      "node",
      "run",
      "command",
      "connect",
      "psql",
      "-h",
      "-p",
      "-u",
      "-d",
      "postgres",
      "databases",
      "users",
      "password",
      "grant",
      "owner",
      "least",
      "16",
      "characters",
      "long",
      "alphanumeric",
      "one",
      "lower",
      "case",
      "letter",
      "upper",
      "digit.",
      "example",
      "auditdbuser",
      "audit1234",
      "auditdb"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install Audit collector on GCP",
    "content": "This section provides the steps required to install the Audit collector. Install and configure Audit collector in each of the namespaces that contain the Audit producer services, for example, SMAX and UD/UCMDB. Audit collector requires the Audit producer file system and it will use the Audit producer PVC and subpath. Generate vault secrets To generate vault secrets for the Audit collector chart, perform the following steps on the control plane node: Run the following command: $CDF_HOME/scripts/gen_secrets.sh -n <Audit Producer Namespace> -c <Audit Collector Chart File> -o <Secrets YAML File> Where, <Audit Audit Producer Namespace> is the namespace in which the Audit Producer (for example, SMAX and UD/UCMDB) is running. <Audit Collector Chart File> is the tar file containing the Audit collector installation charts (audit-collector-<version>.tgz). <Secrets YAML File> is the YAML file that stores the Audit collector secret. Specify the file name (for example, audit-collector-secret.yaml),",
    "url": "131-installauditcollectorgcp",
    "filename": "131-installauditcollectorgcp",
    "headings": [
      "Generate vault secrets",
      "Install Audit collector"
    ],
    "keywords": [
      "cert_two.crt",
      "idm.crt",
      "cert_one.crt",
      "ase.crt",
      "caCertificates.idm",
      "secrets.yaml",
      "secret.yaml",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "audit",
      "collector",
      "gcp",
      "generate",
      "vault",
      "secrets",
      "section",
      "provides",
      "steps",
      "required",
      "collector.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "services",
      "example",
      "smax",
      "ud",
      "ucmdb.",
      "requires",
      "file",
      "system",
      "pvc",
      "subpath.",
      "chart",
      "perform",
      "following",
      "control",
      "plane",
      "node",
      "run",
      "command",
      "scripts",
      "-n",
      "-c",
      "-o",
      "namespace",
      "ucmdb",
      "running.",
      "tar",
      "containing",
      "installation",
      "charts",
      "audit-collector-.tgz",
      "yaml",
      "stores",
      "secret.",
      "specify",
      "name",
      "audit-collector-secret.yaml",
      "script",
      "after",
      "script.",
      "need",
      "opt",
      "cdf",
      "itsma-cfpni",
      "secret",
      "utility",
      "prompts",
      "integration",
      "user",
      "password",
      "complete",
      "deployment",
      "helm",
      "-f",
      "audit-collector-secrets.yaml",
      "--set-file",
      ".crt",
      "path",
      "unique",
      "release",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "charts.",
      "properties",
      "configured",
      "generates",
      "holds",
      "secrets.",
      "pass",
      "certificates",
      "option.",
      "see"
    ],
    "language": "en",
    "word_count": 87,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install audit collector on gcp",
    "contentLower": "this section provides the steps required to install the audit collector. install and configure audit collector in each of the namespaces that contain the audit producer services, for example, smax and ud/ucmdb. audit collector requires the audit producer file system and it will use the audit producer pvc and subpath. generate vault secrets to generate vault secrets for the audit collector chart, perform the following steps on the control plane node: run the following command: $cdf_home/scripts/gen_secrets.sh -n <audit producer namespace> -c <audit collector chart file> -o <secrets yaml file> where, <audit audit producer namespace> is the namespace in which the audit producer (for example, smax and ud/ucmdb) is running. <audit collector chart file> is the tar file containing the audit collector installation charts (audit-collector-<version>.tgz). <secrets yaml file> is the yaml file that stores the audit collector secret. specify the file name (for example, audit-collector-secret.yaml),",
    "keywordsLower": [
      "cert_two.crt",
      "idm.crt",
      "cert_one.crt",
      "ase.crt",
      "cacertificates.idm",
      "secrets.yaml",
      "secret.yaml",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "audit",
      "collector",
      "gcp",
      "generate",
      "vault",
      "secrets",
      "section",
      "provides",
      "steps",
      "required",
      "collector.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "services",
      "example",
      "smax",
      "ud",
      "ucmdb.",
      "requires",
      "file",
      "system",
      "pvc",
      "subpath.",
      "chart",
      "perform",
      "following",
      "control",
      "plane",
      "node",
      "run",
      "command",
      "scripts",
      "-n",
      "-c",
      "-o",
      "namespace",
      "ucmdb",
      "running.",
      "tar",
      "containing",
      "installation",
      "charts",
      "audit-collector-.tgz",
      "yaml",
      "stores",
      "secret.",
      "specify",
      "name",
      "audit-collector-secret.yaml",
      "script",
      "after",
      "script.",
      "need",
      "opt",
      "cdf",
      "itsma-cfpni",
      "secret",
      "utility",
      "prompts",
      "integration",
      "user",
      "password",
      "complete",
      "deployment",
      "helm",
      "-f",
      "audit-collector-secrets.yaml",
      "--set-file",
      ".crt",
      "path",
      "unique",
      "release",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "charts.",
      "properties",
      "configured",
      "generates",
      "holds",
      "secrets.",
      "pass",
      "certificates",
      "option.",
      "see"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install on Bring Your Own Kubernetes (BYOK)",
    "content": "You can install the suite on Bring Your Own Kubernetes (BYOK) platforms that are CNCF certified. An installation on any of these Kubernetes platforms uses their Kubernetes instance instead of the one provided by the ITOM OMT installation package. These BYOK platforms have not been tested by OpenText. Before you begin, ensure that you've reviewed the following planning topics for the BYOK deployment: The support matrix. The definition of suite size and the required hardware resources. Prerequisites You have already set up your Kubernetes cluster. If you don't have one already set up, refer to the platform's official documentation for instructions. In addition to the Kubernetes control plane nodes and worker nodes in the cluster, you should have already set up a Linux machine. This serves as the bastion node for your cluster. Deployment tasks See Deployment checklist for all the preparation, installation, and setup tasks to deploy the suite. Currently, the suite installation on BYOK plat",
    "url": "installonbyok",
    "filename": "installonbyok",
    "headings": [
      "Prerequisites",
      "Deployment tasks",
      "Related topics"
    ],
    "keywords": [
      "install",
      "bring",
      "own",
      "kubernetes",
      "byok",
      "prerequisites",
      "deployment",
      "tasks",
      "related",
      "topics",
      "suite",
      "platforms",
      "cncf",
      "certified.",
      "installation",
      "any",
      "uses",
      "instance",
      "instead",
      "one",
      "provided",
      "itom",
      "omt",
      "package.",
      "tested",
      "opentext.",
      "before",
      "begin",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "support",
      "matrix.",
      "definition",
      "size",
      "required",
      "hardware",
      "resources.",
      "already",
      "set",
      "cluster.",
      "don",
      "refer",
      "platform",
      "official",
      "documentation",
      "instructions.",
      "addition",
      "control",
      "plane",
      "nodes",
      "worker",
      "cluster",
      "linux",
      "machine.",
      "serves",
      "bastion",
      "node",
      "see",
      "checklist",
      "all",
      "preparation",
      "setup",
      "deploy",
      "suite.",
      "currently",
      "supported",
      "helm",
      "approach",
      "hasn",
      "tested.",
      "please",
      "contact",
      "product",
      "management",
      "team",
      "installing",
      "uninstall"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install on bring your own kubernetes (byok)",
    "contentLower": "you can install the suite on bring your own kubernetes (byok) platforms that are cncf certified. an installation on any of these kubernetes platforms uses their kubernetes instance instead of the one provided by the itom omt installation package. these byok platforms have not been tested by opentext. before you begin, ensure that you've reviewed the following planning topics for the byok deployment: the support matrix. the definition of suite size and the required hardware resources. prerequisites you have already set up your kubernetes cluster. if you don't have one already set up, refer to the platform's official documentation for instructions. in addition to the kubernetes control plane nodes and worker nodes in the cluster, you should have already set up a linux machine. this serves as the bastion node for your cluster. deployment tasks see deployment checklist for all the preparation, installation, and setup tasks to deploy the suite. currently, the suite installation on byok plat",
    "keywordsLower": [
      "install",
      "bring",
      "own",
      "kubernetes",
      "byok",
      "prerequisites",
      "deployment",
      "tasks",
      "related",
      "topics",
      "suite",
      "platforms",
      "cncf",
      "certified.",
      "installation",
      "any",
      "uses",
      "instance",
      "instead",
      "one",
      "provided",
      "itom",
      "omt",
      "package.",
      "tested",
      "opentext.",
      "before",
      "begin",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "support",
      "matrix.",
      "definition",
      "size",
      "required",
      "hardware",
      "resources.",
      "already",
      "set",
      "cluster.",
      "don",
      "refer",
      "platform",
      "official",
      "documentation",
      "instructions.",
      "addition",
      "control",
      "plane",
      "nodes",
      "worker",
      "cluster",
      "linux",
      "machine.",
      "serves",
      "bastion",
      "node",
      "see",
      "checklist",
      "all",
      "preparation",
      "setup",
      "deploy",
      "suite.",
      "currently",
      "supported",
      "helm",
      "approach",
      "hasn",
      "tested.",
      "please",
      "contact",
      "product",
      "management",
      "team",
      "installing",
      "uninstall"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install on OpenShift",
    "content": "You can install the suite on Red Hat OpenShift Container Platform, an industry-standard container platform, which comes with a Kubernetes instance. An installation on OpenShift uses this Kubernetes instance instead of the one provided by the OMT installation package. This release supports only on-premises deployments on OpenShift. Before you begin Before you begin, ensure that you've reviewed the following planning topics for the OpenShift deployment: The support matrix. The definition of suite size and the required hardware resources. Prerequisites You have already set up an OpenShift cluster. If you don't have one already set up, refer to the OpenShift documentation for instructions. In addition to the Kubernetes control plane nodes and worker nodes in the cluster, you should have already set up a Linux machine. This serves as the bastion node for the OpenShift cluster. Deployment tasks See Deploy the suite on OpenShift for all the preparation, installation, and setup tasks to instal",
    "url": "installonopenshift",
    "filename": "installonopenshift",
    "headings": [
      "Before you begin",
      "Prerequisites",
      "Deployment tasks",
      "Related topics"
    ],
    "keywords": [
      "install",
      "openshift",
      "before",
      "begin",
      "prerequisites",
      "deployment",
      "tasks",
      "related",
      "topics",
      "suite",
      "red",
      "hat",
      "container",
      "platform",
      "industry-standard",
      "comes",
      "kubernetes",
      "instance.",
      "installation",
      "uses",
      "instance",
      "instead",
      "one",
      "provided",
      "omt",
      "package.",
      "release",
      "supports",
      "on-premises",
      "deployments",
      "openshift.",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "support",
      "matrix.",
      "definition",
      "size",
      "required",
      "hardware",
      "resources.",
      "already",
      "set",
      "cluster.",
      "don",
      "refer",
      "documentation",
      "instructions.",
      "addition",
      "control",
      "plane",
      "nodes",
      "worker",
      "cluster",
      "linux",
      "machine.",
      "serves",
      "bastion",
      "node",
      "see",
      "deploy",
      "all",
      "preparation",
      "setup",
      "uninstall"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install on openshift",
    "contentLower": "you can install the suite on red hat openshift container platform, an industry-standard container platform, which comes with a kubernetes instance. an installation on openshift uses this kubernetes instance instead of the one provided by the omt installation package. this release supports only on-premises deployments on openshift. before you begin before you begin, ensure that you've reviewed the following planning topics for the openshift deployment: the support matrix. the definition of suite size and the required hardware resources. prerequisites you have already set up an openshift cluster. if you don't have one already set up, refer to the openshift documentation for instructions. in addition to the kubernetes control plane nodes and worker nodes in the cluster, you should have already set up a linux machine. this serves as the bastion node for the openshift cluster. deployment tasks see deploy the suite on openshift for all the preparation, installation, and setup tasks to instal",
    "keywordsLower": [
      "install",
      "openshift",
      "before",
      "begin",
      "prerequisites",
      "deployment",
      "tasks",
      "related",
      "topics",
      "suite",
      "red",
      "hat",
      "container",
      "platform",
      "industry-standard",
      "comes",
      "kubernetes",
      "instance.",
      "installation",
      "uses",
      "instance",
      "instead",
      "one",
      "provided",
      "omt",
      "package.",
      "release",
      "supports",
      "on-premises",
      "deployments",
      "openshift.",
      "ensure",
      "ve",
      "reviewed",
      "following",
      "planning",
      "support",
      "matrix.",
      "definition",
      "size",
      "required",
      "hardware",
      "resources.",
      "already",
      "set",
      "cluster.",
      "don",
      "refer",
      "documentation",
      "instructions.",
      "addition",
      "control",
      "plane",
      "nodes",
      "worker",
      "cluster",
      "linux",
      "machine.",
      "serves",
      "bastion",
      "node",
      "see",
      "deploy",
      "all",
      "preparation",
      "setup",
      "uninstall"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Installation tasks (OpenShift)",
    "content": "This section contains some of the mandatory and optional tasks to install the suite in an OpenShift environment. Activate Docker Hub account Prepare external databases Configure the bastion node Download and upload container images to an external registry Configure NFS volumes for the suite Create a deployment for ESM Configure a load balancer on OpenShift You can find the rest of the installation tasks on the Helm installation common tasks page.",
    "url": "prepareopenshiftinstallation",
    "filename": "prepareopenshiftinstallation",
    "headings": [],
    "keywords": [
      "installation",
      "tasks",
      "openshift",
      "section",
      "contains",
      "mandatory",
      "optional",
      "install",
      "suite",
      "environment.",
      "activate",
      "docker",
      "hub",
      "account",
      "prepare",
      "external",
      "databases",
      "configure",
      "bastion",
      "node",
      "download",
      "upload",
      "container",
      "images",
      "registry",
      "nfs",
      "volumes",
      "create",
      "deployment",
      "esm",
      "load",
      "balancer",
      "find",
      "rest",
      "helm",
      "common",
      "page."
    ],
    "language": "en",
    "word_count": 48,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "installation tasks (openshift)",
    "contentLower": "this section contains some of the mandatory and optional tasks to install the suite in an openshift environment. activate docker hub account prepare external databases configure the bastion node download and upload container images to an external registry configure nfs volumes for the suite create a deployment for esm configure a load balancer on openshift you can find the rest of the installation tasks on the helm installation common tasks page.",
    "keywordsLower": [
      "installation",
      "tasks",
      "openshift",
      "section",
      "contains",
      "mandatory",
      "optional",
      "install",
      "suite",
      "environment.",
      "activate",
      "docker",
      "hub",
      "account",
      "prepare",
      "external",
      "databases",
      "configure",
      "bastion",
      "node",
      "download",
      "upload",
      "container",
      "images",
      "registry",
      "nfs",
      "volumes",
      "create",
      "deployment",
      "esm",
      "load",
      "balancer",
      "find",
      "rest",
      "helm",
      "common",
      "page."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install UD/UCMDB on OMT (OpenShift)",
    "content": "RoleLocationSuite adminBastion nodeTo install UD/UCMDB, run the following command: helm install <UD/UCMDB RELEASE NAME> <UD/UCMDB CHART FILE> --namespace <UD/UCMDB NAMESPACE> -f <VALUES YAML FILE> -f <SECRETS YAML FILE> Where, <UD/UCMDB RELEASE NAME> is the unique release name that you need to specify for the UD/UCMDB deployment. You need to use the same release name as specified here when you upgrade UD/UCMDB in the future. <UD/UCMDB CHART FILE> is the tar file containing the UD/UCMDB installation charts. <CMS NAMESPACE> is the unique namespace of the UD/UCMDB deployment. Use the namespace that you specified in the \"Create a deployment for UD/UCMDB\" topic. <VALUES YAML FILE> is your custom my-values.yaml file with properties configured for the UD/UCMDB deployment. <SECRETS YAML FILE> is the YAML file that stores the UD/UCMDB secrets. This file is created when you run gen_secrets.sh to generate vault secrets for UD/UCMDB in the previous step. For example: helm install ucmdb-prod UCMDB_",
    "url": "installcmsopenshift",
    "filename": "installcmsopenshift",
    "headings": [],
    "keywords": [
      "uducmdb",
      "x.tgz",
      "secrets.yaml",
      "1.xx",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "ud",
      "ucmdb",
      "omt",
      "openshift",
      "rolelocationsuite",
      "adminbastion",
      "nodeto",
      "run",
      "following",
      "command",
      "helm",
      "--namespace",
      "-f",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "tar",
      "file",
      "containing",
      "installation",
      "charts.",
      "namespace",
      "create",
      "deployment",
      "topic.",
      "custom",
      "my-values.yaml",
      "properties",
      "configured",
      "yaml",
      "stores",
      "secrets.",
      "created",
      "generate",
      "vault",
      "secrets",
      "previous",
      "step.",
      "example",
      "ucmdb-prod",
      "ucmdb-helm-charts",
      "charts",
      "ucmdb-1.xx.x-xxx",
      "2x.x.x.tgz",
      "cms-secrets.yaml",
      "above",
      "--dry-run",
      "argument",
      "simulate",
      "installation.",
      "useful",
      "validate",
      "content",
      "before",
      "real",
      "after",
      "delete",
      "contains",
      "sensitive",
      "data."
    ],
    "language": "en",
    "word_count": 86,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install ud/ucmdb on omt (openshift)",
    "contentLower": "rolelocationsuite adminbastion nodeto install ud/ucmdb, run the following command: helm install <ud/ucmdb release name> <ud/ucmdb chart file> --namespace <ud/ucmdb namespace> -f <values yaml file> -f <secrets yaml file> where, <ud/ucmdb release name> is the unique release name that you need to specify for the ud/ucmdb deployment. you need to use the same release name as specified here when you upgrade ud/ucmdb in the future. <ud/ucmdb chart file> is the tar file containing the ud/ucmdb installation charts. <cms namespace> is the unique namespace of the ud/ucmdb deployment. use the namespace that you specified in the \"create a deployment for ud/ucmdb\" topic. <values yaml file> is your custom my-values.yaml file with properties configured for the ud/ucmdb deployment. <secrets yaml file> is the yaml file that stores the ud/ucmdb secrets. this file is created when you run gen_secrets.sh to generate vault secrets for ud/ucmdb in the previous step. for example: helm install ucmdb-prod ucmdb_",
    "keywordsLower": [
      "uducmdb",
      "x.tgz",
      "secrets.yaml",
      "1.xx",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "ud",
      "ucmdb",
      "omt",
      "openshift",
      "rolelocationsuite",
      "adminbastion",
      "nodeto",
      "run",
      "following",
      "command",
      "helm",
      "--namespace",
      "-f",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "tar",
      "file",
      "containing",
      "installation",
      "charts.",
      "namespace",
      "create",
      "deployment",
      "topic.",
      "custom",
      "my-values.yaml",
      "properties",
      "configured",
      "yaml",
      "stores",
      "secrets.",
      "created",
      "generate",
      "vault",
      "secrets",
      "previous",
      "step.",
      "example",
      "ucmdb-prod",
      "ucmdb-helm-charts",
      "charts",
      "ucmdb-1.xx.x-xxx",
      "2x.x.x.tgz",
      "cms-secrets.yaml",
      "above",
      "--dry-run",
      "argument",
      "simulate",
      "installation.",
      "useful",
      "validate",
      "content",
      "before",
      "real",
      "after",
      "delete",
      "contains",
      "sensitive",
      "data."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Containerized on OpenShift (OCP)",
    "content": "Perform the following steps to install the containerized OO: Topic Description Plan Review the required resources and create a deployment plan for the installation of OO with OpenShift: Review support matrixPlan the deployment Prepare and deploy Prepare the required resources and then deploy OO with OpenShift: Download OO charts packageDownload container images for OOUpload container images for OO to a registryPrepare an OpenShift project for OOPrepare NFS and PostgreSQL serversConfigure NFS volumes for OOPrepare persistent volumes for OOPrepare external databases for OOCreate OO integration admin user in the suite IdMCreate values.yamlGenerate sensitive data for OO ContainerizedInstall OOVerify the OO installationConfigure a load balancer for OO port 9443 Install external OO components Install external OO components Uninstall Uninstall OO",
    "url": "installooocp",
    "filename": "installooocp",
    "headings": [],
    "keywords": [
      "install",
      "oo",
      "containerized",
      "openshift",
      "ocp",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "plan",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "installation",
      "support",
      "matrixplan",
      "prepare",
      "deploy",
      "download",
      "charts",
      "packagedownload",
      "container",
      "images",
      "ooupload",
      "registryprepare",
      "project",
      "ooprepare",
      "nfs",
      "postgresql",
      "serversconfigure",
      "volumes",
      "persistent",
      "external",
      "databases",
      "oocreate",
      "integration",
      "admin",
      "user",
      "suite",
      "idmcreate",
      "values.yamlgenerate",
      "sensitive",
      "data",
      "containerizedinstall",
      "ooverify",
      "installationconfigure",
      "load",
      "balancer",
      "port",
      "9443",
      "components",
      "uninstall"
    ],
    "language": "en",
    "word_count": 91,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo containerized on openshift (ocp)",
    "contentLower": "perform the following steps to install the containerized oo: topic description plan review the required resources and create a deployment plan for the installation of oo with openshift: review support matrixplan the deployment prepare and deploy prepare the required resources and then deploy oo with openshift: download oo charts packagedownload container images for ooupload container images for oo to a registryprepare an openshift project for ooprepare nfs and postgresql serversconfigure nfs volumes for ooprepare persistent volumes for ooprepare external databases for oocreate oo integration admin user in the suite idmcreate values.yamlgenerate sensitive data for oo containerizedinstall ooverify the oo installationconfigure a load balancer for oo port 9443 install external oo components install external oo components uninstall uninstall oo",
    "keywordsLower": [
      "install",
      "oo",
      "containerized",
      "openshift",
      "ocp",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "plan",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "installation",
      "support",
      "matrixplan",
      "prepare",
      "deploy",
      "download",
      "charts",
      "packagedownload",
      "container",
      "images",
      "ooupload",
      "registryprepare",
      "project",
      "ooprepare",
      "nfs",
      "postgresql",
      "serversconfigure",
      "volumes",
      "persistent",
      "external",
      "databases",
      "oocreate",
      "integration",
      "admin",
      "user",
      "suite",
      "idmcreate",
      "values.yamlgenerate",
      "sensitive",
      "data",
      "containerizedinstall",
      "ooverify",
      "installationconfigure",
      "load",
      "balancer",
      "port",
      "9443",
      "components",
      "uninstall"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO on OpenShift",
    "content": "To install OO Containerized on OpenShift, run the following command on the bastion node. If you use custom values for resources in the values.yaml file, the global.oo.size parameter gets disabled. If you're adding certificates at Helm install time using CLI, ensure to remove the caCertificates parameter from the values.yaml file. $CDF_HOME/bin/helm install <OO INSTALL NAME> <OO CHART FILE> --namespace <OO NAMESPACE> -f <values.yaml> --set global.oo.size=xx --set-file \"caCertificates.cert_one\\.crt\"=/path/to/cert_one.crt Where: $CDF_HOME is the installation directory of OPTIC Management Toolkit (OMT). <OO INSTALL NAME> is the unique release name you need to specify for the OO deployment. <OO CHART FILE> is the absolute path of the OO chart file. <OO NAMESPACE> is the unique namespace of the OO deployment. <values.yaml> is the path to OO values.yaml file which you configured as part of the preparation step Create values.yaml for OO. <global.oo.size> = the number of tenants in the OO deplo",
    "url": "399-installooonocp",
    "filename": "399-installooonocp",
    "headings": [],
    "keywords": [
      "oo_db.crt",
      "smax_lb.crt",
      "cert_one.crt",
      "global.oo",
      "x.tgz",
      "values.yaml",
      "install",
      "oo",
      "openshift",
      "containerized",
      "run",
      "following",
      "command",
      "bastion",
      "node.",
      "custom",
      "values",
      "resources",
      "file",
      "global.oo.size",
      "parameter",
      "gets",
      "disabled.",
      "re",
      "adding",
      "certificates",
      "helm",
      "time",
      "cli",
      "ensure",
      "remove",
      "cacertificates",
      "file.",
      "bin",
      "--namespace",
      "-f",
      "--set",
      "xx",
      "--set-file",
      ".crt",
      "path",
      "installation",
      "directory",
      "optic",
      "management",
      "toolkit",
      "omt",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "absolute",
      "chart",
      "namespace",
      "configured",
      "part",
      "preparation",
      "step",
      "create",
      "oo.",
      "number",
      "tenants",
      "value",
      "between",
      "20.",
      "default",
      "don",
      "configure",
      "parameter.",
      "optional",
      "arguments",
      "because",
      "address",
      "one",
      "way",
      "having",
      "trust-specific",
      "certificates.",
      "depending",
      "choice",
      "during",
      "see",
      "suite",
      "integration",
      "parameters",
      "opted",
      "provide",
      "required",
      "provided",
      "directly",
      "command.",
      "example",
      "oo-helm-charts-1.x.x",
      "26.x.x",
      "oo-helm-charts",
      "charts",
      "oo-1.x.x",
      "26.x.x.tgz"
    ],
    "language": "en",
    "word_count": 86,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo on openshift",
    "contentLower": "to install oo containerized on openshift, run the following command on the bastion node. if you use custom values for resources in the values.yaml file, the global.oo.size parameter gets disabled. if you're adding certificates at helm install time using cli, ensure to remove the cacertificates parameter from the values.yaml file. $cdf_home/bin/helm install <oo install name> <oo chart file> --namespace <oo namespace> -f <values.yaml> --set global.oo.size=xx --set-file \"cacertificates.cert_one\\.crt\"=/path/to/cert_one.crt where: $cdf_home is the installation directory of optic management toolkit (omt). <oo install name> is the unique release name you need to specify for the oo deployment. <oo chart file> is the absolute path of the oo chart file. <oo namespace> is the unique namespace of the oo deployment. <values.yaml> is the path to oo values.yaml file which you configured as part of the preparation step create values.yaml for oo. <global.oo.size> = the number of tenants in the oo deplo",
    "keywordsLower": [
      "oo_db.crt",
      "smax_lb.crt",
      "cert_one.crt",
      "global.oo",
      "x.tgz",
      "values.yaml",
      "install",
      "oo",
      "openshift",
      "containerized",
      "run",
      "following",
      "command",
      "bastion",
      "node.",
      "custom",
      "values",
      "resources",
      "file",
      "global.oo.size",
      "parameter",
      "gets",
      "disabled.",
      "re",
      "adding",
      "certificates",
      "helm",
      "time",
      "cli",
      "ensure",
      "remove",
      "cacertificates",
      "file.",
      "bin",
      "--namespace",
      "-f",
      "--set",
      "xx",
      "--set-file",
      ".crt",
      "path",
      "installation",
      "directory",
      "optic",
      "management",
      "toolkit",
      "omt",
      "unique",
      "release",
      "name",
      "need",
      "specify",
      "deployment.",
      "absolute",
      "chart",
      "namespace",
      "configured",
      "part",
      "preparation",
      "step",
      "create",
      "oo.",
      "number",
      "tenants",
      "value",
      "between",
      "20.",
      "default",
      "don",
      "configure",
      "parameter.",
      "optional",
      "arguments",
      "because",
      "address",
      "one",
      "way",
      "having",
      "trust-specific",
      "certificates.",
      "depending",
      "choice",
      "during",
      "see",
      "suite",
      "integration",
      "parameters",
      "opted",
      "provide",
      "required",
      "provided",
      "directly",
      "command.",
      "example",
      "oo-helm-charts-1.x.x",
      "26.x.x",
      "oo-helm-charts",
      "charts",
      "oo-1.x.x",
      "26.x.x.tgz"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install external OO components (OCP)",
    "content": "Operations Orchestration (OO) supports the following components: OO External RAS OO RAS enables execution in remote data centers and networks. The RAS interacts with Central and polls it for operations to execute. Installation of external OO RAS is optional and depends on whether you need additional RASes apart from what's already available when you install OO Central. OO Workflow Designer OO Workflow Designer is a web based environment for authoring flows, which is available as a standalone product that you can deploy either on a Windows or Linux server. Installing OO Workflow Designer is optional and depends on your requirement to author flows. You need to install the OO Workflow Designer as it's not installed along with OO Central. For information about installing these external components, see Install additional OO components.",
    "url": "installoocomponentsocp",
    "filename": "installoocomponentsocp",
    "headings": [],
    "keywords": [
      "install",
      "external",
      "oo",
      "components",
      "ocp",
      "operations",
      "orchestration",
      "supports",
      "following",
      "ras",
      "enables",
      "execution",
      "remote",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "installation",
      "optional",
      "depends",
      "whether",
      "need",
      "additional",
      "rases",
      "apart",
      "what",
      "already",
      "available",
      "central.",
      "workflow",
      "designer",
      "web",
      "based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deploy",
      "either",
      "windows",
      "linux",
      "server.",
      "installing",
      "requirement",
      "author",
      "flows.",
      "installed",
      "along",
      "information",
      "about",
      "see",
      "components."
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install external oo components (ocp)",
    "contentLower": "operations orchestration (oo) supports the following components: oo external ras oo ras enables execution in remote data centers and networks. the ras interacts with central and polls it for operations to execute. installation of external oo ras is optional and depends on whether you need additional rases apart from what's already available when you install oo central. oo workflow designer oo workflow designer is a web based environment for authoring flows, which is available as a standalone product that you can deploy either on a windows or linux server. installing oo workflow designer is optional and depends on your requirement to author flows. you need to install the oo workflow designer as it's not installed along with oo central. for information about installing these external components, see install additional oo components.",
    "keywordsLower": [
      "install",
      "external",
      "oo",
      "components",
      "ocp",
      "operations",
      "orchestration",
      "supports",
      "following",
      "ras",
      "enables",
      "execution",
      "remote",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "installation",
      "optional",
      "depends",
      "whether",
      "need",
      "additional",
      "rases",
      "apart",
      "what",
      "already",
      "available",
      "central.",
      "workflow",
      "designer",
      "web",
      "based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deploy",
      "either",
      "windows",
      "linux",
      "server.",
      "installing",
      "requirement",
      "author",
      "flows.",
      "installed",
      "along",
      "information",
      "about",
      "see",
      "components."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install Audit on OpenShift",
    "content": "This section describes how to install the Audit service on OpenShift: Plan Review the required resources and create a deployment plan for the installation of the Audit service on OpenShift: Review system requirements Prepare and deploy Prepare the required resources for the installation of the Audit service on OpenShift: Download the Audit Helm chart (OpenShift) Download Audit images (OpenShift) Upload container images for Audit to a registry (OpenShift) Create a deployment (OpenShift) Create NFS volumes for Audit (OpenShift) Prepare persistent volumes for Audit (OpenShift) Prepare an external database for Audit (OpenShift) Configure values.yaml for Audit service (OpenShift) Deploy Audit service (OpenShift) Configure a load balancer for Audit port 9889 (OpenShift) Uninstall Uninstall Audit service on OpenShift.",
    "url": "installauditopenshift",
    "filename": "installauditopenshift",
    "headings": [
      "Plan",
      "Prepare and deploy",
      "Uninstall"
    ],
    "keywords": [
      "values.yaml",
      "install",
      "audit",
      "openshift",
      "plan",
      "prepare",
      "deploy",
      "uninstall",
      "section",
      "describes",
      "service",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "installation",
      "system",
      "requirements",
      "download",
      "helm",
      "chart",
      "images",
      "upload",
      "container",
      "registry",
      "nfs",
      "volumes",
      "persistent",
      "external",
      "database",
      "configure",
      "load",
      "balancer",
      "port",
      "9889",
      "openshift."
    ],
    "language": "en",
    "word_count": 86,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install audit on openshift",
    "contentLower": "this section describes how to install the audit service on openshift: plan review the required resources and create a deployment plan for the installation of the audit service on openshift: review system requirements prepare and deploy prepare the required resources for the installation of the audit service on openshift: download the audit helm chart (openshift) download audit images (openshift) upload container images for audit to a registry (openshift) create a deployment (openshift) create nfs volumes for audit (openshift) prepare persistent volumes for audit (openshift) prepare an external database for audit (openshift) configure values.yaml for audit service (openshift) deploy audit service (openshift) configure a load balancer for audit port 9889 (openshift) uninstall uninstall audit service on openshift.",
    "keywordsLower": [
      "values.yaml",
      "install",
      "audit",
      "openshift",
      "plan",
      "prepare",
      "deploy",
      "uninstall",
      "section",
      "describes",
      "service",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "installation",
      "system",
      "requirements",
      "download",
      "helm",
      "chart",
      "images",
      "upload",
      "container",
      "registry",
      "nfs",
      "volumes",
      "persistent",
      "external",
      "database",
      "configure",
      "load",
      "balancer",
      "port",
      "9889",
      "openshift."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install Audit collector (OpenShift)",
    "content": "Services that are integrated with the Audit service store the audits on the NFS file system when the audit service isn't reachable. Audit collector is a microservice that collects the audits from the NFS file system and passes them to the Audit service via REST. You can deploy the Audit collector using Helm charts. Install and configure Audit collector in each of the namespaces that contain the Audit producer services, for example, Service Management,and UD/UCMDB. Audit collector requires the Audit producer file system and it will use the Audit producer PVC and subpath. This section describes the steps to install the Audit collector.",
    "url": "auditcollectoropenshift",
    "filename": "auditcollectoropenshift",
    "headings": [],
    "keywords": [
      "install",
      "audit",
      "collector",
      "openshift",
      "services",
      "integrated",
      "service",
      "store",
      "audits",
      "nfs",
      "file",
      "system",
      "isn",
      "reachable.",
      "microservice",
      "collects",
      "passes",
      "via",
      "rest.",
      "deploy",
      "helm",
      "charts.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "example",
      "management",
      "ud",
      "ucmdb.",
      "requires",
      "pvc",
      "subpath.",
      "section",
      "describes",
      "steps",
      "collector."
    ],
    "language": "en",
    "word_count": 67,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install audit collector (openshift)",
    "contentLower": "services that are integrated with the audit service store the audits on the nfs file system when the audit service isn't reachable. audit collector is a microservice that collects the audits from the nfs file system and passes them to the audit service via rest. you can deploy the audit collector using helm charts. install and configure audit collector in each of the namespaces that contain the audit producer services, for example, service management,and ud/ucmdb. audit collector requires the audit producer file system and it will use the audit producer pvc and subpath. this section describes the steps to install the audit collector.",
    "keywordsLower": [
      "install",
      "audit",
      "collector",
      "openshift",
      "services",
      "integrated",
      "service",
      "store",
      "audits",
      "nfs",
      "file",
      "system",
      "isn",
      "reachable.",
      "microservice",
      "collects",
      "passes",
      "via",
      "rest.",
      "deploy",
      "helm",
      "charts.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "example",
      "management",
      "ud",
      "ucmdb.",
      "requires",
      "pvc",
      "subpath.",
      "section",
      "describes",
      "steps",
      "collector."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install Audit collector on OpenShift",
    "content": "This section provides the steps required to install the Audit collector. Install and configure Audit collector in each of the namespaces that contain the Audit producer services, for example, Service Management and UCMDB. Audit collector requires the Audit producer file system and it will use the Audit producer PVC and subpath. Generate vault secrets To generate vault secrets for the Audit collector chart, perform the following steps on the bastion node: Run the following command: $CDF_HOME/scripts/gen_secrets.sh -n <Audit Producer Namespace> -c <Audit Collector Chart File> -o <Secrets YAML File> where, <Audit Audit Producer Namespace> is the namespace in which the Audit Producer (for example, Service Management or UCMDB) is running. <Audit Collector Chart File> is the tar file containing the Audit collector installation charts (audit-collector-<version>.tgz). <Secrets YAML File> is the YAML file that stores the Audit collector secrets. Specify the file name (for example, audit-collect",
    "url": "installauditcollectoropenshift",
    "filename": "installauditcollectoropenshift",
    "headings": [
      "Generate vault secrets",
      "Install"
    ],
    "keywords": [
      "cert_two.crt",
      "idm.crt",
      "cert_one.crt",
      "ase.crt",
      "caCertificates.idm",
      "secrets.yaml",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "audit",
      "collector",
      "openshift",
      "generate",
      "vault",
      "secrets",
      "section",
      "provides",
      "steps",
      "required",
      "collector.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "services",
      "example",
      "service",
      "management",
      "ucmdb.",
      "requires",
      "file",
      "system",
      "pvc",
      "subpath.",
      "chart",
      "perform",
      "following",
      "bastion",
      "node",
      "run",
      "command",
      "scripts",
      "-n",
      "-c",
      "-o",
      "namespace",
      "ucmdb",
      "running.",
      "tar",
      "containing",
      "installation",
      "charts",
      "audit-collector-.tgz",
      "yaml",
      "stores",
      "secrets.",
      "specify",
      "name",
      "audit-collector-secrets.yaml",
      "script",
      "after",
      "script.",
      "need",
      "root",
      "cdf",
      "itsma-cfpni",
      "secret",
      "utility",
      "prompts",
      "integration",
      "user",
      "password",
      "complete",
      "deployment",
      "helm",
      "-f",
      "--set-file",
      ".crt",
      "path",
      "unique",
      "release",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "charts.",
      "properties",
      "configured",
      "generates",
      "holds",
      "pass",
      "certificates",
      "option.",
      "see",
      "suite",
      "engine",
      "certificates.",
      "audit-collector"
    ],
    "language": "en",
    "word_count": 86,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install audit collector on openshift",
    "contentLower": "this section provides the steps required to install the audit collector. install and configure audit collector in each of the namespaces that contain the audit producer services, for example, service management and ucmdb. audit collector requires the audit producer file system and it will use the audit producer pvc and subpath. generate vault secrets to generate vault secrets for the audit collector chart, perform the following steps on the bastion node: run the following command: $cdf_home/scripts/gen_secrets.sh -n <audit producer namespace> -c <audit collector chart file> -o <secrets yaml file> where, <audit audit producer namespace> is the namespace in which the audit producer (for example, service management or ucmdb) is running. <audit collector chart file> is the tar file containing the audit collector installation charts (audit-collector-<version>.tgz). <secrets yaml file> is the yaml file that stores the audit collector secrets. specify the file name (for example, audit-collect",
    "keywordsLower": [
      "cert_two.crt",
      "idm.crt",
      "cert_one.crt",
      "ase.crt",
      "cacertificates.idm",
      "secrets.yaml",
      "values.yaml",
      "gen_secrets.sh",
      "install",
      "audit",
      "collector",
      "openshift",
      "generate",
      "vault",
      "secrets",
      "section",
      "provides",
      "steps",
      "required",
      "collector.",
      "configure",
      "namespaces",
      "contain",
      "producer",
      "services",
      "example",
      "service",
      "management",
      "ucmdb.",
      "requires",
      "file",
      "system",
      "pvc",
      "subpath.",
      "chart",
      "perform",
      "following",
      "bastion",
      "node",
      "run",
      "command",
      "scripts",
      "-n",
      "-c",
      "-o",
      "namespace",
      "ucmdb",
      "running.",
      "tar",
      "containing",
      "installation",
      "charts",
      "audit-collector-.tgz",
      "yaml",
      "stores",
      "secrets.",
      "specify",
      "name",
      "audit-collector-secrets.yaml",
      "script",
      "after",
      "script.",
      "need",
      "root",
      "cdf",
      "itsma-cfpni",
      "secret",
      "utility",
      "prompts",
      "integration",
      "user",
      "password",
      "complete",
      "deployment",
      "helm",
      "-f",
      "--set-file",
      ".crt",
      "path",
      "unique",
      "release",
      "deployment.",
      "same",
      "specified",
      "here",
      "upgrade",
      "future.",
      "charts.",
      "properties",
      "configured",
      "generates",
      "holds",
      "pass",
      "certificates",
      "option.",
      "see",
      "suite",
      "engine",
      "certificates.",
      "audit-collector"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install the suite using CLI",
    "content": "This document provides instructions for installing the suite using the command line. Before proceeding, make sure that you have created the my-values.yaml file. For details, see Create my-values.yaml.Prepare container registry secret (optional)If your deployment doesn't include OMT, you must generate a new container registry secret under the Suite namespace to allow to Kubernetes access the container registry.  To create a new container registry, run the following command:kubectl create secret docker-registry <image_secret_name> --docker-username=<username> --docker-password=<password> --docker-server=<registry_server> -n <suite namespace>Where:<image_secret_name> is the name of the secret. Take note of this name, you will need to include it in your my-values.yaml file.<username> is the host account username of the container registry.<password> is the host account password of the container registry. Enclose the password in single quotes. For example, 'mysecret'.<registry_server> is the",
    "url": "installesmcli",
    "filename": "installesmcli",
    "headings": [
      "Prepare container registry secret (optional)",
      "Deploy the suite"
    ],
    "keywords": [
      "securityContext.user",
      "values.yaml",
      "kubernetes.io",
      "install",
      "suite",
      "cli",
      "prepare",
      "container",
      "registry",
      "secret",
      "optional",
      "deploy",
      "document",
      "provides",
      "instructions",
      "installing",
      "command",
      "line.",
      "before",
      "proceeding",
      "make",
      "sure",
      "created",
      "my-values.yaml",
      "file.",
      "details",
      "see",
      "create",
      "my-values.yaml.prepare",
      "deployment",
      "doesn",
      "include",
      "omt",
      "generate",
      "new",
      "under",
      "namespace",
      "allow",
      "kubernetes",
      "access",
      "registry.",
      "run",
      "following",
      "kubectl",
      "docker-registry",
      "--docker-username",
      "--docker-password",
      "--docker-server",
      "-n",
      "name",
      "secret.",
      "take",
      "note",
      "need",
      "host",
      "account",
      "username",
      "password",
      "enclose",
      "single",
      "quotes.",
      "example",
      "mysecret",
      "url",
      "want",
      "pull",
      "images",
      "from.",
      "unique",
      "deployment.",
      "specified",
      "esm.deploy",
      "suiterolelocationsuite",
      "admincontrol",
      "plane",
      "bastion",
      "nodeto",
      "all",
      "flags",
      "apply",
      "vary",
      "based",
      "capabilities",
      "configurations",
      "integrations",
      "enabled",
      "environment.",
      "review",
      "conditions",
      "relevant",
      "flags.",
      "aviator",
      "capability",
      "cross-domain",
      "integration",
      "products",
      "such",
      "ai",
      "operations",
      "management"
    ],
    "language": "en",
    "word_count": 83,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install the suite using cli",
    "contentLower": "this document provides instructions for installing the suite using the command line. before proceeding, make sure that you have created the my-values.yaml file. for details, see create my-values.yaml.prepare container registry secret (optional)if your deployment doesn't include omt, you must generate a new container registry secret under the suite namespace to allow to kubernetes access the container registry.  to create a new container registry, run the following command:kubectl create secret docker-registry <image_secret_name> --docker-username=<username> --docker-password=<password> --docker-server=<registry_server> -n <suite namespace>where:<image_secret_name> is the name of the secret. take note of this name, you will need to include it in your my-values.yaml file.<username> is the host account username of the container registry.<password> is the host account password of the container registry. enclose the password in single quotes. for example, 'mysecret'.<registry_server> is the",
    "keywordsLower": [
      "securitycontext.user",
      "values.yaml",
      "kubernetes.io",
      "install",
      "suite",
      "cli",
      "prepare",
      "container",
      "registry",
      "secret",
      "optional",
      "deploy",
      "document",
      "provides",
      "instructions",
      "installing",
      "command",
      "line.",
      "before",
      "proceeding",
      "make",
      "sure",
      "created",
      "my-values.yaml",
      "file.",
      "details",
      "see",
      "create",
      "my-values.yaml.prepare",
      "deployment",
      "doesn",
      "include",
      "omt",
      "generate",
      "new",
      "under",
      "namespace",
      "allow",
      "kubernetes",
      "access",
      "registry.",
      "run",
      "following",
      "kubectl",
      "docker-registry",
      "--docker-username",
      "--docker-password",
      "--docker-server",
      "-n",
      "name",
      "secret.",
      "take",
      "note",
      "need",
      "host",
      "account",
      "username",
      "password",
      "enclose",
      "single",
      "quotes.",
      "example",
      "mysecret",
      "url",
      "want",
      "pull",
      "images",
      "from.",
      "unique",
      "deployment.",
      "specified",
      "esm.deploy",
      "suiterolelocationsuite",
      "admincontrol",
      "plane",
      "bastion",
      "nodeto",
      "all",
      "flags",
      "apply",
      "vary",
      "based",
      "capabilities",
      "configurations",
      "integrations",
      "enabled",
      "environment.",
      "review",
      "conditions",
      "relevant",
      "flags.",
      "aviator",
      "capability",
      "cross-domain",
      "integration",
      "products",
      "such",
      "ai",
      "operations",
      "management"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install the suite using AppHub",
    "content": "To install an application using OMT AppHub UI, you need to first upload the helm chart to AppHub. To use AppHub, you must already have OMT installed. Upload the application chart using AppHub To do this, follow these steps. Log in to AppHub Copy the installation portal URL to a supported browser. The URL uses the following format: https://<external_access_host>:<apphub-port>/apphub. For example: https://myhost.mycompany.com:5443/apphubLog in using the cluster administrator credentials that you provided when you ran the OMT ./install command: User name: adminPassword: Enter the password you provided during OMT installation. Click LOG IN. Upload a Helm chart You can only upload one application (two files) at a time. Otherwise, you'll get a warning and the upload will fail. If you try uploading a chart file without a provenance file, you will get a warning that this isn't recommended. Because AppHub won't be able to verify the integrity of the chart. It's recommended always uploading the ",
    "url": "installesmapphub",
    "filename": "installesmapphub",
    "headings": [
      "Upload the application chart using AppHub",
      "Log in to AppHub",
      "Upload a Helm chart",
      "Configure General Settings",
      "External Access Settings",
      "Docker Registry Settings",
      "Deployment size",
      "Time Zone",
      "Kubernetes Provider",
      "Configure Capabilities",
      "Configure external load balancer",
      "Configure Database",
      "External Database Settings",
      "Configure PostgreSQL Database",
      "Set Suite Database Owner Password",
      "Vertica Database",
      "Configure Vertica Database",
      "Configure Security",
      "Suite System Administrator(suite-admin)",
      "User ID and Group ID"
    ],
    "keywords": [
      "editor.Add",
      "dummy.com",
      "788232947507.dkr",
      "153722867.End",
      "kubernetes.io",
      "Upload.Wait",
      "https://myhost.mycompany.com:5443/apphubLog",
      "ecr.us",
      "mycompany.com",
      "amazonaws.com",
      "https://<external_access_host>:<apphub-port>/apphub",
      "tgz.prov",
      "config.json",
      "parameters.Copy",
      "install",
      "suite",
      "apphub",
      "upload",
      "application",
      "chart",
      "log",
      "helm",
      "configure",
      "general",
      "settings",
      "external",
      "access",
      "docker",
      "registry",
      "deployment",
      "size",
      "time",
      "zone",
      "kubernetes",
      "provider",
      "capabilities",
      "load",
      "balancer",
      "database",
      "postgresql",
      "set",
      "owner",
      "password",
      "vertica",
      "security",
      "system",
      "administrator",
      "suite-admin",
      "user",
      "id",
      "group",
      "deploy",
      "openshift",
      "edit",
      "namespace",
      "omt",
      "ui",
      "need",
      "first",
      "apphub.",
      "already",
      "installed.",
      "follow",
      "steps.",
      "copy",
      "installation",
      "portal",
      "url",
      "supported",
      "browser.",
      "uses",
      "following",
      "format",
      "https",
      "example",
      "myhost.mycompany.com",
      "5443",
      "apphublog",
      "cluster",
      "credentials",
      "provided",
      "ran",
      "command",
      "name",
      "adminpassword",
      "enter",
      "during",
      "installation.",
      "click",
      "in.",
      "one",
      "two",
      "files",
      "time.",
      "otherwise",
      "ll",
      "get",
      "warning",
      "fail.",
      "try"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install the suite using apphub",
    "contentLower": "to install an application using omt apphub ui, you need to first upload the helm chart to apphub. to use apphub, you must already have omt installed. upload the application chart using apphub to do this, follow these steps. log in to apphub copy the installation portal url to a supported browser. the url uses the following format: https://<external_access_host>:<apphub-port>/apphub. for example: https://myhost.mycompany.com:5443/apphublog in using the cluster administrator credentials that you provided when you ran the omt ./install command: user name: adminpassword: enter the password you provided during omt installation. click log in. upload a helm chart you can only upload one application (two files) at a time. otherwise, you'll get a warning and the upload will fail. if you try uploading a chart file without a provenance file, you will get a warning that this isn't recommended. because apphub won't be able to verify the integrity of the chart. it's recommended always uploading the ",
    "keywordsLower": [
      "editor.add",
      "dummy.com",
      "788232947507.dkr",
      "153722867.end",
      "kubernetes.io",
      "upload.wait",
      "https://myhost.mycompany.com:5443/apphublog",
      "ecr.us",
      "mycompany.com",
      "amazonaws.com",
      "https://<external_access_host>:<apphub-port>/apphub",
      "tgz.prov",
      "config.json",
      "parameters.copy",
      "install",
      "suite",
      "apphub",
      "upload",
      "application",
      "chart",
      "log",
      "helm",
      "configure",
      "general",
      "settings",
      "external",
      "access",
      "docker",
      "registry",
      "deployment",
      "size",
      "time",
      "zone",
      "kubernetes",
      "provider",
      "capabilities",
      "load",
      "balancer",
      "database",
      "postgresql",
      "set",
      "owner",
      "password",
      "vertica",
      "security",
      "system",
      "administrator",
      "suite-admin",
      "user",
      "id",
      "group",
      "deploy",
      "openshift",
      "edit",
      "namespace",
      "omt",
      "ui",
      "need",
      "first",
      "apphub.",
      "already",
      "installed.",
      "follow",
      "steps.",
      "copy",
      "installation",
      "portal",
      "url",
      "supported",
      "browser.",
      "uses",
      "following",
      "format",
      "https",
      "example",
      "myhost.mycompany.com",
      "5443",
      "apphublog",
      "cluster",
      "credentials",
      "provided",
      "ran",
      "command",
      "name",
      "adminpassword",
      "enter",
      "during",
      "installation.",
      "click",
      "in.",
      "one",
      "two",
      "files",
      "time.",
      "otherwise",
      "ll",
      "get",
      "warning",
      "fail.",
      "try"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install add-on capabilities (Helm)",
    "content": "The suite offers a Change feature that enables you to configure more capabilities to an already installed suite instance. These capabilities are referred to as add-ons, and they can be installed during or after the suite installation. You can install these add-ons using either OMT AppHub or command line interface, CLI. The following table lists the add-on capabilities and their supported installation methods. Capability Description Notes Supported installation method FinOps FinOps enables you to optimize your cloud spending and simplify configuration with various Cloud providers. With FinOps, you can review your public cloud costs to get a clearer understanding of your activity and usage of public cloud resources. Optional for Service Management customersNot applicable to SMA-SM customers AppHub and Command line interface Cloud Carbon Footprint (Carbon) The Carbon Footprint capability is deprecated from version 26.1. Carbon aims to provide insights into your IT estate, enabling you to ",
    "url": "addcapabilitieshelm",
    "filename": "addcapabilitieshelm",
    "headings": [
      "Add hardware resources",
      "Prepare external databases if needed",
      "Prepare a PostgreSQL database server",
      "Configure the pg_hba.conf file",
      "Prepare a Vertica database (for FinOps only)",
      "Download and upload images",
      "Install new capabilities",
      "Install new capabilities using AppHub UI",
      "Install new capabilities using CLI",
      "Get your my-values.yaml file",
      "Add add-on capabilities parameters to my-values.yaml",
      "Perform add-on capability installation",
      "Configure the newly added capabilities",
      "FinOps",
      "Carbon",
      "Hybrid Cloud Management",
      "Software Asset Management (SAM)",
      "Virtual Agent(VA)",
      "Related topics"
    ],
    "keywords": [
      "https://<external_access_host>:<apphub-port>/apphub.Go",
      "services.cgro",
      "pg_hba.conf",
      "sha-256",
      "172.0.0.0",
      "vertica.host",
      "services.va",
      "services.sam",
      "REDEPLOY.Once",
      "auditCollector.idm",
      "values.yaml",
      "services.cmp",
      "https://<Audit",
      "automatically.In",
      "services.dnd",
      "vertica.port",
      "172.0.0",
      "26.1",
      "apphub.Go",
      "auditengine.url",
      "install",
      "add-on",
      "capabilities",
      "helm",
      "add",
      "hardware",
      "resources",
      "prepare",
      "external",
      "databases",
      "needed",
      "postgresql",
      "database",
      "server",
      "configure",
      "file",
      "vertica",
      "finops",
      "download",
      "upload",
      "images",
      "new",
      "apphub",
      "ui",
      "cli",
      "get",
      "my-values.yaml",
      "parameters",
      "perform",
      "capability",
      "installation",
      "newly",
      "added",
      "carbon",
      "hybrid",
      "cloud",
      "management",
      "software",
      "asset",
      "sam",
      "virtual",
      "agent",
      "va",
      "related",
      "topics",
      "suite",
      "offers",
      "change",
      "feature",
      "enables",
      "already",
      "installed",
      "instance.",
      "referred",
      "add-ons",
      "during",
      "after",
      "installation.",
      "either",
      "omt",
      "command",
      "line",
      "interface",
      "cli.",
      "following",
      "table",
      "lists",
      "supported",
      "methods.",
      "description",
      "notes",
      "method",
      "optimize",
      "spending",
      "simplify",
      "configuration",
      "various",
      "providers.",
      "review",
      "public"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install add-on capabilities (helm)",
    "contentLower": "the suite offers a change feature that enables you to configure more capabilities to an already installed suite instance. these capabilities are referred to as add-ons, and they can be installed during or after the suite installation. you can install these add-ons using either omt apphub or command line interface, cli. the following table lists the add-on capabilities and their supported installation methods. capability description notes supported installation method finops finops enables you to optimize your cloud spending and simplify configuration with various cloud providers. with finops, you can review your public cloud costs to get a clearer understanding of your activity and usage of public cloud resources. optional for service management customersnot applicable to sma-sm customers apphub and command line interface cloud carbon footprint (carbon) the carbon footprint capability is deprecated from version 26.1. carbon aims to provide insights into your it estate, enabling you to ",
    "keywordsLower": [
      "https://<external_access_host>:<apphub-port>/apphub.go",
      "services.cgro",
      "pg_hba.conf",
      "sha-256",
      "172.0.0.0",
      "vertica.host",
      "services.va",
      "services.sam",
      "redeploy.once",
      "auditcollector.idm",
      "values.yaml",
      "services.cmp",
      "https://<audit",
      "automatically.in",
      "services.dnd",
      "vertica.port",
      "172.0.0",
      "26.1",
      "apphub.go",
      "auditengine.url",
      "install",
      "add-on",
      "capabilities",
      "helm",
      "add",
      "hardware",
      "resources",
      "prepare",
      "external",
      "databases",
      "needed",
      "postgresql",
      "database",
      "server",
      "configure",
      "file",
      "vertica",
      "finops",
      "download",
      "upload",
      "images",
      "new",
      "apphub",
      "ui",
      "cli",
      "get",
      "my-values.yaml",
      "parameters",
      "perform",
      "capability",
      "installation",
      "newly",
      "added",
      "carbon",
      "hybrid",
      "cloud",
      "management",
      "software",
      "asset",
      "sam",
      "virtual",
      "agent",
      "va",
      "related",
      "topics",
      "suite",
      "offers",
      "change",
      "feature",
      "enables",
      "already",
      "installed",
      "instance.",
      "referred",
      "add-ons",
      "during",
      "after",
      "installation.",
      "either",
      "omt",
      "command",
      "line",
      "interface",
      "cli.",
      "following",
      "table",
      "lists",
      "supported",
      "methods.",
      "description",
      "notes",
      "method",
      "optimize",
      "spending",
      "simplify",
      "configuration",
      "various",
      "providers.",
      "review",
      "public"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Containerized using AppHub",
    "content": "This section provides information about the steps involved in installing OO Containerized using OPTIC AppHub. Decide the Kubernetes strategy You will use Kubernetes to orchestrate the containerized application deployment. You can choose the Kubernetes distribution based on your IT strategy and infrastructure requirements. You can choose to deploy on an external Kubernetes or an embedded Kubernetes. External Kubernetes: For a list of qualified external Kubernetes distributions, see Sizing and system requirements.Embedded Kubernetes: Kubernetes which is part of the OPTIC Management Toolkit (OMT). For more information about embedded Kubernetes, see Deployment architecture in OMT documentation. Decide the high availability requirement High availability (HA) of all containerized components requires additional hardware for the following: Redundant storageDatabasesKubernetes cluster with multiple control plane nodesApplication services on multiple worker nodes that support active-active setup",
    "url": "installoocapphub",
    "filename": "installoocapphub",
    "headings": [
      "Decide the Kubernetes strategy",
      "Decide the high availability requirement",
      "Install",
      "Related topics"
    ],
    "keywords": [
      "install",
      "oo",
      "containerized",
      "apphub",
      "decide",
      "kubernetes",
      "strategy",
      "high",
      "availability",
      "requirement",
      "related",
      "topics",
      "section",
      "provides",
      "information",
      "about",
      "steps",
      "involved",
      "installing",
      "optic",
      "apphub.",
      "orchestrate",
      "application",
      "deployment.",
      "choose",
      "distribution",
      "based",
      "infrastructure",
      "requirements.",
      "deploy",
      "external",
      "embedded",
      "kubernetes.",
      "list",
      "qualified",
      "distributions",
      "see",
      "sizing",
      "system",
      "requirements.embedded",
      "part",
      "management",
      "toolkit",
      "omt",
      "deployment",
      "architecture",
      "documentation.",
      "ha",
      "all",
      "components",
      "requires",
      "additional",
      "hardware",
      "following",
      "redundant",
      "storagedatabaseskubernetes",
      "cluster",
      "multiple",
      "control",
      "plane",
      "nodesapplication",
      "services",
      "worker",
      "nodes",
      "support",
      "active-active",
      "setups",
      "achieve",
      "lowest",
      "failover",
      "time",
      "highly",
      "available",
      "relational",
      "database",
      "nfs",
      "servermultiple",
      "nodesmultiple",
      "set",
      "prerequisites",
      "once",
      "complete",
      "prerequisite",
      "tasks",
      "before",
      "deploying",
      "application.",
      "differ",
      "scenario",
      "requirements",
      "capabilities.",
      "download",
      "installer",
      "information.",
      "task",
      "input",
      "output",
      "configure",
      "parameters",
      "through"
    ],
    "language": "en",
    "word_count": 91,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo containerized using apphub",
    "contentLower": "this section provides information about the steps involved in installing oo containerized using optic apphub. decide the kubernetes strategy you will use kubernetes to orchestrate the containerized application deployment. you can choose the kubernetes distribution based on your it strategy and infrastructure requirements. you can choose to deploy on an external kubernetes or an embedded kubernetes. external kubernetes: for a list of qualified external kubernetes distributions, see sizing and system requirements.embedded kubernetes: kubernetes which is part of the optic management toolkit (omt). for more information about embedded kubernetes, see deployment architecture in omt documentation. decide the high availability requirement high availability (ha) of all containerized components requires additional hardware for the following: redundant storagedatabaseskubernetes cluster with multiple control plane nodesapplication services on multiple worker nodes that support active-active setup",
    "keywordsLower": [
      "install",
      "oo",
      "containerized",
      "apphub",
      "decide",
      "kubernetes",
      "strategy",
      "high",
      "availability",
      "requirement",
      "related",
      "topics",
      "section",
      "provides",
      "information",
      "about",
      "steps",
      "involved",
      "installing",
      "optic",
      "apphub.",
      "orchestrate",
      "application",
      "deployment.",
      "choose",
      "distribution",
      "based",
      "infrastructure",
      "requirements.",
      "deploy",
      "external",
      "embedded",
      "kubernetes.",
      "list",
      "qualified",
      "distributions",
      "see",
      "sizing",
      "system",
      "requirements.embedded",
      "part",
      "management",
      "toolkit",
      "omt",
      "deployment",
      "architecture",
      "documentation.",
      "ha",
      "all",
      "components",
      "requires",
      "additional",
      "hardware",
      "following",
      "redundant",
      "storagedatabaseskubernetes",
      "cluster",
      "multiple",
      "control",
      "plane",
      "nodesapplication",
      "services",
      "worker",
      "nodes",
      "support",
      "active-active",
      "setups",
      "achieve",
      "lowest",
      "failover",
      "time",
      "highly",
      "available",
      "relational",
      "database",
      "nfs",
      "servermultiple",
      "nodesmultiple",
      "set",
      "prerequisites",
      "once",
      "complete",
      "prerequisite",
      "tasks",
      "before",
      "deploying",
      "application.",
      "differ",
      "scenario",
      "requirements",
      "capabilities.",
      "download",
      "installer",
      "information.",
      "task",
      "input",
      "output",
      "configure",
      "parameters",
      "through"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install additional OO components",
    "content": "Operations Orchestration Containerized (OO Containerized) supports the following components: OO External RAS: OO RAS enables execution in remote data centers and networks. The RAS interacts with Central and polls it for operations to execute. Installation of external OO RAS is optional and depends on whether you need additional RASes apart from what's already available when you installed OO Central. OO Workflow Designer: OO Workflow Designer is a web-based environment for authoring flows, which is available as a standalone product deployed on a Windows or Linux server. Installation of OO Workflow Designer is optional and depends on your requirement to author flows. You need to install the OO Workflow Designer as it's not installed along with OO Central. OO Studio: OO Studio is a desktop based application for authoring flows, which is available as a standalone product deployed on a Windows server. Instalation of OO Studio is optional and depends on your requirement to author flows. You ",
    "url": "installooexternalcomponents",
    "filename": "installooexternalcomponents",
    "headings": [
      "Prerequisites",
      "Download installation files from the Service Management UI",
      "Install OO RAS, Workflow Designer and Studio"
    ],
    "keywords": [
      "install",
      "additional",
      "oo",
      "components",
      "prerequisites",
      "download",
      "installation",
      "files",
      "service",
      "management",
      "ui",
      "ras",
      "workflow",
      "designer",
      "studio",
      "operations",
      "orchestration",
      "containerized",
      "supports",
      "following",
      "external",
      "enables",
      "execution",
      "remote",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "optional",
      "depends",
      "whether",
      "need",
      "rases",
      "apart",
      "what",
      "already",
      "available",
      "installed",
      "central.",
      "web-based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deployed",
      "windows",
      "linux",
      "server.",
      "requirement",
      "author",
      "flows.",
      "along",
      "desktop",
      "based",
      "application",
      "instalation",
      "installed.",
      "separate",
      "installers",
      "on-premises",
      "only.",
      "ensure",
      "enough",
      "entropy",
      "os.",
      "check",
      "set",
      "levels",
      "see",
      "start-up",
      "stuck",
      "due",
      "insufficient",
      "pool.",
      "run",
      "script",
      "suite",
      "admin",
      "opb",
      "center",
      "displays",
      "direct",
      "links",
      "designer.",
      "log",
      "agent",
      "interface",
      "tenant",
      "admin.",
      "go",
      "click",
      "links.",
      "details",
      "enable",
      "ui.",
      "information"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install additional oo components",
    "contentLower": "operations orchestration containerized (oo containerized) supports the following components: oo external ras: oo ras enables execution in remote data centers and networks. the ras interacts with central and polls it for operations to execute. installation of external oo ras is optional and depends on whether you need additional rases apart from what's already available when you installed oo central. oo workflow designer: oo workflow designer is a web-based environment for authoring flows, which is available as a standalone product deployed on a windows or linux server. installation of oo workflow designer is optional and depends on your requirement to author flows. you need to install the oo workflow designer as it's not installed along with oo central. oo studio: oo studio is a desktop based application for authoring flows, which is available as a standalone product deployed on a windows server. instalation of oo studio is optional and depends on your requirement to author flows. you ",
    "keywordsLower": [
      "install",
      "additional",
      "oo",
      "components",
      "prerequisites",
      "download",
      "installation",
      "files",
      "service",
      "management",
      "ui",
      "ras",
      "workflow",
      "designer",
      "studio",
      "operations",
      "orchestration",
      "containerized",
      "supports",
      "following",
      "external",
      "enables",
      "execution",
      "remote",
      "data",
      "centers",
      "networks.",
      "interacts",
      "central",
      "polls",
      "execute.",
      "optional",
      "depends",
      "whether",
      "need",
      "rases",
      "apart",
      "what",
      "already",
      "available",
      "installed",
      "central.",
      "web-based",
      "environment",
      "authoring",
      "flows",
      "standalone",
      "product",
      "deployed",
      "windows",
      "linux",
      "server.",
      "requirement",
      "author",
      "flows.",
      "along",
      "desktop",
      "based",
      "application",
      "instalation",
      "installed.",
      "separate",
      "installers",
      "on-premises",
      "only.",
      "ensure",
      "enough",
      "entropy",
      "os.",
      "check",
      "set",
      "levels",
      "see",
      "start-up",
      "stuck",
      "due",
      "insufficient",
      "pool.",
      "run",
      "script",
      "suite",
      "admin",
      "opb",
      "center",
      "displays",
      "direct",
      "links",
      "designer.",
      "log",
      "agent",
      "interface",
      "tenant",
      "admin.",
      "go",
      "click",
      "links.",
      "details",
      "enable",
      "ui.",
      "information"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO RAS",
    "content": "This topic contains the system requirements and the instructions to install an External Operations Orchestration (OO) Remote Action Server (RAS). You can install the software using the installer or silent installation property file. Topic Description System requirements for OO RAS Prepare the environment according to the OO RAS System Requirements. Install OO RAS on-premises Install OO RAS. Uninstall OO RAS Uninstall OO RAS.",
    "url": "installooras",
    "filename": "installooras",
    "headings": [],
    "keywords": [
      "install",
      "oo",
      "ras",
      "topic",
      "contains",
      "system",
      "requirements",
      "instructions",
      "external",
      "operations",
      "orchestration",
      "remote",
      "action",
      "server",
      "software",
      "installer",
      "silent",
      "installation",
      "property",
      "file.",
      "description",
      "prepare",
      "environment",
      "according",
      "requirements.",
      "on-premises",
      "ras.",
      "uninstall"
    ],
    "language": "en",
    "word_count": 50,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo ras",
    "contentLower": "this topic contains the system requirements and the instructions to install an external operations orchestration (oo) remote action server (ras). you can install the software using the installer or silent installation property file. topic description system requirements for oo ras prepare the environment according to the oo ras system requirements. install oo ras on-premises install oo ras. uninstall oo ras uninstall oo ras.",
    "keywordsLower": [
      "install",
      "oo",
      "ras",
      "topic",
      "contains",
      "system",
      "requirements",
      "instructions",
      "external",
      "operations",
      "orchestration",
      "remote",
      "action",
      "server",
      "software",
      "installer",
      "silent",
      "installation",
      "property",
      "file.",
      "description",
      "prepare",
      "environment",
      "according",
      "requirements.",
      "on-premises",
      "ras.",
      "uninstall"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO RAS on-premises",
    "content": "Installing an external Operations Orchestration Remote Action Server (OO RAS) is optional. You may consider installing an external OO RAS for high availability. Prerequisites Before you begin, make sure you: Download and extract the installation package (installer-ras-<OS>-<latest_version>.zip) onto your local file system. Contact your administrator for the installation package, or download the installation package from the user interface of the product. Have an IdM user with a linked oo_topologyManage IdM permission. In IdM, the Permission/Role/Group/User association is as follows: IdM Permissions are linked to an IdM role. An IdM Role is associated with an IdM group, a group may have multiple roles associated with it. An IdM Group is a collection of users. If you are installing OO RAS by connecting it to a SaaS OO, then contact the SaaS Support group to prepare the association with your username. If you are installing OO RAS by connecting it to your existing OO installation, then you",
    "url": "399-installoorasonpremises",
    "filename": "399-installoorasonpremises",
    "headings": [
      "Prerequisites",
      "Install OO RAS on Windows",
      "Silent install OO RAS on Windows",
      "Silent install OO RAS on Linux",
      "Import a certificate into the RAS TrustStore"
    ],
    "keywords": [
      "setup.exe",
      "https://my.central.com:8443/oo/?tenantId=<tenantId",
      "MT.bin",
      "X.509",
      "http://proxyserver.com",
      "proxyserver.com",
      "installer.exe",
      "central.com",
      "wrapper.conf",
      "xxxx.xx",
      "wrapper.java",
      "install",
      "oo",
      "ras",
      "on-premises",
      "prerequisites",
      "windows",
      "silent",
      "linux",
      "import",
      "certificate",
      "truststore",
      "installing",
      "external",
      "operations",
      "orchestration",
      "remote",
      "action",
      "server",
      "optional.",
      "consider",
      "high",
      "availability.",
      "before",
      "begin",
      "make",
      "sure",
      "download",
      "extract",
      "installation",
      "package",
      "installer-ras--.zip",
      "onto",
      "local",
      "file",
      "system.",
      "contact",
      "administrator",
      "user",
      "interface",
      "product.",
      "idm",
      "linked",
      "permission.",
      "permission",
      "role",
      "group",
      "association",
      "follows",
      "permissions",
      "role.",
      "associated",
      "multiple",
      "roles",
      "it.",
      "collection",
      "users.",
      "connecting",
      "saas",
      "support",
      "prepare",
      "username.",
      "existing",
      "check",
      "admin",
      "console",
      "username",
      "ie",
      "tenant",
      "required",
      "know",
      "id",
      "requires",
      "id.",
      "installer-ras-win-.zip",
      "file.",
      "right-click",
      "extracted",
      "select",
      "run",
      "administrator.",
      "configuration",
      "wizard",
      "opens.",
      "click",
      "next.",
      "license",
      "screen",
      "agree",
      "next"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo ras on-premises",
    "contentLower": "installing an external operations orchestration remote action server (oo ras) is optional. you may consider installing an external oo ras for high availability. prerequisites before you begin, make sure you: download and extract the installation package (installer-ras-<os>-<latest_version>.zip) onto your local file system. contact your administrator for the installation package, or download the installation package from the user interface of the product. have an idm user with a linked oo_topologymanage idm permission. in idm, the permission/role/group/user association is as follows: idm permissions are linked to an idm role. an idm role is associated with an idm group, a group may have multiple roles associated with it. an idm group is a collection of users. if you are installing oo ras by connecting it to a saas oo, then contact the saas support group to prepare the association with your username. if you are installing oo ras by connecting it to your existing oo installation, then you",
    "keywordsLower": [
      "setup.exe",
      "https://my.central.com:8443/oo/?tenantid=<tenantid",
      "mt.bin",
      "x.509",
      "http://proxyserver.com",
      "proxyserver.com",
      "installer.exe",
      "central.com",
      "wrapper.conf",
      "xxxx.xx",
      "wrapper.java",
      "install",
      "oo",
      "ras",
      "on-premises",
      "prerequisites",
      "windows",
      "silent",
      "linux",
      "import",
      "certificate",
      "truststore",
      "installing",
      "external",
      "operations",
      "orchestration",
      "remote",
      "action",
      "server",
      "optional.",
      "consider",
      "high",
      "availability.",
      "before",
      "begin",
      "make",
      "sure",
      "download",
      "extract",
      "installation",
      "package",
      "installer-ras--.zip",
      "onto",
      "local",
      "file",
      "system.",
      "contact",
      "administrator",
      "user",
      "interface",
      "product.",
      "idm",
      "linked",
      "permission.",
      "permission",
      "role",
      "group",
      "association",
      "follows",
      "permissions",
      "role.",
      "associated",
      "multiple",
      "roles",
      "it.",
      "collection",
      "users.",
      "connecting",
      "saas",
      "support",
      "prepare",
      "username.",
      "existing",
      "check",
      "admin",
      "console",
      "username",
      "ie",
      "tenant",
      "required",
      "know",
      "id",
      "requires",
      "id.",
      "installer-ras-win-.zip",
      "file.",
      "right-click",
      "extracted",
      "select",
      "run",
      "administrator.",
      "configuration",
      "wizard",
      "opens.",
      "click",
      "next.",
      "license",
      "screen",
      "agree",
      "next"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Workflow Designer",
    "content": "This section provides the information required to install the following Operations Orchestration (OO) Workflow Designer. You can install this using the installer or silent installation property file.",
    "url": "ooworkflowdesigner",
    "filename": "ooworkflowdesigner",
    "headings": [],
    "keywords": [
      "install",
      "oo",
      "workflow",
      "designer",
      "section",
      "provides",
      "information",
      "required",
      "following",
      "operations",
      "orchestration",
      "designer.",
      "installer",
      "silent",
      "installation",
      "property",
      "file."
    ],
    "language": "en",
    "word_count": 21,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo workflow designer",
    "contentLower": "this section provides the information required to install the following operations orchestration (oo) workflow designer. you can install this using the installer or silent installation property file.",
    "keywordsLower": [
      "install",
      "oo",
      "workflow",
      "designer",
      "section",
      "provides",
      "information",
      "required",
      "following",
      "operations",
      "orchestration",
      "designer.",
      "installer",
      "silent",
      "installation",
      "property",
      "file."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Installation prerequisites for OO Workflow Designer",
    "content": "This topic provides information on how to set up the required environment before installing OO Workflow Designer. Before installing OO Workflow Designer, you must complete the following required activities: Back up OO Workflow Designer database If you had installed OO Workflow Designer before, then back up all the files under the installation folder and delete that folder before installing the new version. See the topics listed under the related topics section. Download and install drivers Oracle drivers The Oracle JDBC drivers are only required if you are using an Oracle database. Download and install Oracle JDBC driver from Oracle website and replace it in all locations in the installation folder. The recommended Oracle JDBC driver version is ojdbc8-12.1.0.2 to ojdbc8-19.3.0.0. Verify the integrity of the OO Workflow Designer installation package To verify the integrity of the installation zip and the embedded digital signature certificates: Download the latest JDK from Oracle site. ",
    "url": "installprerequisitesoowfd",
    "filename": "installprerequisitesoowfd",
    "headings": [
      "Back up OO Workflow Designer database",
      "Download and install drivers",
      "Oracle drivers",
      "Verify the integrity of the OO Workflow Designer installation package"
    ],
    "keywords": [
      "12.1.0",
      "12.1.0.2",
      "19.3.0.0",
      "19.3.0",
      "installation",
      "prerequisites",
      "oo",
      "workflow",
      "designer",
      "back",
      "database",
      "download",
      "install",
      "drivers",
      "oracle",
      "verify",
      "integrity",
      "package",
      "topic",
      "provides",
      "information",
      "set",
      "required",
      "environment",
      "before",
      "installing",
      "designer.",
      "complete",
      "following",
      "activities",
      "installed",
      "all",
      "files",
      "under",
      "folder",
      "delete",
      "new",
      "version.",
      "see",
      "topics",
      "listed",
      "related",
      "section.",
      "jdbc",
      "database.",
      "driver",
      "website",
      "replace",
      "locations",
      "folder.",
      "recommended",
      "version",
      "ojdbc8-12.1.0.2",
      "ojdbc8-19.3.0.0.",
      "zip",
      "embedded",
      "digital",
      "signature",
      "certificates",
      "latest",
      "jdk",
      "site.",
      "suitable",
      "location",
      "local",
      "file",
      "system",
      "locate",
      "jarsigner",
      "utility",
      "bin",
      "run",
      "command",
      "line",
      "-verify",
      "-verbose",
      "file.",
      "output",
      "return",
      "messages",
      "jar",
      "verified",
      "safe",
      "secure.",
      "unsigned",
      "isn",
      "safe."
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "installation prerequisites for oo workflow designer",
    "contentLower": "this topic provides information on how to set up the required environment before installing oo workflow designer. before installing oo workflow designer, you must complete the following required activities: back up oo workflow designer database if you had installed oo workflow designer before, then back up all the files under the installation folder and delete that folder before installing the new version. see the topics listed under the related topics section. download and install drivers oracle drivers the oracle jdbc drivers are only required if you are using an oracle database. download and install oracle jdbc driver from oracle website and replace it in all locations in the installation folder. the recommended oracle jdbc driver version is ojdbc8-12.1.0.2 to ojdbc8-19.3.0.0. verify the integrity of the oo workflow designer installation package to verify the integrity of the installation zip and the embedded digital signature certificates: download the latest jdk from oracle site. ",
    "keywordsLower": [
      "12.1.0",
      "12.1.0.2",
      "19.3.0.0",
      "19.3.0",
      "installation",
      "prerequisites",
      "oo",
      "workflow",
      "designer",
      "back",
      "database",
      "download",
      "install",
      "drivers",
      "oracle",
      "verify",
      "integrity",
      "package",
      "topic",
      "provides",
      "information",
      "set",
      "required",
      "environment",
      "before",
      "installing",
      "designer.",
      "complete",
      "following",
      "activities",
      "installed",
      "all",
      "files",
      "under",
      "folder",
      "delete",
      "new",
      "version.",
      "see",
      "topics",
      "listed",
      "related",
      "section.",
      "jdbc",
      "database.",
      "driver",
      "website",
      "replace",
      "locations",
      "folder.",
      "recommended",
      "version",
      "ojdbc8-12.1.0.2",
      "ojdbc8-19.3.0.0.",
      "zip",
      "embedded",
      "digital",
      "signature",
      "certificates",
      "latest",
      "jdk",
      "site.",
      "suitable",
      "location",
      "local",
      "file",
      "system",
      "locate",
      "jarsigner",
      "utility",
      "bin",
      "run",
      "command",
      "line",
      "-verify",
      "-verbose",
      "file.",
      "output",
      "return",
      "messages",
      "jar",
      "verified",
      "safe",
      "secure.",
      "unsigned",
      "isn",
      "safe."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Installation wizard Other database option in OO Workflow Designer",
    "content": "This topic contains information about the Other database option in the OO Workflow Designer installation wizard. This option enables using specific JDBC driver and connection options. Use this option if: You want to use a different version of the JDBC driver, other than the ones provided in the OO Workflow Designer installation (see the note below). You want to offer a JDBC connection URL yourself, to include an option or options which aren't currently offered by the standard database connection options. Connection is restricted to the versions of database described in System requirements topic. Use a non provided JDBC driver at your own risk. The following JDBC drivers are supported: mssql-jdbc-7.4.1.jre8.jar–for SQL Server 2016 and above ojdbc8-19.3.0.0.jar postgresql-42.2.14.jar When you install OO Workflow Designer using the Other database option, the installation wizard doesn't create the database and related user or roles. You must create these beforehand. You can find detailed i",
    "url": "otherdbdesigneroowfd",
    "filename": "otherdbdesigneroowfd",
    "headings": [
      "Microsoft SQL server named instance example",
      "Oracle examples–with SID, Service Name and Oracle RAC",
      "PostgreSQL example"
    ],
    "keywords": [
      "0.jar",
      "RAC1.MY",
      "sqlserver://<DB_IP_OR_HOSTNAME>:<INSTANCE_",
      "1.jre8",
      "sqlserver://<DB_IP_OR_HOSTNAME>;instanceName=<INSTANCE_NAME>;databaseName=<DB_NAME>;sendStringParametersAsUnicode=true",
      "42.2.14",
      "oracle.jdbc",
      "19.3.0.0",
      "sqlserver.jdbc",
      "7.4.1",
      "postgresql://<HOST>:<PORT>/<DB_NAME>[?[propertyName=propertyValue",
      "2.14",
      "19.3.0",
      "installation",
      "wizard",
      "database",
      "option",
      "oo",
      "workflow",
      "designer",
      "microsoft",
      "sql",
      "server",
      "named",
      "instance",
      "example",
      "oracle",
      "examples",
      "sid",
      "service",
      "name",
      "rac",
      "postgresql",
      "topic",
      "contains",
      "information",
      "about",
      "wizard.",
      "enables",
      "specific",
      "jdbc",
      "driver",
      "connection",
      "options.",
      "want",
      "different",
      "version",
      "ones",
      "provided",
      "see",
      "note",
      "below",
      "offer",
      "url",
      "yourself",
      "include",
      "options",
      "aren",
      "currently",
      "offered",
      "standard",
      "restricted",
      "versions",
      "described",
      "system",
      "requirements",
      "topic.",
      "non",
      "own",
      "risk.",
      "following",
      "drivers",
      "supported",
      "mssql-jdbc-7.4.1.jre8.jar",
      "2016",
      "above",
      "ojdbc8-19.3.0.0.jar",
      "postgresql-42.2.14.jar",
      "install",
      "doesn",
      "create",
      "related",
      "user",
      "roles.",
      "beforehand.",
      "find",
      "detailed",
      "instructions",
      "role",
      "under",
      "manually",
      "creating",
      "section",
      "give",
      "complete",
      "database.",
      "idm",
      "scenario",
      "enter",
      "credentials."
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "installation wizard other database option in oo workflow designer",
    "contentLower": "this topic contains information about the other database option in the oo workflow designer installation wizard. this option enables using specific jdbc driver and connection options. use this option if: you want to use a different version of the jdbc driver, other than the ones provided in the oo workflow designer installation (see the note below). you want to offer a jdbc connection url yourself, to include an option or options which aren't currently offered by the standard database connection options. connection is restricted to the versions of database described in system requirements topic. use a non provided jdbc driver at your own risk. the following jdbc drivers are supported: mssql-jdbc-7.4.1.jre8.jar–for sql server 2016 and above ojdbc8-19.3.0.0.jar postgresql-42.2.14.jar when you install oo workflow designer using the other database option, the installation wizard doesn't create the database and related user or roles. you must create these beforehand. you can find detailed i",
    "keywordsLower": [
      "0.jar",
      "rac1.my",
      "sqlserver://<db_ip_or_hostname>:<instance_",
      "1.jre8",
      "sqlserver://<db_ip_or_hostname>;instancename=<instance_name>;databasename=<db_name>;sendstringparametersasunicode=true",
      "42.2.14",
      "oracle.jdbc",
      "19.3.0.0",
      "sqlserver.jdbc",
      "7.4.1",
      "postgresql://<host>:<port>/<db_name>[?[propertyname=propertyvalue",
      "2.14",
      "19.3.0",
      "installation",
      "wizard",
      "database",
      "option",
      "oo",
      "workflow",
      "designer",
      "microsoft",
      "sql",
      "server",
      "named",
      "instance",
      "example",
      "oracle",
      "examples",
      "sid",
      "service",
      "name",
      "rac",
      "postgresql",
      "topic",
      "contains",
      "information",
      "about",
      "wizard.",
      "enables",
      "specific",
      "jdbc",
      "driver",
      "connection",
      "options.",
      "want",
      "different",
      "version",
      "ones",
      "provided",
      "see",
      "note",
      "below",
      "offer",
      "url",
      "yourself",
      "include",
      "options",
      "aren",
      "currently",
      "offered",
      "standard",
      "restricted",
      "versions",
      "described",
      "system",
      "requirements",
      "topic.",
      "non",
      "own",
      "risk.",
      "following",
      "drivers",
      "supported",
      "mssql-jdbc-7.4.1.jre8.jar",
      "2016",
      "above",
      "ojdbc8-19.3.0.0.jar",
      "postgresql-42.2.14.jar",
      "install",
      "doesn",
      "create",
      "related",
      "user",
      "roles.",
      "beforehand.",
      "find",
      "detailed",
      "instructions",
      "role",
      "under",
      "manually",
      "creating",
      "section",
      "give",
      "complete",
      "database.",
      "idm",
      "scenario",
      "enter",
      "credentials."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Workflow Designer",
    "content": "This topic contains instructions on how to install an OO Workflow Designer on Windows and Linux environments. The installation of an OO Workflow Designer is optional. Prerequisites Before you begin, make sure you have downloaded and extracted the installation package (installer-designer-<OS>-<latest_version>.zip) onto your local file system. Contact your administrator for the installation package, orDownload the installation package from the product user interface. Installation instructions To start the installer: On Windows: Right-click the installer-oo-designer-win64-<version>-MT.exe installation file and run it as an administrator. On Linux: Run the following command from a Linux terminal window: bash installer-oo-designer-linux-<version>-MT.binAfter you start the installer, the installation package extracts, and the OpenText Operations Orchestration Designer Installation Wizard automatically opens. On the License page, select I Agree and click Next. On the Location page, select the",
    "url": "deployoowfdclean",
    "filename": "deployoowfdclean",
    "headings": [
      "Prerequisites",
      "Installation instructions"
    ],
    "keywords": [
      "Dmgmt.url",
      "https://OODesignerHost:<port>/oo-designer",
      "MT.exe",
      "installer.log",
      "wrapper.conf",
      "https://<OODesignerHost>:<port>/oo-designer",
      "wrapper.java",
      "install",
      "oo",
      "workflow",
      "designer",
      "prerequisites",
      "installation",
      "instructions",
      "topic",
      "contains",
      "windows",
      "linux",
      "environments.",
      "optional.",
      "before",
      "begin",
      "make",
      "sure",
      "downloaded",
      "extracted",
      "package",
      "installer-designer--.zip",
      "onto",
      "local",
      "file",
      "system.",
      "contact",
      "administrator",
      "ordownload",
      "product",
      "user",
      "interface.",
      "start",
      "installer",
      "right-click",
      "installer-oo-designer-win64--mt.exe",
      "run",
      "administrator.",
      "following",
      "command",
      "terminal",
      "window",
      "bash",
      "installer-oo-designer-linux--mt.binafter",
      "extracts",
      "opentext",
      "operations",
      "orchestration",
      "wizard",
      "automatically",
      "opens.",
      "license",
      "page",
      "select",
      "agree",
      "click",
      "next.",
      "location",
      "root",
      "directory",
      "doesn",
      "exist",
      "gets",
      "created",
      "automatically.",
      "prompted",
      "confirm",
      "creation",
      "new",
      "location.",
      "valid",
      "characters",
      "path",
      "include",
      "english",
      "letters",
      "digits",
      "spaces",
      "hyphens",
      "underscores",
      "default",
      "program",
      "files",
      "micro",
      "focus",
      "opt",
      "microfocus",
      "oo-designer",
      "options",
      "selection",
      "check",
      "box",
      "connectivity",
      "configure"
    ],
    "language": "en",
    "word_count": 88,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo workflow designer",
    "contentLower": "this topic contains instructions on how to install an oo workflow designer on windows and linux environments. the installation of an oo workflow designer is optional. prerequisites before you begin, make sure you have downloaded and extracted the installation package (installer-designer-<os>-<latest_version>.zip) onto your local file system. contact your administrator for the installation package, ordownload the installation package from the product user interface. installation instructions to start the installer: on windows: right-click the installer-oo-designer-win64-<version>-mt.exe installation file and run it as an administrator. on linux: run the following command from a linux terminal window: bash installer-oo-designer-linux-<version>-mt.binafter you start the installer, the installation package extracts, and the opentext operations orchestration designer installation wizard automatically opens. on the license page, select i agree and click next. on the location page, select the",
    "keywordsLower": [
      "dmgmt.url",
      "https://oodesignerhost:<port>/oo-designer",
      "mt.exe",
      "installer.log",
      "wrapper.conf",
      "https://<oodesignerhost>:<port>/oo-designer",
      "wrapper.java",
      "install",
      "oo",
      "workflow",
      "designer",
      "prerequisites",
      "installation",
      "instructions",
      "topic",
      "contains",
      "windows",
      "linux",
      "environments.",
      "optional.",
      "before",
      "begin",
      "make",
      "sure",
      "downloaded",
      "extracted",
      "package",
      "installer-designer--.zip",
      "onto",
      "local",
      "file",
      "system.",
      "contact",
      "administrator",
      "ordownload",
      "product",
      "user",
      "interface.",
      "start",
      "installer",
      "right-click",
      "installer-oo-designer-win64--mt.exe",
      "run",
      "administrator.",
      "following",
      "command",
      "terminal",
      "window",
      "bash",
      "installer-oo-designer-linux--mt.binafter",
      "extracts",
      "opentext",
      "operations",
      "orchestration",
      "wizard",
      "automatically",
      "opens.",
      "license",
      "page",
      "select",
      "agree",
      "click",
      "next.",
      "location",
      "root",
      "directory",
      "doesn",
      "exist",
      "gets",
      "created",
      "automatically.",
      "prompted",
      "confirm",
      "creation",
      "new",
      "location.",
      "valid",
      "characters",
      "path",
      "include",
      "english",
      "letters",
      "digits",
      "spaces",
      "hyphens",
      "underscores",
      "default",
      "program",
      "files",
      "micro",
      "focus",
      "opt",
      "microfocus",
      "oo-designer",
      "options",
      "selection",
      "check",
      "box",
      "connectivity",
      "configure"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Studio",
    "content": "This topic contains the system requirements and instructions for installing Operations Orchestration (OO) Studio. The compatibility between the OO Studio version and the suite version is the following: OO Studio version Suite version 24.1.x 24.x",
    "url": "installoostudio",
    "filename": "installoostudio",
    "headings": [],
    "keywords": [
      "24.1",
      "install",
      "oo",
      "studio",
      "topic",
      "contains",
      "system",
      "requirements",
      "instructions",
      "installing",
      "operations",
      "orchestration",
      "studio.",
      "compatibility",
      "between",
      "version",
      "suite",
      "following",
      "24.1.x",
      "24.x"
    ],
    "language": "en",
    "word_count": 28,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo studio",
    "contentLower": "this topic contains the system requirements and instructions for installing operations orchestration (oo) studio. the compatibility between the oo studio version and the suite version is the following: oo studio version suite version 24.1.x 24.x",
    "keywordsLower": [
      "24.1",
      "install",
      "oo",
      "studio",
      "topic",
      "contains",
      "system",
      "requirements",
      "instructions",
      "installing",
      "operations",
      "orchestration",
      "studio.",
      "compatibility",
      "between",
      "version",
      "suite",
      "following",
      "24.1.x",
      "24.x"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OO Studio on-premises",
    "content": "This section describes how to install Operations Orchestration (OO) Studio. You can only install OO Desktop Studio on Windows. Download the installer-oo-studio-win64-<version>.exe file from the Software Licenses and Downloads Portal to a local drive on your computer. To start the installer, double-click the installer-oo-studio-win64-<version>.exe installation file. After you start the installer, it extracts the installation package, and the Operations Orchestration Installation and Configuration Wizard automatically opens. Click Next. On the License page, select I Agree, and click Next. In the Location step, select the location for the installation root directory, and click Next. The default path is C:\\Program Files\\Micro Focus\\Operations Orchestration. Valid characters for the installation path include English letters, digits, spaces, hyphens (-), and underscores (_). If the directory doesn't exist, the installer will create it automatically. You need to confirm the creation of the ne",
    "url": "installoostudioonpremises",
    "filename": "installoostudioonpremises",
    "headings": [],
    "keywords": [
      "github.com",
      "v2.47",
      "2.47",
      "studio.git",
      "v2.47.0",
      "https://github.com/git-for-windows/git/releases/download/v2.47.0.windows.2/Git-2.47.0.2-64-bit.exe",
      "bit.exe",
      "installer.log",
      "2.47.0",
      "install",
      "oo",
      "studio",
      "on-premises",
      "section",
      "describes",
      "operations",
      "orchestration",
      "studio.",
      "desktop",
      "windows.",
      "download",
      "installer-oo-studio-win64-.exe",
      "file",
      "software",
      "licenses",
      "downloads",
      "portal",
      "local",
      "drive",
      "computer.",
      "start",
      "installer",
      "double-click",
      "installation",
      "file.",
      "after",
      "extracts",
      "package",
      "configuration",
      "wizard",
      "automatically",
      "opens.",
      "click",
      "next.",
      "license",
      "page",
      "select",
      "agree",
      "location",
      "step",
      "root",
      "directory",
      "default",
      "path",
      "program",
      "files",
      "micro",
      "focus",
      "orchestration.",
      "valid",
      "characters",
      "include",
      "english",
      "letters",
      "digits",
      "spaces",
      "hyphens",
      "underscores",
      "doesn",
      "exist",
      "create",
      "automatically.",
      "need",
      "confirm",
      "creation",
      "new",
      "location.",
      "options",
      "check",
      "box.",
      "content",
      "packs",
      "browse",
      "custom",
      "folder",
      "per",
      "choice",
      "ok.",
      "available",
      "located",
      "selected",
      "appear",
      "list.",
      "pack",
      "want",
      "import",
      "appropriate",
      "packs.",
      "additional",
      "updated"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install oo studio on-premises",
    "contentLower": "this section describes how to install operations orchestration (oo) studio. you can only install oo desktop studio on windows. download the installer-oo-studio-win64-<version>.exe file from the software licenses and downloads portal to a local drive on your computer. to start the installer, double-click the installer-oo-studio-win64-<version>.exe installation file. after you start the installer, it extracts the installation package, and the operations orchestration installation and configuration wizard automatically opens. click next. on the license page, select i agree, and click next. in the location step, select the location for the installation root directory, and click next. the default path is c:\\program files\\micro focus\\operations orchestration. valid characters for the installation path include english letters, digits, spaces, hyphens (-), and underscores (_). if the directory doesn't exist, the installer will create it automatically. you need to confirm the creation of the ne",
    "keywordsLower": [
      "github.com",
      "v2.47",
      "2.47",
      "studio.git",
      "v2.47.0",
      "https://github.com/git-for-windows/git/releases/download/v2.47.0.windows.2/git-2.47.0.2-64-bit.exe",
      "bit.exe",
      "installer.log",
      "2.47.0",
      "install",
      "oo",
      "studio",
      "on-premises",
      "section",
      "describes",
      "operations",
      "orchestration",
      "studio.",
      "desktop",
      "windows.",
      "download",
      "installer-oo-studio-win64-.exe",
      "file",
      "software",
      "licenses",
      "downloads",
      "portal",
      "local",
      "drive",
      "computer.",
      "start",
      "installer",
      "double-click",
      "installation",
      "file.",
      "after",
      "extracts",
      "package",
      "configuration",
      "wizard",
      "automatically",
      "opens.",
      "click",
      "next.",
      "license",
      "page",
      "select",
      "agree",
      "location",
      "step",
      "root",
      "directory",
      "default",
      "path",
      "program",
      "files",
      "micro",
      "focus",
      "orchestration.",
      "valid",
      "characters",
      "include",
      "english",
      "letters",
      "digits",
      "spaces",
      "hyphens",
      "underscores",
      "doesn",
      "exist",
      "create",
      "automatically.",
      "need",
      "confirm",
      "creation",
      "new",
      "location.",
      "options",
      "check",
      "box.",
      "content",
      "packs",
      "browse",
      "custom",
      "folder",
      "per",
      "choice",
      "ok.",
      "available",
      "located",
      "selected",
      "appear",
      "list.",
      "pack",
      "want",
      "import",
      "appropriate",
      "packs.",
      "additional",
      "updated"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install Operations Platform",
    "content": "This topic provides a high level overview of tasks that you must complete to install Operations Platform. Step Task Reference 1 Download Operations Platform installer package Download the installer package 2 Concepts related to preparing your infrastructure Prepare infrastructure 3 Install OPTIC Management Toolkit (OMT) Install OPTIC Management Toolkit 4 Download the Operations Platform images from Docker Hub and then upload them to an image registry. You can upload the images to an external or local registry based on your Kubernetes strategy. Download the application images and then upload them to an image registry 5 Create Operations Platform namespace Create Operations Platform namespace 6 Configure the database, install Vertica, set up secure communication, create storage class and persistent volumes, install load balancer, and configure service. Set up prerequisites 7 Configure the values YAML file Configure values.yaml 8 Configure and deploy Operations Platform using CLI Deploy O",
    "url": "402-installop",
    "filename": "402-installop",
    "headings": [],
    "keywords": [
      "values.yaml",
      "install",
      "operations",
      "platform",
      "topic",
      "provides",
      "high",
      "level",
      "overview",
      "tasks",
      "complete",
      "platform.",
      "step",
      "task",
      "reference",
      "download",
      "installer",
      "package",
      "concepts",
      "related",
      "preparing",
      "infrastructure",
      "prepare",
      "optic",
      "management",
      "toolkit",
      "omt",
      "images",
      "docker",
      "hub",
      "upload",
      "image",
      "registry.",
      "external",
      "local",
      "registry",
      "based",
      "kubernetes",
      "strategy.",
      "application",
      "create",
      "namespace",
      "configure",
      "database",
      "vertica",
      "set",
      "secure",
      "communication",
      "storage",
      "class",
      "persistent",
      "volumes",
      "load",
      "balancer",
      "service.",
      "prerequisites",
      "values",
      "yaml",
      "file",
      "deploy",
      "cli",
      "verify",
      "deployment",
      "10",
      "register"
    ],
    "language": "en",
    "word_count": 110,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install operations platform",
    "contentLower": "this topic provides a high level overview of tasks that you must complete to install operations platform. step task reference 1 download operations platform installer package download the installer package 2 concepts related to preparing your infrastructure prepare infrastructure 3 install optic management toolkit (omt) install optic management toolkit 4 download the operations platform images from docker hub and then upload them to an image registry. you can upload the images to an external or local registry based on your kubernetes strategy. download the application images and then upload them to an image registry 5 create operations platform namespace create operations platform namespace 6 configure the database, install vertica, set up secure communication, create storage class and persistent volumes, install load balancer, and configure service. set up prerequisites 7 configure the values yaml file configure values.yaml 8 configure and deploy operations platform using cli deploy o",
    "keywordsLower": [
      "values.yaml",
      "install",
      "operations",
      "platform",
      "topic",
      "provides",
      "high",
      "level",
      "overview",
      "tasks",
      "complete",
      "platform.",
      "step",
      "task",
      "reference",
      "download",
      "installer",
      "package",
      "concepts",
      "related",
      "preparing",
      "infrastructure",
      "prepare",
      "optic",
      "management",
      "toolkit",
      "omt",
      "images",
      "docker",
      "hub",
      "upload",
      "image",
      "registry.",
      "external",
      "local",
      "registry",
      "based",
      "kubernetes",
      "strategy.",
      "application",
      "create",
      "namespace",
      "configure",
      "database",
      "vertica",
      "set",
      "secure",
      "communication",
      "storage",
      "class",
      "persistent",
      "volumes",
      "load",
      "balancer",
      "service.",
      "prerequisites",
      "values",
      "yaml",
      "file",
      "deploy",
      "cli",
      "verify",
      "deployment",
      "10",
      "register"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Kubernetes related infrastructure",
    "content": "This topic provides you the required specifications to configure the Kubernetes requirements. If you are deploying Operations Platform with Embedded Kubernetes, you can skip this page. While setting up any of the Kubernetes clusters, ensure that you adhere to the networking model described in the Kubernetes documentation. Amazon Elastic Kubernetes (EKS) To set up an EKS cluster, see Amazon Elastic Kubernetes Service (EKS). Deployment on Amazon Web Services China Regions isn't supported. Operations Platform and applications support TLS 1.3 for an AWS Load Balancer instance created without using ITOM Cloud Deployment toolkit and with a security policy that supports TLS 1.3. Points to note while creating an EKS While creating the resources for EKS, you must use the same EKS cluster name when creating the VPC, EKS cluster, and worker nodes; otherwise, the installation fails.Operations Platform and applications support single and multi availability zone deployment for the Amazon Web Service",
    "url": "402-prepkubernetes",
    "filename": "402-prepkubernetes",
    "headings": [
      "Amazon Elastic Kubernetes (EKS)",
      "Points to note while creating an EKS",
      "Microsoft Azure Kubernetes Service (AKS)",
      "Points to note while creating an AKS",
      "Google Kubernetes Engine",
      "Point to note while creating a GKE",
      "Red Hat OpenShift Container Platform"
    ],
    "keywords": [
      "account.The",
      "deployment.You",
      "1.3",
      "VNet.AKS",
      "kubernetes",
      "related",
      "infrastructure",
      "amazon",
      "elastic",
      "eks",
      "points",
      "note",
      "while",
      "creating",
      "microsoft",
      "azure",
      "service",
      "aks",
      "google",
      "engine",
      "point",
      "gke",
      "red",
      "hat",
      "openshift",
      "container",
      "platform",
      "topic",
      "provides",
      "required",
      "specifications",
      "configure",
      "requirements.",
      "deploying",
      "operations",
      "embedded",
      "skip",
      "page.",
      "setting",
      "any",
      "clusters",
      "ensure",
      "adhere",
      "networking",
      "model",
      "described",
      "documentation.",
      "set",
      "cluster",
      "see",
      "deployment",
      "web",
      "services",
      "china",
      "regions",
      "isn",
      "supported.",
      "applications",
      "support",
      "tls",
      "aws",
      "load",
      "balancer",
      "instance",
      "created",
      "itom",
      "cloud",
      "toolkit",
      "security",
      "policy",
      "supports",
      "1.3.",
      "resources",
      "same",
      "name",
      "vpc",
      "worker",
      "nodes",
      "otherwise",
      "installation",
      "fails.operations",
      "single",
      "multi",
      "availability",
      "zone",
      "nodes.",
      "choose",
      "deploy",
      "either",
      "az",
      "multiple",
      "according",
      "requirement.",
      "install",
      "release",
      "change",
      "later.",
      "doesn",
      "converting",
      "deployment."
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "kubernetes related infrastructure",
    "contentLower": "this topic provides you the required specifications to configure the kubernetes requirements. if you are deploying operations platform with embedded kubernetes, you can skip this page. while setting up any of the kubernetes clusters, ensure that you adhere to the networking model described in the kubernetes documentation. amazon elastic kubernetes (eks) to set up an eks cluster, see amazon elastic kubernetes service (eks). deployment on amazon web services china regions isn't supported. operations platform and applications support tls 1.3 for an aws load balancer instance created without using itom cloud deployment toolkit and with a security policy that supports tls 1.3. points to note while creating an eks while creating the resources for eks, you must use the same eks cluster name when creating the vpc, eks cluster, and worker nodes; otherwise, the installation fails.operations platform and applications support single and multi availability zone deployment for the amazon web service",
    "keywordsLower": [
      "account.the",
      "deployment.you",
      "1.3",
      "vnet.aks",
      "kubernetes",
      "related",
      "infrastructure",
      "amazon",
      "elastic",
      "eks",
      "points",
      "note",
      "while",
      "creating",
      "microsoft",
      "azure",
      "service",
      "aks",
      "google",
      "engine",
      "point",
      "gke",
      "red",
      "hat",
      "openshift",
      "container",
      "platform",
      "topic",
      "provides",
      "required",
      "specifications",
      "configure",
      "requirements.",
      "deploying",
      "operations",
      "embedded",
      "skip",
      "page.",
      "setting",
      "any",
      "clusters",
      "ensure",
      "adhere",
      "networking",
      "model",
      "described",
      "documentation.",
      "set",
      "cluster",
      "see",
      "deployment",
      "web",
      "services",
      "china",
      "regions",
      "isn",
      "supported.",
      "applications",
      "support",
      "tls",
      "aws",
      "load",
      "balancer",
      "instance",
      "created",
      "itom",
      "cloud",
      "toolkit",
      "security",
      "policy",
      "supports",
      "1.3.",
      "resources",
      "same",
      "name",
      "vpc",
      "worker",
      "nodes",
      "otherwise",
      "installation",
      "fails.operations",
      "single",
      "multi",
      "availability",
      "zone",
      "nodes.",
      "choose",
      "deploy",
      "either",
      "az",
      "multiple",
      "according",
      "requirement.",
      "install",
      "release",
      "change",
      "later.",
      "doesn",
      "converting",
      "deployment."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Image registry",
    "content": "This topic provides you required specifications to configure the image registry requirements. Images are downloaded during install, upgrade, or anytime a cluster or pod goes down. If an image is missing in the cluster, a call is made to download the image from the image repository server. Fetching images from Docker Hub each time an image is missing, uses plenty of network resources. To prevent this, it's recommended to use a local image repository so that the traffic stays within the customer network or download the images to the cluster and let the cluster handle the image requests. Do one of the following to download and upload installation images: Download images from Docker Hub to an image repository server in a customer network. You can use this local image repository server to download images during OMT and application installation as required. This image repository serves as a “cache” for all the users in that network.For AWS, see Create ECR repositories and upload container im",
    "url": "402-image",
    "filename": "402-image",
    "headings": [],
    "keywords": [
      "network.For",
      "image",
      "registry",
      "topic",
      "provides",
      "required",
      "specifications",
      "configure",
      "requirements.",
      "images",
      "downloaded",
      "during",
      "install",
      "upgrade",
      "anytime",
      "cluster",
      "pod",
      "goes",
      "down.",
      "missing",
      "call",
      "made",
      "download",
      "repository",
      "server.",
      "fetching",
      "docker",
      "hub",
      "time",
      "uses",
      "plenty",
      "network",
      "resources.",
      "prevent",
      "recommended",
      "local",
      "traffic",
      "stays",
      "customer",
      "let",
      "handle",
      "requests.",
      "one",
      "following",
      "upload",
      "installation",
      "server",
      "network.",
      "omt",
      "application",
      "required.",
      "serves",
      "cache",
      "all",
      "users",
      "aws",
      "see",
      "create",
      "ecr",
      "repositories",
      "container",
      "imagesfor",
      "azure",
      "uploading",
      "registryfor",
      "gcp",
      "artifact",
      "machine",
      "directly",
      "script",
      "provided",
      "omt.upload",
      "embedded",
      "kubernetes",
      "omt.",
      "information",
      "registry."
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "image registry",
    "contentLower": "this topic provides you required specifications to configure the image registry requirements. images are downloaded during install, upgrade, or anytime a cluster or pod goes down. if an image is missing in the cluster, a call is made to download the image from the image repository server. fetching images from docker hub each time an image is missing, uses plenty of network resources. to prevent this, it's recommended to use a local image repository so that the traffic stays within the customer network or download the images to the cluster and let the cluster handle the image requests. do one of the following to download and upload installation images: download images from docker hub to an image repository server in a customer network. you can use this local image repository server to download images during omt and application installation as required. this image repository serves as a “cache” for all the users in that network.for aws, see create ecr repositories and upload container im",
    "keywordsLower": [
      "network.for",
      "image",
      "registry",
      "topic",
      "provides",
      "required",
      "specifications",
      "configure",
      "requirements.",
      "images",
      "downloaded",
      "during",
      "install",
      "upgrade",
      "anytime",
      "cluster",
      "pod",
      "goes",
      "down.",
      "missing",
      "call",
      "made",
      "download",
      "repository",
      "server.",
      "fetching",
      "docker",
      "hub",
      "time",
      "uses",
      "plenty",
      "network",
      "resources.",
      "prevent",
      "recommended",
      "local",
      "traffic",
      "stays",
      "customer",
      "let",
      "handle",
      "requests.",
      "one",
      "following",
      "upload",
      "installation",
      "server",
      "network.",
      "omt",
      "application",
      "required.",
      "serves",
      "cache",
      "all",
      "users",
      "aws",
      "see",
      "create",
      "ecr",
      "repositories",
      "container",
      "imagesfor",
      "azure",
      "uploading",
      "registryfor",
      "gcp",
      "artifact",
      "machine",
      "directly",
      "script",
      "provided",
      "omt.upload",
      "embedded",
      "kubernetes",
      "omt.",
      "information",
      "registry."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install Vertica",
    "content": "To install and set up a new Vertica database perform the following: Compute resources and storage using the Sizing calculator. Sync the time between Kubernetes and Vertica. Check the supported Vertica versions and operating systems. Download the Vertica package from the Software Licenses and Downloads website. See the following Vertica documentation sections before you install Vertica: Vertica documentation link Description Installing Vertica Contains information to install Vertica based on your platform. Recommendations for Sizing Vertica Nodes and Cluster Contains recommendations for hardware for your Vertica nodes. It also provides information that helps you appropriately size the hardware components to meet the needs of your environment. Vertica recommends that the disk utilization per node mustn't be more than 60%. Supported Operating Systems and File System Requirements for Vertica Server Contains information about the operating systems and file systems required for the installat",
    "url": "402-installverticaforop",
    "filename": "402-installverticaforop",
    "headings": [],
    "keywords": [
      "example.net",
      "RHEL6.rpm",
      "install",
      "vertica",
      "set",
      "new",
      "database",
      "perform",
      "following",
      "compute",
      "resources",
      "storage",
      "sizing",
      "calculator.",
      "sync",
      "time",
      "between",
      "kubernetes",
      "vertica.",
      "check",
      "supported",
      "versions",
      "operating",
      "systems.",
      "download",
      "package",
      "software",
      "licenses",
      "downloads",
      "website.",
      "see",
      "documentation",
      "sections",
      "before",
      "link",
      "description",
      "installing",
      "contains",
      "information",
      "based",
      "platform.",
      "recommendations",
      "nodes",
      "cluster",
      "hardware",
      "nodes.",
      "provides",
      "helps",
      "appropriately",
      "size",
      "components",
      "meet",
      "needs",
      "environment.",
      "recommends",
      "disk",
      "utilization",
      "per",
      "node",
      "mustn",
      "60",
      "systems",
      "file",
      "system",
      "requirements",
      "server",
      "about",
      "required",
      "installation",
      "server.",
      "virtualized",
      "environment",
      "packages",
      "require",
      "all",
      "overview",
      "checklist",
      "tasks.",
      "steps",
      "ensure",
      "meets",
      "prerequisites.",
      "swappiness",
      "value",
      "running",
      "command",
      "cat",
      "proc",
      "sys",
      "vm",
      "configuring",
      "network",
      "import",
      "export",
      "data",
      "knowledge",
      "base",
      "article",
      "data.",
      "configuration"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install vertica",
    "contentLower": "to install and set up a new vertica database perform the following: compute resources and storage using the sizing calculator. sync the time between kubernetes and vertica. check the supported vertica versions and operating systems. download the vertica package from the software licenses and downloads website. see the following vertica documentation sections before you install vertica: vertica documentation link description installing vertica contains information to install vertica based on your platform. recommendations for sizing vertica nodes and cluster contains recommendations for hardware for your vertica nodes. it also provides information that helps you appropriately size the hardware components to meet the needs of your environment. vertica recommends that the disk utilization per node mustn't be more than 60%. supported operating systems and file system requirements for vertica server contains information about the operating systems and file systems required for the installat",
    "keywordsLower": [
      "example.net",
      "rhel6.rpm",
      "install",
      "vertica",
      "set",
      "new",
      "database",
      "perform",
      "following",
      "compute",
      "resources",
      "storage",
      "sizing",
      "calculator.",
      "sync",
      "time",
      "between",
      "kubernetes",
      "vertica.",
      "check",
      "supported",
      "versions",
      "operating",
      "systems.",
      "download",
      "package",
      "software",
      "licenses",
      "downloads",
      "website.",
      "see",
      "documentation",
      "sections",
      "before",
      "link",
      "description",
      "installing",
      "contains",
      "information",
      "based",
      "platform.",
      "recommendations",
      "nodes",
      "cluster",
      "hardware",
      "nodes.",
      "provides",
      "helps",
      "appropriately",
      "size",
      "components",
      "meet",
      "needs",
      "environment.",
      "recommends",
      "disk",
      "utilization",
      "per",
      "node",
      "mustn",
      "60",
      "systems",
      "file",
      "system",
      "requirements",
      "server",
      "about",
      "required",
      "installation",
      "server.",
      "virtualized",
      "environment",
      "packages",
      "require",
      "all",
      "overview",
      "checklist",
      "tasks.",
      "steps",
      "ensure",
      "meets",
      "prerequisites.",
      "swappiness",
      "value",
      "running",
      "command",
      "cat",
      "proc",
      "sys",
      "vm",
      "configuring",
      "network",
      "import",
      "export",
      "data",
      "knowledge",
      "base",
      "article",
      "data.",
      "configuration"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install OPTIC Data Lake Vertica Plugin",
    "content": "After you install Vertica, you must install the package that contains OPTIC DL Vertica Plugin on any one Vertica node. The plugin library then becomes a part of the database and all nodes in the cluster can use the plugin library. The OPTIC DL Vertica Plugin RPM includes an installation script that initializes Vertica for OPTIC DL. You must use the same values during OPTIC DL Vertica Plugin installation and Operations Platform deployment. Download and install the plugin After installing external Vertica and setting up the database, follow these steps on any one of the Vertica nodes: Copy the OPTIC DL Vertica Plugin RPM appropriate for your operating system from the location where you have extracted the installation package into a Vertica node: For RHEL and CentOS 8.x and RHEL and CentOS 9.x, copy itom-di-pulsarudx-<version>.x86_64.rpm Run the following command to switch as the root user: su - root Run the following commands to install the OPTIC Data Lake Vertica plugin rpm: To install ",
    "url": "402-prepverticadb",
    "filename": "402-prepverticadb",
    "headings": [
      "Download and install the plugin"
    ],
    "keywords": [
      "x.xx",
      "x86_64.rpm",
      "dbinit.sh",
      "dbinit_conf.yaml",
      "install",
      "optic",
      "data",
      "lake",
      "vertica",
      "plugin",
      "download",
      "after",
      "package",
      "contains",
      "dl",
      "any",
      "one",
      "node.",
      "library",
      "becomes",
      "part",
      "database",
      "all",
      "nodes",
      "cluster",
      "library.",
      "rpm",
      "includes",
      "installation",
      "script",
      "initializes",
      "dl.",
      "same",
      "values",
      "during",
      "operations",
      "platform",
      "deployment.",
      "installing",
      "external",
      "setting",
      "follow",
      "steps",
      "copy",
      "appropriate",
      "operating",
      "system",
      "location",
      "extracted",
      "node",
      "rhel",
      "centos",
      "8.x",
      "9.x",
      "run",
      "following",
      "command",
      "switch",
      "root",
      "user",
      "su",
      "commands",
      "default",
      "path",
      "systems",
      "-ivh",
      "custom",
      "--prefix",
      "choose",
      "ensure",
      "755",
      "file",
      "permission",
      "parent",
      "directory",
      "path.",
      "replace",
      "usr",
      "given",
      "output",
      "generate",
      "set",
      "environment",
      "variables",
      "directed",
      "below",
      "local",
      "itom-di-pulsarudx",
      "bin",
      "genconfig",
      "example",
      "test",
      "create",
      "configure",
      "file."
    ],
    "language": "en",
    "word_count": 111,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install optic data lake vertica plugin",
    "contentLower": "after you install vertica, you must install the package that contains optic dl vertica plugin on any one vertica node. the plugin library then becomes a part of the database and all nodes in the cluster can use the plugin library. the optic dl vertica plugin rpm includes an installation script that initializes vertica for optic dl. you must use the same values during optic dl vertica plugin installation and operations platform deployment. download and install the plugin after installing external vertica and setting up the database, follow these steps on any one of the vertica nodes: copy the optic dl vertica plugin rpm appropriate for your operating system from the location where you have extracted the installation package into a vertica node: for rhel and centos 8.x and rhel and centos 9.x, copy itom-di-pulsarudx-<version>.x86_64.rpm run the following command to switch as the root user: su - root run the following commands to install the optic data lake vertica plugin rpm: to install ",
    "keywordsLower": [
      "x.xx",
      "x86_64.rpm",
      "dbinit.sh",
      "dbinit_conf.yaml",
      "install",
      "optic",
      "data",
      "lake",
      "vertica",
      "plugin",
      "download",
      "after",
      "package",
      "contains",
      "dl",
      "any",
      "one",
      "node.",
      "library",
      "becomes",
      "part",
      "database",
      "all",
      "nodes",
      "cluster",
      "library.",
      "rpm",
      "includes",
      "installation",
      "script",
      "initializes",
      "dl.",
      "same",
      "values",
      "during",
      "operations",
      "platform",
      "deployment.",
      "installing",
      "external",
      "setting",
      "follow",
      "steps",
      "copy",
      "appropriate",
      "operating",
      "system",
      "location",
      "extracted",
      "node",
      "rhel",
      "centos",
      "8.x",
      "9.x",
      "run",
      "following",
      "command",
      "switch",
      "root",
      "user",
      "su",
      "commands",
      "default",
      "path",
      "systems",
      "-ivh",
      "custom",
      "--prefix",
      "choose",
      "ensure",
      "755",
      "file",
      "permission",
      "parent",
      "directory",
      "path.",
      "replace",
      "usr",
      "given",
      "output",
      "generate",
      "set",
      "environment",
      "variables",
      "directed",
      "below",
      "local",
      "itom-di-pulsarudx",
      "bin",
      "genconfig",
      "example",
      "test",
      "create",
      "configure",
      "file."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install storage provisioner chart",
    "content": "If you have created local persistent volumes (LPVs) on Embedded Kubernetes for the OPTIC Data Lake, as described in Create Local Persistent Volumes on worker nodes page, you must follow the instructions below to deploy the local storage provisioner. Otherwise, skip this topic, for example, when using External Kubernetes. The helm install of the storage provisioner chart converts the filesystem mount points you created on the worker nodes (see Create Local Persistent Volumes on worker nodes) into Kubernetes persistent volumes. This gets claimed via the helm installation step according to the Operations Platform values YAML file and deployment YAML specifications. As mentioned in the Create Local Persistent Volumes on worker nodes topic, you should have at least three workers for a production implementation of OPTIC Data Lake, each provisioned with filesystems mounted under the OPTIC Data Lake path. The OPTIC Data Lake path for LPVs is /mnt/disks, or you may have chosen an alternative. T",
    "url": "402-storageprovisionerchartoppl",
    "filename": "402-storageprovisionerchartoppl",
    "headings": [
      "Verify the installation of storage provisioner chart"
    ],
    "keywords": [
      "lpv.yaml",
      "values.yaml",
      "install",
      "storage",
      "provisioner",
      "chart",
      "verify",
      "installation",
      "created",
      "local",
      "persistent",
      "volumes",
      "lpvs",
      "embedded",
      "kubernetes",
      "optic",
      "data",
      "lake",
      "described",
      "create",
      "worker",
      "nodes",
      "page",
      "follow",
      "instructions",
      "below",
      "deploy",
      "provisioner.",
      "otherwise",
      "skip",
      "topic",
      "example",
      "external",
      "kubernetes.",
      "helm",
      "converts",
      "filesystem",
      "mount",
      "points",
      "see",
      "volumes.",
      "gets",
      "claimed",
      "via",
      "step",
      "according",
      "operations",
      "platform",
      "values",
      "yaml",
      "file",
      "deployment",
      "specifications.",
      "mentioned",
      "least",
      "three",
      "workers",
      "production",
      "implementation",
      "provisioned",
      "filesystems",
      "mounted",
      "under",
      "path.",
      "path",
      "mnt",
      "disks",
      "chosen",
      "alternative.",
      "available",
      "free",
      "space",
      "needs",
      "greater",
      "amount",
      "obtained",
      "sizing",
      "calculator",
      "ledger",
      "bookkeeper",
      "zookeeper",
      "get",
      "attached",
      "directories",
      "chose",
      "specifies",
      "hostdir.",
      "unsure",
      "able",
      "command",
      "directory",
      "name.",
      "default",
      "need",
      "same",
      "structure",
      "node",
      "environment",
      "grep",
      "dev"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install storage provisioner chart",
    "contentLower": "if you have created local persistent volumes (lpvs) on embedded kubernetes for the optic data lake, as described in create local persistent volumes on worker nodes page, you must follow the instructions below to deploy the local storage provisioner. otherwise, skip this topic, for example, when using external kubernetes. the helm install of the storage provisioner chart converts the filesystem mount points you created on the worker nodes (see create local persistent volumes on worker nodes) into kubernetes persistent volumes. this gets claimed via the helm installation step according to the operations platform values yaml file and deployment yaml specifications. as mentioned in the create local persistent volumes on worker nodes topic, you should have at least three workers for a production implementation of optic data lake, each provisioned with filesystems mounted under the optic data lake path. the optic data lake path for lpvs is /mnt/disks, or you may have chosen an alternative. t",
    "keywordsLower": [
      "lpv.yaml",
      "values.yaml",
      "install",
      "storage",
      "provisioner",
      "chart",
      "verify",
      "installation",
      "created",
      "local",
      "persistent",
      "volumes",
      "lpvs",
      "embedded",
      "kubernetes",
      "optic",
      "data",
      "lake",
      "described",
      "create",
      "worker",
      "nodes",
      "page",
      "follow",
      "instructions",
      "below",
      "deploy",
      "provisioner.",
      "otherwise",
      "skip",
      "topic",
      "example",
      "external",
      "kubernetes.",
      "helm",
      "converts",
      "filesystem",
      "mount",
      "points",
      "see",
      "volumes.",
      "gets",
      "claimed",
      "via",
      "step",
      "according",
      "operations",
      "platform",
      "values",
      "yaml",
      "file",
      "deployment",
      "specifications.",
      "mentioned",
      "least",
      "three",
      "workers",
      "production",
      "implementation",
      "provisioned",
      "filesystems",
      "mounted",
      "under",
      "path.",
      "path",
      "mnt",
      "disks",
      "chosen",
      "alternative.",
      "available",
      "free",
      "space",
      "needs",
      "greater",
      "amount",
      "obtained",
      "sizing",
      "calculator",
      "ledger",
      "bookkeeper",
      "zookeeper",
      "get",
      "attached",
      "directories",
      "chose",
      "specifies",
      "hostdir.",
      "unsure",
      "able",
      "command",
      "directory",
      "name.",
      "default",
      "need",
      "same",
      "structure",
      "node",
      "environment",
      "grep",
      "dev"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install UD/UCMDB for SAM",
    "content": "The new SAM capability requires one of the following UD/UCMDB deployments as described in Service Management integration with Universal Discovery and CMDB. Install UD/UCMDB You can deploy containerized or classic UD/UCMDB to enable the SAM capability. For classic UD/UCMDB, you will need to install both UCMDB Server and UCMDB Gateway. Containerized UD/UCMDB has an embedded UCMDB Gateway. Containerized UD/UCMDB To install containerized UD/UCMDB, follow the instructions in the following topics depending on your deployment platform. Make sure you've configured the required UCMDB Gateway settings in your values.yaml file. Install containerized UD/UCMDB (on-premises) Install containerized UD/UCMDB (EKS) Install containerized UD/UCMDB (AKS) Install containerized UD/UCMDB (OpenShift) Then, run the following command to upgrade the deployment: helm upgrade <UD/UCMDB_RELEASE_NAME> <PATH_TO_UD/UCMDB_CHART.TGZ> -n <UD/UCMDB_NAMESPACE> -f <PATH_TO_VALUES.YAML> Classic UD/UCMDB To install classic UD/",
    "url": "saminstallcms",
    "filename": "saminstallcms",
    "headings": [
      "Install UD/UCMDB",
      "Containerized UD/UCMDB",
      "Classic UD/UCMDB",
      "Install UCMDB Server",
      "Install UCMDB Gateway",
      "Set up a secure connection between Service Management and UCMDB Gateway"
    ],
    "keywords": [
      "uducmdb",
      "UCMDB_CHART.TGZ",
      "values.yaml",
      "PATH_TO_VALUES.YAML",
      "install",
      "ud",
      "ucmdb",
      "sam",
      "containerized",
      "classic",
      "server",
      "gateway",
      "set",
      "secure",
      "connection",
      "between",
      "service",
      "management",
      "new",
      "capability",
      "requires",
      "one",
      "following",
      "deployments",
      "described",
      "integration",
      "universal",
      "discovery",
      "cmdb.",
      "deploy",
      "enable",
      "capability.",
      "need",
      "both",
      "gateway.",
      "embedded",
      "follow",
      "instructions",
      "topics",
      "depending",
      "deployment",
      "platform.",
      "make",
      "sure",
      "ve",
      "configured",
      "required",
      "settings",
      "file.",
      "on-premises",
      "eks",
      "aks",
      "openshift",
      "run",
      "command",
      "upgrade",
      "helm",
      "-n",
      "-f",
      "steps",
      "obtain",
      "installer",
      "section",
      "cmdb",
      "online",
      "documentation.",
      "already",
      "earlier",
      "version",
      "installed",
      "current",
      "version.",
      "see",
      "prepare",
      "windows",
      "linux",
      "configure",
      "after",
      "same",
      "kubernetes",
      "cluster",
      "accessible",
      "through",
      "external",
      "access",
      "host",
      "name",
      "skip",
      "certificate",
      "exchange",
      "steps.",
      "procedure",
      "native",
      "sacm",
      "solution",
      "includes",
      "import",
      "management.",
      "restart",
      "backend"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install ud/ucmdb for sam",
    "contentLower": "the new sam capability requires one of the following ud/ucmdb deployments as described in service management integration with universal discovery and cmdb. install ud/ucmdb you can deploy containerized or classic ud/ucmdb to enable the sam capability. for classic ud/ucmdb, you will need to install both ucmdb server and ucmdb gateway. containerized ud/ucmdb has an embedded ucmdb gateway. containerized ud/ucmdb to install containerized ud/ucmdb, follow the instructions in the following topics depending on your deployment platform. make sure you've configured the required ucmdb gateway settings in your values.yaml file. install containerized ud/ucmdb (on-premises) install containerized ud/ucmdb (eks) install containerized ud/ucmdb (aks) install containerized ud/ucmdb (openshift) then, run the following command to upgrade the deployment: helm upgrade <ud/ucmdb_release_name> <path_to_ud/ucmdb_chart.tgz> -n <ud/ucmdb_namespace> -f <path_to_values.yaml> classic ud/ucmdb to install classic ud/",
    "keywordsLower": [
      "uducmdb",
      "ucmdb_chart.tgz",
      "values.yaml",
      "path_to_values.yaml",
      "install",
      "ud",
      "ucmdb",
      "sam",
      "containerized",
      "classic",
      "server",
      "gateway",
      "set",
      "secure",
      "connection",
      "between",
      "service",
      "management",
      "new",
      "capability",
      "requires",
      "one",
      "following",
      "deployments",
      "described",
      "integration",
      "universal",
      "discovery",
      "cmdb.",
      "deploy",
      "enable",
      "capability.",
      "need",
      "both",
      "gateway.",
      "embedded",
      "follow",
      "instructions",
      "topics",
      "depending",
      "deployment",
      "platform.",
      "make",
      "sure",
      "ve",
      "configured",
      "required",
      "settings",
      "file.",
      "on-premises",
      "eks",
      "aks",
      "openshift",
      "run",
      "command",
      "upgrade",
      "helm",
      "-n",
      "-f",
      "steps",
      "obtain",
      "installer",
      "section",
      "cmdb",
      "online",
      "documentation.",
      "already",
      "earlier",
      "version",
      "installed",
      "current",
      "version.",
      "see",
      "prepare",
      "windows",
      "linux",
      "configure",
      "after",
      "same",
      "kubernetes",
      "cluster",
      "accessible",
      "through",
      "external",
      "access",
      "host",
      "name",
      "skip",
      "certificate",
      "exchange",
      "steps.",
      "procedure",
      "native",
      "sacm",
      "solution",
      "includes",
      "import",
      "management.",
      "restart",
      "backend"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install UD/UCMDB for Native SACM",
    "content": "Native SACM can be enabled in the following two modes: Multi-deployment mode: Both Service Management and UD/UCMDB are containerized and deployed on OMT Classic mode: Service Management connecting with classic UD/UCMDB Containerized UD/UCMDB To install a containerized UD/UCMDB, follow the instructions documented here: Install containerized UD/UCMDB (on-premises) Install containerized UD/UCMDB (EKS) Install containerized UD/UCMDB (AKS) Install containerized UD/UCMDB (OpenShift) Classic UD/UCMDB Install the UCMDB server You need a UCMDB server set up before you enable Native SACM. To install an external UCMDB server, follow the instructions documented here: Install UCMDB server Enable UCMDB Gateway on UCMDB Server Obtain the UCMDB server installer and install the UCMDB server by following the instructions in the UD/UCMDB Deployment Guide. On the UCMDB server machine, locate the wrapper-custom.conf file in the <Ucmdb Server home>\\conf\\ folder and open the file using a text editor. Add the",
    "url": "nativesacmucmdb",
    "filename": "nativesacmucmdb",
    "headings": [
      "Containerized UD/UCMDB",
      "Classic UD/UCMDB",
      "Install the UCMDB server",
      "Enable UCMDB Gateway on UCMDB Server",
      "Check the super integration user for UCMDB Gateway",
      "Install UCMDB Gateway",
      "Windows",
      "Linux",
      "Configure SSL for outbound integrations"
    ],
    "keywords": [
      "uducmdb",
      "_Windows.zip",
      "server.crt",
      "installService.sh",
      "server.key",
      "custom.conf",
      "encryptPassword.bat",
      "_Linux.zip",
      "additional.91",
      "installService.bat",
      "encryptPassword.sh",
      "wrapper.java",
      "install",
      "ud",
      "ucmdb",
      "native",
      "sacm",
      "containerized",
      "classic",
      "server",
      "enable",
      "gateway",
      "check",
      "super",
      "integration",
      "user",
      "windows",
      "linux",
      "configure",
      "ssl",
      "outbound",
      "integrations",
      "enabled",
      "following",
      "two",
      "modes",
      "multi-deployment",
      "mode",
      "both",
      "service",
      "management",
      "deployed",
      "omt",
      "connecting",
      "follow",
      "instructions",
      "documented",
      "here",
      "on-premises",
      "eks",
      "aks",
      "openshift",
      "need",
      "set",
      "before",
      "sacm.",
      "external",
      "obtain",
      "installer",
      "deployment",
      "guide.",
      "machine",
      "locate",
      "wrapper-custom.conf",
      "file",
      "conf",
      "folder",
      "open",
      "text",
      "editor.",
      "add",
      "line",
      "save",
      "wrapper.java.additional.91",
      "-dsmax.integration.gateway.enabled",
      "true",
      "restart",
      "server.",
      "whether",
      "uisysadmin",
      "log",
      "jmx",
      "console.",
      "invoke",
      "retrievecmdbsuperintegrationuser",
      "method.",
      "make",
      "sure",
      "returned",
      "method",
      "uisysadmin.",
      "run",
      "setcmdbsuperintegrationuser",
      "change",
      "back",
      "operating",
      "system",
      "unzip",
      "package",
      "preferred"
    ],
    "language": "en",
    "word_count": 116,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install ud/ucmdb for native sacm",
    "contentLower": "native sacm can be enabled in the following two modes: multi-deployment mode: both service management and ud/ucmdb are containerized and deployed on omt classic mode: service management connecting with classic ud/ucmdb containerized ud/ucmdb to install a containerized ud/ucmdb, follow the instructions documented here: install containerized ud/ucmdb (on-premises) install containerized ud/ucmdb (eks) install containerized ud/ucmdb (aks) install containerized ud/ucmdb (openshift) classic ud/ucmdb install the ucmdb server you need a ucmdb server set up before you enable native sacm. to install an external ucmdb server, follow the instructions documented here: install ucmdb server enable ucmdb gateway on ucmdb server obtain the ucmdb server installer and install the ucmdb server by following the instructions in the ud/ucmdb deployment guide. on the ucmdb server machine, locate the wrapper-custom.conf file in the <ucmdb server home>\\conf\\ folder and open the file using a text editor. add the",
    "keywordsLower": [
      "uducmdb",
      "_windows.zip",
      "server.crt",
      "installservice.sh",
      "server.key",
      "custom.conf",
      "encryptpassword.bat",
      "_linux.zip",
      "additional.91",
      "installservice.bat",
      "encryptpassword.sh",
      "wrapper.java",
      "install",
      "ud",
      "ucmdb",
      "native",
      "sacm",
      "containerized",
      "classic",
      "server",
      "enable",
      "gateway",
      "check",
      "super",
      "integration",
      "user",
      "windows",
      "linux",
      "configure",
      "ssl",
      "outbound",
      "integrations",
      "enabled",
      "following",
      "two",
      "modes",
      "multi-deployment",
      "mode",
      "both",
      "service",
      "management",
      "deployed",
      "omt",
      "connecting",
      "follow",
      "instructions",
      "documented",
      "here",
      "on-premises",
      "eks",
      "aks",
      "openshift",
      "need",
      "set",
      "before",
      "sacm.",
      "external",
      "obtain",
      "installer",
      "deployment",
      "guide.",
      "machine",
      "locate",
      "wrapper-custom.conf",
      "file",
      "conf",
      "folder",
      "open",
      "text",
      "editor.",
      "add",
      "line",
      "save",
      "wrapper.java.additional.91",
      "-dsmax.integration.gateway.enabled",
      "true",
      "restart",
      "server.",
      "whether",
      "uisysadmin",
      "log",
      "jmx",
      "console.",
      "invoke",
      "retrievecmdbsuperintegrationuser",
      "method.",
      "make",
      "sure",
      "returned",
      "method",
      "uisysadmin.",
      "run",
      "setcmdbsuperintegrationuser",
      "change",
      "back",
      "operating",
      "system",
      "unzip",
      "package",
      "preferred"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install an ESM patch",
    "content": "This page contains the steps to apply a patch to an ESM Helm chart deployment using either OMT's embedded Kubernetes or a managed Kubernetes. For detailed information about fixed issues, patch name, and download link, see the corresponding patch release notes. Back up You can back up the system by using Velero. For details, see SMA Backup Procedure. Prepare Follow these steps to prepare for the installation: Download the patch package ESM_Helm_Chart-2x.x.x.zip.Run the following command to unzip the package: unzip ESM_Helm_Chart-2x.x.x.zip -d ESM_Helm_Chart-2x.x.x The unzipped file contains the following files: esm-1.x.x+2x.x.x.zip: Contains the suite installation package.esm-1.x.x+2x.x.x.zip.sig: The signature file to verify the package. (Optional) Verify the ESM helm package. Skip this step if you don't want to verify the package. Visit KM000003948 to download the ot-package-sign.pub key and follow the instructions in the doc to verify the ESM helm package. Run the following command t",
    "url": "helmpatchinstall2611",
    "filename": "helmpatchinstall2611",
    "headings": [
      "Back up",
      "Prepare",
      "Install",
      "Install with CLI",
      "Install with AppHub",
      "Verify"
    ],
    "keywords": [
      "warning.Go",
      "https://<EXTERNAL_ACCESS_HOST>/bo",
      "sign.pub",
      "Upgrade.From",
      "OK.You",
      "x.zip",
      "x.tgz",
      "values.yaml",
      "package.esm",
      "install",
      "esm",
      "patch",
      "back",
      "prepare",
      "cli",
      "apphub",
      "verify",
      "page",
      "contains",
      "steps",
      "apply",
      "helm",
      "chart",
      "deployment",
      "either",
      "omt",
      "embedded",
      "kubernetes",
      "managed",
      "kubernetes.",
      "detailed",
      "information",
      "about",
      "fixed",
      "issues",
      "name",
      "download",
      "link",
      "see",
      "corresponding",
      "release",
      "notes.",
      "system",
      "velero.",
      "details",
      "sma",
      "backup",
      "procedure.",
      "follow",
      "installation",
      "package",
      "following",
      "command",
      "unzip",
      "-d",
      "unzipped",
      "file",
      "files",
      "esm-1.x.x",
      "2x.x.x.zip",
      "suite",
      "package.esm-1.x.x",
      "2x.x.x.zip.sig",
      "signature",
      "package.",
      "optional",
      "skip",
      "step",
      "don",
      "want",
      "visit",
      "km000003948",
      "ot-package-sign.pub",
      "key",
      "instructions",
      "doc",
      "run",
      "cd",
      "charts",
      "need",
      "2x.x.x.tgz",
      "2x.x.x.tgz.prov.",
      "samples",
      "sample.",
      "sample",
      "includes",
      "definition",
      "help",
      "customization",
      "file.",
      "scripts",
      "configuration",
      "scripts.",
      "container",
      "images",
      "images.",
      "section",
      "according",
      "deployment.",
      "remember"
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install an esm patch",
    "contentLower": "this page contains the steps to apply a patch to an esm helm chart deployment using either omt's embedded kubernetes or a managed kubernetes. for detailed information about fixed issues, patch name, and download link, see the corresponding patch release notes. back up you can back up the system by using velero. for details, see sma backup procedure. prepare follow these steps to prepare for the installation: download the patch package esm_helm_chart-2x.x.x.zip.run the following command to unzip the package: unzip esm_helm_chart-2x.x.x.zip -d esm_helm_chart-2x.x.x the unzipped file contains the following files: esm-1.x.x+2x.x.x.zip: contains the suite installation package.esm-1.x.x+2x.x.x.zip.sig: the signature file to verify the package. (optional) verify the esm helm package. skip this step if you don't want to verify the package. visit km000003948 to download the ot-package-sign.pub key and follow the instructions in the doc to verify the esm helm package. run the following command t",
    "keywordsLower": [
      "warning.go",
      "https://<external_access_host>/bo",
      "sign.pub",
      "upgrade.from",
      "ok.you",
      "x.zip",
      "x.tgz",
      "values.yaml",
      "package.esm",
      "install",
      "esm",
      "patch",
      "back",
      "prepare",
      "cli",
      "apphub",
      "verify",
      "page",
      "contains",
      "steps",
      "apply",
      "helm",
      "chart",
      "deployment",
      "either",
      "omt",
      "embedded",
      "kubernetes",
      "managed",
      "kubernetes.",
      "detailed",
      "information",
      "about",
      "fixed",
      "issues",
      "name",
      "download",
      "link",
      "see",
      "corresponding",
      "release",
      "notes.",
      "system",
      "velero.",
      "details",
      "sma",
      "backup",
      "procedure.",
      "follow",
      "installation",
      "package",
      "following",
      "command",
      "unzip",
      "-d",
      "unzipped",
      "file",
      "files",
      "esm-1.x.x",
      "2x.x.x.zip",
      "suite",
      "package.esm-1.x.x",
      "2x.x.x.zip.sig",
      "signature",
      "package.",
      "optional",
      "skip",
      "step",
      "don",
      "want",
      "visit",
      "km000003948",
      "ot-package-sign.pub",
      "key",
      "instructions",
      "doc",
      "run",
      "cd",
      "charts",
      "need",
      "2x.x.x.tgz",
      "2x.x.x.tgz.prov.",
      "samples",
      "sample.",
      "sample",
      "includes",
      "definition",
      "help",
      "customization",
      "file.",
      "scripts",
      "configuration",
      "scripts.",
      "container",
      "images",
      "images.",
      "section",
      "according",
      "deployment.",
      "remember"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install a UD/UCMDB patch (embedded Kubernetes)",
    "content": "This page contains the steps to apply a UD/UCMDB patch to an on-premises environment. For detailed information about fixed issues, patch name, and download link, see the corresponding patch release notes. The UD/UCMDB GA version update includes a class model update, which may take hours to complete in the backend. To ensure a smooth class model update, either perform the patch version update at least one day after the GA version update, or you can directly upgrade to the latest patch version if you haven't already upgraded to the GA version. Back up To back up your UD/UCMDB deployment, see Back up containerized UD/UCMDB deployment. Install patch To install a UD/UCMDB patch, follow these steps: Extract the downloaded package and obtain the patch file (ucmdb-1.nn.n-nnn+2n.n.n.tgz) from the \\ucmdb-helm-charts-2n.n\\ucmdb-helm-charts\\charts directory. Prepare the container images by following the instructions in Download container images for UD/UCMDB. Run the following command to apply the ",
    "url": "installcmspatch2611",
    "filename": "installcmspatch2611",
    "headings": [
      "Back up",
      "Install patch",
      "Verify",
      "Roll back"
    ],
    "keywords": [
      "uducmdb",
      "kubernetes.io",
      "n.tgz",
      "1.nn",
      "meta.helm",
      "n.Pn",
      "values.yaml",
      "install",
      "ud",
      "ucmdb",
      "patch",
      "embedded",
      "kubernetes",
      "back",
      "verify",
      "roll",
      "page",
      "contains",
      "steps",
      "apply",
      "on-premises",
      "environment.",
      "detailed",
      "information",
      "about",
      "fixed",
      "issues",
      "name",
      "download",
      "link",
      "see",
      "corresponding",
      "release",
      "notes.",
      "ga",
      "version",
      "update",
      "includes",
      "class",
      "model",
      "take",
      "hours",
      "complete",
      "backend.",
      "ensure",
      "smooth",
      "either",
      "perform",
      "least",
      "one",
      "day",
      "after",
      "directly",
      "upgrade",
      "latest",
      "haven",
      "already",
      "upgraded",
      "version.",
      "deployment",
      "containerized",
      "deployment.",
      "follow",
      "extract",
      "downloaded",
      "package",
      "obtain",
      "file",
      "ucmdb-1.nn.n-nnn",
      "2n.n.n.tgz",
      "ucmdb-helm-charts-2n.n",
      "ucmdb-helm-charts",
      "charts",
      "directory.",
      "prepare",
      "container",
      "images",
      "following",
      "instructions",
      "ucmdb.",
      "run",
      "command",
      "helm",
      "-n",
      "-f",
      "list",
      "awk",
      "print",
      "patch.",
      "unique",
      "namespace",
      "custom",
      "my-values.yaml",
      "properties",
      "configured",
      "current",
      "don",
      "remember",
      "path",
      "create"
    ],
    "language": "en",
    "word_count": 115,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install a ud/ucmdb patch (embedded kubernetes)",
    "contentLower": "this page contains the steps to apply a ud/ucmdb patch to an on-premises environment. for detailed information about fixed issues, patch name, and download link, see the corresponding patch release notes. the ud/ucmdb ga version update includes a class model update, which may take hours to complete in the backend. to ensure a smooth class model update, either perform the patch version update at least one day after the ga version update, or you can directly upgrade to the latest patch version if you haven't already upgraded to the ga version. back up to back up your ud/ucmdb deployment, see back up containerized ud/ucmdb deployment. install patch to install a ud/ucmdb patch, follow these steps: extract the downloaded package and obtain the patch file (ucmdb-1.nn.n-nnn+2n.n.n.tgz) from the \\ucmdb-helm-charts-2n.n\\ucmdb-helm-charts\\charts directory. prepare the container images by following the instructions in download container images for ud/ucmdb. run the following command to apply the ",
    "keywordsLower": [
      "uducmdb",
      "kubernetes.io",
      "n.tgz",
      "1.nn",
      "meta.helm",
      "n.pn",
      "values.yaml",
      "install",
      "ud",
      "ucmdb",
      "patch",
      "embedded",
      "kubernetes",
      "back",
      "verify",
      "roll",
      "page",
      "contains",
      "steps",
      "apply",
      "on-premises",
      "environment.",
      "detailed",
      "information",
      "about",
      "fixed",
      "issues",
      "name",
      "download",
      "link",
      "see",
      "corresponding",
      "release",
      "notes.",
      "ga",
      "version",
      "update",
      "includes",
      "class",
      "model",
      "take",
      "hours",
      "complete",
      "backend.",
      "ensure",
      "smooth",
      "either",
      "perform",
      "least",
      "one",
      "day",
      "after",
      "directly",
      "upgrade",
      "latest",
      "haven",
      "already",
      "upgraded",
      "version.",
      "deployment",
      "containerized",
      "deployment.",
      "follow",
      "extract",
      "downloaded",
      "package",
      "obtain",
      "file",
      "ucmdb-1.nn.n-nnn",
      "2n.n.n.tgz",
      "ucmdb-helm-charts-2n.n",
      "ucmdb-helm-charts",
      "charts",
      "directory.",
      "prepare",
      "container",
      "images",
      "following",
      "instructions",
      "ucmdb.",
      "run",
      "command",
      "helm",
      "-n",
      "-f",
      "list",
      "awk",
      "print",
      "patch.",
      "unique",
      "namespace",
      "custom",
      "my-values.yaml",
      "properties",
      "configured",
      "current",
      "don",
      "remember",
      "path",
      "create"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install a UD/UCMDB patch (managed Kubernetes)",
    "content": "This page contains the steps to apply a UD/UCMDB patch to a managed Kubernetes (Azure with AKS, AWS with EKS, or OpenShift) environment. For detailed information about fixed issues, patch name, and download link, see the corresponding patch release notes. The UD/UCMDB GA version update includes a class model update, which may take hours to complete in the backend. To ensure a smooth class model update, either perform the patch version update at least one day after the GA version update, or you can directly upgrade to the latest patch version if you haven't already upgraded to the GA version. Back up To back up your UD/UCMDB deployment, see Back up containerized UD/UCMDB deployment. Install To install a UD/UCMDB patch, follow these steps: Extract the downloaded package and obtain the patch file (ucmdb-1.nn.n-nnn+2n.n.n.tgz) in the \\ucmdb-helm-charts-2n.n\\ucmdb-helm-charts\\charts directory. Prepare the container images: EKS: Follow the instructions in Download container images for UD/UCM",
    "url": "installcmspatchmanagedk8s2611",
    "filename": "installcmspatchmanagedk8s2611",
    "headings": [
      "Back up",
      "Install",
      "Verify",
      "Roll back"
    ],
    "keywords": [
      "uducmdb",
      "kubernetes.io",
      "n.tgz",
      "1.nn",
      "meta.helm",
      "n.Pn",
      "values.yaml",
      "install",
      "ud",
      "ucmdb",
      "patch",
      "managed",
      "kubernetes",
      "back",
      "verify",
      "roll",
      "page",
      "contains",
      "steps",
      "apply",
      "azure",
      "aks",
      "aws",
      "eks",
      "openshift",
      "environment.",
      "detailed",
      "information",
      "about",
      "fixed",
      "issues",
      "name",
      "download",
      "link",
      "see",
      "corresponding",
      "release",
      "notes.",
      "ga",
      "version",
      "update",
      "includes",
      "class",
      "model",
      "take",
      "hours",
      "complete",
      "backend.",
      "ensure",
      "smooth",
      "either",
      "perform",
      "least",
      "one",
      "day",
      "after",
      "directly",
      "upgrade",
      "latest",
      "haven",
      "already",
      "upgraded",
      "version.",
      "deployment",
      "containerized",
      "deployment.",
      "follow",
      "extract",
      "downloaded",
      "package",
      "obtain",
      "file",
      "ucmdb-1.nn.n-nnn",
      "2n.n.n.tgz",
      "ucmdb-helm-charts-2n.n",
      "ucmdb-helm-charts",
      "charts",
      "directory.",
      "prepare",
      "container",
      "images",
      "instructions",
      "bastion",
      "node",
      "change",
      "working",
      "directory",
      "omt",
      "cd",
      "bin",
      "run",
      "following",
      "command",
      "helm",
      "-n",
      "-f",
      "list",
      "awk",
      "print",
      "patch."
    ],
    "language": "en",
    "word_count": 117,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install a ud/ucmdb patch (managed kubernetes)",
    "contentLower": "this page contains the steps to apply a ud/ucmdb patch to a managed kubernetes (azure with aks, aws with eks, or openshift) environment. for detailed information about fixed issues, patch name, and download link, see the corresponding patch release notes. the ud/ucmdb ga version update includes a class model update, which may take hours to complete in the backend. to ensure a smooth class model update, either perform the patch version update at least one day after the ga version update, or you can directly upgrade to the latest patch version if you haven't already upgraded to the ga version. back up to back up your ud/ucmdb deployment, see back up containerized ud/ucmdb deployment. install to install a ud/ucmdb patch, follow these steps: extract the downloaded package and obtain the patch file (ucmdb-1.nn.n-nnn+2n.n.n.tgz) in the \\ucmdb-helm-charts-2n.n\\ucmdb-helm-charts\\charts directory. prepare the container images: eks: follow the instructions in download container images for ud/ucm",
    "keywordsLower": [
      "uducmdb",
      "kubernetes.io",
      "n.tgz",
      "1.nn",
      "meta.helm",
      "n.pn",
      "values.yaml",
      "install",
      "ud",
      "ucmdb",
      "patch",
      "managed",
      "kubernetes",
      "back",
      "verify",
      "roll",
      "page",
      "contains",
      "steps",
      "apply",
      "azure",
      "aks",
      "aws",
      "eks",
      "openshift",
      "environment.",
      "detailed",
      "information",
      "about",
      "fixed",
      "issues",
      "name",
      "download",
      "link",
      "see",
      "corresponding",
      "release",
      "notes.",
      "ga",
      "version",
      "update",
      "includes",
      "class",
      "model",
      "take",
      "hours",
      "complete",
      "backend.",
      "ensure",
      "smooth",
      "either",
      "perform",
      "least",
      "one",
      "day",
      "after",
      "directly",
      "upgrade",
      "latest",
      "haven",
      "already",
      "upgraded",
      "version.",
      "deployment",
      "containerized",
      "deployment.",
      "follow",
      "extract",
      "downloaded",
      "package",
      "obtain",
      "file",
      "ucmdb-1.nn.n-nnn",
      "2n.n.n.tgz",
      "ucmdb-helm-charts-2n.n",
      "ucmdb-helm-charts",
      "charts",
      "directory.",
      "prepare",
      "container",
      "images",
      "instructions",
      "bastion",
      "node",
      "change",
      "working",
      "directory",
      "omt",
      "cd",
      "bin",
      "run",
      "following",
      "command",
      "helm",
      "-n",
      "-f",
      "list",
      "awk",
      "print",
      "patch."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install an OO Containerized patch",
    "content": "The OO Containerized patch is a Helm chart deployed on top of an existing OO Containerized installation. Download the patch package The Helm chart package includes the oo-helm-charts-oo-1.x.x-2x.x.x/oo-helm-charts/charts/oo-1.x.x+2x.x.x.tgz file. After downloading the zip package on the control plane node/bastion host, unzip the package: unzip oo-helm-charts-1.x.x-2x.x.x.zip Get the values.yaml file of the existing deployment Run the following command to get values.yaml file from the previous OO deployment. helm get values <oo_install_name> -n <oo_namespace> > <previous_deployment_values.yaml> Download the required container images If you deployed the suite on-premises: Follow the steps provided in the topic Download OO container images, by generating the download bundle for the OO Containerized Helm chart.If you deployed the suite on a managed Kubernetes: EKS: Follow the steps provided in the topic Download and upload OO container images by generating the download bundle for the OO Co",
    "url": "installoopatch2611",
    "filename": "installoopatch2611",
    "headings": [
      "Download the patch package",
      "Get the values.yaml file of the existing deployment",
      "Download the required container images",
      "Apply the patch",
      "Upgrade OO RAS and OO Designer"
    ],
    "keywords": [
      "previous_deployment_values.yaml",
      "service.beta",
      "kubernetes.io",
      "chart.If",
      "x.zip",
      "https://<EXTERNAL_ACCESS_HOST>:<EXTERNAL_ACCESS_PORT>/oo",
      "x.tgz",
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "patch",
      "download",
      "package",
      "get",
      "file",
      "existing",
      "deployment",
      "required",
      "container",
      "images",
      "apply",
      "upgrade",
      "ras",
      "designer",
      "helm",
      "chart",
      "deployed",
      "top",
      "installation.",
      "includes",
      "oo-helm-charts-oo-1.x.x-2x.x.x",
      "oo-helm-charts",
      "charts",
      "oo-1.x.x",
      "2x.x.x.tgz",
      "file.",
      "after",
      "downloading",
      "zip",
      "control",
      "plane",
      "node",
      "bastion",
      "host",
      "unzip",
      "oo-helm-charts-1.x.x-2x.x.x.zip",
      "run",
      "following",
      "command",
      "previous",
      "deployment.",
      "values",
      "-n",
      "suite",
      "on-premises",
      "follow",
      "steps",
      "provided",
      "topic",
      "generating",
      "bundle",
      "managed",
      "kubernetes",
      "eks",
      "upload",
      "chart.",
      "aks",
      "upgrading",
      "consists",
      "three",
      "stages",
      "pre-upgrade",
      "hooks",
      "jobs",
      "first.",
      "responsible",
      "databases",
      "components",
      "tailoring",
      "idm",
      "resources.",
      "successfully",
      "process",
      "continues",
      "rolling",
      "out",
      "new",
      "version",
      "pods.",
      "fail",
      "stops.rolling",
      "update",
      "stage",
      "pods",
      "upgraded",
      "through",
      "strategy.",
      "post",
      "job"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install an oo containerized patch",
    "contentLower": "the oo containerized patch is a helm chart deployed on top of an existing oo containerized installation. download the patch package the helm chart package includes the oo-helm-charts-oo-1.x.x-2x.x.x/oo-helm-charts/charts/oo-1.x.x+2x.x.x.tgz file. after downloading the zip package on the control plane node/bastion host, unzip the package: unzip oo-helm-charts-1.x.x-2x.x.x.zip get the values.yaml file of the existing deployment run the following command to get values.yaml file from the previous oo deployment. helm get values <oo_install_name> -n <oo_namespace> > <previous_deployment_values.yaml> download the required container images if you deployed the suite on-premises: follow the steps provided in the topic download oo container images, by generating the download bundle for the oo containerized helm chart.if you deployed the suite on a managed kubernetes: eks: follow the steps provided in the topic download and upload oo container images by generating the download bundle for the oo co",
    "keywordsLower": [
      "previous_deployment_values.yaml",
      "service.beta",
      "kubernetes.io",
      "chart.if",
      "x.zip",
      "https://<external_access_host>:<external_access_port>/oo",
      "x.tgz",
      "values.yaml",
      "install",
      "oo",
      "containerized",
      "patch",
      "download",
      "package",
      "get",
      "file",
      "existing",
      "deployment",
      "required",
      "container",
      "images",
      "apply",
      "upgrade",
      "ras",
      "designer",
      "helm",
      "chart",
      "deployed",
      "top",
      "installation.",
      "includes",
      "oo-helm-charts-oo-1.x.x-2x.x.x",
      "oo-helm-charts",
      "charts",
      "oo-1.x.x",
      "2x.x.x.tgz",
      "file.",
      "after",
      "downloading",
      "zip",
      "control",
      "plane",
      "node",
      "bastion",
      "host",
      "unzip",
      "oo-helm-charts-1.x.x-2x.x.x.zip",
      "run",
      "following",
      "command",
      "previous",
      "deployment.",
      "values",
      "-n",
      "suite",
      "on-premises",
      "follow",
      "steps",
      "provided",
      "topic",
      "generating",
      "bundle",
      "managed",
      "kubernetes",
      "eks",
      "upload",
      "chart.",
      "aks",
      "upgrading",
      "consists",
      "three",
      "stages",
      "pre-upgrade",
      "hooks",
      "jobs",
      "first.",
      "responsible",
      "databases",
      "components",
      "tailoring",
      "idm",
      "resources.",
      "successfully",
      "process",
      "continues",
      "rolling",
      "out",
      "new",
      "version",
      "pods.",
      "fail",
      "stops.rolling",
      "update",
      "stage",
      "pods",
      "upgraded",
      "through",
      "strategy.",
      "post",
      "job"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install an Audit patch (embedded Kubernetes)",
    "content": "This page contains the steps to apply an Audit service patch and an Audit collector patch to an on-premises environment. You need to apply both the Audit service patch and the Audit collector patch. For detailed information about Audit fixed issues, Audit service and Audit collector patch download links, and Audit service and Audit collector patch filenames, see the corresponding main patch release notes. Back up Audit service This section gives the steps to back up Audit files and data: Back up audit database. For instructions about backing up PostgreSQL database, see the PostgreSQL documentation. Back up all the configuration files that you are using. For example, values.yaml file, audit-secret.yaml, chart tar file, and certificates. Back up the itom-audit-pv.yaml file. Back up the following data on the NFS volumes for the Audit deployment: Component NFS volume name Description Example directory path Audit as-vault-volume-<Audit Namespace> Stores Audit configuration files. /var/vols/",
    "url": "installauditpatch2611",
    "filename": "installauditpatch2611",
    "headings": [
      "Back up Audit service",
      "Prepare the Audit service patch",
      "Deploy the Audit service patch",
      "Verify the Audit service patch installation",
      "Audit service roll back",
      "Deploy the Audit collector patch",
      "Audit collector roll back"
    ],
    "keywords": [
      "cert_two.crt",
      "running.Run",
      "x.xxx",
      "RE_ca_db.crt",
      "cert_one.crt",
      "idm.crt",
      "backup.yaml",
      "pv.yaml",
      "RE_ca_idm.crt",
      "ase.crt",
      "caCertificates.idm",
      "secrets.yaml",
      "rootCA.crt",
      "secret.yaml",
      "customCA.crt",
      "values.yaml",
      "install",
      "audit",
      "patch",
      "embedded",
      "kubernetes",
      "back",
      "service",
      "prepare",
      "deploy",
      "verify",
      "installation",
      "roll",
      "collector",
      "page",
      "contains",
      "steps",
      "apply",
      "on-premises",
      "environment.",
      "need",
      "both",
      "patch.",
      "detailed",
      "information",
      "about",
      "fixed",
      "issues",
      "download",
      "links",
      "filenames",
      "see",
      "corresponding",
      "main",
      "release",
      "notes.",
      "section",
      "gives",
      "files",
      "data",
      "database.",
      "instructions",
      "backing",
      "postgresql",
      "database",
      "documentation.",
      "all",
      "configuration",
      "using.",
      "example",
      "file",
      "audit-secret.yaml",
      "chart",
      "tar",
      "certificates.",
      "itom-audit-pv.yaml",
      "file.",
      "following",
      "nfs",
      "volumes",
      "deployment",
      "component",
      "volume",
      "name",
      "description",
      "directory",
      "path",
      "as-vault-volume-",
      "stores",
      "files.",
      "var",
      "vols",
      "itom",
      "auditns",
      "vault",
      "as-log-volume-",
      "logs",
      "generated",
      "audit.",
      "log",
      "control",
      "plane",
      "node.",
      "after",
      "follow"
    ],
    "language": "en",
    "word_count": 111,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install an audit patch (embedded kubernetes)",
    "contentLower": "this page contains the steps to apply an audit service patch and an audit collector patch to an on-premises environment. you need to apply both the audit service patch and the audit collector patch. for detailed information about audit fixed issues, audit service and audit collector patch download links, and audit service and audit collector patch filenames, see the corresponding main patch release notes. back up audit service this section gives the steps to back up audit files and data: back up audit database. for instructions about backing up postgresql database, see the postgresql documentation. back up all the configuration files that you are using. for example, values.yaml file, audit-secret.yaml, chart tar file, and certificates. back up the itom-audit-pv.yaml file. back up the following data on the nfs volumes for the audit deployment: component nfs volume name description example directory path audit as-vault-volume-<audit namespace> stores audit configuration files. /var/vols/",
    "keywordsLower": [
      "cert_two.crt",
      "running.run",
      "x.xxx",
      "re_ca_db.crt",
      "cert_one.crt",
      "idm.crt",
      "backup.yaml",
      "pv.yaml",
      "re_ca_idm.crt",
      "ase.crt",
      "cacertificates.idm",
      "secrets.yaml",
      "rootca.crt",
      "secret.yaml",
      "customca.crt",
      "values.yaml",
      "install",
      "audit",
      "patch",
      "embedded",
      "kubernetes",
      "back",
      "service",
      "prepare",
      "deploy",
      "verify",
      "installation",
      "roll",
      "collector",
      "page",
      "contains",
      "steps",
      "apply",
      "on-premises",
      "environment.",
      "need",
      "both",
      "patch.",
      "detailed",
      "information",
      "about",
      "fixed",
      "issues",
      "download",
      "links",
      "filenames",
      "see",
      "corresponding",
      "main",
      "release",
      "notes.",
      "section",
      "gives",
      "files",
      "data",
      "database.",
      "instructions",
      "backing",
      "postgresql",
      "database",
      "documentation.",
      "all",
      "configuration",
      "using.",
      "example",
      "file",
      "audit-secret.yaml",
      "chart",
      "tar",
      "certificates.",
      "itom-audit-pv.yaml",
      "file.",
      "following",
      "nfs",
      "volumes",
      "deployment",
      "component",
      "volume",
      "name",
      "description",
      "directory",
      "path",
      "as-vault-volume-",
      "stores",
      "files.",
      "var",
      "vols",
      "itom",
      "auditns",
      "vault",
      "as-log-volume-",
      "logs",
      "generated",
      "audit.",
      "log",
      "control",
      "plane",
      "node.",
      "after",
      "follow"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Install an Audit patch (managed Kubernetes)",
    "content": "This page contains the steps to apply an Audit service patch and an Audit collector patch to a managed Kubernetes (AWS with EKS or OpenShift) environment. You need to apply both the Audit service patch and the Audit collector patch. For detailed information about Audit fixed issues, Audit service and Audit collector patch download links, and Audit service and Audit collector patch filenames, see the corresponding main patch release notes. Back up Audit service This section gives the steps to back up Audit files and data: Back up audit database. For instructions about backing up PostgreSQL database, see the PostgreSQL documentation. Back up all the configuration files that you are using. For example, values.yaml file, chart tar file, audit-secrets.yaml, and certificates. Back up the itom-audit-pv.yaml file. Back up the following data on the Amazon Elastic File System (EFS) volumes for Audit deployment: View Fullscreen Component EFS volume name Description Example directory path Audit as",
    "url": "installauditpatchmanagedk8s2611",
    "filename": "installauditpatchmanagedk8s2611",
    "headings": [
      "Back up Audit service",
      "Prepare the Audit service patch",
      "Deploy the Audit service patch",
      "Verify the Audit service patch installation",
      "Audit service roll back",
      "Deploy the Audit collector patch",
      "Audit collector roll back"
    ],
    "keywords": [
      "cert_three.crt",
      "secrets.yaml",
      "service.For",
      "x.xxx",
      "backup.yaml",
      "caCertificates.idm",
      "values.yaml",
      "RE_ca_intAlb.crt",
      "collector.For",
      "RE_ca_db.crt",
      "directory.For",
      "cert_two.crt",
      "images.On",
      "running.Run",
      "idm.crt",
      "cert_one.crt",
      "pv.yaml",
      "RE_ca_idm.crt",
      "ase.crt",
      "install",
      "audit",
      "patch",
      "managed",
      "kubernetes",
      "back",
      "service",
      "prepare",
      "deploy",
      "verify",
      "installation",
      "roll",
      "collector",
      "page",
      "contains",
      "steps",
      "apply",
      "aws",
      "eks",
      "openshift",
      "environment.",
      "need",
      "both",
      "patch.",
      "detailed",
      "information",
      "about",
      "fixed",
      "issues",
      "download",
      "links",
      "filenames",
      "see",
      "corresponding",
      "main",
      "release",
      "notes.",
      "section",
      "gives",
      "files",
      "data",
      "database.",
      "instructions",
      "backing",
      "postgresql",
      "database",
      "documentation.",
      "all",
      "configuration",
      "using.",
      "example",
      "file",
      "chart",
      "tar",
      "audit-secrets.yaml",
      "certificates.",
      "itom-audit-pv.yaml",
      "file.",
      "following",
      "amazon",
      "elastic",
      "system",
      "efs",
      "volumes",
      "deployment",
      "view",
      "fullscreen",
      "component",
      "volume",
      "name",
      "description",
      "directory",
      "path",
      "as-vault-volume-",
      "stores",
      "files.",
      "var",
      "vols",
      "itom",
      "auditns",
      "vault"
    ],
    "language": "en",
    "word_count": 114,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "install an audit patch (managed kubernetes)",
    "contentLower": "this page contains the steps to apply an audit service patch and an audit collector patch to a managed kubernetes (aws with eks or openshift) environment. you need to apply both the audit service patch and the audit collector patch. for detailed information about audit fixed issues, audit service and audit collector patch download links, and audit service and audit collector patch filenames, see the corresponding main patch release notes. back up audit service this section gives the steps to back up audit files and data: back up audit database. for instructions about backing up postgresql database, see the postgresql documentation. back up all the configuration files that you are using. for example, values.yaml file, chart tar file, audit-secrets.yaml, and certificates. back up the itom-audit-pv.yaml file. back up the following data on the amazon elastic file system (efs) volumes for audit deployment: view fullscreen component efs volume name description example directory path audit as",
    "keywordsLower": [
      "cert_three.crt",
      "secrets.yaml",
      "service.for",
      "x.xxx",
      "backup.yaml",
      "cacertificates.idm",
      "values.yaml",
      "re_ca_intalb.crt",
      "collector.for",
      "re_ca_db.crt",
      "directory.for",
      "cert_two.crt",
      "images.on",
      "running.run",
      "idm.crt",
      "cert_one.crt",
      "pv.yaml",
      "re_ca_idm.crt",
      "ase.crt",
      "install",
      "audit",
      "patch",
      "managed",
      "kubernetes",
      "back",
      "service",
      "prepare",
      "deploy",
      "verify",
      "installation",
      "roll",
      "collector",
      "page",
      "contains",
      "steps",
      "apply",
      "aws",
      "eks",
      "openshift",
      "environment.",
      "need",
      "both",
      "patch.",
      "detailed",
      "information",
      "about",
      "fixed",
      "issues",
      "download",
      "links",
      "filenames",
      "see",
      "corresponding",
      "main",
      "release",
      "notes.",
      "section",
      "gives",
      "files",
      "data",
      "database.",
      "instructions",
      "backing",
      "postgresql",
      "database",
      "documentation.",
      "all",
      "configuration",
      "using.",
      "example",
      "file",
      "chart",
      "tar",
      "audit-secrets.yaml",
      "certificates.",
      "itom-audit-pv.yaml",
      "file.",
      "following",
      "amazon",
      "elastic",
      "system",
      "efs",
      "volumes",
      "deployment",
      "view",
      "fullscreen",
      "component",
      "volume",
      "name",
      "description",
      "directory",
      "path",
      "as-vault-volume-",
      "stores",
      "files.",
      "var",
      "vols",
      "itom",
      "auditns",
      "vault"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate",
    "content": "Service Management supports integrations with the following products. For support matrix information about these integrations, visit the Integration Central website.You can also use Connect-It to integrate with other products. For more information about the Connect-It configuration, see Integrate with Connect-It.It's recommended to create a distinct user for each integration.For all integrations that use OPB, you need to create only one integration user for the OPB agent to access Service Management.An integration user that's defined in Service Management consumes a license unless it has the \"Integration user\" role defined in Suite Administration.For integration with authentication services such as LDAP, SAML, and OAUTH, see Configure authentication in IdM Admin Portal.OpenText integrationsYou can integrate the suite with the following OpenText products.ProductDescriptionMethodDocumentationALM OctaneEnables you to set up a bi-directional integration with Octane to submit Defects or Enh",
    "url": "integrations",
    "filename": "integrations",
    "headings": [
      "OpenText integrations",
      "Third-party integrations",
      "Related topics"
    ],
    "keywords": [
      "systems.For",
      "Administration.For",
      "Management.An",
      "repositories.IDOL",
      "website.You",
      "Orchestration.To",
      "integration.To",
      "integration.For",
      "It.It",
      "Content.IDOL",
      "integrate",
      "opentext",
      "integrations",
      "third-party",
      "related",
      "topics",
      "service",
      "management",
      "supports",
      "following",
      "products.",
      "support",
      "matrix",
      "information",
      "about",
      "visit",
      "integration",
      "central",
      "connect-it",
      "configuration",
      "see",
      "connect-it.it",
      "recommended",
      "create",
      "distinct",
      "user",
      "all",
      "opb",
      "need",
      "one",
      "agent",
      "access",
      "defined",
      "consumes",
      "license",
      "unless",
      "role",
      "suite",
      "authentication",
      "services",
      "such",
      "ldap",
      "saml",
      "oauth",
      "configure",
      "idm",
      "admin",
      "portal.opentext",
      "integrationsyou",
      "products.productdescriptionmethoddocumentationalm",
      "octaneenables",
      "set",
      "bi-directional",
      "octane",
      "submit",
      "defects",
      "enhancement",
      "requests",
      "portal",
      "record",
      "types.",
      "allows",
      "keep",
      "data",
      "sync",
      "both",
      "systems.integration",
      "studiointegrate",
      "alm",
      "octanealm",
      "quality",
      "centerenables",
      "center",
      "centerarcsight",
      "soarenables",
      "update",
      "cases",
      "arcsight",
      "soar.integration",
      "soarasset",
      "managerenables",
      "import",
      "demo",
      "asset",
      "manager.integration",
      "managerconnect-itenables",
      "products.n",
      "aintegrate",
      "connect-itcore",
      "contentenables"
    ],
    "language": "en",
    "word_count": 87,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate",
    "contentLower": "service management supports integrations with the following products. for support matrix information about these integrations, visit the integration central website.you can also use connect-it to integrate with other products. for more information about the connect-it configuration, see integrate with connect-it.it's recommended to create a distinct user for each integration.for all integrations that use opb, you need to create only one integration user for the opb agent to access service management.an integration user that's defined in service management consumes a license unless it has the \"integration user\" role defined in suite administration.for integration with authentication services such as ldap, saml, and oauth, see configure authentication in idm admin portal.opentext integrationsyou can integrate the suite with the following opentext products.productdescriptionmethoddocumentationalm octaneenables you to set up a bi-directional integration with octane to submit defects or enh",
    "keywordsLower": [
      "systems.for",
      "administration.for",
      "management.an",
      "repositories.idol",
      "website.you",
      "orchestration.to",
      "integration.to",
      "integration.for",
      "it.it",
      "content.idol",
      "integrate",
      "opentext",
      "integrations",
      "third-party",
      "related",
      "topics",
      "service",
      "management",
      "supports",
      "following",
      "products.",
      "support",
      "matrix",
      "information",
      "about",
      "visit",
      "integration",
      "central",
      "connect-it",
      "configuration",
      "see",
      "connect-it.it",
      "recommended",
      "create",
      "distinct",
      "user",
      "all",
      "opb",
      "need",
      "one",
      "agent",
      "access",
      "defined",
      "consumes",
      "license",
      "unless",
      "role",
      "suite",
      "authentication",
      "services",
      "such",
      "ldap",
      "saml",
      "oauth",
      "configure",
      "idm",
      "admin",
      "portal.opentext",
      "integrationsyou",
      "products.productdescriptionmethoddocumentationalm",
      "octaneenables",
      "set",
      "bi-directional",
      "octane",
      "submit",
      "defects",
      "enhancement",
      "requests",
      "portal",
      "record",
      "types.",
      "allows",
      "keep",
      "data",
      "sync",
      "both",
      "systems.integration",
      "studiointegrate",
      "alm",
      "octanealm",
      "quality",
      "centerenables",
      "center",
      "centerarcsight",
      "soarenables",
      "update",
      "cases",
      "arcsight",
      "soar.integration",
      "soarasset",
      "managerenables",
      "import",
      "demo",
      "asset",
      "manager.integration",
      "managerconnect-itenables",
      "products.n",
      "aintegrate",
      "connect-itcore",
      "contentenables"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Management",
    "content": "The Integration Management module includes the following integration platforms: The On-Premises Bridge Agents and Endpoints for communication between on-premises applications and Service Management Automation. For more information, see On-Premises Bridge Agents and Endpoints. External Systems for the Service Manager and Operations Manager i integrations to exchange data between Service Management Automation and other products. For more information, see External systems. External Configurations to help simplify the integrations with external systems. For more information, see External configurations. BI Integration to enable you to export data to an external business intelligence system. For more information, see Business Intelligence integration. Integration Studio to enable you to set up integrations based on the Integration Studio. For more information, see Integration Studio. Related topics How to use endpoints On-Premises Bridge security additional information",
    "url": "integrationmgmt",
    "filename": "integrationmgmt",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "integration",
      "management",
      "related",
      "topics",
      "module",
      "includes",
      "following",
      "platforms",
      "on-premises",
      "bridge",
      "agents",
      "endpoints",
      "communication",
      "between",
      "applications",
      "service",
      "automation.",
      "information",
      "see",
      "endpoints.",
      "external",
      "systems",
      "manager",
      "operations",
      "integrations",
      "exchange",
      "data",
      "automation",
      "products.",
      "systems.",
      "configurations",
      "help",
      "simplify",
      "configurations.",
      "bi",
      "enable",
      "export",
      "business",
      "intelligence",
      "system.",
      "integration.",
      "studio",
      "set",
      "based",
      "studio.",
      "security",
      "additional"
    ],
    "language": "en",
    "word_count": 89,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration management",
    "contentLower": "the integration management module includes the following integration platforms: the on-premises bridge agents and endpoints for communication between on-premises applications and service management automation. for more information, see on-premises bridge agents and endpoints. external systems for the service manager and operations manager i integrations to exchange data between service management automation and other products. for more information, see external systems. external configurations to help simplify the integrations with external systems. for more information, see external configurations. bi integration to enable you to export data to an external business intelligence system. for more information, see business intelligence integration. integration studio to enable you to set up integrations based on the integration studio. for more information, see integration studio. related topics how to use endpoints on-premises bridge security additional information",
    "keywordsLower": [
      "integration",
      "management",
      "related",
      "topics",
      "module",
      "includes",
      "following",
      "platforms",
      "on-premises",
      "bridge",
      "agents",
      "endpoints",
      "communication",
      "between",
      "applications",
      "service",
      "automation.",
      "information",
      "see",
      "endpoints.",
      "external",
      "systems",
      "manager",
      "operations",
      "integrations",
      "exchange",
      "data",
      "automation",
      "products.",
      "systems.",
      "configurations",
      "help",
      "simplify",
      "configurations.",
      "bi",
      "enable",
      "export",
      "business",
      "intelligence",
      "system.",
      "integration.",
      "studio",
      "set",
      "based",
      "studio.",
      "security",
      "additional"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio",
    "content": "The Integration Studio is a built-in integration platform that enables you to set up an integration with another Service Management system or an external application. Complete the following steps to access the Integration Studio: Log in to the Agent Interface. Go to Administration > Utilities > Integration. In the Integration Management list, select Integration Studio. To use the Integration Studio, you must have the Integration Administrator role. For example, to create or configure Integration Studio-related endpoints, integrations, and scenarios, you must have the Integration Administrator role. The Integration Studio has the following capabilities: On-Premises Bridge (OPB) Agent support: Enables you to specify an OPB agent for the endpoint systems so that Service Management can connect to systems behind a firewall. Built-in platform: Enables you to manage case exchange type of integrations without the involvement of third-party products. Codeless: Allows you to build integrations w",
    "url": "integrationhub",
    "filename": "integrationhub",
    "headings": [
      "Main components",
      "Architecture diagram",
      "Data flow",
      "Integration modes",
      "Limitations"
    ],
    "keywords": [
      "integration",
      "studio",
      "main",
      "components",
      "architecture",
      "diagram",
      "data",
      "flow",
      "modes",
      "limitations",
      "built-in",
      "platform",
      "enables",
      "set",
      "another",
      "service",
      "management",
      "system",
      "external",
      "application.",
      "complete",
      "following",
      "steps",
      "access",
      "log",
      "agent",
      "interface.",
      "go",
      "administration",
      "utilities",
      "integration.",
      "list",
      "select",
      "studio.",
      "administrator",
      "role.",
      "example",
      "create",
      "configure",
      "studio-related",
      "endpoints",
      "integrations",
      "scenarios",
      "capabilities",
      "on-premises",
      "bridge",
      "opb",
      "support",
      "specify",
      "endpoint",
      "systems",
      "connect",
      "behind",
      "firewall.",
      "manage",
      "case",
      "exchange",
      "type",
      "involvement",
      "third-party",
      "products.",
      "codeless",
      "allows",
      "build",
      "need",
      "write",
      "complex",
      "code",
      "logic.",
      "modern",
      "user",
      "interface",
      "provides",
      "simple",
      "intuitive",
      "rich",
      "integrations.",
      "predefined",
      "templates",
      "comes",
      "facilitate",
      "creation.",
      "table",
      "describes",
      "component",
      "description",
      "defines",
      "details",
      "application",
      "including",
      "authentication",
      "certificates",
      "transport",
      "protocols",
      "base",
      "urls",
      "on.",
      "connector",
      "metadata-driven",
      "definition"
    ],
    "language": "en",
    "word_count": 88,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio",
    "contentLower": "the integration studio is a built-in integration platform that enables you to set up an integration with another service management system or an external application. complete the following steps to access the integration studio: log in to the agent interface. go to administration > utilities > integration. in the integration management list, select integration studio. to use the integration studio, you must have the integration administrator role. for example, to create or configure integration studio-related endpoints, integrations, and scenarios, you must have the integration administrator role. the integration studio has the following capabilities: on-premises bridge (opb) agent support: enables you to specify an opb agent for the endpoint systems so that service management can connect to systems behind a firewall. built-in platform: enables you to manage case exchange type of integrations without the involvement of third-party products. codeless: allows you to build integrations w",
    "keywordsLower": [
      "integration",
      "studio",
      "main",
      "components",
      "architecture",
      "diagram",
      "data",
      "flow",
      "modes",
      "limitations",
      "built-in",
      "platform",
      "enables",
      "set",
      "another",
      "service",
      "management",
      "system",
      "external",
      "application.",
      "complete",
      "following",
      "steps",
      "access",
      "log",
      "agent",
      "interface.",
      "go",
      "administration",
      "utilities",
      "integration.",
      "list",
      "select",
      "studio.",
      "administrator",
      "role.",
      "example",
      "create",
      "configure",
      "studio-related",
      "endpoints",
      "integrations",
      "scenarios",
      "capabilities",
      "on-premises",
      "bridge",
      "opb",
      "support",
      "specify",
      "endpoint",
      "systems",
      "connect",
      "behind",
      "firewall.",
      "manage",
      "case",
      "exchange",
      "type",
      "involvement",
      "third-party",
      "products.",
      "codeless",
      "allows",
      "build",
      "need",
      "write",
      "complex",
      "code",
      "logic.",
      "modern",
      "user",
      "interface",
      "provides",
      "simple",
      "intuitive",
      "rich",
      "integrations.",
      "predefined",
      "templates",
      "comes",
      "facilitate",
      "creation.",
      "table",
      "describes",
      "component",
      "description",
      "defines",
      "details",
      "application",
      "including",
      "authentication",
      "certificates",
      "transport",
      "protocols",
      "base",
      "urls",
      "on.",
      "connector",
      "metadata-driven",
      "definition"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Listener",
    "content": "Listeners provide a mechanism to trigger integration scenarios from external systems. For each listener-enabled scenario, the Integration Studio exposes a webhook URL for external systems to call, and constantly listens for requests sent to the webhook URL. Once a request is received at the URL, the Integration Studio will authenticate the user data contained in the request, and if the authentication and various validations pass, execute the scenario. Set up the listener What external systems can use listeners To use listeners, the external system must have a mechanism (usually called webhook or callback) to push messages to a specified URL when certain events occur. The messages the external system sends must satisfy the following conditions: The messages must be sent with the HTTP POST method. The HTTP request must contain a content-type header with one of the following values: application/json: For sending data in JSON format. application/x-www-form-urlencoded: For sending sending f",
    "url": "xielistener",
    "filename": "xielistener",
    "headings": [
      "Set up the listener",
      "What external systems can use listeners",
      "Configure the listener",
      "Configure token expiration reminder",
      "Delete a token",
      "Regenerate an expired token",
      "Verify the listener configuration",
      "Configure webhooks in external systems to trigger listener-based scenarios",
      "Webhook with URL token example - Jira webhook configuration",
      "Webhook with basic authentication example - Octane webhook configuration",
      "Use payload data sent by the external system in scenario rules",
      "Send responses with payload data to the external system in scenario rules",
      "Monitor external system-triggered scenarios",
      "Disable the listener"
    ],
    "keywords": [
      "key1.key2",
      "payload.data",
      "listener",
      "set",
      "what",
      "external",
      "systems",
      "listeners",
      "configure",
      "token",
      "expiration",
      "reminder",
      "delete",
      "regenerate",
      "expired",
      "verify",
      "configuration",
      "webhooks",
      "trigger",
      "listener-based",
      "scenarios",
      "webhook",
      "url",
      "example",
      "jira",
      "basic",
      "authentication",
      "octane",
      "payload",
      "data",
      "sent",
      "system",
      "scenario",
      "rules",
      "send",
      "responses",
      "monitor",
      "system-triggered",
      "disable",
      "provide",
      "mechanism",
      "integration",
      "systems.",
      "listener-enabled",
      "studio",
      "exposes",
      "call",
      "constantly",
      "listens",
      "requests",
      "url.",
      "once",
      "request",
      "received",
      "authenticate",
      "user",
      "contained",
      "various",
      "validations",
      "pass",
      "execute",
      "scenario.",
      "usually",
      "called",
      "callback",
      "push",
      "messages",
      "specified",
      "certain",
      "events",
      "occur.",
      "sends",
      "satisfy",
      "following",
      "conditions",
      "http",
      "post",
      "method.",
      "contain",
      "content-type",
      "header",
      "one",
      "values",
      "application",
      "json",
      "sending",
      "format.",
      "x-www-form-urlencoded",
      "form",
      "such",
      "key-value",
      "pairs.",
      "text",
      "plain",
      "files.",
      "maximum",
      "size",
      "accepted",
      "100",
      "mb."
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "listener",
    "contentLower": "listeners provide a mechanism to trigger integration scenarios from external systems. for each listener-enabled scenario, the integration studio exposes a webhook url for external systems to call, and constantly listens for requests sent to the webhook url. once a request is received at the url, the integration studio will authenticate the user data contained in the request, and if the authentication and various validations pass, execute the scenario. set up the listener what external systems can use listeners to use listeners, the external system must have a mechanism (usually called webhook or callback) to push messages to a specified url when certain events occur. the messages the external system sends must satisfy the following conditions: the messages must be sent with the http post method. the http request must contain a content-type header with one of the following values: application/json: for sending data in json format. application/x-www-form-urlencoded: for sending sending f",
    "keywordsLower": [
      "key1.key2",
      "payload.data",
      "listener",
      "set",
      "what",
      "external",
      "systems",
      "listeners",
      "configure",
      "token",
      "expiration",
      "reminder",
      "delete",
      "regenerate",
      "expired",
      "verify",
      "configuration",
      "webhooks",
      "trigger",
      "listener-based",
      "scenarios",
      "webhook",
      "url",
      "example",
      "jira",
      "basic",
      "authentication",
      "octane",
      "payload",
      "data",
      "sent",
      "system",
      "scenario",
      "rules",
      "send",
      "responses",
      "monitor",
      "system-triggered",
      "disable",
      "provide",
      "mechanism",
      "integration",
      "systems.",
      "listener-enabled",
      "studio",
      "exposes",
      "call",
      "constantly",
      "listens",
      "requests",
      "url.",
      "once",
      "request",
      "received",
      "authenticate",
      "user",
      "contained",
      "various",
      "validations",
      "pass",
      "execute",
      "scenario.",
      "usually",
      "called",
      "callback",
      "push",
      "messages",
      "specified",
      "certain",
      "events",
      "occur.",
      "sends",
      "satisfy",
      "following",
      "conditions",
      "http",
      "post",
      "method.",
      "contain",
      "content-type",
      "header",
      "one",
      "values",
      "application",
      "json",
      "sending",
      "format.",
      "x-www-form-urlencoded",
      "form",
      "such",
      "key-value",
      "pairs.",
      "text",
      "plain",
      "files.",
      "maximum",
      "size",
      "accepted",
      "100",
      "mb."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "JSON payload expressions",
    "content": "Some scenario rule input parameters accept JSON data. The JSON payload expressions enable you to construct JSON data for such parameters. JSON payload expression syntax The form of the JSON payload expression differs, according to the location of the key inside the JSON data. If the key is at the root of the JSON data, the JSON payload expression is just the name of the key. Example To construct the following data: { \"Id\": 12345; \"Name\": \"Service 1\" \"Status\": \"Ready\" } Add the following key-value pairs: If the key is inside another JSON object (which in turn might be nested inside another JSON object), then use the following symbols and operators in the JSON payload expression: Enclose the JSON payload expression in angle brackets (<>). Include all ancestor objects of the key in the expression, and separate the objects and the key with the dot (.) operator. Example To construct the following data: { ... \"key1\": { \"key2\": { \"key3\": \"value3\" } } ... } Enter the following in the key and v",
    "url": "keyexpressionsxie",
    "filename": "keyexpressionsxie",
    "headings": [
      "JSON payload expression syntax",
      "Configuration best practices",
      "Verify JSON payload expressions by using Payload preview",
      "Escape special characters"
    ],
    "keywords": [
      "json",
      "payload",
      "expressions",
      "expression",
      "syntax",
      "configuration",
      "best",
      "practices",
      "verify",
      "preview",
      "escape",
      "special",
      "characters",
      "scenario",
      "rule",
      "input",
      "parameters",
      "accept",
      "data.",
      "enable",
      "construct",
      "data",
      "such",
      "parameters.",
      "form",
      "differs",
      "according",
      "location",
      "key",
      "inside",
      "root",
      "just",
      "name",
      "key.",
      "example",
      "following",
      "id",
      "12345",
      "service",
      "status",
      "ready",
      "add",
      "key-value",
      "pairs",
      "another",
      "object",
      "turn",
      "nested",
      "symbols",
      "operators",
      "enclose",
      "angle",
      "brackets",
      "include",
      "all",
      "ancestor",
      "objects",
      "separate",
      "dot",
      "operator.",
      "key1",
      "key2",
      "key3",
      "value3",
      "enter",
      "value",
      "fields",
      "current",
      "belongs",
      "array",
      "append",
      "after",
      "zero-based",
      "index",
      "array.",
      "keyx",
      "valuex",
      "key4",
      "value4",
      "two",
      "op",
      "path",
      "system.title",
      "null",
      "sample",
      "task",
      "system.description",
      "description",
      "note",
      "don",
      "need",
      "keys",
      "values",
      "quotes.",
      "integration",
      "studio",
      "adds",
      "quotes",
      "applicable",
      "output."
    ],
    "language": "en",
    "word_count": 107,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "json payload expressions",
    "contentLower": "some scenario rule input parameters accept json data. the json payload expressions enable you to construct json data for such parameters. json payload expression syntax the form of the json payload expression differs, according to the location of the key inside the json data. if the key is at the root of the json data, the json payload expression is just the name of the key. example to construct the following data: { \"id\": 12345; \"name\": \"service 1\" \"status\": \"ready\" } add the following key-value pairs: if the key is inside another json object (which in turn might be nested inside another json object), then use the following symbols and operators in the json payload expression: enclose the json payload expression in angle brackets (<>). include all ancestor objects of the key in the expression, and separate the objects and the key with the dot (.) operator. example to construct the following data: { ... \"key1\": { \"key2\": { \"key3\": \"value3\" } } ... } enter the following in the key and v",
    "keywordsLower": [
      "json",
      "payload",
      "expressions",
      "expression",
      "syntax",
      "configuration",
      "best",
      "practices",
      "verify",
      "preview",
      "escape",
      "special",
      "characters",
      "scenario",
      "rule",
      "input",
      "parameters",
      "accept",
      "data.",
      "enable",
      "construct",
      "data",
      "such",
      "parameters.",
      "form",
      "differs",
      "according",
      "location",
      "key",
      "inside",
      "root",
      "just",
      "name",
      "key.",
      "example",
      "following",
      "id",
      "12345",
      "service",
      "status",
      "ready",
      "add",
      "key-value",
      "pairs",
      "another",
      "object",
      "turn",
      "nested",
      "symbols",
      "operators",
      "enclose",
      "angle",
      "brackets",
      "include",
      "all",
      "ancestor",
      "objects",
      "separate",
      "dot",
      "operator.",
      "key1",
      "key2",
      "key3",
      "value3",
      "enter",
      "value",
      "fields",
      "current",
      "belongs",
      "array",
      "append",
      "after",
      "zero-based",
      "index",
      "array.",
      "keyx",
      "valuex",
      "key4",
      "value4",
      "two",
      "op",
      "path",
      "system.title",
      "null",
      "sample",
      "task",
      "system.description",
      "description",
      "note",
      "don",
      "need",
      "keys",
      "values",
      "quotes.",
      "integration",
      "studio",
      "adds",
      "quotes",
      "applicable",
      "output."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration with OpenText products",
    "content": "Service Management supports integration with the following OpenText products. For support matrix information about these integrations, visit the Integration Central website. You can also use Connect-It to integrate with other products. For more information about the Connect-It configuration, see Integrate with Connect-It. It's recommended to create a distinct user for each integration. For all integrations that use OPB, you need to create only one integration user for the OPB agent to access Service Management. An integration user that's defined in Service Management consumes a license unless it has the \"Integration user\" role defined in Suite Administration. You can integrate the suite with the following OpenText products. Product Description Method Documentation ALM Octane Enables you to set up a bi-directional integration with Octane to submit Defects or Enhancement requests from Service Portal or other Service Management record types. The integration also allows to keep the data in",
    "url": "mfintegrations",
    "filename": "mfintegrations",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "integration",
      "opentext",
      "products",
      "related",
      "topics",
      "service",
      "management",
      "supports",
      "following",
      "products.",
      "support",
      "matrix",
      "information",
      "about",
      "integrations",
      "visit",
      "central",
      "website.",
      "connect-it",
      "integrate",
      "configuration",
      "see",
      "connect-it.",
      "recommended",
      "create",
      "distinct",
      "user",
      "integration.",
      "all",
      "opb",
      "need",
      "one",
      "agent",
      "access",
      "management.",
      "defined",
      "consumes",
      "license",
      "unless",
      "role",
      "suite",
      "administration.",
      "product",
      "description",
      "method",
      "documentation",
      "alm",
      "octane",
      "enables",
      "set",
      "bi-directional",
      "submit",
      "defects",
      "enhancement",
      "requests",
      "portal",
      "record",
      "types.",
      "allows",
      "keep",
      "data",
      "sync",
      "both",
      "systems.",
      "studio",
      "quality",
      "center",
      "arcsight",
      "soar",
      "update",
      "cases",
      "soar.",
      "asset",
      "manager",
      "import",
      "demo",
      "manager.",
      "smax",
      "core",
      "content",
      "embed",
      "widget",
      "users",
      "manage",
      "documents",
      "content.",
      "index",
      "knowledge",
      "idol",
      "connector",
      "endpoint",
      "devices",
      "zenworks",
      "ucmdb",
      "sacm",
      "module.",
      "extended",
      "ecm",
      "repositories.",
      "hcm"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration with opentext products",
    "contentLower": "service management supports integration with the following opentext products. for support matrix information about these integrations, visit the integration central website. you can also use connect-it to integrate with other products. for more information about the connect-it configuration, see integrate with connect-it. it's recommended to create a distinct user for each integration. for all integrations that use opb, you need to create only one integration user for the opb agent to access service management. an integration user that's defined in service management consumes a license unless it has the \"integration user\" role defined in suite administration. you can integrate the suite with the following opentext products. product description method documentation alm octane enables you to set up a bi-directional integration with octane to submit defects or enhancement requests from service portal or other service management record types. the integration also allows to keep the data in",
    "keywordsLower": [
      "integration",
      "opentext",
      "products",
      "related",
      "topics",
      "service",
      "management",
      "supports",
      "following",
      "products.",
      "support",
      "matrix",
      "information",
      "about",
      "integrations",
      "visit",
      "central",
      "website.",
      "connect-it",
      "integrate",
      "configuration",
      "see",
      "connect-it.",
      "recommended",
      "create",
      "distinct",
      "user",
      "integration.",
      "all",
      "opb",
      "need",
      "one",
      "agent",
      "access",
      "management.",
      "defined",
      "consumes",
      "license",
      "unless",
      "role",
      "suite",
      "administration.",
      "product",
      "description",
      "method",
      "documentation",
      "alm",
      "octane",
      "enables",
      "set",
      "bi-directional",
      "submit",
      "defects",
      "enhancement",
      "requests",
      "portal",
      "record",
      "types.",
      "allows",
      "keep",
      "data",
      "sync",
      "both",
      "systems.",
      "studio",
      "quality",
      "center",
      "arcsight",
      "soar",
      "update",
      "cases",
      "soar.",
      "asset",
      "manager",
      "import",
      "demo",
      "manager.",
      "smax",
      "core",
      "content",
      "embed",
      "widget",
      "users",
      "manage",
      "documents",
      "content.",
      "index",
      "knowledge",
      "idol",
      "connector",
      "endpoint",
      "devices",
      "zenworks",
      "ucmdb",
      "sacm",
      "module.",
      "extended",
      "ecm",
      "repositories.",
      "hcm"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—Core Content integration",
    "content": "OpenText Core Content is a cloud-based content management and collaboration platform designed to help organizations manage and share their documents and content securely. The integration of SMAX with Core Content enables you to embed a Core Content widget in Service Management so that users can leverage the industry-leading content management capabilities powered by Core Content. Overview With this integration, SMAX users can click a Related documents tab in a record to launch an embedded Core Content widget, which offers the following benefits. Enterprise document management The embedded widget centralizes document storage and access. It ensures that all documents, regardless of their source, are stored securely in one location for easy retrieval and management. Workflow automation The embedded widget automates, manages, and tracks document-centric processes. It streamlines tasks, routes documents, assigns responsibilities, ensures compliance, and enhances collaboration for efficient ",
    "url": "integratecorecontent",
    "filename": "integratecorecontent",
    "headings": [
      "Overview",
      "Enterprise document management",
      "Workflow automation",
      "Version control",
      "Secure external sharing",
      "Set up the integration",
      "Prerequisites",
      "A valid Core Content subscription",
      "Client Id and client secret",
      "Single Sign-On (SSO)",
      "Configure in the Core Content Admin Center",
      "Obtain the authentication Id",
      "Obtain the subscription name or Id",
      "Add SMAX as a trusted site",
      "Configure in Core Content Administration",
      "Configure in Core Content",
      "Configure SMAX",
      "Prepare an integration user",
      "Configure an endpoint",
      "Create an integration"
    ],
    "keywords": [
      "studiocore",
      "microsoftonline.com",
      "ancestors.Add",
      "page.In",
      "integration.In",
      "https://<fully",
      "opentext.com",
      "pane.In",
      "administrator.Go",
      "src.Add",
      "location.Only",
      "WksTypeResponse.data",
      "https://<region>.api.opentext.com",
      "again.In",
      "https://<region>.api.opentext.com/tenants/<tenant_id>/oauth2/token",
      "2.0",
      "type.In",
      "https://na-1.api.opentext.com",
      "below.No",
      "Contract.Name",
      "1.api",
      "template.Only",
      "integration",
      "studio",
      "core",
      "content",
      "overview",
      "enterprise",
      "document",
      "management",
      "workflow",
      "automation",
      "version",
      "control",
      "secure",
      "external",
      "sharing",
      "set",
      "prerequisites",
      "valid",
      "subscription",
      "client",
      "id",
      "secret",
      "single",
      "sign-on",
      "sso",
      "configure",
      "admin",
      "center",
      "obtain",
      "authentication",
      "name",
      "add",
      "smax",
      "trusted",
      "site",
      "administration",
      "prepare",
      "user",
      "endpoint",
      "create",
      "scenarios",
      "customize",
      "scenario",
      "support",
      "attributes",
      "optional",
      "enable",
      "widget",
      "settings",
      "allow",
      "display",
      "manage",
      "access",
      "reconfigure",
      "record",
      "types",
      "after",
      "making",
      "changes",
      "troubleshoot",
      "opentext",
      "cloud-based",
      "collaboration",
      "platform",
      "designed",
      "help",
      "organizations",
      "share",
      "documents",
      "securely.",
      "enables",
      "embed",
      "service",
      "users",
      "leverage",
      "industry-leading",
      "capabilities",
      "powered"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—core content integration",
    "contentLower": "opentext core content is a cloud-based content management and collaboration platform designed to help organizations manage and share their documents and content securely. the integration of smax with core content enables you to embed a core content widget in service management so that users can leverage the industry-leading content management capabilities powered by core content. overview with this integration, smax users can click a related documents tab in a record to launch an embedded core content widget, which offers the following benefits. enterprise document management the embedded widget centralizes document storage and access. it ensures that all documents, regardless of their source, are stored securely in one location for easy retrieval and management. workflow automation the embedded widget automates, manages, and tracks document-centric processes. it streamlines tasks, routes documents, assigns responsibilities, ensures compliance, and enhances collaboration for efficient ",
    "keywordsLower": [
      "studiocore",
      "microsoftonline.com",
      "ancestors.add",
      "page.in",
      "integration.in",
      "https://<fully",
      "opentext.com",
      "pane.in",
      "administrator.go",
      "src.add",
      "location.only",
      "wkstyperesponse.data",
      "https://<region>.api.opentext.com",
      "again.in",
      "https://<region>.api.opentext.com/tenants/<tenant_id>/oauth2/token",
      "2.0",
      "type.in",
      "https://na-1.api.opentext.com",
      "below.no",
      "contract.name",
      "1.api",
      "template.only",
      "integration",
      "studio",
      "core",
      "content",
      "overview",
      "enterprise",
      "document",
      "management",
      "workflow",
      "automation",
      "version",
      "control",
      "secure",
      "external",
      "sharing",
      "set",
      "prerequisites",
      "valid",
      "subscription",
      "client",
      "id",
      "secret",
      "single",
      "sign-on",
      "sso",
      "configure",
      "admin",
      "center",
      "obtain",
      "authentication",
      "name",
      "add",
      "smax",
      "trusted",
      "site",
      "administration",
      "prepare",
      "user",
      "endpoint",
      "create",
      "scenarios",
      "customize",
      "scenario",
      "support",
      "attributes",
      "optional",
      "enable",
      "widget",
      "settings",
      "allow",
      "display",
      "manage",
      "access",
      "reconfigure",
      "record",
      "types",
      "after",
      "making",
      "changes",
      "troubleshoot",
      "opentext",
      "cloud-based",
      "collaboration",
      "platform",
      "designed",
      "help",
      "organizations",
      "share",
      "documents",
      "securely.",
      "enables",
      "embed",
      "service",
      "users",
      "leverage",
      "industry-leading",
      "capabilities",
      "powered"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—SPM/PPM integration",
    "content": "You can integrate Service Management with Strategic Portfolio Management (SPM, also known as ValueEdge Strategy) or Project and Portfolio Management (PPM). This integration enables you to create Proposals in PPM or Epics in SPM for Service Management Ideas. It also enables you to create projects in PPM for Service Management Proposals. The lifecycle of these records can then be managed in PPM or SPM.Use case 1 - Idea to Epic in SPM or Idea to Proposal in PPMIn case an idea is classified as a valid item to create an Epic or Proposal, the idea reviewer can decide to open an Epic in SPM or a Proposal in PPM. The integration copies the information into the target system and updates the Idea record.Use case 2 – Proposal to Project in PPM In case a Proposal in Service Management is classified as a valid item to create a Project in PPM, the proposal reviewer can decide to open a Project in PPM. The integration copies the information into the target system and updates the Proposal record. The ",
    "url": "integrateppmxie",
    "filename": "integrateppmxie",
    "headings": [
      "Use case 1 - Idea to Epic in SPM or Idea to Proposal in PPM",
      "Use case 2 – Proposal to Project in PPM",
      "Set up the integration",
      "Prepare integration user",
      "Export the certificate from the PPM or SPM instance",
      "Create an endpoint",
      "Create an integration",
      "Configure the scenarios",
      "Parameters to be configured for the Idea to Proposal in PPM scenario",
      "Parameters to be configured for the Proposal to Project in PPM scenario",
      "Parameters to be configured for the Idea to Epic in SPM scenario",
      "Use the scenarios",
      "Idea to Proposal in PPM / Idea to Epic in SPM",
      "Proposal to Project in PPM",
      "Understand the integration"
    ],
    "keywords": [
      "studiospmppm",
      "SPM.Adds",
      "X.509",
      "page.In",
      "4.In",
      "integration.In",
      "needed.Each",
      "https://<FQDN",
      "condition.The",
      "rule.The",
      "text.In",
      "3.If",
      "record.Use",
      "SPM.Use",
      "https://FQDNCredentialsSelect",
      "2.0",
      "Management.The",
      "Business.Use",
      "parameters.Gets",
      "integration",
      "studio",
      "spm",
      "ppm",
      "case",
      "idea",
      "epic",
      "proposal",
      "project",
      "set",
      "prepare",
      "user",
      "export",
      "certificate",
      "instance",
      "create",
      "endpoint",
      "configure",
      "scenarios",
      "parameters",
      "configured",
      "scenario",
      "understand",
      "integrate",
      "service",
      "management",
      "strategic",
      "portfolio",
      "known",
      "valueedge",
      "strategy",
      "enables",
      "proposals",
      "epics",
      "ideas.",
      "projects",
      "proposals.",
      "lifecycle",
      "records",
      "managed",
      "ppmin",
      "classified",
      "valid",
      "item",
      "reviewer",
      "decide",
      "open",
      "ppm.",
      "copies",
      "information",
      "target",
      "system",
      "updates",
      "record.",
      "cases",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "edition.",
      "integrationas",
      "example",
      "procedure",
      "below",
      "implement",
      "documented",
      "cases.",
      "business",
      "logic"
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—spm/ppm integration",
    "contentLower": "you can integrate service management with strategic portfolio management (spm, also known as valueedge strategy) or project and portfolio management (ppm). this integration enables you to create proposals in ppm or epics in spm for service management ideas. it also enables you to create projects in ppm for service management proposals. the lifecycle of these records can then be managed in ppm or spm.use case 1 - idea to epic in spm or idea to proposal in ppmin case an idea is classified as a valid item to create an epic or proposal, the idea reviewer can decide to open an epic in spm or a proposal in ppm. the integration copies the information into the target system and updates the idea record.use case 2 – proposal to project in ppm in case a proposal in service management is classified as a valid item to create a project in ppm, the proposal reviewer can decide to open a project in ppm. the integration copies the information into the target system and updates the proposal record. the ",
    "keywordsLower": [
      "studiospmppm",
      "spm.adds",
      "x.509",
      "page.in",
      "4.in",
      "integration.in",
      "needed.each",
      "https://<fqdn",
      "condition.the",
      "rule.the",
      "text.in",
      "3.if",
      "record.use",
      "spm.use",
      "https://fqdncredentialsselect",
      "2.0",
      "management.the",
      "business.use",
      "parameters.gets",
      "integration",
      "studio",
      "spm",
      "ppm",
      "case",
      "idea",
      "epic",
      "proposal",
      "project",
      "set",
      "prepare",
      "user",
      "export",
      "certificate",
      "instance",
      "create",
      "endpoint",
      "configure",
      "scenarios",
      "parameters",
      "configured",
      "scenario",
      "understand",
      "integrate",
      "service",
      "management",
      "strategic",
      "portfolio",
      "known",
      "valueedge",
      "strategy",
      "enables",
      "proposals",
      "epics",
      "ideas.",
      "projects",
      "proposals.",
      "lifecycle",
      "records",
      "managed",
      "ppmin",
      "classified",
      "valid",
      "item",
      "reviewer",
      "decide",
      "open",
      "ppm.",
      "copies",
      "information",
      "target",
      "system",
      "updates",
      "record.",
      "cases",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "edition.",
      "integrationas",
      "example",
      "procedure",
      "below",
      "implement",
      "documented",
      "cases.",
      "business",
      "logic"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio - Service Management to Service Management",
    "content": "Integrations between two Open Text Service Management systems are typically used to exchange data between different two tenants. With the Integration Studio, you can easily configure integration solutions to query and exchange data between tenants. For example, you can forward requests from a tenant where HR tickets are managed to a tenant that is used by your internal IT department. Communications can be in both directions to efficiently keep all involved tickets in the tenants up-to-date and in sync. The Integration Studio provides several Service Management to Service Management scenario templates to support the following use cases: Incident exchange, for example, to exchange incidents between internal IT and service providers. Click here to learn more about this use case. Request exchange, for example, to exchange requests between an HR or Facility Management tenant and an internal IT tenant. Click here to learn more about this use case. Move knowledge articles, to copy knowledge a",
    "url": "integratesmaxlandingpage",
    "filename": "integratesmaxlandingpage",
    "headings": [],
    "keywords": [
      "integration",
      "studio",
      "service",
      "management",
      "integrations",
      "between",
      "two",
      "open",
      "text",
      "systems",
      "typically",
      "exchange",
      "data",
      "different",
      "tenants.",
      "easily",
      "configure",
      "solutions",
      "query",
      "example",
      "forward",
      "requests",
      "tenant",
      "hr",
      "tickets",
      "managed",
      "internal",
      "department.",
      "communications",
      "both",
      "directions",
      "efficiently",
      "keep",
      "all",
      "involved",
      "tenants",
      "up-to-date",
      "sync.",
      "provides",
      "several",
      "scenario",
      "templates",
      "support",
      "following",
      "cases",
      "incident",
      "incidents",
      "providers.",
      "click",
      "here",
      "learn",
      "about",
      "case.",
      "request",
      "facility",
      "tenant.",
      "move",
      "knowledge",
      "articles",
      "copy",
      "one",
      "another."
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio - service management to service management",
    "contentLower": "integrations between two open text service management systems are typically used to exchange data between different two tenants. with the integration studio, you can easily configure integration solutions to query and exchange data between tenants. for example, you can forward requests from a tenant where hr tickets are managed to a tenant that is used by your internal it department. communications can be in both directions to efficiently keep all involved tickets in the tenants up-to-date and in sync. the integration studio provides several service management to service management scenario templates to support the following use cases: incident exchange, for example, to exchange incidents between internal it and service providers. click here to learn more about this use case. request exchange, for example, to exchange requests between an hr or facility management tenant and an internal it tenant. click here to learn more about this use case. move knowledge articles, to copy knowledge a",
    "keywordsLower": [
      "integration",
      "studio",
      "service",
      "management",
      "integrations",
      "between",
      "two",
      "open",
      "text",
      "systems",
      "typically",
      "exchange",
      "data",
      "different",
      "tenants.",
      "easily",
      "configure",
      "solutions",
      "query",
      "example",
      "forward",
      "requests",
      "tenant",
      "hr",
      "tickets",
      "managed",
      "internal",
      "department.",
      "communications",
      "both",
      "directions",
      "efficiently",
      "keep",
      "all",
      "involved",
      "tenants",
      "up-to-date",
      "sync.",
      "provides",
      "several",
      "scenario",
      "templates",
      "support",
      "following",
      "cases",
      "incident",
      "incidents",
      "providers.",
      "click",
      "here",
      "learn",
      "about",
      "case.",
      "request",
      "facility",
      "tenant.",
      "move",
      "knowledge",
      "articles",
      "copy",
      "one",
      "another."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—Asset Manager to Service Management demo data import",
    "content": "The Integration Studio provides the capability to import data from Asset Manager (AM) to Service Management. A set of templates are also available to ease the data import from Asset Manager to the corresponding Service Management record types.This capability is not designed for the migration of an Asset Manager system to Service Management. You can use it only for demonstration purposes and only for partial data import. The integration use cases described in this document serve as examples to inspire and guide your integration efforts. However, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. The examples may or may not be applicable for your license edition. Supported record typesThis capability supports data import for the following objects or record types, and you can customize the system to support additional objects / record types.Financial moduleBudgetBudget LineBudget CenterBudget TypeCost CenterCost TypeFiscal YearAccounting Pe",
    "url": "integrateam4dataimport",
    "filename": "integrateam4dataimport",
    "headings": [
      "Supported record types",
      "Before you start",
      "Expertise",
      "Supported versions",
      "Network connectivity",
      "Cloud/On-Premises",
      "Key concepts",
      "Nature vs. Record type",
      "Dependencies and migration sequence",
      "Unidirectional relation",
      "Bidirectional relation and loop relation",
      "Self-reference",
      "Dependencies between record types",
      "Reconciliation key",
      "Data model mapping",
      "Data inapplicable to migration",
      "Prepare Service Management",
      "Prepare an integration user for your Service Management tenant",
      "Create an agent (optional)",
      "Create an endpoint"
    ],
    "keywords": [
      "studioasset",
      "AM.The",
      "center.In",
      "BudgetCategory.Code",
      "table.In",
      "order.01",
      "Center.Cost",
      "https://<FQDN>:portCredentials(only",
      "center.We",
      "rules.Data",
      "Years.In",
      "lines.Cost",
      "user.The",
      "Peripheral.The",
      "Management.To",
      "page.In",
      "integration.In",
      "Manager.Two",
      "Bridge.Key",
      "parameter.Now",
      "field.If",
      "first.Run",
      "year.In",
      "record.The",
      "records.In",
      "period.Cost",
      "scenario.How",
      "data.In",
      "kept.The",
      "imported.The",
      "statements.By",
      "2.0",
      "9.80",
      "https.Base",
      "field.You",
      "X.509",
      "created.The",
      "Subtype.The",
      "Studio.To",
      "this.The",
      "types.This",
      "type.Data",
      "principles.Get",
      "systems.Use",
      "integration",
      "studio",
      "asset",
      "manager",
      "service",
      "management",
      "demo",
      "data",
      "import",
      "supported",
      "record",
      "types",
      "before",
      "start",
      "expertise",
      "versions",
      "network",
      "connectivity",
      "cloud",
      "on-premises",
      "key",
      "concepts",
      "nature",
      "vs.",
      "type",
      "dependencies",
      "migration",
      "sequence",
      "unidirectional",
      "relation",
      "bidirectional",
      "loop",
      "self-reference",
      "between",
      "reconciliation",
      "model",
      "mapping",
      "inapplicable",
      "prepare",
      "user",
      "tenant",
      "create",
      "agent",
      "optional",
      "endpoint",
      "configure",
      "scenarios",
      "run",
      "in-dept",
      "explanation",
      "scenario",
      "get",
      "details",
      "fiscal",
      "years",
      "accounting"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—asset manager to service management demo data import",
    "contentLower": "the integration studio provides the capability to import data from asset manager (am) to service management. a set of templates are also available to ease the data import from asset manager to the corresponding service management record types.this capability is not designed for the migration of an asset manager system to service management. you can use it only for demonstration purposes and only for partial data import. the integration use cases described in this document serve as examples to inspire and guide your integration efforts. however, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. the examples may or may not be applicable for your license edition. supported record typesthis capability supports data import for the following objects or record types, and you can customize the system to support additional objects / record types.financial modulebudgetbudget linebudget centerbudget typecost centercost typefiscal yearaccounting pe",
    "keywordsLower": [
      "studioasset",
      "am.the",
      "center.in",
      "budgetcategory.code",
      "table.in",
      "order.01",
      "center.cost",
      "https://<fqdn>:portcredentials(only",
      "center.we",
      "rules.data",
      "years.in",
      "lines.cost",
      "user.the",
      "peripheral.the",
      "management.to",
      "page.in",
      "integration.in",
      "manager.two",
      "bridge.key",
      "parameter.now",
      "field.if",
      "first.run",
      "year.in",
      "record.the",
      "records.in",
      "period.cost",
      "scenario.how",
      "data.in",
      "kept.the",
      "imported.the",
      "statements.by",
      "2.0",
      "9.80",
      "https.base",
      "field.you",
      "x.509",
      "created.the",
      "subtype.the",
      "studio.to",
      "this.the",
      "types.this",
      "type.data",
      "principles.get",
      "systems.use",
      "integration",
      "studio",
      "asset",
      "manager",
      "service",
      "management",
      "demo",
      "data",
      "import",
      "supported",
      "record",
      "types",
      "before",
      "start",
      "expertise",
      "versions",
      "network",
      "connectivity",
      "cloud",
      "on-premises",
      "key",
      "concepts",
      "nature",
      "vs.",
      "type",
      "dependencies",
      "migration",
      "sequence",
      "unidirectional",
      "relation",
      "bidirectional",
      "loop",
      "self-reference",
      "between",
      "reconciliation",
      "model",
      "mapping",
      "inapplicable",
      "prepare",
      "user",
      "tenant",
      "create",
      "agent",
      "optional",
      "endpoint",
      "configure",
      "scenarios",
      "run",
      "in-dept",
      "explanation",
      "scenario",
      "get",
      "details",
      "fiscal",
      "years",
      "accounting"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—Service Manager to Service Management data import",
    "content": "The Integration Studio provides a capability to move data from Service Manager to Service Management. A set of templates are also available to ease the data import from Service Manager to the corresponding Service Management record types. This capability is not designed for the migration of a Service Manager system to Service Management. You can use it only for demonstration purposes and only for partial data import.The scenario templates used in this integration don't import attachments or pictures embedded in Service Manager records, with the exception for knowledge articles. If you want to retain attachments in other imported records, add attachment retrieval rules to the scenario. For details, see How to upload attachments. The integration use cases described in this document serve as examples to inspire and guide your integration efforts. However, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. The examples may or may not be appl",
    "url": "importsmdataintosmax",
    "filename": "importsmdataintosmax",
    "headings": [
      "Overview",
      "Supported objects or record types",
      "Supported versions",
      "Before you start",
      "Product knowledge and skills",
      "Network connectivity",
      "SaaS",
      "Cloud or on-premises",
      "Prepare Service Manager",
      "Configure Service Manager Web Services",
      "Prepare data for the Service Manager instance",
      "Prepare Service Management",
      "Prepare an integration user for your Service Management tenant",
      "Export the certificates of the two systems",
      "Download and install the OPB Agent if needed",
      "Create an endpoint",
      "Create an integration",
      "Configure the scenarios",
      "Run the data import",
      "How the solution works"
    ],
    "keywords": [
      "studioservice",
      "mycorp.com",
      "agent.To",
      "implemented.This",
      "definitions.If",
      "2.In",
      "africa.com",
      "Management.The",
      "user.The",
      "page.In",
      "list.If",
      "integration.In",
      "base-64",
      "Management.Only",
      "XIELocation.City",
      "together.You",
      "xie_webservices.unl",
      "record.The",
      "accordingly.Get",
      "https://<FQDN>:port",
      "scenario.Now",
      "sm_CA.cer",
      "Locations.In",
      "https://mysmserver.mycorp.com:13443",
      "smax_CA.cer",
      "record.Some",
      "imported.The",
      "Management.What",
      "2.0",
      "license.To",
      "group.To",
      "X.509",
      "import.The",
      "9.40",
      "fail.You",
      "key.What",
      "iteration.The",
      "asia.com",
      "integration",
      "studio",
      "service",
      "manager",
      "management",
      "data",
      "import",
      "overview",
      "supported",
      "objects",
      "record",
      "types",
      "versions",
      "before",
      "start",
      "product",
      "knowledge",
      "skills",
      "network",
      "connectivity",
      "saas",
      "cloud",
      "on-premises",
      "prepare",
      "configure",
      "web",
      "services",
      "instance",
      "user",
      "tenant",
      "export",
      "certificates",
      "two",
      "systems",
      "download",
      "install",
      "opb",
      "agent",
      "needed",
      "create",
      "endpoint",
      "scenarios",
      "run",
      "solution",
      "works",
      "process",
      "logic",
      "scenario",
      "get",
      "additional",
      "response",
      "update",
      "records",
      "details",
      "pre-configured",
      "locations",
      "departments",
      "assignment",
      "groups",
      "categories",
      "subcategories",
      "areas"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—service manager to service management data import",
    "contentLower": "the integration studio provides a capability to move data from service manager to service management. a set of templates are also available to ease the data import from service manager to the corresponding service management record types. this capability is not designed for the migration of a service manager system to service management. you can use it only for demonstration purposes and only for partial data import.the scenario templates used in this integration don't import attachments or pictures embedded in service manager records, with the exception for knowledge articles. if you want to retain attachments in other imported records, add attachment retrieval rules to the scenario. for details, see how to upload attachments. the integration use cases described in this document serve as examples to inspire and guide your integration efforts. however, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. the examples may or may not be appl",
    "keywordsLower": [
      "studioservice",
      "mycorp.com",
      "agent.to",
      "implemented.this",
      "definitions.if",
      "2.in",
      "africa.com",
      "management.the",
      "user.the",
      "page.in",
      "list.if",
      "integration.in",
      "base-64",
      "management.only",
      "xielocation.city",
      "together.you",
      "xie_webservices.unl",
      "record.the",
      "accordingly.get",
      "https://<fqdn>:port",
      "scenario.now",
      "sm_ca.cer",
      "locations.in",
      "https://mysmserver.mycorp.com:13443",
      "smax_ca.cer",
      "record.some",
      "imported.the",
      "management.what",
      "2.0",
      "license.to",
      "group.to",
      "x.509",
      "import.the",
      "9.40",
      "fail.you",
      "key.what",
      "iteration.the",
      "asia.com",
      "integration",
      "studio",
      "service",
      "manager",
      "management",
      "data",
      "import",
      "overview",
      "supported",
      "objects",
      "record",
      "types",
      "versions",
      "before",
      "start",
      "product",
      "knowledge",
      "skills",
      "network",
      "connectivity",
      "saas",
      "cloud",
      "on-premises",
      "prepare",
      "configure",
      "web",
      "services",
      "instance",
      "user",
      "tenant",
      "export",
      "certificates",
      "two",
      "systems",
      "download",
      "install",
      "opb",
      "agent",
      "needed",
      "create",
      "endpoint",
      "scenarios",
      "run",
      "solution",
      "works",
      "process",
      "logic",
      "scenario",
      "get",
      "additional",
      "response",
      "update",
      "records",
      "details",
      "pre-configured",
      "locations",
      "departments",
      "assignment",
      "groups",
      "categories",
      "subcategories",
      "areas"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—ALM Quality Center integration",
    "content": "OverviewThis topic presents an illustrated set of integration use cases between SMAX and Quality Center:Portal-based Defect or Enhancement requestsIncident to Defect integrationThe use cases in a nutshell:The integration use cases described in this document serve as examples to inspire and guide your integration efforts. However, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. The examples may or may not be applicable for your license edition.Use case 1: Portal-based Defect or Enhancement requestsIn this sample use case, end users who subscribe to a specific Business Service Application can report defects or request enhancements to the application by posting them via the SMAX Service Portal. We will follow the steps below, which are performed by Eve, an employee who uses her company’s eExpense application for expense tracking and reporting. She has been frustrated that expense reporting can only be done while logged on to the eExpense",
    "url": "integratewithalmqc",
    "filename": "integratewithalmqc",
    "headings": [
      "Overview",
      "Use case 1: Portal-based Defect or Enhancement requests",
      "Step 1: Portal access",
      "Step 2:  Submitting an enhancement request or defect",
      "Step 3: Quality Center Feature created and confirmation provided back to the user",
      "Step 4: Status confirmation",
      "Step 5: Behind the scenes - association of the Quality Center ticket from the SMAX Request ticket",
      "Use case 2: Change request to requirement",
      "Use case 3: Incident/Request/Problem to defect",
      "Use case 4: Get status update",
      "Set up the integration",
      "Prepare an integration user",
      "Export the certificate from the Quality Center instance",
      "Configure SMAX",
      "Create an endpoint",
      "Create an integration",
      "Configure the scenarios",
      "Use the scenarios",
      "Use case 1 – Portal-based Defect or Enhancement requests",
      "Specify user options"
    ],
    "keywords": [
      "studioalm",
      "Eve.Step",
      "listed.She",
      "backlog.Step",
      "reference.Step",
      "needed.As",
      "intervention.Task",
      "task.Task",
      "Studio.The",
      "current_user.Upn",
      "day.Step",
      "Management.Let",
      "Task.Give",
      "record.Use",
      "system.Use",
      "defect.Step",
      "record.In",
      "Definition.We",
      "tracking.Save",
      "failure.Task",
      "code.Save",
      "section.Set",
      "needs.To",
      "https://<FQDN>/authentication/qcbin/api/authentication/sign-inCertificate",
      "page.In",
      "Center.Use",
      "integration.In",
      "integrations.Tip",
      "below.Note",
      "group.Save",
      "rule.The",
      "request.To",
      "ExpertGroup.Name",
      "plan.In",
      "method.Use",
      "Center.Note",
      "2.0",
      "user.Log",
      "https.Base",
      "X.509",
      "tracking.In",
      "steps.Go",
      "task.This",
      "steps.As",
      "entity.Id",
      "edition.Use",
      "value.Save",
      "fulfilled.If",
      "created.If",
      "integration.It",
      "https://<FQDN>Authentication",
      "integration",
      "studio",
      "alm",
      "quality",
      "center",
      "overview",
      "case",
      "portal-based",
      "defect",
      "enhancement",
      "requests",
      "step",
      "portal",
      "access",
      "submitting",
      "request",
      "feature",
      "created",
      "confirmation",
      "provided",
      "back",
      "user",
      "status",
      "behind",
      "scenes",
      "association",
      "ticket",
      "smax",
      "change",
      "requirement",
      "incident",
      "problem",
      "get",
      "update",
      "set",
      "prepare",
      "export",
      "certificate",
      "instance",
      "configure",
      "create",
      "endpoint",
      "scenarios",
      "specify",
      "options",
      "define",
      "task",
      "plan",
      "pull"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—alm quality center integration",
    "contentLower": "overviewthis topic presents an illustrated set of integration use cases between smax and quality center:portal-based defect or enhancement requestsincident to defect integrationthe use cases in a nutshell:the integration use cases described in this document serve as examples to inspire and guide your integration efforts. however, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. the examples may or may not be applicable for your license edition.use case 1: portal-based defect or enhancement requestsin this sample use case, end users who subscribe to a specific business service application can report defects or request enhancements to the application by posting them via the smax service portal. we will follow the steps below, which are performed by eve, an employee who uses her company’s eexpense application for expense tracking and reporting. she has been frustrated that expense reporting can only be done while logged on to the eexpense",
    "keywordsLower": [
      "studioalm",
      "eve.step",
      "listed.she",
      "backlog.step",
      "reference.step",
      "needed.as",
      "intervention.task",
      "task.task",
      "studio.the",
      "current_user.upn",
      "day.step",
      "management.let",
      "task.give",
      "record.use",
      "system.use",
      "defect.step",
      "record.in",
      "definition.we",
      "tracking.save",
      "failure.task",
      "code.save",
      "section.set",
      "needs.to",
      "https://<fqdn>/authentication/qcbin/api/authentication/sign-incertificate",
      "page.in",
      "center.use",
      "integration.in",
      "integrations.tip",
      "below.note",
      "group.save",
      "rule.the",
      "request.to",
      "expertgroup.name",
      "plan.in",
      "method.use",
      "center.note",
      "2.0",
      "user.log",
      "https.base",
      "x.509",
      "tracking.in",
      "steps.go",
      "task.this",
      "steps.as",
      "entity.id",
      "edition.use",
      "value.save",
      "fulfilled.if",
      "created.if",
      "integration.it",
      "https://<fqdn>authentication",
      "integration",
      "studio",
      "alm",
      "quality",
      "center",
      "overview",
      "case",
      "portal-based",
      "defect",
      "enhancement",
      "requests",
      "step",
      "portal",
      "access",
      "submitting",
      "request",
      "feature",
      "created",
      "confirmation",
      "provided",
      "back",
      "user",
      "status",
      "behind",
      "scenes",
      "association",
      "ticket",
      "smax",
      "change",
      "requirement",
      "incident",
      "problem",
      "get",
      "update",
      "set",
      "prepare",
      "export",
      "certificate",
      "instance",
      "configure",
      "create",
      "endpoint",
      "scenarios",
      "specify",
      "options",
      "define",
      "task",
      "plan",
      "pull"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—ArcSight SOAR integration",
    "content": "OverviewThis topic presents an illustrated set of integration use cases between SMAX and ArcSight SOAR:Security case to IncidentThe use cases in a nutshell:The integration use cases described in this document serve as examples to inspire and guide your integration efforts. However, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. The examples may or may not be applicable for your license edition.Use case 1: Update cases in ArcSight SOARRaising a security Incident in Service Management provides an option for a formal handling of a breach or vulnerability. In this sample use case, a security officer is reviewing security alerts in ArcSight SOAR and decides to raise an Incident in Service Management for further investigations since he detected a new vulnerability. By raising the Security Incident, the security team can immediately perform risk analysis and drive implementation of permanent long-term security changes.Step 1: Incident Queue",
    "url": "integratewitharcsightsoar",
    "filename": "integratewitharcsightsoar",
    "headings": [
      "Overview",
      "Use case 1: Update cases in ArcSight SOAR",
      "Step 1: Incident Queue review",
      "Step 2: Update the Incident in Service Management",
      "Step 3: Resolve the Incident in Service Management",
      "Set up the integration",
      "Configuration in ArcSight",
      "Configuration in Service Management",
      "Prepare integration user",
      "Export the certificate from the ArcSight instance",
      "Create an endpoint",
      "Create an integration",
      "Configure the scenarios",
      "Use the scenarios",
      "Use case 1 – Update cases in ArcSight SOAR"
    ],
    "keywords": [
      "studioarcsight",
      "current_user.Upn",
      "action.Step",
      "system.Note",
      "Management.The",
      "page.In",
      "integration.In",
      "here.As",
      "condition.To",
      "https://<FQDN>UsernameEnter",
      "scenario.The",
      "changes.Step",
      "system.The",
      "https://<FQDN>CredentialsSelect",
      "case.In",
      "Agent.If",
      "2.0",
      "Resolved.Set",
      "user.Log",
      "https.Base",
      "X.509",
      "Management.Step",
      "edition.Use",
      "entity.Id",
      "integration",
      "studio",
      "arcsight",
      "soar",
      "overview",
      "case",
      "update",
      "cases",
      "step",
      "incident",
      "queue",
      "review",
      "service",
      "management",
      "resolve",
      "set",
      "configuration",
      "prepare",
      "user",
      "export",
      "certificate",
      "instance",
      "create",
      "endpoint",
      "configure",
      "scenarios",
      "overviewthis",
      "topic",
      "presents",
      "illustrated",
      "between",
      "smax",
      "security",
      "incidentthe",
      "nutshell",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "soarraising",
      "provides",
      "option",
      "formal",
      "handling",
      "breach",
      "vulnerability.",
      "sample",
      "officer",
      "reviewing",
      "alerts",
      "decides",
      "raise",
      "further",
      "investigations",
      "since",
      "detected",
      "new",
      "raising",
      "team",
      "immediately",
      "perform",
      "risk",
      "analysis",
      "drive"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—arcsight soar integration",
    "contentLower": "overviewthis topic presents an illustrated set of integration use cases between smax and arcsight soar:security case to incidentthe use cases in a nutshell:the integration use cases described in this document serve as examples to inspire and guide your integration efforts. however, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. the examples may or may not be applicable for your license edition.use case 1: update cases in arcsight soarraising a security incident in service management provides an option for a formal handling of a breach or vulnerability. in this sample use case, a security officer is reviewing security alerts in arcsight soar and decides to raise an incident in service management for further investigations since he detected a new vulnerability. by raising the security incident, the security team can immediately perform risk analysis and drive implementation of permanent long-term security changes.step 1: incident queue",
    "keywordsLower": [
      "studioarcsight",
      "current_user.upn",
      "action.step",
      "system.note",
      "management.the",
      "page.in",
      "integration.in",
      "here.as",
      "condition.to",
      "https://<fqdn>usernameenter",
      "scenario.the",
      "changes.step",
      "system.the",
      "https://<fqdn>credentialsselect",
      "case.in",
      "agent.if",
      "2.0",
      "resolved.set",
      "user.log",
      "https.base",
      "x.509",
      "management.step",
      "edition.use",
      "entity.id",
      "integration",
      "studio",
      "arcsight",
      "soar",
      "overview",
      "case",
      "update",
      "cases",
      "step",
      "incident",
      "queue",
      "review",
      "service",
      "management",
      "resolve",
      "set",
      "configuration",
      "prepare",
      "user",
      "export",
      "certificate",
      "instance",
      "create",
      "endpoint",
      "configure",
      "scenarios",
      "overviewthis",
      "topic",
      "presents",
      "illustrated",
      "between",
      "smax",
      "security",
      "incidentthe",
      "nutshell",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "soarraising",
      "provides",
      "option",
      "formal",
      "handling",
      "breach",
      "vulnerability.",
      "sample",
      "officer",
      "reviewing",
      "alerts",
      "decides",
      "raise",
      "further",
      "investigations",
      "since",
      "detected",
      "new",
      "raising",
      "team",
      "immediately",
      "perform",
      "risk",
      "analysis",
      "drive"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with OPTIC Data Lake",
    "content": "OPTIC Data Lake (OPTIC DL) provides a central data lake to receive, process, and store data that is high in volume and velocity from several independent data sources. With this integration, historical record data of SMAX can be populated to OPTIC Data Lake based on a backend job. Then, the SMAX data can be represented in a holistic view along with data from other data sources through reporting tools (such as UIF dashboard and Flex reporting) or any other third-party BI tools to generate meaningful analysis. About this integration: This integration requires the tenant to have a Premium license. Historical record data will be populated to OPTIC Data Lake, which means every time an update is made on a record, a new record is created in OPTIC Data Lake. If you want to populate an existing record, you need to make an update on it.If the integration is deactivated and then activated again, the updates made during this period of time will not be pushed to OPTIC Data Lake.The backend data push",
    "url": "integrateodl",
    "filename": "integrateodl",
    "headings": [
      "Prerequisites of this integration",
      "Application information",
      "Direct network connection",
      "Native SACM tasks",
      "Configure OPTIC Data Lake certificates",
      "Download OPTIC Data Lake certificates",
      "Import OPTIC Data Lake certificates",
      "Classic deployment",
      "Helm deployment",
      "Restart pods",
      "Create a credential for OPTIC Data Lake",
      "Deploy the OPTIC Data Lake capability",
      "Configure the integration",
      "Data synchronization",
      "Verify the integration",
      "Database structure",
      "SMAX data",
      "Use the integration"
    ],
    "keywords": [
      "credential.On",
      "population.New",
      "interface.In",
      "Lake.Only",
      "performance.Now",
      "Configurations.On",
      "it.If",
      "valid.Go",
      "New.In",
      "https://<externalAccessHost>:<Port",
      "https://<external",
      "Deploy.The",
      "Next.In",
      "retired.Each",
      "administration.In",
      "Lake.The",
      "integration.Data",
      "https://<externalAccessHost>:30004",
      "https://<externalAccessHost>:<port>/itom-data-ingestion-administration.In",
      "https://<externalAccessHost>:<Port>/itom-data-ingestion-receiver",
      "https://<externalAccessHost>:443",
      "https://<externalAccessHost>:<port>/itom-data-ingestion-receiver.In",
      "ttps://<externalAccessHost>:<Port>/itom-data-ingestion-administration",
      "design.If",
      "receiver.In",
      "COMPLEXT_TYPE.For",
      "https://<externalAccessHost>:30004/itom-data-ingestion-administrationhttps://<externalAccessHost>:30001/itom-data-ingestion-receiver",
      "applications.The",
      "issue.The",
      "integrate",
      "optic",
      "data",
      "lake",
      "prerequisites",
      "integration",
      "application",
      "information",
      "direct",
      "network",
      "connection",
      "native",
      "sacm",
      "tasks",
      "configure",
      "certificates",
      "download",
      "import",
      "classic",
      "deployment",
      "helm",
      "restart",
      "pods",
      "create",
      "credential",
      "deploy",
      "capability",
      "synchronization",
      "verify",
      "database",
      "structure",
      "smax",
      "dl",
      "provides",
      "central",
      "receive",
      "process",
      "store",
      "high",
      "volume",
      "velocity",
      "several",
      "independent",
      "sources.",
      "historical",
      "record",
      "populated",
      "based",
      "backend",
      "job.",
      "represented",
      "holistic",
      "view",
      "along",
      "sources",
      "through",
      "reporting",
      "tools",
      "such",
      "uif",
      "dashboard",
      "flex",
      "any",
      "third-party",
      "bi",
      "generate",
      "meaningful",
      "analysis.",
      "about",
      "requires",
      "tenant"
    ],
    "language": "en",
    "word_count": 110,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with optic data lake",
    "contentLower": "optic data lake (optic dl) provides a central data lake to receive, process, and store data that is high in volume and velocity from several independent data sources. with this integration, historical record data of smax can be populated to optic data lake based on a backend job. then, the smax data can be represented in a holistic view along with data from other data sources through reporting tools (such as uif dashboard and flex reporting) or any other third-party bi tools to generate meaningful analysis. about this integration: this integration requires the tenant to have a premium license. historical record data will be populated to optic data lake, which means every time an update is made on a record, a new record is created in optic data lake. if you want to populate an existing record, you need to make an update on it.if the integration is deactivated and then activated again, the updates made during this period of time will not be pushed to optic data lake.the backend data push",
    "keywordsLower": [
      "credential.on",
      "population.new",
      "interface.in",
      "lake.only",
      "performance.now",
      "configurations.on",
      "it.if",
      "valid.go",
      "new.in",
      "https://<externalaccesshost>:<port",
      "https://<external",
      "deploy.the",
      "next.in",
      "retired.each",
      "administration.in",
      "lake.the",
      "integration.data",
      "https://<externalaccesshost>:30004",
      "https://<externalaccesshost>:<port>/itom-data-ingestion-administration.in",
      "https://<externalaccesshost>:<port>/itom-data-ingestion-receiver",
      "https://<externalaccesshost>:443",
      "https://<externalaccesshost>:<port>/itom-data-ingestion-receiver.in",
      "ttps://<externalaccesshost>:<port>/itom-data-ingestion-administration",
      "design.if",
      "receiver.in",
      "complext_type.for",
      "https://<externalaccesshost>:30004/itom-data-ingestion-administrationhttps://<externalaccesshost>:30001/itom-data-ingestion-receiver",
      "applications.the",
      "issue.the",
      "integrate",
      "optic",
      "data",
      "lake",
      "prerequisites",
      "integration",
      "application",
      "information",
      "direct",
      "network",
      "connection",
      "native",
      "sacm",
      "tasks",
      "configure",
      "certificates",
      "download",
      "import",
      "classic",
      "deployment",
      "helm",
      "restart",
      "pods",
      "create",
      "credential",
      "deploy",
      "capability",
      "synchronization",
      "verify",
      "database",
      "structure",
      "smax",
      "dl",
      "provides",
      "central",
      "receive",
      "process",
      "store",
      "high",
      "volume",
      "velocity",
      "several",
      "independent",
      "sources.",
      "historical",
      "record",
      "populated",
      "based",
      "backend",
      "job.",
      "represented",
      "holistic",
      "view",
      "along",
      "sources",
      "through",
      "reporting",
      "tools",
      "such",
      "uif",
      "dashboard",
      "flex",
      "any",
      "third-party",
      "bi",
      "generate",
      "meaningful",
      "analysis.",
      "about",
      "requires",
      "tenant"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with UCMDB",
    "content": "You can integrate Service Management with UCMDB through either On-Premises Bridge (OPB) or the Native SACM solution (using UCMDB Gateway). The Service Management-UCMDB integration through OPB is deprecated and might be removed in a future release. Existing customers can continue to use it, but the support might be discontinued in a future release. Native SACM is one single solution for the entire Service Asset & Configuration Management (SACM) process with instant access to the data and embedded graphical widgets for CI topology representation. It is a natively connected solution and enables bi-directional data federations between SMA and UCMDB. Therefore, we highly recommend that you use Native SACM to integrate SMA and UCMDB. On-Premises Bridge Through On-Premises Bridge (OPB), integration with UCMDB enables you to synchronize configuration item (CI) data from UCMDB with Service Management. You can perform the automatic synchronization or tailor your custom synchronization with UCMDB",
    "url": "integucmdb",
    "filename": "integucmdb",
    "headings": [
      "On-Premises Bridge",
      "Native SACM"
    ],
    "keywords": [
      "integrate",
      "ucmdb",
      "on-premises",
      "bridge",
      "native",
      "sacm",
      "service",
      "management",
      "through",
      "either",
      "opb",
      "solution",
      "gateway",
      "management-ucmdb",
      "integration",
      "deprecated",
      "removed",
      "future",
      "release.",
      "existing",
      "customers",
      "continue",
      "support",
      "discontinued",
      "one",
      "single",
      "entire",
      "asset",
      "configuration",
      "process",
      "instant",
      "access",
      "data",
      "embedded",
      "graphical",
      "widgets",
      "ci",
      "topology",
      "representation.",
      "natively",
      "connected",
      "enables",
      "bi-directional",
      "federations",
      "between",
      "sma",
      "ucmdb.",
      "therefore",
      "highly",
      "recommend",
      "synchronize",
      "item",
      "management.",
      "perform",
      "automatic",
      "synchronization",
      "tailor",
      "custom",
      "platform",
      "communications",
      "applications",
      "both",
      "directions.",
      "because",
      "located",
      "behind",
      "firewalls",
      "initiating",
      "connection",
      "application",
      "isn",
      "possible",
      "component",
      "required.",
      "external",
      "non-containerized",
      "classic",
      "need",
      "download",
      "install",
      "agent",
      "complete",
      "cases.",
      "set",
      "communicate",
      "https",
      "apart",
      "traditional",
      "now",
      "called",
      "achieve",
      "seamless",
      "federation",
      "gateway.",
      "needs",
      "work",
      "externally",
      "deployed",
      "server",
      "containerized."
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with ucmdb",
    "contentLower": "you can integrate service management with ucmdb through either on-premises bridge (opb) or the native sacm solution (using ucmdb gateway). the service management-ucmdb integration through opb is deprecated and might be removed in a future release. existing customers can continue to use it, but the support might be discontinued in a future release. native sacm is one single solution for the entire service asset & configuration management (sacm) process with instant access to the data and embedded graphical widgets for ci topology representation. it is a natively connected solution and enables bi-directional data federations between sma and ucmdb. therefore, we highly recommend that you use native sacm to integrate sma and ucmdb. on-premises bridge through on-premises bridge (opb), integration with ucmdb enables you to synchronize configuration item (ci) data from ucmdb with service management. you can perform the automatic synchronization or tailor your custom synchronization with ucmdb",
    "keywordsLower": [
      "integrate",
      "ucmdb",
      "on-premises",
      "bridge",
      "native",
      "sacm",
      "service",
      "management",
      "through",
      "either",
      "opb",
      "solution",
      "gateway",
      "management-ucmdb",
      "integration",
      "deprecated",
      "removed",
      "future",
      "release.",
      "existing",
      "customers",
      "continue",
      "support",
      "discontinued",
      "one",
      "single",
      "entire",
      "asset",
      "configuration",
      "process",
      "instant",
      "access",
      "data",
      "embedded",
      "graphical",
      "widgets",
      "ci",
      "topology",
      "representation.",
      "natively",
      "connected",
      "enables",
      "bi-directional",
      "federations",
      "between",
      "sma",
      "ucmdb.",
      "therefore",
      "highly",
      "recommend",
      "synchronize",
      "item",
      "management.",
      "perform",
      "automatic",
      "synchronization",
      "tailor",
      "custom",
      "platform",
      "communications",
      "applications",
      "both",
      "directions.",
      "because",
      "located",
      "behind",
      "firewalls",
      "initiating",
      "connection",
      "application",
      "isn",
      "possible",
      "component",
      "required.",
      "external",
      "non-containerized",
      "classic",
      "need",
      "download",
      "install",
      "agent",
      "complete",
      "cases.",
      "set",
      "communicate",
      "https",
      "apart",
      "traditional",
      "now",
      "called",
      "achieve",
      "seamless",
      "federation",
      "gateway.",
      "needs",
      "work",
      "externally",
      "deployed",
      "server",
      "containerized."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Identification",
    "content": "Identification is a Web service that does the following: Finds and collects information about relevant configuration items (CIs) in the UCMDB. Prepares the data so you can synchronize it with Service Management. The web service identifies the following CIs: Actual service Device Service component System element For more information about each of the different types of CI, see these topics: How to create an actual service record How to create a device record How to create a service component record How to create a system element record Related topics How to consume identification Matching CIs Output",
    "url": "identification",
    "filename": "identification",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "identification",
      "related",
      "topics",
      "web",
      "service",
      "following",
      "finds",
      "collects",
      "information",
      "about",
      "relevant",
      "configuration",
      "items",
      "cis",
      "ucmdb.",
      "prepares",
      "data",
      "synchronize",
      "management.",
      "identifies",
      "actual",
      "device",
      "component",
      "system",
      "element",
      "different",
      "types",
      "ci",
      "see",
      "create",
      "record",
      "consume",
      "matching",
      "output"
    ],
    "language": "en",
    "word_count": 60,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "identification",
    "contentLower": "identification is a web service that does the following: finds and collects information about relevant configuration items (cis) in the ucmdb. prepares the data so you can synchronize it with service management. the web service identifies the following cis: actual service device service component system element for more information about each of the different types of ci, see these topics: how to create an actual service record how to create a device record how to create a service component record how to create a system element record related topics how to consume identification matching cis output",
    "keywordsLower": [
      "identification",
      "related",
      "topics",
      "web",
      "service",
      "following",
      "finds",
      "collects",
      "information",
      "about",
      "relevant",
      "configuration",
      "items",
      "cis",
      "ucmdb.",
      "prepares",
      "data",
      "synchronize",
      "management.",
      "identifies",
      "actual",
      "device",
      "component",
      "system",
      "element",
      "different",
      "types",
      "ci",
      "see",
      "create",
      "record",
      "consume",
      "matching",
      "output"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with classic UCMDB through OPB",
    "content": "If you are using a classic UCMDB (that is, a non-containerized UCMDB), follow these steps to set up the connection with UCMDB. Prerequisites You have installed a classic UCMDB. For details, see the UCMDB documentation. You have installed a UCMDB data flow probe. Download and install the On-Premises Bridge (OPB) agent In the agent interface, navigate to Administration > Utilities > Integration > Agents, and then download and install the OPB agent and create an agent. For detail, see Related topics. Import the UCMDB server certificate to the OPB agent Open the UCMDB server URL in a browser: https://<UCMDB server>:8443/ucmdb-ui. From the browser, open the certificate window, go to the Details tab, and then click Copy to File. Follow the wizard prompts, and select the Base-64 encoded X.509 (.CER) format to export the certificate to a file. For example, ucmdb.cer. Import the UCMDB server certificate to the OPB agent's trust store. Suppose the OPB agent is installed on a Windows machine, do ",
    "url": "integexternalucmdb",
    "filename": "integexternalucmdb",
    "headings": [
      "Prerequisites",
      "Download and install the On-Premises Bridge (OPB) agent",
      "Import the UCMDB server certificate to the OPB agent",
      "Create a UCMDB endpoint",
      "Set up automatic or custom synchronization with UCMDB",
      "Related topics"
    ],
    "keywords": [
      "https://<UCMDB",
      "X.509",
      "Base-64",
      "ucmdb.cer",
      "integrate",
      "classic",
      "ucmdb",
      "through",
      "opb",
      "prerequisites",
      "download",
      "install",
      "on-premises",
      "bridge",
      "agent",
      "import",
      "server",
      "certificate",
      "create",
      "endpoint",
      "set",
      "automatic",
      "custom",
      "synchronization",
      "related",
      "topics",
      "non-containerized",
      "follow",
      "steps",
      "connection",
      "ucmdb.",
      "installed",
      "details",
      "see",
      "documentation.",
      "data",
      "flow",
      "probe.",
      "interface",
      "navigate",
      "administration",
      "utilities",
      "integration",
      "agents",
      "agent.",
      "detail",
      "topics.",
      "open",
      "url",
      "browser",
      "https",
      "8443",
      "ucmdb-ui.",
      "window",
      "go",
      "tab",
      "click",
      "copy",
      "file.",
      "wizard",
      "prompts",
      "select",
      "encoded",
      ".cer",
      "format",
      "export",
      "example",
      "ucmdb.cer.",
      "trust",
      "store.",
      "suppose",
      "windows",
      "machine",
      "following",
      "command",
      "prompt",
      "selecting",
      "run",
      "administrator.",
      "commands",
      "cd",
      "programdata",
      "microfocus",
      "on-premise",
      "product",
      "util",
      "3rd-party",
      "jre",
      "bin",
      "keytool",
      "-importcert",
      "-keystore",
      "lib",
      "security",
      "cacerts",
      "-alias",
      "-file",
      "credentials",
      "manager",
      "restart"
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 5.3,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with classic ucmdb through opb",
    "contentLower": "if you are using a classic ucmdb (that is, a non-containerized ucmdb), follow these steps to set up the connection with ucmdb. prerequisites you have installed a classic ucmdb. for details, see the ucmdb documentation. you have installed a ucmdb data flow probe. download and install the on-premises bridge (opb) agent in the agent interface, navigate to administration > utilities > integration > agents, and then download and install the opb agent and create an agent. for detail, see related topics. import the ucmdb server certificate to the opb agent open the ucmdb server url in a browser: https://<ucmdb server>:8443/ucmdb-ui. from the browser, open the certificate window, go to the details tab, and then click copy to file. follow the wizard prompts, and select the base-64 encoded x.509 (.cer) format to export the certificate to a file. for example, ucmdb.cer. import the ucmdb server certificate to the opb agent's trust store. suppose the opb agent is installed on a windows machine, do ",
    "keywordsLower": [
      "https://<ucmdb",
      "x.509",
      "base-64",
      "ucmdb.cer",
      "integrate",
      "classic",
      "ucmdb",
      "through",
      "opb",
      "prerequisites",
      "download",
      "install",
      "on-premises",
      "bridge",
      "agent",
      "import",
      "server",
      "certificate",
      "create",
      "endpoint",
      "set",
      "automatic",
      "custom",
      "synchronization",
      "related",
      "topics",
      "non-containerized",
      "follow",
      "steps",
      "connection",
      "ucmdb.",
      "installed",
      "details",
      "see",
      "documentation.",
      "data",
      "flow",
      "probe.",
      "interface",
      "navigate",
      "administration",
      "utilities",
      "integration",
      "agents",
      "agent.",
      "detail",
      "topics.",
      "open",
      "url",
      "browser",
      "https",
      "8443",
      "ucmdb-ui.",
      "window",
      "go",
      "tab",
      "click",
      "copy",
      "file.",
      "wizard",
      "prompts",
      "select",
      "encoded",
      ".cer",
      "format",
      "export",
      "example",
      "ucmdb.cer.",
      "trust",
      "store.",
      "suppose",
      "windows",
      "machine",
      "following",
      "command",
      "prompt",
      "selecting",
      "run",
      "administrator.",
      "commands",
      "cd",
      "programdata",
      "microfocus",
      "on-premise",
      "product",
      "util",
      "3rd-party",
      "jre",
      "bin",
      "keytool",
      "-importcert",
      "-keystore",
      "lib",
      "security",
      "cacerts",
      "-alias",
      "-file",
      "credentials",
      "manager",
      "restart"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Operations Orchestration",
    "content": "This topic provides information for integrating Service Management with Operations Orchestration (OO). You can integrate Service Management with: Classic (external) OO - For the product versions supported for this integration, visit the Service Management integration with Operations Orchestration (OO). Depending on your requirement to execute flows, you can install Operations Orchestration Remote Action Service (OO RAS). The version of OO RAS supported is the same as the version of the standalone OO. Operations Orchestration (OO) Containerized - The Operations Orchestration (OO), which was embedded as part of Design and Deploy, has been removed. If you have integrated Service Management with the embedded OO, you need to install OO Containerized and re-configure the integration. For more information on how to install Operations Orchestration, see Install OO on-premises, Install OO on EKS, and Install OO on AKS.  OO Containerized comes with an Internal OO Remote Action Service (RAS), whi",
    "url": "integrateoo",
    "filename": "integrateoo",
    "headings": [
      "Benefits of integrating Service Management and OO",
      "Use cases",
      "Operations Orchestration",
      "Operations Orchestration Remote Action Service",
      "Related topics"
    ],
    "keywords": [
      "integrate",
      "operations",
      "orchestration",
      "benefits",
      "integrating",
      "service",
      "management",
      "oo",
      "cases",
      "remote",
      "action",
      "related",
      "topics",
      "topic",
      "provides",
      "information",
      "classic",
      "external",
      "product",
      "versions",
      "supported",
      "integration",
      "visit",
      "depending",
      "requirement",
      "execute",
      "flows",
      "install",
      "ras",
      "version",
      "same",
      "standalone",
      "oo.",
      "containerized",
      "embedded",
      "part",
      "design",
      "deploy",
      "removed.",
      "integrated",
      "need",
      "re-configure",
      "integration.",
      "see",
      "on-premises",
      "eks",
      "aks.",
      "comes",
      "internal",
      "deployed",
      "automatically",
      "central",
      "tenant.",
      "additionally",
      "requirements",
      "data",
      "centers",
      "networks.",
      "adds",
      "following",
      "capabilities",
      "current",
      "deployment",
      "define",
      "business",
      "rules",
      "detailed",
      "task",
      "plan",
      "workflows",
      "coordinate",
      "repeatable",
      "work",
      "run",
      "automation",
      "result",
      "actions",
      "ranging",
      "sending",
      "notifications.",
      "updating",
      "records",
      "invoking",
      "processes.",
      "extending",
      "include",
      "run-book",
      "on-premises.",
      "import",
      "content",
      "bridge",
      "imported",
      "workflow",
      "record",
      "type",
      "automatic",
      "record.",
      "asymmetric",
      "encryption",
      "protect"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with operations orchestration",
    "contentLower": "this topic provides information for integrating service management with operations orchestration (oo). you can integrate service management with: classic (external) oo - for the product versions supported for this integration, visit the service management integration with operations orchestration (oo). depending on your requirement to execute flows, you can install operations orchestration remote action service (oo ras). the version of oo ras supported is the same as the version of the standalone oo. operations orchestration (oo) containerized - the operations orchestration (oo), which was embedded as part of design and deploy, has been removed. if you have integrated service management with the embedded oo, you need to install oo containerized and re-configure the integration. for more information on how to install operations orchestration, see install oo on-premises, install oo on eks, and install oo on aks.  oo containerized comes with an internal oo remote action service (ras), whi",
    "keywordsLower": [
      "integrate",
      "operations",
      "orchestration",
      "benefits",
      "integrating",
      "service",
      "management",
      "oo",
      "cases",
      "remote",
      "action",
      "related",
      "topics",
      "topic",
      "provides",
      "information",
      "classic",
      "external",
      "product",
      "versions",
      "supported",
      "integration",
      "visit",
      "depending",
      "requirement",
      "execute",
      "flows",
      "install",
      "ras",
      "version",
      "same",
      "standalone",
      "oo.",
      "containerized",
      "embedded",
      "part",
      "design",
      "deploy",
      "removed.",
      "integrated",
      "need",
      "re-configure",
      "integration.",
      "see",
      "on-premises",
      "eks",
      "aks.",
      "comes",
      "internal",
      "deployed",
      "automatically",
      "central",
      "tenant.",
      "additionally",
      "requirements",
      "data",
      "centers",
      "networks.",
      "adds",
      "following",
      "capabilities",
      "current",
      "deployment",
      "define",
      "business",
      "rules",
      "detailed",
      "task",
      "plan",
      "workflows",
      "coordinate",
      "repeatable",
      "work",
      "run",
      "automation",
      "result",
      "actions",
      "ranging",
      "sending",
      "notifications.",
      "updating",
      "records",
      "invoking",
      "processes.",
      "extending",
      "include",
      "run-book",
      "on-premises.",
      "import",
      "content",
      "bridge",
      "imported",
      "workflow",
      "record",
      "type",
      "automatic",
      "record.",
      "asymmetric",
      "encryption",
      "protect"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with OO RAS",
    "content": "This topic provides information on why and how to install Operations Orchestration Remote Action Service (OO RAS). Need for OO RAS If you integrated Service Management with Operations Orchestration (OO), you may need to install OO RAS to execute flows in remote data centers and networks. OO RAS that you need to install depends on whether you have integrated Service Management with the standalone OO or OO External RAS. Service Management integrated with standalone OO - Install OO RAS with the installer that's released with the standalone version of OO. For the OO versions supported, see SMAX integration with Operations Orchestration (OO). Service Management integrated with embedded OO - On installing OO Containerized, you will get an out-of-the-box internal RAS. Additionally, you can install external OO RAS if needed. For more information, see Install External OO RAS. Benefits of installing OO RAS Installation of OO RAS adds the following capabilities to your current deployment: OO RAS ",
    "url": "integrateooras",
    "filename": "integrateooras",
    "headings": [
      "Need for OO RAS",
      "Benefits of installing OO RAS",
      "Use cases",
      "Integration procedure",
      "Configure a user in Service Management",
      "Get the suite CA certificate",
      "Install OO RAS",
      "Enable the integration in Service Management",
      "Related topics"
    ],
    "keywords": [
      "https://<External_Access_Host>/idm-admin",
      "below.Log",
      "Groups.For",
      "run.OO",
      "to.Go",
      "aliases.For",
      "list.Go",
      "Topology.The",
      "networks.OO",
      "tab.If",
      "it.If",
      "integrate",
      "oo",
      "ras",
      "need",
      "benefits",
      "installing",
      "cases",
      "integration",
      "procedure",
      "configure",
      "user",
      "service",
      "management",
      "get",
      "suite",
      "ca",
      "certificate",
      "install",
      "enable",
      "related",
      "topics",
      "topic",
      "provides",
      "information",
      "operations",
      "orchestration",
      "remote",
      "action",
      "integrated",
      "execute",
      "flows",
      "data",
      "centers",
      "networks.",
      "depends",
      "whether",
      "standalone",
      "external",
      "ras.",
      "installer",
      "released",
      "version",
      "oo.",
      "versions",
      "supported",
      "see",
      "smax",
      "embedded",
      "containerized",
      "out-of-the-box",
      "internal",
      "additionally",
      "needed.",
      "installation",
      "adds",
      "following",
      "capabilities",
      "current",
      "deployment",
      "contains",
      "protocol",
      "connecting",
      "central",
      "worker",
      "software",
      "component",
      "responsible",
      "running",
      "enables",
      "execution",
      "interacts",
      "polls",
      "supports",
      "grouping",
      "mechanism",
      "correlate",
      "step",
      "flow",
      "type",
      "perform",
      "step.",
      "therefore",
      "binding",
      "between",
      "steps",
      "dynamic.",
      "installed",
      "linux",
      "want"
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with oo ras",
    "contentLower": "this topic provides information on why and how to install operations orchestration remote action service (oo ras). need for oo ras if you integrated service management with operations orchestration (oo), you may need to install oo ras to execute flows in remote data centers and networks. oo ras that you need to install depends on whether you have integrated service management with the standalone oo or oo external ras. service management integrated with standalone oo - install oo ras with the installer that's released with the standalone version of oo. for the oo versions supported, see smax integration with operations orchestration (oo). service management integrated with embedded oo - on installing oo containerized, you will get an out-of-the-box internal ras. additionally, you can install external oo ras if needed. for more information, see install external oo ras. benefits of installing oo ras installation of oo ras adds the following capabilities to your current deployment: oo ras ",
    "keywordsLower": [
      "https://<external_access_host>/idm-admin",
      "below.log",
      "groups.for",
      "run.oo",
      "to.go",
      "aliases.for",
      "list.go",
      "topology.the",
      "networks.oo",
      "tab.if",
      "it.if",
      "integrate",
      "oo",
      "ras",
      "need",
      "benefits",
      "installing",
      "cases",
      "integration",
      "procedure",
      "configure",
      "user",
      "service",
      "management",
      "get",
      "suite",
      "ca",
      "certificate",
      "install",
      "enable",
      "related",
      "topics",
      "topic",
      "provides",
      "information",
      "operations",
      "orchestration",
      "remote",
      "action",
      "integrated",
      "execute",
      "flows",
      "data",
      "centers",
      "networks.",
      "depends",
      "whether",
      "standalone",
      "external",
      "ras.",
      "installer",
      "released",
      "version",
      "oo.",
      "versions",
      "supported",
      "see",
      "smax",
      "embedded",
      "containerized",
      "out-of-the-box",
      "internal",
      "additionally",
      "needed.",
      "installation",
      "adds",
      "following",
      "capabilities",
      "current",
      "deployment",
      "contains",
      "protocol",
      "connecting",
      "central",
      "worker",
      "software",
      "component",
      "responsible",
      "running",
      "enables",
      "execution",
      "interacts",
      "polls",
      "supports",
      "grouping",
      "mechanism",
      "correlate",
      "step",
      "flow",
      "type",
      "perform",
      "step.",
      "therefore",
      "binding",
      "between",
      "steps",
      "dynamic.",
      "installed",
      "linux",
      "want"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Service Manager",
    "content": "Service Management provides a case exchange framework to exchange data between Service Management and Service Manager. The case exchange framework enables you to define an external system with which information will be exchanged. The external system has the ability to create and update records in Service Management as well as to relate those records to records in the external system. Use cases The integration with Service Manager is designed to support use cases involving a Line of Business IT organization that's supported by a Central IT organization. It also supports use-cases involving Central IT and other IT suppliers. The integration enables you to exchange incidents and requests between Service Management and Service Manager. Incident or request records created in Service Management can be assigned to a group that has been registered to a Service Manager system. This sends the record to Service Manager for investigation and resolution. Additional information that's added to the r",
    "url": "integratesm",
    "filename": "integratesm",
    "headings": [
      "Use cases",
      "Set up the integration",
      "Implement the case exchange framework",
      "Configure the integration",
      "Related topics"
    ],
    "keywords": [
      "integrate",
      "service",
      "manager",
      "cases",
      "set",
      "integration",
      "implement",
      "case",
      "exchange",
      "framework",
      "configure",
      "related",
      "topics",
      "management",
      "provides",
      "data",
      "between",
      "manager.",
      "enables",
      "define",
      "external",
      "system",
      "information",
      "exchanged.",
      "ability",
      "create",
      "update",
      "records",
      "well",
      "relate",
      "system.",
      "designed",
      "support",
      "involving",
      "line",
      "business",
      "organization",
      "supported",
      "central",
      "organization.",
      "supports",
      "use-cases",
      "suppliers.",
      "incidents",
      "requests",
      "incident",
      "request",
      "created",
      "assigned",
      "group",
      "registered",
      "sends",
      "record",
      "investigation",
      "resolution.",
      "additional",
      "added",
      "including",
      "comments",
      "attachments",
      "exchanged",
      "updates",
      "done",
      "activities",
      "sent",
      "management.",
      "possible",
      "either",
      "side",
      "includes",
      "necessary",
      "workflow",
      "phases",
      "statuses",
      "two",
      "systems",
      "independent",
      "other.",
      "instead",
      "uses",
      "concept",
      "operations.",
      "operations",
      "bi-directional",
      "provided",
      "record.",
      "tell",
      "receiving",
      "what",
      "needs",
      "determine",
      "steps",
      "modifications",
      "need",
      "implemented.",
      "defining",
      "actual",
      "reduces",
      "perform",
      "complex"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with service manager",
    "contentLower": "service management provides a case exchange framework to exchange data between service management and service manager. the case exchange framework enables you to define an external system with which information will be exchanged. the external system has the ability to create and update records in service management as well as to relate those records to records in the external system. use cases the integration with service manager is designed to support use cases involving a line of business it organization that's supported by a central it organization. it also supports use-cases involving central it and other it suppliers. the integration enables you to exchange incidents and requests between service management and service manager. incident or request records created in service management can be assigned to a group that has been registered to a service manager system. this sends the record to service manager for investigation and resolution. additional information that's added to the r",
    "keywordsLower": [
      "integrate",
      "service",
      "manager",
      "cases",
      "set",
      "integration",
      "implement",
      "case",
      "exchange",
      "framework",
      "configure",
      "related",
      "topics",
      "management",
      "provides",
      "data",
      "between",
      "manager.",
      "enables",
      "define",
      "external",
      "system",
      "information",
      "exchanged.",
      "ability",
      "create",
      "update",
      "records",
      "well",
      "relate",
      "system.",
      "designed",
      "support",
      "involving",
      "line",
      "business",
      "organization",
      "supported",
      "central",
      "organization.",
      "supports",
      "use-cases",
      "suppliers.",
      "incidents",
      "requests",
      "incident",
      "request",
      "created",
      "assigned",
      "group",
      "registered",
      "sends",
      "record",
      "investigation",
      "resolution.",
      "additional",
      "added",
      "including",
      "comments",
      "attachments",
      "exchanged",
      "updates",
      "done",
      "activities",
      "sent",
      "management.",
      "possible",
      "either",
      "side",
      "includes",
      "necessary",
      "workflow",
      "phases",
      "statuses",
      "two",
      "systems",
      "independent",
      "other.",
      "instead",
      "uses",
      "concept",
      "operations.",
      "operations",
      "bi-directional",
      "provided",
      "record.",
      "tell",
      "receiving",
      "what",
      "needs",
      "determine",
      "steps",
      "modifications",
      "need",
      "implemented.",
      "defining",
      "actual",
      "reduces",
      "perform",
      "complex"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Project and Portfolio Management (PPM)",
    "content": "Service Management enables you to synchronize Service Management ideas and proposals with Project and Portfolio Management (PPM) using the On-Premises Bridge. You can then manage these records in PPM: After an idea is reviewed and approved in Service Management, you can choose to create a proposal in Service Management or PPM. If you choose to create a PPM proposal, the proposal's lifecycle will then be managed by PPM. After a Service Management proposal is reviewed and approved, you can create a project in PPM. You can then manage the project as a regular PPM project. Note: The SMAX-PPM integration through the PPM Outbound Integration endpoint is deprecated and will be obsoleted in a future release. Use the Integration Studio-based PPM integration instead. Set up the integration The following steps describe how to synchronize Service Management with PPM. Notes For the product versions supported by this integration, visit the OpenText integrations website. For more information about do",
    "url": "integrateppm",
    "filename": "integrateppm",
    "headings": [
      "Set up the integration",
      "Limitations",
      "General",
      "Mapping ideas to PPM proposals",
      "Mapping proposals to PPM projects",
      "Related topics"
    ],
    "keywords": [
      "mercury.itg",
      "entity.cost",
      "1.0",
      "000.00",
      "30000.00",
      "Java.lang",
      "integrate",
      "project",
      "portfolio",
      "management",
      "ppm",
      "set",
      "integration",
      "limitations",
      "general",
      "mapping",
      "ideas",
      "proposals",
      "projects",
      "related",
      "topics",
      "service",
      "enables",
      "synchronize",
      "on-premises",
      "bridge.",
      "manage",
      "records",
      "after",
      "idea",
      "reviewed",
      "approved",
      "choose",
      "create",
      "proposal",
      "ppm.",
      "lifecycle",
      "managed",
      "regular",
      "project.",
      "note",
      "smax-ppm",
      "through",
      "outbound",
      "endpoint",
      "deprecated",
      "obsoleted",
      "future",
      "release.",
      "studio-based",
      "instead.",
      "following",
      "steps",
      "describe",
      "notes",
      "product",
      "versions",
      "supported",
      "visit",
      "opentext",
      "integrations",
      "website.",
      "information",
      "about",
      "downloading",
      "installing",
      "creating",
      "bridge",
      "agent",
      "see",
      "agents",
      "windows",
      "linux.",
      "download",
      "install",
      "agent.",
      "specify",
      "credentials",
      "type",
      "projectportfoliomanagement-1.0",
      "linux",
      "main",
      "menu",
      "select",
      "administration",
      "utilities",
      "endpoints.",
      "click",
      "add.",
      "enter",
      "details.",
      "field",
      "description",
      "relevant",
      "version.",
      "name",
      "endpoint.",
      "latin",
      "letters",
      "spaces."
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with project and portfolio management (ppm)",
    "contentLower": "service management enables you to synchronize service management ideas and proposals with project and portfolio management (ppm) using the on-premises bridge. you can then manage these records in ppm: after an idea is reviewed and approved in service management, you can choose to create a proposal in service management or ppm. if you choose to create a ppm proposal, the proposal's lifecycle will then be managed by ppm. after a service management proposal is reviewed and approved, you can create a project in ppm. you can then manage the project as a regular ppm project. note: the smax-ppm integration through the ppm outbound integration endpoint is deprecated and will be obsoleted in a future release. use the integration studio-based ppm integration instead. set up the integration the following steps describe how to synchronize service management with ppm. notes for the product versions supported by this integration, visit the opentext integrations website. for more information about do",
    "keywordsLower": [
      "mercury.itg",
      "entity.cost",
      "1.0",
      "000.00",
      "30000.00",
      "java.lang",
      "integrate",
      "project",
      "portfolio",
      "management",
      "ppm",
      "set",
      "integration",
      "limitations",
      "general",
      "mapping",
      "ideas",
      "proposals",
      "projects",
      "related",
      "topics",
      "service",
      "enables",
      "synchronize",
      "on-premises",
      "bridge.",
      "manage",
      "records",
      "after",
      "idea",
      "reviewed",
      "approved",
      "choose",
      "create",
      "proposal",
      "ppm.",
      "lifecycle",
      "managed",
      "regular",
      "project.",
      "note",
      "smax-ppm",
      "through",
      "outbound",
      "endpoint",
      "deprecated",
      "obsoleted",
      "future",
      "release.",
      "studio-based",
      "instead.",
      "following",
      "steps",
      "describe",
      "notes",
      "product",
      "versions",
      "supported",
      "visit",
      "opentext",
      "integrations",
      "website.",
      "information",
      "about",
      "downloading",
      "installing",
      "creating",
      "bridge",
      "agent",
      "see",
      "agents",
      "windows",
      "linux.",
      "download",
      "install",
      "agent.",
      "specify",
      "credentials",
      "type",
      "projectportfoliomanagement-1.0",
      "linux",
      "main",
      "menu",
      "select",
      "administration",
      "utilities",
      "endpoints.",
      "click",
      "add.",
      "enter",
      "details.",
      "field",
      "description",
      "relevant",
      "version.",
      "name",
      "endpoint.",
      "latin",
      "letters",
      "spaces."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Limitations",
    "content": "This topic contains limitations on integration with PPM. General During endpoint mapping configuration, if you map an idea or proposal field to an invalid PPM field and click Sync now, the synchronization operation is completed with no error message returned as long as the PPM field isn't a required field. During endpoint mapping configuration, if the maximum length of the idea or proposal field supported exceeds that of a PPM record field, the value is synchronized to PPM without any truncation. When mapping a Boolean type check box from Service Management to PPM as radio buttons, if the check box is first selected and then deselected, the radio button mapping is incorrect. Mapping ideas to PPM proposals If no value is mapped to the target PPM proposal title field (REQ.KNTA_PROJECT_NAME), the synchronization operation fails with an error message displayed on the Task details page. The following is one of the messages: Java.lang.Integer can not be cast to Java.lang.String Mapping propo",
    "url": "troubleshootsync",
    "filename": "troubleshootsync",
    "headings": [
      "General",
      "Mapping ideas to PPM proposals",
      "Mapping proposals to PPM projects",
      "Related topics"
    ],
    "keywords": [
      "mercury.itg",
      "Java.lang",
      "limitations",
      "general",
      "mapping",
      "ideas",
      "ppm",
      "proposals",
      "projects",
      "related",
      "topics",
      "topic",
      "contains",
      "integration",
      "ppm.",
      "during",
      "endpoint",
      "configuration",
      "map",
      "idea",
      "proposal",
      "field",
      "invalid",
      "click",
      "sync",
      "now",
      "synchronization",
      "operation",
      "completed",
      "error",
      "message",
      "returned",
      "long",
      "isn",
      "required",
      "field.",
      "maximum",
      "length",
      "supported",
      "exceeds",
      "record",
      "value",
      "synchronized",
      "any",
      "truncation.",
      "boolean",
      "type",
      "check",
      "box",
      "service",
      "management",
      "radio",
      "buttons",
      "first",
      "selected",
      "deselected",
      "button",
      "incorrect.",
      "mapped",
      "target",
      "title",
      "fails",
      "displayed",
      "task",
      "details",
      "page.",
      "following",
      "one",
      "messages",
      "java.lang.integer",
      "cast",
      "java.lang.string",
      "project",
      "unknown",
      "null",
      "specified",
      "start",
      "date",
      "end",
      "page",
      "active",
      "time",
      "period",
      "duration",
      "between",
      "longer",
      "less",
      "drop-down",
      "options",
      "disabled",
      "internal",
      "com.mercury.itg.exceptions.warningexception",
      "suggestion",
      "configuring",
      "select",
      "lookup",
      "data",
      "set",
      "value-to-value",
      "relation."
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "limitations",
    "contentLower": "this topic contains limitations on integration with ppm. general during endpoint mapping configuration, if you map an idea or proposal field to an invalid ppm field and click sync now, the synchronization operation is completed with no error message returned as long as the ppm field isn't a required field. during endpoint mapping configuration, if the maximum length of the idea or proposal field supported exceeds that of a ppm record field, the value is synchronized to ppm without any truncation. when mapping a boolean type check box from service management to ppm as radio buttons, if the check box is first selected and then deselected, the radio button mapping is incorrect. mapping ideas to ppm proposals if no value is mapped to the target ppm proposal title field (req.knta_project_name), the synchronization operation fails with an error message displayed on the task details page. the following is one of the messages: java.lang.integer can not be cast to java.lang.string mapping propo",
    "keywordsLower": [
      "mercury.itg",
      "java.lang",
      "limitations",
      "general",
      "mapping",
      "ideas",
      "ppm",
      "proposals",
      "projects",
      "related",
      "topics",
      "topic",
      "contains",
      "integration",
      "ppm.",
      "during",
      "endpoint",
      "configuration",
      "map",
      "idea",
      "proposal",
      "field",
      "invalid",
      "click",
      "sync",
      "now",
      "synchronization",
      "operation",
      "completed",
      "error",
      "message",
      "returned",
      "long",
      "isn",
      "required",
      "field.",
      "maximum",
      "length",
      "supported",
      "exceeds",
      "record",
      "value",
      "synchronized",
      "any",
      "truncation.",
      "boolean",
      "type",
      "check",
      "box",
      "service",
      "management",
      "radio",
      "buttons",
      "first",
      "selected",
      "deselected",
      "button",
      "incorrect.",
      "mapped",
      "target",
      "title",
      "fails",
      "displayed",
      "task",
      "details",
      "page.",
      "following",
      "one",
      "messages",
      "java.lang.integer",
      "cast",
      "java.lang.string",
      "project",
      "unknown",
      "null",
      "specified",
      "start",
      "date",
      "end",
      "page",
      "active",
      "time",
      "period",
      "duration",
      "between",
      "longer",
      "less",
      "drop-down",
      "options",
      "disabled",
      "internal",
      "com.mercury.itg.exceptions.warningexception",
      "suggestion",
      "configuring",
      "select",
      "lookup",
      "data",
      "set",
      "value-to-value",
      "relation."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Extended ECM",
    "content": "OpenText Extended ECM (xECM) is an enterprise content management solution that securely governs the information lifecycle. The Service Management and xECM integration enables you to embed an xECM widget in Service Management so that users can leverage the industry-leading content management capabilities powered by xECM. Overview With this integration, Service Management users can launch an embedded xECM widget by clicking the Related documents tab in a record. This offers the following benefits: Enterprise document management: The embedded widget centralizes document storage and access. It ensures that all documents, regardless of their source, are stored securely in one location for easy retrieval and management. Workflow automation: The embedded widget automates, manages, and tracks document-centric processes. It streamlines tasks, routes documents, assigns responsibilities, ensures compliance, and enhances collaboration for efficient business operations. Version control: The embedde",
    "url": "integratexecmwidget",
    "filename": "integratexecmwidget",
    "headings": [
      "Overview",
      "Set up the integration",
      "Prerequisites",
      "Configure Single Sign-On (SSO)",
      "Configure Service Management",
      "Enable the xECM widget",
      "Display xECM in Service Management",
      "Use and manage the integration",
      "Access the xECM widget",
      "Control the display of the xECM widget"
    ],
    "keywords": [
      "https://<Service",
      "microsoftonline.com",
      "integrate",
      "extended",
      "ecm",
      "overview",
      "set",
      "integration",
      "prerequisites",
      "configure",
      "single",
      "sign-on",
      "sso",
      "service",
      "management",
      "enable",
      "xecm",
      "widget",
      "display",
      "manage",
      "access",
      "control",
      "opentext",
      "enterprise",
      "content",
      "solution",
      "securely",
      "governs",
      "information",
      "lifecycle.",
      "enables",
      "embed",
      "users",
      "leverage",
      "industry-leading",
      "capabilities",
      "powered",
      "xecm.",
      "launch",
      "embedded",
      "clicking",
      "related",
      "documents",
      "tab",
      "record.",
      "offers",
      "following",
      "benefits",
      "document",
      "centralizes",
      "storage",
      "access.",
      "ensures",
      "all",
      "regardless",
      "source",
      "stored",
      "one",
      "location",
      "easy",
      "retrieval",
      "management.",
      "workflow",
      "automation",
      "automates",
      "manages",
      "tracks",
      "document-centric",
      "processes.",
      "streamlines",
      "tasks",
      "routes",
      "assigns",
      "responsibilities",
      "compliance",
      "enhances",
      "collaboration",
      "efficient",
      "business",
      "operations.",
      "version",
      "history",
      "everyone",
      "accesses",
      "works",
      "latest",
      "versions",
      "minimizing",
      "confusion.",
      "secure",
      "external",
      "sharing",
      "capability",
      "controlled",
      "audited",
      "partners",
      "clients",
      "vendors.",
      "sensitive",
      "data"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with extended ecm",
    "contentLower": "opentext extended ecm (xecm) is an enterprise content management solution that securely governs the information lifecycle. the service management and xecm integration enables you to embed an xecm widget in service management so that users can leverage the industry-leading content management capabilities powered by xecm. overview with this integration, service management users can launch an embedded xecm widget by clicking the related documents tab in a record. this offers the following benefits: enterprise document management: the embedded widget centralizes document storage and access. it ensures that all documents, regardless of their source, are stored securely in one location for easy retrieval and management. workflow automation: the embedded widget automates, manages, and tracks document-centric processes. it streamlines tasks, routes documents, assigns responsibilities, ensures compliance, and enhances collaboration for efficient business operations. version control: the embedde",
    "keywordsLower": [
      "https://<service",
      "microsoftonline.com",
      "integrate",
      "extended",
      "ecm",
      "overview",
      "set",
      "integration",
      "prerequisites",
      "configure",
      "single",
      "sign-on",
      "sso",
      "service",
      "management",
      "enable",
      "xecm",
      "widget",
      "display",
      "manage",
      "access",
      "control",
      "opentext",
      "enterprise",
      "content",
      "solution",
      "securely",
      "governs",
      "information",
      "lifecycle.",
      "enables",
      "embed",
      "users",
      "leverage",
      "industry-leading",
      "capabilities",
      "powered",
      "xecm.",
      "launch",
      "embedded",
      "clicking",
      "related",
      "documents",
      "tab",
      "record.",
      "offers",
      "following",
      "benefits",
      "document",
      "centralizes",
      "storage",
      "access.",
      "ensures",
      "all",
      "regardless",
      "source",
      "stored",
      "one",
      "location",
      "easy",
      "retrieval",
      "management.",
      "workflow",
      "automation",
      "automates",
      "manages",
      "tracks",
      "document-centric",
      "processes.",
      "streamlines",
      "tasks",
      "routes",
      "assigns",
      "responsibilities",
      "compliance",
      "enhances",
      "collaboration",
      "efficient",
      "business",
      "operations.",
      "version",
      "history",
      "everyone",
      "accesses",
      "works",
      "latest",
      "versions",
      "minimizing",
      "confusion.",
      "secure",
      "external",
      "sharing",
      "capability",
      "controlled",
      "audited",
      "partners",
      "clients",
      "vendors.",
      "sensitive",
      "data"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Operations Bridge Manager (OBM)",
    "content": "Operations Bridge Manager (OBM) automatically discovers and monitors the IT infrastructure of an enterprise by collecting topology and event data. You can use event data to resolve infrastructure or service issues. The CI topology helps check the health and performance of the IT infrastructure. The Operations Bridge Manager framework provides you with a single pane of glass to detect, correlate, solve, and prevent problems occurring in networks, systems, and applications. OBM and Service Management can be integrated to share important information between users working in Enterprise Service Management as well as users working in Service Assurance. The integrated solution offers three major use cases. Closed Loop Incident Process (CLIP) This use case enables you to forward events from OBM to OpenText Service Management to create and update incidents and synchronize incident changes back from Service Management to OBM events. With this integration, engineers working on incidents or events",
    "url": "integrateobm",
    "filename": "integrateobm",
    "headings": [
      "Closed Loop Incident Process (CLIP)",
      "Embedded Event Browser",
      "Schedule downtime",
      "Related documents"
    ],
    "keywords": [
      "integrate",
      "operations",
      "bridge",
      "manager",
      "obm",
      "closed",
      "loop",
      "incident",
      "process",
      "clip",
      "embedded",
      "event",
      "browser",
      "schedule",
      "downtime",
      "related",
      "documents",
      "automatically",
      "discovers",
      "monitors",
      "infrastructure",
      "enterprise",
      "collecting",
      "topology",
      "data.",
      "data",
      "resolve",
      "service",
      "issues.",
      "ci",
      "helps",
      "check",
      "health",
      "performance",
      "infrastructure.",
      "framework",
      "provides",
      "single",
      "pane",
      "glass",
      "detect",
      "correlate",
      "solve",
      "prevent",
      "problems",
      "occurring",
      "networks",
      "systems",
      "applications.",
      "management",
      "integrated",
      "share",
      "important",
      "information",
      "between",
      "users",
      "working",
      "well",
      "assurance.",
      "solution",
      "offers",
      "three",
      "major",
      "cases.",
      "case",
      "enables",
      "forward",
      "events",
      "opentext",
      "create",
      "update",
      "incidents",
      "synchronize",
      "changes",
      "back",
      "events.",
      "integration",
      "engineers",
      "informed",
      "about",
      "ongoing",
      "activities",
      "both",
      "systems.",
      "efficiently",
      "troubleshoot",
      "fast",
      "possible.",
      "comparison",
      "analysts",
      "see",
      "lot",
      "additional",
      "incidents.",
      "often",
      "struggle",
      "finding",
      "meaningful",
      "analyze",
      "raised"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with operations bridge manager (obm)",
    "contentLower": "operations bridge manager (obm) automatically discovers and monitors the it infrastructure of an enterprise by collecting topology and event data. you can use event data to resolve infrastructure or service issues. the ci topology helps check the health and performance of the it infrastructure. the operations bridge manager framework provides you with a single pane of glass to detect, correlate, solve, and prevent problems occurring in networks, systems, and applications. obm and service management can be integrated to share important information between users working in enterprise service management as well as users working in service assurance. the integrated solution offers three major use cases. closed loop incident process (clip) this use case enables you to forward events from obm to opentext service management to create and update incidents and synchronize incident changes back from service management to obm events. with this integration, engineers working on incidents or events",
    "keywordsLower": [
      "integrate",
      "operations",
      "bridge",
      "manager",
      "obm",
      "closed",
      "loop",
      "incident",
      "process",
      "clip",
      "embedded",
      "event",
      "browser",
      "schedule",
      "downtime",
      "related",
      "documents",
      "automatically",
      "discovers",
      "monitors",
      "infrastructure",
      "enterprise",
      "collecting",
      "topology",
      "data.",
      "data",
      "resolve",
      "service",
      "issues.",
      "ci",
      "helps",
      "check",
      "health",
      "performance",
      "infrastructure.",
      "framework",
      "provides",
      "single",
      "pane",
      "glass",
      "detect",
      "correlate",
      "solve",
      "prevent",
      "problems",
      "occurring",
      "networks",
      "systems",
      "applications.",
      "management",
      "integrated",
      "share",
      "important",
      "information",
      "between",
      "users",
      "working",
      "well",
      "assurance.",
      "solution",
      "offers",
      "three",
      "major",
      "cases.",
      "case",
      "enables",
      "forward",
      "events",
      "opentext",
      "create",
      "update",
      "incidents",
      "synchronize",
      "changes",
      "back",
      "events.",
      "integration",
      "engineers",
      "informed",
      "about",
      "ongoing",
      "activities",
      "both",
      "systems.",
      "efficiently",
      "troubleshoot",
      "fast",
      "possible.",
      "comparison",
      "analysts",
      "see",
      "lot",
      "additional",
      "incidents.",
      "often",
      "struggle",
      "finding",
      "meaningful",
      "analyze",
      "raised"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Connect-It",
    "content": "You can also use Connect-It to integrate OpenText Service Management with other products. The instructions in this section outline the configurations for Service Management. Prerequisite Connect-It Management as a Service connector and Service Anywhere User connector rely on a user with sufficient rights to access the SaaS environment and write a Service Management database. As a consequence, an Integration user must be created in the Suite Administration portal for your Service Management tenant. This user must be granted administration rights to access and update the Service Management tenant. In Service Management administration console, the Connect-It Integration user must be granted tenant admin privileges. These 2 conditons must be met to prevent any error when running your Connect-It integration scenario. Install Connect-It You can download Connect-It 9.90 from the Software Licenses and Downloads (SLD) website. The following are Connect-It 9.90 documentation links: Help center P",
    "url": "integratecit",
    "filename": "integratecit",
    "headings": [
      "Prerequisite",
      "Install Connect-It",
      "Import the suite CA certificate to a local truststore",
      "Configure Service Management as a Service connector",
      "Apply security patch on JRE/JDK 1.8"
    ],
    "keywords": [
      "1.8",
      "sma_CA.cert",
      "readme.txt",
      "https://<IP",
      "2133166.html",
      "http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html",
      "oracle.com",
      "9.90",
      "keystore.jks",
      "integrate",
      "connect-it",
      "prerequisite",
      "install",
      "import",
      "suite",
      "ca",
      "certificate",
      "local",
      "truststore",
      "configure",
      "service",
      "management",
      "connector",
      "apply",
      "security",
      "patch",
      "jre",
      "jdk",
      "opentext",
      "products.",
      "instructions",
      "section",
      "outline",
      "configurations",
      "management.",
      "anywhere",
      "user",
      "rely",
      "sufficient",
      "rights",
      "access",
      "saas",
      "environment",
      "write",
      "database.",
      "consequence",
      "integration",
      "created",
      "administration",
      "portal",
      "tenant.",
      "granted",
      "update",
      "console",
      "tenant",
      "admin",
      "privileges.",
      "conditons",
      "met",
      "prevent",
      "any",
      "error",
      "running",
      "scenario.",
      "download",
      "software",
      "licenses",
      "downloads",
      "sld",
      "website.",
      "following",
      "documentation",
      "links",
      "help",
      "center",
      "product",
      "manuals",
      "requires",
      "put",
      "truststore.",
      "steps",
      "get",
      "certificate.",
      "detailed",
      "see",
      "make",
      "sure",
      "latest",
      "version",
      "openssl",
      "installed.",
      "run",
      "command",
      "keytool",
      "-importcert",
      "-alias",
      "-file",
      "-keystore",
      "-storepass",
      "placeholder"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with connect-it",
    "contentLower": "you can also use connect-it to integrate opentext service management with other products. the instructions in this section outline the configurations for service management. prerequisite connect-it management as a service connector and service anywhere user connector rely on a user with sufficient rights to access the saas environment and write a service management database. as a consequence, an integration user must be created in the suite administration portal for your service management tenant. this user must be granted administration rights to access and update the service management tenant. in service management administration console, the connect-it integration user must be granted tenant admin privileges. these 2 conditons must be met to prevent any error when running your connect-it integration scenario. install connect-it you can download connect-it 9.90 from the software licenses and downloads (sld) website. the following are connect-it 9.90 documentation links: help center p",
    "keywordsLower": [
      "1.8",
      "sma_ca.cert",
      "readme.txt",
      "https://<ip",
      "2133166.html",
      "http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html",
      "oracle.com",
      "9.90",
      "keystore.jks",
      "integrate",
      "connect-it",
      "prerequisite",
      "install",
      "import",
      "suite",
      "ca",
      "certificate",
      "local",
      "truststore",
      "configure",
      "service",
      "management",
      "connector",
      "apply",
      "security",
      "patch",
      "jre",
      "jdk",
      "opentext",
      "products.",
      "instructions",
      "section",
      "outline",
      "configurations",
      "management.",
      "anywhere",
      "user",
      "rely",
      "sufficient",
      "rights",
      "access",
      "saas",
      "environment",
      "write",
      "database.",
      "consequence",
      "integration",
      "created",
      "administration",
      "portal",
      "tenant.",
      "granted",
      "update",
      "console",
      "tenant",
      "admin",
      "privileges.",
      "conditons",
      "met",
      "prevent",
      "any",
      "error",
      "running",
      "scenario.",
      "download",
      "software",
      "licenses",
      "downloads",
      "sld",
      "website.",
      "following",
      "documentation",
      "links",
      "help",
      "center",
      "product",
      "manuals",
      "requires",
      "put",
      "truststore.",
      "steps",
      "get",
      "certificate.",
      "detailed",
      "see",
      "make",
      "sure",
      "latest",
      "version",
      "openssl",
      "installed.",
      "run",
      "command",
      "keytool",
      "-importcert",
      "-alias",
      "-file",
      "-keystore",
      "-storepass",
      "placeholder"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—ALM Octane integration",
    "content": "OverviewThis topic presents an illustrated set of integration use cases between Service Management and Octane:Portal-based Defect or Enhancement requestsIncident to Defect integrationThe use cases in a nutshell:Service Management:Main/IntegrateJira Use case 1: Portal-based Defect or Enhancement requestsIn this sample use case, end users who subscribe to a specific Business Service Application can report defects or request enhancements to the application by posting them via the Service Management Service Portal.We will follow the steps below, which are performed by Eve, an employee who uses her company’s eExpense application for expense tracking and reporting. She has been frustrated that expense reporting can only be done while logged on to the eExpense web application and wishes that her company would provide an enhancement to enable native mobile access for faster expense reporting. While Eve can post the enhancement request via the Portal, her company is tracking such Application re",
    "url": "integrateoctane",
    "filename": "integrateoctane",
    "headings": [
      "Overview",
      "Use case 1: Portal-based Defect or Enhancement requests",
      "Step 3: Octane Feature created and confirmation provided back to the user",
      "Step 4: Status confirmation",
      "Step 5: Behind the scenes - association of the Octane ticket with the Service Management Request ticket",
      "Use case 2: Change request to Enhancement",
      "Use case 3: Incident/Request/Problem to defect",
      "Use case 4: Keep exchanged records in sync",
      "Set up the integration",
      "Prepare an integration user",
      "Export the certificate from the Octane instance",
      "Configure Service Management",
      "Create an endpoint",
      "Create an integration",
      "Configure the scenarios",
      "Use the scenarios",
      "Use case 1 – Portal-based Defect or Enhancement requests",
      "Specify user options",
      "Define a task plan",
      "Use case 2 – Change request to feature"
    ],
    "keywords": [
      "studioalm",
      "backlog.Step",
      "needed.As",
      "Studio.The",
      "current_user.Upn",
      "listener.In",
      "Task.Give",
      "record.Use",
      "system.Use",
      "Definition.We",
      "record.In",
      "tracking.Save",
      "Management.The",
      "section.Set",
      "page.In",
      "integration.In",
      "integrations.Tip",
      "Portal.We",
      "application.The",
      "https://<FQDN>/authentication/sign_inCertificate",
      "Octane.Pull",
      "Agent.If",
      "2.0",
      "Octane.Use",
      "https.Base",
      "X.509",
      "tracking.In",
      "entity.Id",
      "https://<FQDN>Authentication",
      "integration",
      "studio",
      "alm",
      "octane",
      "overview",
      "case",
      "portal-based",
      "defect",
      "enhancement",
      "requests",
      "step",
      "feature",
      "created",
      "confirmation",
      "provided",
      "back",
      "user",
      "status",
      "behind",
      "scenes",
      "association",
      "ticket",
      "service",
      "management",
      "request",
      "change",
      "incident",
      "problem",
      "keep",
      "exchanged",
      "records",
      "sync",
      "set",
      "prepare",
      "export",
      "certificate",
      "instance",
      "configure",
      "create",
      "endpoint",
      "scenarios",
      "specify",
      "options",
      "define",
      "task",
      "plan",
      "pull",
      "updates",
      "demand",
      "process",
      "incoming",
      "events",
      "overviewthis",
      "topic",
      "presents",
      "illustrated",
      "cases",
      "between",
      "requestsincident",
      "integrationthe",
      "nutshell",
      "main",
      "integratejira",
      "requestsin",
      "sample",
      "end",
      "users",
      "subscribe",
      "specific",
      "business",
      "application"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—alm octane integration",
    "contentLower": "overviewthis topic presents an illustrated set of integration use cases between service management and octane:portal-based defect or enhancement requestsincident to defect integrationthe use cases in a nutshell:service management:main/integratejira use case 1: portal-based defect or enhancement requestsin this sample use case, end users who subscribe to a specific business service application can report defects or request enhancements to the application by posting them via the service management service portal.we will follow the steps below, which are performed by eve, an employee who uses her company’s eexpense application for expense tracking and reporting. she has been frustrated that expense reporting can only be done while logged on to the eexpense web application and wishes that her company would provide an enhancement to enable native mobile access for faster expense reporting. while eve can post the enhancement request via the portal, her company is tracking such application re",
    "keywordsLower": [
      "studioalm",
      "backlog.step",
      "needed.as",
      "studio.the",
      "current_user.upn",
      "listener.in",
      "task.give",
      "record.use",
      "system.use",
      "definition.we",
      "record.in",
      "tracking.save",
      "management.the",
      "section.set",
      "page.in",
      "integration.in",
      "integrations.tip",
      "portal.we",
      "application.the",
      "https://<fqdn>/authentication/sign_incertificate",
      "octane.pull",
      "agent.if",
      "2.0",
      "octane.use",
      "https.base",
      "x.509",
      "tracking.in",
      "entity.id",
      "https://<fqdn>authentication",
      "integration",
      "studio",
      "alm",
      "octane",
      "overview",
      "case",
      "portal-based",
      "defect",
      "enhancement",
      "requests",
      "step",
      "feature",
      "created",
      "confirmation",
      "provided",
      "back",
      "user",
      "status",
      "behind",
      "scenes",
      "association",
      "ticket",
      "service",
      "management",
      "request",
      "change",
      "incident",
      "problem",
      "keep",
      "exchanged",
      "records",
      "sync",
      "set",
      "prepare",
      "export",
      "certificate",
      "instance",
      "configure",
      "create",
      "endpoint",
      "scenarios",
      "specify",
      "options",
      "define",
      "task",
      "plan",
      "pull",
      "updates",
      "demand",
      "process",
      "incoming",
      "events",
      "overviewthis",
      "topic",
      "presents",
      "illustrated",
      "cases",
      "between",
      "requestsincident",
      "integrationthe",
      "nutshell",
      "main",
      "integratejira",
      "requestsin",
      "sample",
      "end",
      "users",
      "subscribe",
      "specific",
      "business",
      "application"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with HCM",
    "content": "Note The SMAX-HCM integration has been deprecated since the 2020.05 release, and may be obsoleted in a future release. Customers can deploy and enable the Design and Deploy (DND) capability to use aggregated offerings instead of this integration (see Create and edit aggregation providers). This topic describes two use cases based on an integration between SMAX and OpenText Hybrid Cloud Management (HCM): VM provisioning in HCM via SMAX Service Portal VM provisioning failure handling in SMAX-HCM It also provides step-by-step guidance for setting up the SMAX-HCM integration to support the use cases. Use case 1: VM provisioning in HCM via SMAX Service Portal In this use case, Amy, as one R&D engineer, needs to request one new Linux VM from vCenter for her daily work. Knowing that the SMAX Self-Service Portal is integrated with HCM so that end users may request VMs from the Portal directly and that she has access to a variety of self-service support features such as catalog, knowledge, and ",
    "url": "integratehcm",
    "filename": "integratehcm",
    "headings": [
      "Use case 1: VM provisioning in HCM via SMAX Service Portal",
      "Step 1: End user accesses self-service portal",
      "Step 2: End user submits VM Request with the help of Virtual Agent",
      "Step 3: IT Agent approves VM request",
      "Step 4: HCM fulfillment tasks run to completion",
      "Step 5: Agent checks HCM subscription details via SMAX request",
      "Step 6: Agent gets HCM subscription status via SMAX request",
      "Step 7: End users gets success feedback and logs in to VM",
      "Use case 2: VM provisioning failure handling in SMAX-HCM",
      "Behind the scenes: setting up the integration",
      "SMAX",
      "Step 1: Set up an On-Premises Bridge agent",
      "Step 2: Add an OO endpoint for invoking OO flows",
      "Step 3: Add a REST execution endpoint for invoking OO flows via REST APIs",
      "Step 4: Create new fields for requests and offerings",
      "Step 5: Expose new fields in request and offering forms",
      "Step 6: Configure one catalog offering",
      "User Options",
      "Rules",
      "After change"
    ],
    "keywords": [
      "10.2.11",
      "4.92",
      "https://myhost.mycompany.net:8444",
      "2.11",
      "https://myconsumerportalhost.mycompany.net:8089",
      "2020.05",
      "https://myoohost.mycompany.net:8444",
      "https://mycsahost.mycompany.net:8444",
      "entity.Id",
      "https://mysmaxhost.mycompany.net",
      "https://10.2.11.247:8444",
      "mycompany.net",
      "integrate",
      "hcm",
      "case",
      "vm",
      "provisioning",
      "via",
      "smax",
      "service",
      "portal",
      "step",
      "end",
      "user",
      "accesses",
      "self-service",
      "submits",
      "request",
      "help",
      "virtual",
      "agent",
      "approves",
      "fulfillment",
      "tasks",
      "run",
      "completion",
      "checks",
      "subscription",
      "details",
      "gets",
      "status",
      "users",
      "success",
      "feedback",
      "logs",
      "failure",
      "handling",
      "smax-hcm",
      "behind",
      "scenes",
      "setting",
      "integration",
      "set",
      "on-premises",
      "bridge",
      "add",
      "oo",
      "endpoint",
      "invoking",
      "flows",
      "rest",
      "execution",
      "apis",
      "create",
      "new",
      "fields",
      "requests",
      "offerings",
      "expose",
      "offering",
      "forms",
      "configure",
      "one",
      "catalog",
      "options",
      "rules",
      "after",
      "change",
      "rendering",
      "task",
      "plan",
      "approve",
      "phase",
      "fulfill",
      "default",
      "values",
      "workflow",
      "rule-1",
      "rule-2",
      "rule-3",
      "rule-4",
      "rule-5",
      "custom",
      "action",
      "incident",
      "model",
      "10",
      "settings",
      "apply",
      "content"
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with hcm",
    "contentLower": "note the smax-hcm integration has been deprecated since the 2020.05 release, and may be obsoleted in a future release. customers can deploy and enable the design and deploy (dnd) capability to use aggregated offerings instead of this integration (see create and edit aggregation providers). this topic describes two use cases based on an integration between smax and opentext hybrid cloud management (hcm): vm provisioning in hcm via smax service portal vm provisioning failure handling in smax-hcm it also provides step-by-step guidance for setting up the smax-hcm integration to support the use cases. use case 1: vm provisioning in hcm via smax service portal in this use case, amy, as one r&d engineer, needs to request one new linux vm from vcenter for her daily work. knowing that the smax self-service portal is integrated with hcm so that end users may request vms from the portal directly and that she has access to a variety of self-service support features such as catalog, knowledge, and ",
    "keywordsLower": [
      "10.2.11",
      "4.92",
      "https://myhost.mycompany.net:8444",
      "2.11",
      "https://myconsumerportalhost.mycompany.net:8089",
      "2020.05",
      "https://myoohost.mycompany.net:8444",
      "https://mycsahost.mycompany.net:8444",
      "entity.id",
      "https://mysmaxhost.mycompany.net",
      "https://10.2.11.247:8444",
      "mycompany.net",
      "integrate",
      "hcm",
      "case",
      "vm",
      "provisioning",
      "via",
      "smax",
      "service",
      "portal",
      "step",
      "end",
      "user",
      "accesses",
      "self-service",
      "submits",
      "request",
      "help",
      "virtual",
      "agent",
      "approves",
      "fulfillment",
      "tasks",
      "run",
      "completion",
      "checks",
      "subscription",
      "details",
      "gets",
      "status",
      "users",
      "success",
      "feedback",
      "logs",
      "failure",
      "handling",
      "smax-hcm",
      "behind",
      "scenes",
      "setting",
      "integration",
      "set",
      "on-premises",
      "bridge",
      "add",
      "oo",
      "endpoint",
      "invoking",
      "flows",
      "rest",
      "execution",
      "apis",
      "create",
      "new",
      "fields",
      "requests",
      "offerings",
      "expose",
      "offering",
      "forms",
      "configure",
      "one",
      "catalog",
      "options",
      "rules",
      "after",
      "change",
      "rendering",
      "task",
      "plan",
      "approve",
      "phase",
      "fulfill",
      "default",
      "values",
      "workflow",
      "rule-1",
      "rule-2",
      "rule-3",
      "rule-4",
      "rule-5",
      "custom",
      "action",
      "incident",
      "model",
      "10",
      "settings",
      "apply",
      "content"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio - ZENworks",
    "content": "You can integrate Service Management with ZENworks. This integration enables you to import devices from ZENworks into the UCMDB and the SACM module. During the import, the system attempts to identify the device owner to assign it to the correct user in Service Management. The user in Service Management can then view all assigned devices in the Service Portal under My Services and Assets. Furthermore, configuration managers or administrators can run remote actions against the imported devices. For example, they can restart a device, lock it, or start a virus scan directly from the device record in Service Management. When using remote actions to manage devices via ZENworks, it’s your responsibility to set the correct permissions in the ZENworks application to ensure the integration is not misused for harmful actions. Supported use cases Import devices from ZENworks into Service Management, to manage them in Service Management and give endusers the abiltiy to see their devices in the sel",
    "url": "integratezenworks",
    "filename": "integratezenworks",
    "headings": [
      "Supported use cases",
      "Data integration flow",
      "Import devices",
      "Import bundles",
      "Set up the integration",
      "Prerequisites",
      "Network connectivity",
      "ZENworks Credentials",
      "Configurations in UCMDB",
      "Configurations in Service Management",
      "Prepare an integration user in Service Management",
      "Create endpoints",
      "Zenworks",
      "Use agentless endpoint",
      "Universal CMDB",
      "Create an integration",
      "Create using ZENworks - Import devices scenario template",
      "Configure the scenario",
      "Parameters to consider before using the scenario",
      "Run the scenario"
    ],
    "keywords": [
      "https://<FQDN>:<port",
      "page.In",
      "current_user.Name",
      "integration.In",
      "https://<FQDN>/ucmdb-server/rest-api/authenticate",
      "tab.Go",
      "group.Add",
      "https://<FQDN",
      "access.For",
      "scenario.In",
      "entity.Go",
      "https://<ZENworks",
      "record.In",
      "2.0",
      "data.Go",
      "integration",
      "studio",
      "zenworks",
      "supported",
      "cases",
      "data",
      "flow",
      "import",
      "devices",
      "bundles",
      "set",
      "prerequisites",
      "network",
      "connectivity",
      "credentials",
      "configurations",
      "ucmdb",
      "service",
      "management",
      "prepare",
      "user",
      "create",
      "endpoints",
      "agentless",
      "endpoint",
      "universal",
      "cmdb",
      "scenario",
      "template",
      "configure",
      "parameters",
      "consider",
      "before",
      "run",
      "additional",
      "information",
      "execute",
      "action",
      "custom",
      "deploy",
      "software",
      "bundle",
      "catalog",
      "offering",
      "via",
      "task",
      "plan",
      "integrate",
      "zenworks.",
      "enables",
      "sacm",
      "module.",
      "during",
      "system",
      "attempts",
      "identify",
      "device",
      "owner",
      "assign",
      "correct",
      "management.",
      "view",
      "all",
      "assigned",
      "portal",
      "under",
      "services",
      "assets.",
      "furthermore",
      "configuration",
      "managers",
      "administrators",
      "remote",
      "actions",
      "against",
      "imported",
      "devices.",
      "example",
      "restart",
      "lock",
      "start",
      "virus",
      "scan",
      "directly",
      "record"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio - zenworks",
    "contentLower": "you can integrate service management with zenworks. this integration enables you to import devices from zenworks into the ucmdb and the sacm module. during the import, the system attempts to identify the device owner to assign it to the correct user in service management. the user in service management can then view all assigned devices in the service portal under my services and assets. furthermore, configuration managers or administrators can run remote actions against the imported devices. for example, they can restart a device, lock it, or start a virus scan directly from the device record in service management. when using remote actions to manage devices via zenworks, it’s your responsibility to set the correct permissions in the zenworks application to ensure the integration is not misused for harmful actions. supported use cases import devices from zenworks into service management, to manage them in service management and give endusers the abiltiy to see their devices in the sel",
    "keywordsLower": [
      "https://<fqdn>:<port",
      "page.in",
      "current_user.name",
      "integration.in",
      "https://<fqdn>/ucmdb-server/rest-api/authenticate",
      "tab.go",
      "group.add",
      "https://<fqdn",
      "access.for",
      "scenario.in",
      "entity.go",
      "https://<zenworks",
      "record.in",
      "2.0",
      "data.go",
      "integration",
      "studio",
      "zenworks",
      "supported",
      "cases",
      "data",
      "flow",
      "import",
      "devices",
      "bundles",
      "set",
      "prerequisites",
      "network",
      "connectivity",
      "credentials",
      "configurations",
      "ucmdb",
      "service",
      "management",
      "prepare",
      "user",
      "create",
      "endpoints",
      "agentless",
      "endpoint",
      "universal",
      "cmdb",
      "scenario",
      "template",
      "configure",
      "parameters",
      "consider",
      "before",
      "run",
      "additional",
      "information",
      "execute",
      "action",
      "custom",
      "deploy",
      "software",
      "bundle",
      "catalog",
      "offering",
      "via",
      "task",
      "plan",
      "integrate",
      "zenworks.",
      "enables",
      "sacm",
      "module.",
      "during",
      "system",
      "attempts",
      "identify",
      "device",
      "owner",
      "assign",
      "correct",
      "management.",
      "view",
      "all",
      "assigned",
      "portal",
      "under",
      "services",
      "assets.",
      "furthermore",
      "configuration",
      "managers",
      "administrators",
      "remote",
      "actions",
      "against",
      "imported",
      "devices.",
      "example",
      "restart",
      "lock",
      "start",
      "virus",
      "scan",
      "directly",
      "record"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio - Endpoint Management",
    "content": "You can integrate Service Management with Endpoint Management. This integration enables you to import devices from Endpoint Management into the UCMDB and the SACM module. During the import, the system attempts to identify the device owner to assign it to the correct user in Service Management. The user in Service Management can then view all assigned devices in the Service Portal under My Services and Assets. Furthermore, configuration managers or administrators can run remote actions against the imported devices. For example, they can restart a device, lock it, or start a virus scan directly from the device record in Service Management. When using remote actions to manage devices via Endpoint Management, it’s your responsibility to set the correct permissions in the Endpoint Management application to ensure the integration is not misused for harmful actions. Supported use cases Import devices from Endpoint Management into Service Management, to manage them in Service Management and gi",
    "url": "integrateendpointmanagement",
    "filename": "integrateendpointmanagement",
    "headings": [
      "Supported use cases",
      "Data integration flow",
      "Importing devices",
      "Importing bundles",
      "Set up the integration",
      "Prerequisites",
      "Network connectivity",
      "Endpoint Management Credentials",
      "Configurations in UCMDB",
      "Configurations in Service Management",
      "Prepare an integration user in Service Management",
      "Create endpoints",
      "Endpoint Management",
      "Use agentless endpoint",
      "Universal CMDB",
      "Create an integration",
      "Use the scenario to import devices",
      "Configure the scenario",
      "Parameters to consider before using the scenario",
      "Run the scenario"
    ],
    "keywords": [
      "https://<FQDN>:<port",
      "page.In",
      "current_user.Name",
      "integration.In",
      "https://<FQDN>/ucmdb-server/rest-api/authenticate",
      "tab.Go",
      "https://<FQDN",
      "entity.Go",
      "record.In",
      "2.0",
      "data.Go",
      "integration",
      "studio",
      "endpoint",
      "management",
      "supported",
      "cases",
      "data",
      "flow",
      "importing",
      "devices",
      "bundles",
      "set",
      "prerequisites",
      "network",
      "connectivity",
      "credentials",
      "configurations",
      "ucmdb",
      "service",
      "prepare",
      "user",
      "create",
      "endpoints",
      "agentless",
      "universal",
      "cmdb",
      "scenario",
      "import",
      "configure",
      "parameters",
      "consider",
      "before",
      "run",
      "additional",
      "information",
      "remote",
      "actions",
      "custom",
      "action",
      "deploy",
      "software",
      "bundle",
      "catalog",
      "offering",
      "via",
      "task",
      "plan",
      "integrate",
      "management.",
      "enables",
      "sacm",
      "module.",
      "during",
      "system",
      "attempts",
      "identify",
      "device",
      "owner",
      "assign",
      "correct",
      "view",
      "all",
      "assigned",
      "portal",
      "under",
      "services",
      "assets.",
      "furthermore",
      "configuration",
      "managers",
      "administrators",
      "against",
      "imported",
      "devices.",
      "example",
      "restart",
      "lock",
      "start",
      "virus",
      "scan",
      "directly",
      "record",
      "manage",
      "responsibility",
      "permissions",
      "application",
      "ensure",
      "misused",
      "harmful"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio - endpoint management",
    "contentLower": "you can integrate service management with endpoint management. this integration enables you to import devices from endpoint management into the ucmdb and the sacm module. during the import, the system attempts to identify the device owner to assign it to the correct user in service management. the user in service management can then view all assigned devices in the service portal under my services and assets. furthermore, configuration managers or administrators can run remote actions against the imported devices. for example, they can restart a device, lock it, or start a virus scan directly from the device record in service management. when using remote actions to manage devices via endpoint management, it’s your responsibility to set the correct permissions in the endpoint management application to ensure the integration is not misused for harmful actions. supported use cases import devices from endpoint management into service management, to manage them in service management and gi",
    "keywordsLower": [
      "https://<fqdn>:<port",
      "page.in",
      "current_user.name",
      "integration.in",
      "https://<fqdn>/ucmdb-server/rest-api/authenticate",
      "tab.go",
      "https://<fqdn",
      "entity.go",
      "record.in",
      "2.0",
      "data.go",
      "integration",
      "studio",
      "endpoint",
      "management",
      "supported",
      "cases",
      "data",
      "flow",
      "importing",
      "devices",
      "bundles",
      "set",
      "prerequisites",
      "network",
      "connectivity",
      "credentials",
      "configurations",
      "ucmdb",
      "service",
      "prepare",
      "user",
      "create",
      "endpoints",
      "agentless",
      "universal",
      "cmdb",
      "scenario",
      "import",
      "configure",
      "parameters",
      "consider",
      "before",
      "run",
      "additional",
      "information",
      "remote",
      "actions",
      "custom",
      "action",
      "deploy",
      "software",
      "bundle",
      "catalog",
      "offering",
      "via",
      "task",
      "plan",
      "integrate",
      "management.",
      "enables",
      "sacm",
      "module.",
      "during",
      "system",
      "attempts",
      "identify",
      "device",
      "owner",
      "assign",
      "correct",
      "view",
      "all",
      "assigned",
      "portal",
      "under",
      "services",
      "assets.",
      "furthermore",
      "configuration",
      "managers",
      "administrators",
      "against",
      "imported",
      "devices.",
      "example",
      "restart",
      "lock",
      "start",
      "virus",
      "scan",
      "directly",
      "record",
      "manage",
      "responsibility",
      "permissions",
      "application",
      "ensure",
      "misused",
      "harmful"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration with third-party products",
    "content": "Service Management supports integration with the following third-party products. For support matrix information about these integrations, visit the Integration Central website. You can also use Connect-It to integrate with other products. For more information about the Connect-It configuration, see Integrate with Connect-It.It's recommended to create a distinct user for each integration.For all integrations that use OPB, you need to create only one integration user for the OPB agent to access Service Management.An integration user that's defined in Service Management consumes a license unless it has the \"Integration user\" role defined in Suite Administration. Third-party integrations You can integrate the suite with the following third-party products. Product Description Method Documentation Asana Enables you to import user entitlements from Asana. Integration Studio Integrate with Asana for SAM Atlassian Confluence Enables you to index knowledge from Confluence. IDOL connector and OPB",
    "url": "thirdpartyintegrations",
    "filename": "thirdpartyintegrations",
    "headings": [
      "Third-party integrations",
      "Related topics"
    ],
    "keywords": [
      "It.It",
      "integration.For",
      "Management.An",
      "integration",
      "third-party",
      "products",
      "integrations",
      "related",
      "topics",
      "service",
      "management",
      "supports",
      "following",
      "products.",
      "support",
      "matrix",
      "information",
      "about",
      "visit",
      "central",
      "website.",
      "connect-it",
      "integrate",
      "configuration",
      "see",
      "connect-it.it",
      "recommended",
      "create",
      "distinct",
      "user",
      "all",
      "opb",
      "need",
      "one",
      "agent",
      "access",
      "defined",
      "consumes",
      "license",
      "unless",
      "role",
      "suite",
      "administration.",
      "product",
      "description",
      "method",
      "documentation",
      "asana",
      "enables",
      "import",
      "entitlements",
      "asana.",
      "studio",
      "sam",
      "atlassian",
      "confluence",
      "index",
      "knowledge",
      "confluence.",
      "idol",
      "connector",
      "jira",
      "set",
      "bi-directional",
      "submit",
      "defects",
      "enhancement",
      "requests",
      "portal",
      "record",
      "types.",
      "allows",
      "keep",
      "data",
      "sync",
      "both",
      "systems.",
      "aws",
      "governance",
      "billing",
      "cloud",
      "accounts",
      "cost",
      "reports",
      "account",
      "features",
      "reporting.",
      "provider",
      "reporting",
      "business",
      "intelligence",
      "bi",
      "define",
      "fields",
      "relationships",
      "synced",
      "type",
      "integrating",
      "external",
      "system."
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration with third-party products",
    "contentLower": "service management supports integration with the following third-party products. for support matrix information about these integrations, visit the integration central website. you can also use connect-it to integrate with other products. for more information about the connect-it configuration, see integrate with connect-it.it's recommended to create a distinct user for each integration.for all integrations that use opb, you need to create only one integration user for the opb agent to access service management.an integration user that's defined in service management consumes a license unless it has the \"integration user\" role defined in suite administration. third-party integrations you can integrate the suite with the following third-party products. product description method documentation asana enables you to import user entitlements from asana. integration studio integrate with asana for sam atlassian confluence enables you to index knowledge from confluence. idol connector and opb",
    "keywordsLower": [
      "it.it",
      "integration.for",
      "management.an",
      "integration",
      "third-party",
      "products",
      "integrations",
      "related",
      "topics",
      "service",
      "management",
      "supports",
      "following",
      "products.",
      "support",
      "matrix",
      "information",
      "about",
      "visit",
      "central",
      "website.",
      "connect-it",
      "integrate",
      "configuration",
      "see",
      "connect-it.it",
      "recommended",
      "create",
      "distinct",
      "user",
      "all",
      "opb",
      "need",
      "one",
      "agent",
      "access",
      "defined",
      "consumes",
      "license",
      "unless",
      "role",
      "suite",
      "administration.",
      "product",
      "description",
      "method",
      "documentation",
      "asana",
      "enables",
      "import",
      "entitlements",
      "asana.",
      "studio",
      "sam",
      "atlassian",
      "confluence",
      "index",
      "knowledge",
      "confluence.",
      "idol",
      "connector",
      "jira",
      "set",
      "bi-directional",
      "submit",
      "defects",
      "enhancement",
      "requests",
      "portal",
      "record",
      "types.",
      "allows",
      "keep",
      "data",
      "sync",
      "both",
      "systems.",
      "aws",
      "governance",
      "billing",
      "cloud",
      "accounts",
      "cost",
      "reports",
      "account",
      "features",
      "reporting.",
      "provider",
      "reporting",
      "business",
      "intelligence",
      "bi",
      "define",
      "fields",
      "relationships",
      "synced",
      "type",
      "integrating",
      "external",
      "system."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with cloud platforms for Cloud Cost Reporting",
    "content": "To use the Cloud Cost Reporting solution, firstly you need to retrieve cloud data from the cloud platforms that we support: Amazon Web Services (AWS), Azure, and Oracle Cloud Infrastructure (OCI). The Cloud Cost Reporting solution uses two types of cloud data: Cloud billing data – Cloud billing data is used to generate prebuilt Cloud Cost Reports and to support custom reporting in Operations Cloud.Cloud account data –  Cloud account data is retrieved so you can centrally manage cloud accounts on supported platforms using the account management functionality. You need to set up two types of integrations to retrieve the cloud data: Cloud Cost Data Provider-based integrations that retrieve cloud billing dataIntegration Studio-based integrations that import cloud account data See these topics for how to set up the integrations to retrieve cloud data from each cloud platform: Integrate with AWS for Cloud Cost ReportingIntegrate with Azure for Cloud Cost Reporting Related topics Integration ",
    "url": "integratecloudproviderfinops",
    "filename": "integratecloudproviderfinops",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "integrate",
      "cloud",
      "platforms",
      "cost",
      "reporting",
      "related",
      "topics",
      "solution",
      "firstly",
      "need",
      "retrieve",
      "data",
      "support",
      "amazon",
      "web",
      "services",
      "aws",
      "azure",
      "oracle",
      "infrastructure",
      "oci",
      "uses",
      "two",
      "types",
      "billing",
      "generate",
      "prebuilt",
      "reports",
      "custom",
      "operations",
      "cloud.cloud",
      "account",
      "retrieved",
      "centrally",
      "manage",
      "accounts",
      "supported",
      "management",
      "functionality.",
      "set",
      "integrations",
      "provider-based",
      "dataintegration",
      "studio-based",
      "import",
      "see",
      "platform",
      "reportingintegrate",
      "integration",
      "studiocloud",
      "providerscloud",
      "operationscost"
    ],
    "language": "en",
    "word_count": 111,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with cloud platforms for cloud cost reporting",
    "contentLower": "to use the cloud cost reporting solution, firstly you need to retrieve cloud data from the cloud platforms that we support: amazon web services (aws), azure, and oracle cloud infrastructure (oci). the cloud cost reporting solution uses two types of cloud data: cloud billing data – cloud billing data is used to generate prebuilt cloud cost reports and to support custom reporting in operations cloud.cloud account data –  cloud account data is retrieved so you can centrally manage cloud accounts on supported platforms using the account management functionality. you need to set up two types of integrations to retrieve the cloud data: cloud cost data provider-based integrations that retrieve cloud billing dataintegration studio-based integrations that import cloud account data see these topics for how to set up the integrations to retrieve cloud data from each cloud platform: integrate with aws for cloud cost reportingintegrate with azure for cloud cost reporting related topics integration ",
    "keywordsLower": [
      "integrate",
      "cloud",
      "platforms",
      "cost",
      "reporting",
      "related",
      "topics",
      "solution",
      "firstly",
      "need",
      "retrieve",
      "data",
      "support",
      "amazon",
      "web",
      "services",
      "aws",
      "azure",
      "oracle",
      "infrastructure",
      "oci",
      "uses",
      "two",
      "types",
      "billing",
      "generate",
      "prebuilt",
      "reports",
      "custom",
      "operations",
      "cloud.cloud",
      "account",
      "retrieved",
      "centrally",
      "manage",
      "accounts",
      "supported",
      "management",
      "functionality.",
      "set",
      "integrations",
      "provider-based",
      "dataintegration",
      "studio-based",
      "import",
      "see",
      "platform",
      "reportingintegrate",
      "integration",
      "studiocloud",
      "providerscloud",
      "operationscost"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with AWS for Cloud Cost Reporting",
    "content": "Before you can use the Cloud Cost Reporting capability, you must set up integrations with the cloud platforms from which you want to pull cloud data. We support integrations with Amazon Web Services (AWS), Azure, and Oracle Cloud Infrastructure (OCI). The Cloud Cost Reporting capability uses two types of cloud data. You will need to set up a separate integration to retrieve Cloud Account Data and Cloud Billing Data. Cloud account data – Cloud account data is retrieved so you can centrally manage cloud accounts on supported platforms using the account management functionality. You must set up an Integration Studio-based integration to import cloud account data.Cloud billing data –  Cloud billing data is used to generate prebuilt Cloud Cost Reports and to support custom reporting in Operations Cloud. You must set up a Cloud Cost Data Provider-based integration to retrieve cloud billing data. This topic describes how to set up the two AWS integrations that support the Cloud Cost Reporting",
    "url": "syncawsprovider",
    "filename": "syncawsprovider",
    "headings": [
      "Complete prerequisite tasks in AWS",
      "Set up IAM user and permissions",
      "Enable rightsizing recommendations",
      "Configure data export",
      "Set up an integration via Integration Studio to import cloud accounts",
      "Prepare an integration user",
      "Configure endpoint",
      "Create an integration",
      "Configure a scenario",
      "Use IdleDaysToCleanUp to control the stale cloud account cleanup duration",
      "Schedule the import of cloud accounts",
      "Check the scenario execution status",
      "Troubleshooting tips",
      "Configure a cloud cost data provider to retrieve billing data",
      "Create a cost reporting integration",
      "Related topics"
    ],
    "keywords": [
      "page.Add",
      "activity.The",
      "integrations.For",
      "s3://<billing",
      "error.Open",
      "page.In",
      "integration.In",
      "s3://bucket3///reportPath1",
      "amazon.com",
      "Scheduler.For",
      "administrator.Go",
      "sync.In",
      "user.Note",
      "Providers.On",
      "CUR.For",
      "https://console.aws.amazon.com/s3/.In",
      "Studio.For",
      "integration.Log",
      "status.If",
      "2.0",
      "https://<External",
      "console.aws",
      "options.Save",
      "page.On",
      "integrate",
      "aws",
      "cloud",
      "cost",
      "reporting",
      "complete",
      "prerequisite",
      "tasks",
      "set",
      "iam",
      "user",
      "permissions",
      "enable",
      "rightsizing",
      "recommendations",
      "configure",
      "data",
      "export",
      "integration",
      "via",
      "studio",
      "import",
      "accounts",
      "prepare",
      "endpoint",
      "create",
      "scenario",
      "idledaystocleanup",
      "control",
      "stale",
      "account",
      "cleanup",
      "duration",
      "schedule",
      "check",
      "execution",
      "status",
      "troubleshooting",
      "tips",
      "provider",
      "retrieve",
      "billing",
      "related",
      "topics",
      "before",
      "capability",
      "integrations",
      "platforms",
      "want",
      "pull",
      "data.",
      "support",
      "amazon",
      "web",
      "services",
      "azure",
      "oracle",
      "infrastructure",
      "oci",
      "uses",
      "two",
      "types",
      "need",
      "separate",
      "retrieved",
      "centrally",
      "manage",
      "supported",
      "management",
      "functionality.",
      "studio-based",
      "data.cloud",
      "generate",
      "prebuilt",
      "reports",
      "custom"
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with aws for cloud cost reporting",
    "contentLower": "before you can use the cloud cost reporting capability, you must set up integrations with the cloud platforms from which you want to pull cloud data. we support integrations with amazon web services (aws), azure, and oracle cloud infrastructure (oci). the cloud cost reporting capability uses two types of cloud data. you will need to set up a separate integration to retrieve cloud account data and cloud billing data. cloud account data – cloud account data is retrieved so you can centrally manage cloud accounts on supported platforms using the account management functionality. you must set up an integration studio-based integration to import cloud account data.cloud billing data –  cloud billing data is used to generate prebuilt cloud cost reports and to support custom reporting in operations cloud. you must set up a cloud cost data provider-based integration to retrieve cloud billing data. this topic describes how to set up the two aws integrations that support the cloud cost reporting",
    "keywordsLower": [
      "page.add",
      "activity.the",
      "integrations.for",
      "s3://<billing",
      "error.open",
      "page.in",
      "integration.in",
      "s3://bucket3///reportpath1",
      "amazon.com",
      "scheduler.for",
      "administrator.go",
      "sync.in",
      "user.note",
      "providers.on",
      "cur.for",
      "https://console.aws.amazon.com/s3/.in",
      "studio.for",
      "integration.log",
      "status.if",
      "2.0",
      "https://<external",
      "console.aws",
      "options.save",
      "page.on",
      "integrate",
      "aws",
      "cloud",
      "cost",
      "reporting",
      "complete",
      "prerequisite",
      "tasks",
      "set",
      "iam",
      "user",
      "permissions",
      "enable",
      "rightsizing",
      "recommendations",
      "configure",
      "data",
      "export",
      "integration",
      "via",
      "studio",
      "import",
      "accounts",
      "prepare",
      "endpoint",
      "create",
      "scenario",
      "idledaystocleanup",
      "control",
      "stale",
      "account",
      "cleanup",
      "duration",
      "schedule",
      "check",
      "execution",
      "status",
      "troubleshooting",
      "tips",
      "provider",
      "retrieve",
      "billing",
      "related",
      "topics",
      "before",
      "capability",
      "integrations",
      "platforms",
      "want",
      "pull",
      "data.",
      "support",
      "amazon",
      "web",
      "services",
      "azure",
      "oracle",
      "infrastructure",
      "oci",
      "uses",
      "two",
      "types",
      "need",
      "separate",
      "retrieved",
      "centrally",
      "manage",
      "supported",
      "management",
      "functionality.",
      "studio-based",
      "data.cloud",
      "generate",
      "prebuilt",
      "reports",
      "custom"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Azure for Cloud Cost Reporting",
    "content": "Before you can use the Cloud Cost Reporting solution, you must set up integrations with the cloud platforms from which you want to pull cloud data. We support integrations with Amazon Web Services (AWS), Azure, and Oracle Cloud Infrastructure (OCI).The Cloud Cost Reporting capability uses two types of cloud data. You will need to set up a separate integration to retrieve Cloud Account Data and Cloud Billing Data.Cloud account data – Cloud account data is retrieved so you can centrally manage cloud accounts on supported platforms using the account management functionality. You must set up an Integration Studio-based integration to import cloud account data.Cloud billing data – Cloud billing data is used to generate prebuilt Cloud Cost Reports and to support custom reporting in Operations Cloud. You must set up a Cloud Cost Data Provider-based integration to retrieve cloud billing data.This topic describes how to set up the two Azure integrations that support the Cloud Cost Reporting cap",
    "url": "syncazureprovider",
    "filename": "syncazureprovider",
    "headings": [
      "Complete prerequisite tasks in the Azure portal",
      "Register an app to create a service principal",
      "Assign permissions",
      "Set up storage for the billing data",
      "Set up an Integration Studio-based integration to import cloud accounts",
      "Prepare an integration user",
      "Create an endpoint",
      "Create an integration",
      "Configure a scenario",
      "Schedule the import of cloud accounts",
      "Check scenario execution status",
      "Configure a cloud cost data provider to retrieve billing data",
      "Troubleshooting tips",
      "Related topics"
    ],
    "keywords": [
      "issues.Open",
      "interface.Log",
      "status.You",
      "activity.The",
      "integrations.For",
      "microsoftonline.com",
      "export.If",
      "export.In",
      "ResourceValuehttps://management.azure.comClick",
      "page.In",
      "integration.In",
      "Scheduler.For",
      "administrator.Go",
      "tab.In",
      "sync.In",
      "https://management.azure.comCertificate",
      "it.Log",
      "support.Set",
      "used.In",
      "Providers.On",
      "Studio.For",
      "documentation.Set",
      "integration.Log",
      "status.If",
      "2.0",
      "https://<External",
      "empty.Send",
      "ReadWrite.All",
      "storage.In",
      "https.Base",
      "URLhttps://login.microsoftonline.com/<Azure",
      "ID.For",
      "providers.For",
      "01.In",
      "data.This",
      "integrate",
      "azure",
      "cloud",
      "cost",
      "reporting",
      "complete",
      "prerequisite",
      "tasks",
      "portal",
      "register",
      "app",
      "create",
      "service",
      "principal",
      "assign",
      "permissions",
      "set",
      "storage",
      "billing",
      "data",
      "integration",
      "studio-based",
      "import",
      "accounts",
      "prepare",
      "user",
      "endpoint",
      "configure",
      "scenario",
      "schedule",
      "check",
      "execution",
      "status",
      "provider",
      "retrieve",
      "troubleshooting",
      "tips",
      "related",
      "topics",
      "before",
      "solution",
      "integrations",
      "platforms",
      "want",
      "pull",
      "data.",
      "support",
      "amazon",
      "web",
      "services",
      "aws",
      "oracle",
      "infrastructure",
      "oci",
      ".the",
      "capability",
      "uses",
      "two",
      "types",
      "need",
      "separate",
      "account",
      "data.cloud",
      "retrieved",
      "centrally"
    ],
    "language": "en",
    "word_count": 109,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with azure for cloud cost reporting",
    "contentLower": "before you can use the cloud cost reporting solution, you must set up integrations with the cloud platforms from which you want to pull cloud data. we support integrations with amazon web services (aws), azure, and oracle cloud infrastructure (oci).the cloud cost reporting capability uses two types of cloud data. you will need to set up a separate integration to retrieve cloud account data and cloud billing data.cloud account data – cloud account data is retrieved so you can centrally manage cloud accounts on supported platforms using the account management functionality. you must set up an integration studio-based integration to import cloud account data.cloud billing data – cloud billing data is used to generate prebuilt cloud cost reports and to support custom reporting in operations cloud. you must set up a cloud cost data provider-based integration to retrieve cloud billing data.this topic describes how to set up the two azure integrations that support the cloud cost reporting cap",
    "keywordsLower": [
      "issues.open",
      "interface.log",
      "status.you",
      "activity.the",
      "integrations.for",
      "microsoftonline.com",
      "export.if",
      "export.in",
      "resourcevaluehttps://management.azure.comclick",
      "page.in",
      "integration.in",
      "scheduler.for",
      "administrator.go",
      "tab.in",
      "sync.in",
      "https://management.azure.comcertificate",
      "it.log",
      "support.set",
      "used.in",
      "providers.on",
      "studio.for",
      "documentation.set",
      "integration.log",
      "status.if",
      "2.0",
      "https://<external",
      "empty.send",
      "readwrite.all",
      "storage.in",
      "https.base",
      "urlhttps://login.microsoftonline.com/<azure",
      "id.for",
      "providers.for",
      "01.in",
      "data.this",
      "integrate",
      "azure",
      "cloud",
      "cost",
      "reporting",
      "complete",
      "prerequisite",
      "tasks",
      "portal",
      "register",
      "app",
      "create",
      "service",
      "principal",
      "assign",
      "permissions",
      "set",
      "storage",
      "billing",
      "data",
      "integration",
      "studio-based",
      "import",
      "accounts",
      "prepare",
      "user",
      "endpoint",
      "configure",
      "scenario",
      "schedule",
      "check",
      "execution",
      "status",
      "provider",
      "retrieve",
      "troubleshooting",
      "tips",
      "related",
      "topics",
      "before",
      "solution",
      "integrations",
      "platforms",
      "want",
      "pull",
      "data.",
      "support",
      "amazon",
      "web",
      "services",
      "aws",
      "oracle",
      "infrastructure",
      "oci",
      ".the",
      "capability",
      "uses",
      "two",
      "types",
      "need",
      "separate",
      "account",
      "data.cloud",
      "retrieved",
      "centrally"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with GCP for CMP FinOps",
    "content": "This topic describes how to set up two GCP integrations that support the CMP FinOps capability: the cloud accounts import integration and billing data retrieval integration.Complete prerequisite tasks in GCPTo integrate with GCP, you must complete the following tasks in GCP.Complete tasks as Google Cloud administratorAsk your Google Cloud administrator to do the following.Assign the following roles to your Google account on the organization. These roles ensure the cloud accounts import integration has enough permission to retrieve the required accounts.Predefined roles: Organization ViewerCustom roles: A custom role that contains only the resourcemanager.projects.get permission.For the cloud accounts import integration, assigning these roles on the organization level enables you to retrieve all cloud accounts with one Google account and one integration. If the organization has a large number of projects and performance is a concern, you can assign the roles on the folder level to the G",
    "url": "syncgcpprovider",
    "filename": "syncgcpprovider",
    "headings": [
      "Complete prerequisite tasks in GCP",
      "Complete tasks as Google Cloud administrator",
      "Create credentials used by GCP integrations",
      "Export billing data to BigQuery table and copy table ID",
      "Set up an Integration Studio-based integration to import cloud accounts",
      "Prepare an integration user",
      "Create an endpoint",
      "Create an integration",
      "Configure a scenario",
      "Schedule the import of cloud accounts",
      "Check the scenario execution status",
      "Troubleshooting tips",
      "Configure a cloud cost data provider to import billing data",
      "Related topics"
    ],
    "keywords": [
      "issues.Open",
      "interface.Log",
      "BigQuery.In",
      "platform.read",
      "status.You",
      "activity.The",
      "page.Log",
      "integrations.For",
      "URLhttps://cloudresourcemanager.googleapis.comCertificate",
      "BigQuery.For",
      "page.In",
      "projects.get",
      "integration.In",
      "export.For",
      "Scheduler.For",
      "URLhttps://oauth2.googleapis.com/tokenScopehttps://www.googleapis.com/auth/cloud-platform.read-onlyStateLeave",
      "administrator.Go",
      "sync.In",
      "URLhttps://accounts.google.com/o/oauth2/v2/auth?access_type=offline&prompt=consentRedirect",
      "permission.For",
      "Providers.On",
      "Studio.For",
      "integration.Log",
      "google.com",
      "status.If",
      "2.0",
      "https://<External",
      "empty.Send",
      "https.Base",
      "needs.Get",
      "googleapis.com",
      "providers.For",
      "integrate",
      "gcp",
      "cmp",
      "finops",
      "complete",
      "prerequisite",
      "tasks",
      "google",
      "cloud",
      "administrator",
      "create",
      "credentials",
      "integrations",
      "export",
      "billing",
      "data",
      "bigquery",
      "table",
      "copy",
      "id",
      "set",
      "integration",
      "studio-based",
      "import",
      "accounts",
      "prepare",
      "user",
      "endpoint",
      "configure",
      "scenario",
      "schedule",
      "check",
      "execution",
      "status",
      "troubleshooting",
      "tips",
      "cost",
      "provider",
      "related",
      "topics",
      "topic",
      "describes",
      "two",
      "support",
      "capability",
      "retrieval",
      "integration.complete",
      "gcpto",
      "following",
      "gcp.complete",
      "administratorask",
      "following.assign",
      "roles",
      "account",
      "organization.",
      "ensure",
      "enough",
      "permission",
      "retrieve",
      "required",
      "accounts.predefined",
      "organization",
      "viewercustom",
      "custom",
      "role",
      "contains",
      "resourcemanager.projects.get",
      "assigning"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with gcp for cmp finops",
    "contentLower": "this topic describes how to set up two gcp integrations that support the cmp finops capability: the cloud accounts import integration and billing data retrieval integration.complete prerequisite tasks in gcpto integrate with gcp, you must complete the following tasks in gcp.complete tasks as google cloud administratorask your google cloud administrator to do the following.assign the following roles to your google account on the organization. these roles ensure the cloud accounts import integration has enough permission to retrieve the required accounts.predefined roles: organization viewercustom roles: a custom role that contains only the resourcemanager.projects.get permission.for the cloud accounts import integration, assigning these roles on the organization level enables you to retrieve all cloud accounts with one google account and one integration. if the organization has a large number of projects and performance is a concern, you can assign the roles on the folder level to the g",
    "keywordsLower": [
      "issues.open",
      "interface.log",
      "bigquery.in",
      "platform.read",
      "status.you",
      "activity.the",
      "page.log",
      "integrations.for",
      "urlhttps://cloudresourcemanager.googleapis.comcertificate",
      "bigquery.for",
      "page.in",
      "projects.get",
      "integration.in",
      "export.for",
      "scheduler.for",
      "urlhttps://oauth2.googleapis.com/tokenscopehttps://www.googleapis.com/auth/cloud-platform.read-onlystateleave",
      "administrator.go",
      "sync.in",
      "urlhttps://accounts.google.com/o/oauth2/v2/auth?access_type=offline&prompt=consentredirect",
      "permission.for",
      "providers.on",
      "studio.for",
      "integration.log",
      "google.com",
      "status.if",
      "2.0",
      "https://<external",
      "empty.send",
      "https.base",
      "needs.get",
      "googleapis.com",
      "providers.for",
      "integrate",
      "gcp",
      "cmp",
      "finops",
      "complete",
      "prerequisite",
      "tasks",
      "google",
      "cloud",
      "administrator",
      "create",
      "credentials",
      "integrations",
      "export",
      "billing",
      "data",
      "bigquery",
      "table",
      "copy",
      "id",
      "set",
      "integration",
      "studio-based",
      "import",
      "accounts",
      "prepare",
      "user",
      "endpoint",
      "configure",
      "scenario",
      "schedule",
      "check",
      "execution",
      "status",
      "troubleshooting",
      "tips",
      "cost",
      "provider",
      "related",
      "topics",
      "topic",
      "describes",
      "two",
      "support",
      "capability",
      "retrieval",
      "integration.complete",
      "gcpto",
      "following",
      "gcp.complete",
      "administratorask",
      "following.assign",
      "roles",
      "account",
      "organization.",
      "ensure",
      "enough",
      "permission",
      "retrieve",
      "required",
      "accounts.predefined",
      "organization",
      "viewercustom",
      "custom",
      "role",
      "contains",
      "resourcemanager.projects.get",
      "assigning"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—WhatsApp integration",
    "content": "You can integrate Service Management with WhatsApp. This integration enables you to proactively inform users about important issues in your environment.The following depicts a typical use case. When a major incident happens, the system can send a WhatsApp message to all major incident team members. The integration use cases described in this document serve as examples to inspire and guide your integration efforts. However, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. The examples may or may not be applicable for your license edition. Set up the integrationAs an example, the procedure below will set up the integration to implement the documented use case.The business logic for the integration is defined by the predefined scenario template Send major incident notification, which sends WhatsApp messages to the Major Incident team members. You can create an integration scenario based on the scenario template, and then update some envir",
    "url": "integratewhatsapp",
    "filename": "integratewhatsapp",
    "headings": [
      "Set up the integration",
      "Prerequisites",
      "Prepare an integration user",
      "Create an endpoint",
      "Create an integration",
      "Configure the scenario",
      "Use the scenario",
      "Understand the integration"
    ],
    "keywords": [
      "studiowhatsapp",
      "user.Log",
      "message.The",
      "environment.The",
      "page.In",
      "integration.In",
      "list.Note",
      "Management.Gets",
      "Save.Use",
      "scenario.In",
      "case.The",
      "rule.Note",
      "https://graph.facebook.com",
      "2.0",
      "parameters.Gets",
      "modules.At",
      "facebook.com",
      "integration",
      "studio",
      "whatsapp",
      "set",
      "prerequisites",
      "prepare",
      "user",
      "create",
      "endpoint",
      "configure",
      "scenario",
      "understand",
      "integrate",
      "service",
      "management",
      "whatsapp.",
      "enables",
      "proactively",
      "inform",
      "users",
      "about",
      "important",
      "issues",
      "following",
      "depicts",
      "typical",
      "case.",
      "major",
      "incident",
      "happens",
      "system",
      "send",
      "message",
      "all",
      "team",
      "members.",
      "cases",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "edition.",
      "integrationas",
      "example",
      "procedure",
      "below",
      "implement",
      "documented",
      "business",
      "logic",
      "defined",
      "predefined",
      "template",
      "notification",
      "sends",
      "messages",
      "based",
      "update",
      "environment-specific",
      "control",
      "parameters",
      "fine-tune",
      "behavior.prerequisitesto",
      "make",
      "sure",
      "available",
      "account",
      "including",
      "registered",
      "app",
      "see"
    ],
    "language": "en",
    "word_count": 90,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—whatsapp integration",
    "contentLower": "you can integrate service management with whatsapp. this integration enables you to proactively inform users about important issues in your environment.the following depicts a typical use case. when a major incident happens, the system can send a whatsapp message to all major incident team members. the integration use cases described in this document serve as examples to inspire and guide your integration efforts. however, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. the examples may or may not be applicable for your license edition. set up the integrationas an example, the procedure below will set up the integration to implement the documented use case.the business logic for the integration is defined by the predefined scenario template send major incident notification, which sends whatsapp messages to the major incident team members. you can create an integration scenario based on the scenario template, and then update some envir",
    "keywordsLower": [
      "studiowhatsapp",
      "user.log",
      "message.the",
      "environment.the",
      "page.in",
      "integration.in",
      "list.note",
      "management.gets",
      "save.use",
      "scenario.in",
      "case.the",
      "rule.note",
      "https://graph.facebook.com",
      "2.0",
      "parameters.gets",
      "modules.at",
      "facebook.com",
      "integration",
      "studio",
      "whatsapp",
      "set",
      "prerequisites",
      "prepare",
      "user",
      "create",
      "endpoint",
      "configure",
      "scenario",
      "understand",
      "integrate",
      "service",
      "management",
      "whatsapp.",
      "enables",
      "proactively",
      "inform",
      "users",
      "about",
      "important",
      "issues",
      "following",
      "depicts",
      "typical",
      "case.",
      "major",
      "incident",
      "happens",
      "system",
      "send",
      "message",
      "all",
      "team",
      "members.",
      "cases",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "edition.",
      "integrationas",
      "example",
      "procedure",
      "below",
      "implement",
      "documented",
      "business",
      "logic",
      "defined",
      "predefined",
      "template",
      "notification",
      "sends",
      "messages",
      "based",
      "update",
      "environment-specific",
      "control",
      "parameters",
      "fine-tune",
      "behavior.prerequisitesto",
      "make",
      "sure",
      "available",
      "account",
      "including",
      "registered",
      "app",
      "see"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—Microsoft Intune integration",
    "content": "You can integrate Service Management with Microsoft Intune. This integration enables you to import devices/endpoints from Intune into the SACM module. During the import, the system tries to identify the owner of the device to assign it to the correct user in Service Management. Once the owner is set, the user can see all the assigned devices in the Service Portal under “My Services and Assets”.The integration can be scheduled to regularly import the data. The integration can be configured in a way to set devices not found for a period of time as “missed” in Service Management, so that the configuration manager can start a manual review process.Furthermore, configuration managers or administrators can run remote actions against the imported Intune devices. For example, they can restart a device, lock it, or start a virus scan directly from the device record in Service Management.When using remote actions to manage devices via Intune, it’s your responsibility to set the correct permissio",
    "url": "integrateintune",
    "filename": "integrateintune",
    "headings": [
      "Set up the integration",
      "Prerequisites",
      "Prepare an integration user",
      "Create an endpoint",
      "Create an integration",
      "Configure the scenario",
      "Parameters to consider before using the scenario",
      "Use the scenario",
      "Understand the integration",
      "The query parameter definition against the MS Graph API (2)"
    ],
    "keywords": [
      "studiomicrosoft",
      "devices.In",
      "Microsoft.You",
      "Studio.The",
      "Intune.The",
      "Management.When",
      "flag.The",
      "page.In",
      "integration.In",
      "microsoft.com",
      "API.You",
      "devices.The",
      "needed.The",
      "DeviceManagementManagedDevices.Read",
      "offline_access.See",
      "satisfied.You",
      "default.In",
      "2.0",
      "modules.At",
      "ReadWrite.All",
      "user.Log",
      "https.Base",
      "PrivilegedOperations.All",
      "API.The",
      "Hostname.In",
      "https://graph.microsoft.com.Certificate",
      "rules.The",
      "true.Use",
      "above.The",
      "integration",
      "studio",
      "microsoft",
      "intune",
      "set",
      "prerequisites",
      "prepare",
      "user",
      "create",
      "endpoint",
      "configure",
      "scenario",
      "parameters",
      "consider",
      "before",
      "understand",
      "query",
      "parameter",
      "definition",
      "against",
      "ms",
      "graph",
      "api",
      "integrate",
      "service",
      "management",
      "intune.",
      "enables",
      "import",
      "devices",
      "endpoints",
      "sacm",
      "module.",
      "during",
      "system",
      "tries",
      "identify",
      "owner",
      "device",
      "assign",
      "correct",
      "management.",
      "once",
      "see",
      "all",
      "assigned",
      "portal",
      "under",
      "services",
      "assets",
      ".the",
      "scheduled",
      "regularly",
      "data.",
      "configured",
      "way",
      "found",
      "period",
      "time",
      "missed",
      "configuration",
      "manager",
      "start",
      "manual",
      "review",
      "process.furthermore",
      "managers",
      "administrators",
      "run",
      "remote",
      "actions"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—microsoft intune integration",
    "contentLower": "you can integrate service management with microsoft intune. this integration enables you to import devices/endpoints from intune into the sacm module. during the import, the system tries to identify the owner of the device to assign it to the correct user in service management. once the owner is set, the user can see all the assigned devices in the service portal under “my services and assets”.the integration can be scheduled to regularly import the data. the integration can be configured in a way to set devices not found for a period of time as “missed” in service management, so that the configuration manager can start a manual review process.furthermore, configuration managers or administrators can run remote actions against the imported intune devices. for example, they can restart a device, lock it, or start a virus scan directly from the device record in service management.when using remote actions to manage devices via intune, it’s your responsibility to set the correct permissio",
    "keywordsLower": [
      "studiomicrosoft",
      "devices.in",
      "microsoft.you",
      "studio.the",
      "intune.the",
      "management.when",
      "flag.the",
      "page.in",
      "integration.in",
      "microsoft.com",
      "api.you",
      "devices.the",
      "needed.the",
      "devicemanagementmanageddevices.read",
      "offline_access.see",
      "satisfied.you",
      "default.in",
      "2.0",
      "modules.at",
      "readwrite.all",
      "user.log",
      "https.base",
      "privilegedoperations.all",
      "api.the",
      "hostname.in",
      "https://graph.microsoft.com.certificate",
      "rules.the",
      "true.use",
      "above.the",
      "integration",
      "studio",
      "microsoft",
      "intune",
      "set",
      "prerequisites",
      "prepare",
      "user",
      "create",
      "endpoint",
      "configure",
      "scenario",
      "parameters",
      "consider",
      "before",
      "understand",
      "query",
      "parameter",
      "definition",
      "against",
      "ms",
      "graph",
      "api",
      "integrate",
      "service",
      "management",
      "intune.",
      "enables",
      "import",
      "devices",
      "endpoints",
      "sacm",
      "module.",
      "during",
      "system",
      "tries",
      "identify",
      "owner",
      "device",
      "assign",
      "correct",
      "management.",
      "once",
      "see",
      "all",
      "assigned",
      "portal",
      "under",
      "services",
      "assets",
      ".the",
      "scheduled",
      "regularly",
      "data.",
      "configured",
      "way",
      "found",
      "period",
      "time",
      "missed",
      "configuration",
      "manager",
      "start",
      "manual",
      "review",
      "process.furthermore",
      "managers",
      "administrators",
      "run",
      "remote",
      "actions"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—Datadog integration",
    "content": "Overview It is now possible to integrate with Datadog by using the Integration Studio. The Datadog integration templates support the following use cases: Datadog Incident to SMAX Incident Schedule downtime in Datadog Note: The integration use cases described in this document serve as examples to inspire and guide your integration efforts. However, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. The examples may or may not be applicable to your license edition. Use case—Datadog Incident to SMAX Incident exchange This is a typical case exchange use case, where an Event in Datadog first becomes an Incident in Datadog, which is then forwarded to SMAX to create an Incident in Service Management. The use case in a nutshell is illustrated below: Use case—Schedule downtime This use case enables you to schedule downtime in Datadog. In case a change request in Service Management has scheduled downtime planned for the involved CIs, the downtime ",
    "url": "integratedatadog",
    "filename": "integratedatadog",
    "headings": [
      "Overview",
      "Use case—Datadog Incident to SMAX Incident exchange",
      "Use case—Schedule downtime",
      "Technical overview",
      "Set up the integration",
      "Generate API and application keys in Datadog",
      "Prepare an integration user",
      "Create an endpoint",
      "Create an integration",
      "Add and configure the scenarios",
      "Configure Datadog",
      "Create a webhook",
      "Modify Incident settings",
      "Create Incident forwarding rules",
      "Use the scenarios",
      "Process webhooks for Incidents",
      "Update Incident in Datadog",
      "Schedule downtime"
    ],
    "keywords": [
      "studiodatadog",
      "2.0",
      "current_user.Upn",
      "https://api.<datadog",
      "entity.Id",
      "integration",
      "studio",
      "datadog",
      "overview",
      "case",
      "incident",
      "smax",
      "exchange",
      "schedule",
      "downtime",
      "technical",
      "set",
      "generate",
      "api",
      "application",
      "keys",
      "prepare",
      "user",
      "create",
      "endpoint",
      "add",
      "configure",
      "scenarios",
      "webhook",
      "modify",
      "settings",
      "forwarding",
      "rules",
      "process",
      "webhooks",
      "incidents",
      "update",
      "now",
      "possible",
      "integrate",
      "studio.",
      "templates",
      "support",
      "following",
      "cases",
      "note",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "edition.",
      "typical",
      "event",
      "first",
      "becomes",
      "forwarded",
      "service",
      "management.",
      "nutshell",
      "illustrated",
      "below",
      "enables",
      "datadog.",
      "change",
      "request",
      "management",
      "scheduled",
      "planned",
      "involved",
      "cis",
      "data",
      "raise",
      "events",
      "during",
      "downtime.",
      "diagram",
      "illustrates",
      "components",
      "provided",
      "example",
      "cases.",
      "created",
      "out",
      "one",
      "events.",
      "every",
      "action",
      "triggered"
    ],
    "language": "en",
    "word_count": 91,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—datadog integration",
    "contentLower": "overview it is now possible to integrate with datadog by using the integration studio. the datadog integration templates support the following use cases: datadog incident to smax incident schedule downtime in datadog note: the integration use cases described in this document serve as examples to inspire and guide your integration efforts. however, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. the examples may or may not be applicable to your license edition. use case—datadog incident to smax incident exchange this is a typical case exchange use case, where an event in datadog first becomes an incident in datadog, which is then forwarded to smax to create an incident in service management. the use case in a nutshell is illustrated below: use case—schedule downtime this use case enables you to schedule downtime in datadog. in case a change request in service management has scheduled downtime planned for the involved cis, the downtime ",
    "keywordsLower": [
      "studiodatadog",
      "2.0",
      "current_user.upn",
      "https://api.<datadog",
      "entity.id",
      "integration",
      "studio",
      "datadog",
      "overview",
      "case",
      "incident",
      "smax",
      "exchange",
      "schedule",
      "downtime",
      "technical",
      "set",
      "generate",
      "api",
      "application",
      "keys",
      "prepare",
      "user",
      "create",
      "endpoint",
      "add",
      "configure",
      "scenarios",
      "webhook",
      "modify",
      "settings",
      "forwarding",
      "rules",
      "process",
      "webhooks",
      "incidents",
      "update",
      "now",
      "possible",
      "integrate",
      "studio.",
      "templates",
      "support",
      "following",
      "cases",
      "note",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "edition.",
      "typical",
      "event",
      "first",
      "becomes",
      "forwarded",
      "service",
      "management.",
      "nutshell",
      "illustrated",
      "below",
      "enables",
      "datadog.",
      "change",
      "request",
      "management",
      "scheduled",
      "planned",
      "involved",
      "cis",
      "data",
      "raise",
      "events",
      "during",
      "downtime.",
      "diagram",
      "illustrates",
      "components",
      "provided",
      "example",
      "cases.",
      "created",
      "out",
      "one",
      "events.",
      "every",
      "action",
      "triggered"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—SAP SuccessFactors integration",
    "content": "This topic introduces the integration between Service Management and SAP SuccessFactors through the following set of illustrated use cases. Each use case corresponds to an out-of-the-box scenario template for the integration. Create new SuccessFactors users Create requests for new hires Get employee position information Get holiday balance Use cases The integration use cases described in this document serve as examples to inspire and guide your integration efforts. However, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. The examples may or may not be applicable to your license edition. Use case 1: Create new SuccessFactors users This use case describes how to start the onboarding process for a new employee in SAP SuccessFactors. Paul goes to the Service Portal and selects the Create new employee account in Service Management and SAP Success Factors offering. Paul enters all required data and submits the request to get the onboarding ",
    "url": "integratesuccessfactors",
    "filename": "integratesuccessfactors",
    "headings": [
      "Use cases",
      "Use case 1: Create new SuccessFactors users",
      "Use case 2: Create requests for new hires",
      "Use case 3: Get employee position information",
      "Use case 4: Get holiday balance",
      "Set up the integration",
      "Prepare an integration user",
      "Create an endpoint",
      "Create an integration",
      "Add and configure scenarios",
      "Add the scenarios",
      "Configure the scenarios",
      "Create new SuccessFactors users scenario",
      "Create requests for new hires scenario",
      "Get employee position information scenario",
      "Get holiday balance scenario",
      "Configure the offerings",
      "Understand the scenarios",
      "Scenario 1: Create new SuccessFactors users",
      "Scenario 2: Create requests for new hires"
    ],
    "keywords": [
      "studiosap",
      "X.509",
      "successfactors.com",
      "https://apiyy.successfactors.com",
      "https://<api-server>/oauth/token",
      "2.0",
      "integration",
      "studio",
      "sap",
      "successfactors",
      "cases",
      "case",
      "create",
      "new",
      "users",
      "requests",
      "hires",
      "get",
      "employee",
      "position",
      "information",
      "holiday",
      "balance",
      "set",
      "prepare",
      "user",
      "endpoint",
      "add",
      "configure",
      "scenarios",
      "scenario",
      "offerings",
      "understand",
      "topic",
      "introduces",
      "between",
      "service",
      "management",
      "through",
      "following",
      "illustrated",
      "cases.",
      "corresponds",
      "out-of-the-box",
      "template",
      "integration.",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "edition.",
      "describes",
      "start",
      "onboarding",
      "process",
      "successfactors.",
      "paul",
      "goes",
      "portal",
      "selects",
      "account",
      "success",
      "factors",
      "offering.",
      "enters",
      "all",
      "required",
      "data",
      "submits",
      "request",
      "started",
      "hr",
      "system.",
      "system",
      "registers",
      "starts",
      "scheduled",
      "queries",
      "sucessfactor",
      "come",
      "onboard",
      "given",
      "number",
      "days",
      "creates",
      "offering",
      "bundle",
      "found."
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—sap successfactors integration",
    "contentLower": "this topic introduces the integration between service management and sap successfactors through the following set of illustrated use cases. each use case corresponds to an out-of-the-box scenario template for the integration. create new successfactors users create requests for new hires get employee position information get holiday balance use cases the integration use cases described in this document serve as examples to inspire and guide your integration efforts. however, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. the examples may or may not be applicable to your license edition. use case 1: create new successfactors users this use case describes how to start the onboarding process for a new employee in sap successfactors. paul goes to the service portal and selects the create new employee account in service management and sap success factors offering. paul enters all required data and submits the request to get the onboarding ",
    "keywordsLower": [
      "studiosap",
      "x.509",
      "successfactors.com",
      "https://apiyy.successfactors.com",
      "https://<api-server>/oauth/token",
      "2.0",
      "integration",
      "studio",
      "sap",
      "successfactors",
      "cases",
      "case",
      "create",
      "new",
      "users",
      "requests",
      "hires",
      "get",
      "employee",
      "position",
      "information",
      "holiday",
      "balance",
      "set",
      "prepare",
      "user",
      "endpoint",
      "add",
      "configure",
      "scenarios",
      "scenario",
      "offerings",
      "understand",
      "topic",
      "introduces",
      "between",
      "service",
      "management",
      "through",
      "following",
      "illustrated",
      "cases.",
      "corresponds",
      "out-of-the-box",
      "template",
      "integration.",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "edition.",
      "describes",
      "start",
      "onboarding",
      "process",
      "successfactors.",
      "paul",
      "goes",
      "portal",
      "selects",
      "account",
      "success",
      "factors",
      "offering.",
      "enters",
      "all",
      "required",
      "data",
      "submits",
      "request",
      "started",
      "hr",
      "system.",
      "system",
      "registers",
      "starts",
      "scheduled",
      "queries",
      "sucessfactor",
      "come",
      "onboard",
      "given",
      "number",
      "days",
      "creates",
      "offering",
      "bundle",
      "found."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—OpenAI ChatGPT integration",
    "content": "SMAX now offers a native integration with OpenAI ChatGPT.Important: As we are in the early stages of these tools, there are several reasons to be cautious, given concerns around security, IP ownership, and the potential sharing of confidential company information. Please be mindful that generative AI companies have not published or determined the use of your data.You must maintain confidentiality by not sharing proprietary or confidential company information with search or AI tools. There are no clear indications of how entered data will be used, who will have access to it, or if it would be shared with any third parties.In terms of privacy, employees must comply with all data privacy laws and regulations when using AI tools. Never share personal data with AI engines, such as employee names, social security numbers, addresses, and so on. Furthermore, the source of the IP in these tools is unclear.OpenText cannot be made responsible for any consequences resulting from using this integra",
    "url": "integratechatgpt",
    "filename": "integratechatgpt",
    "headings": [
      "Use Case–Agent support by ChatGPT",
      "Set up the integration",
      "Prerequisites",
      "Prepare integration user",
      "Add ChatGPT to the \"Comment purpose\" enum list",
      "Create the question field in the record type",
      "Create an endpoint",
      "Create an Integration",
      "Configure the scenario",
      "Understand the scenario",
      "Configure business rules to trigger the scenario",
      "Add a rule to extract the question from the comment",
      "Add a rule to trigger the Integration Studio scenario"
    ],
    "keywords": [
      "studioopenai",
      "type.Log",
      "https://api.openai.com/TokenEnter",
      "data.You",
      "needed.Sets",
      "support.To",
      "access.Add",
      "page.In",
      "integration.In",
      "scenario.Note",
      "tab.Next",
      "section.Gets",
      "Support.In",
      "openai.com",
      "2.0",
      "models.Adds",
      "user.Log",
      "https.Base",
      "parties.In",
      "comment.Adds",
      "tab.Add",
      "ticket.Set",
      "integration",
      "studio",
      "openai",
      "chatgpt",
      "case",
      "agent",
      "support",
      "set",
      "prerequisites",
      "prepare",
      "user",
      "add",
      "comment",
      "purpose",
      "enum",
      "list",
      "create",
      "question",
      "field",
      "record",
      "type",
      "endpoint",
      "configure",
      "scenario",
      "understand",
      "business",
      "rules",
      "trigger",
      "rule",
      "extract",
      "smax",
      "now",
      "offers",
      "native",
      "chatgpt.important",
      "early",
      "stages",
      "tools",
      "there",
      "several",
      "reasons",
      "cautious",
      "given",
      "concerns",
      "around",
      "security",
      "ip",
      "ownership",
      "potential",
      "sharing",
      "confidential",
      "company",
      "information.",
      "please",
      "mindful",
      "generative",
      "ai",
      "companies",
      "published",
      "determined",
      "maintain",
      "confidentiality",
      "proprietary",
      "information",
      "search",
      "tools.",
      "clear",
      "indications",
      "entered",
      "data",
      "access",
      "shared",
      "any",
      "third",
      "terms",
      "privacy",
      "employees",
      "comply"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—openai chatgpt integration",
    "contentLower": "smax now offers a native integration with openai chatgpt.important: as we are in the early stages of these tools, there are several reasons to be cautious, given concerns around security, ip ownership, and the potential sharing of confidential company information. please be mindful that generative ai companies have not published or determined the use of your data.you must maintain confidentiality by not sharing proprietary or confidential company information with search or ai tools. there are no clear indications of how entered data will be used, who will have access to it, or if it would be shared with any third parties.in terms of privacy, employees must comply with all data privacy laws and regulations when using ai tools. never share personal data with ai engines, such as employee names, social security numbers, addresses, and so on. furthermore, the source of the ip in these tools is unclear.opentext cannot be made responsible for any consequences resulting from using this integra",
    "keywordsLower": [
      "studioopenai",
      "type.log",
      "https://api.openai.com/tokenenter",
      "data.you",
      "needed.sets",
      "support.to",
      "access.add",
      "page.in",
      "integration.in",
      "scenario.note",
      "tab.next",
      "section.gets",
      "support.in",
      "openai.com",
      "2.0",
      "models.adds",
      "user.log",
      "https.base",
      "parties.in",
      "comment.adds",
      "tab.add",
      "ticket.set",
      "integration",
      "studio",
      "openai",
      "chatgpt",
      "case",
      "agent",
      "support",
      "set",
      "prerequisites",
      "prepare",
      "user",
      "add",
      "comment",
      "purpose",
      "enum",
      "list",
      "create",
      "question",
      "field",
      "record",
      "type",
      "endpoint",
      "configure",
      "scenario",
      "understand",
      "business",
      "rules",
      "trigger",
      "rule",
      "extract",
      "smax",
      "now",
      "offers",
      "native",
      "chatgpt.important",
      "early",
      "stages",
      "tools",
      "there",
      "several",
      "reasons",
      "cautious",
      "given",
      "concerns",
      "around",
      "security",
      "ip",
      "ownership",
      "potential",
      "sharing",
      "confidential",
      "company",
      "information.",
      "please",
      "mindful",
      "generative",
      "ai",
      "companies",
      "published",
      "determined",
      "maintain",
      "confidentiality",
      "proprietary",
      "information",
      "search",
      "tools.",
      "clear",
      "indications",
      "entered",
      "data",
      "access",
      "shared",
      "any",
      "third",
      "terms",
      "privacy",
      "employees",
      "comply"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—Microsoft Azure Active Directory (AD) integration",
    "content": "You can integrate Service Management Automation with Azure Active Directory. This integration allows you to import users and groups (as well as user-group relationships) from Azure AD into Service Management. The import is achieved by using the following integration scenarios. User import that initially imports all the users in Service Management.User import (delta) that initially imports all the users in Service Management. In any subsequent import, only new or modified users are considered.Group import and user assignment that initially imports all the groups and assigns the users to the groups.Group import and user assignment (delta) that initially imports all the groups and assigns the users to the groups. In any subsequent import, only new or modified groups are considered. Comparison: Options User import User import (delta) Group import & User assignment Group import & User assignment (delta) Allows to filter records in Azure AD Yes, Azure AD filter query parameters can be users ",
    "url": "integrateazuread",
    "filename": "integrateazuread",
    "headings": [
      "Prerequisites",
      "Prepare an integration user",
      "Create an endpoint",
      "Create an integration",
      "Configure the scenarios",
      "Use the scenarios",
      "Use the scenarios without delta import functionality",
      "Use the scenarios with delta import functionality"
    ],
    "keywords": [
      "studiomicrosoft",
      "payload.data",
      "https://graph.microsoft.com",
      "scenario.Use",
      "Management.User",
      "response.data",
      "token.Run",
      "page.In",
      "integration.In",
      "Group.Read",
      "microsoft.com",
      "ReadBasic.All",
      "GroupMember.Read",
      "User.Read",
      "2.0",
      "true.Run",
      "ReadWrite.All",
      "user.Log",
      "Directory.Read",
      "Assignment.In",
      "type.Open",
      "integration",
      "studio",
      "microsoft",
      "azure",
      "active",
      "directory",
      "ad",
      "prerequisites",
      "prepare",
      "user",
      "create",
      "endpoint",
      "configure",
      "scenarios",
      "delta",
      "import",
      "functionality",
      "integrate",
      "service",
      "management",
      "automation",
      "directory.",
      "allows",
      "users",
      "groups",
      "well",
      "user-group",
      "relationships",
      "management.",
      "achieved",
      "following",
      "scenarios.",
      "initially",
      "imports",
      "all",
      "any",
      "subsequent",
      "new",
      "modified",
      "considered.group",
      "assignment",
      "assigns",
      "groups.group",
      "groups.",
      "considered.",
      "comparison",
      "options",
      "group",
      "filter",
      "records",
      "query",
      "parameters",
      "limited",
      "object",
      "id",
      "supported",
      "changes",
      "data",
      "matching",
      "criteria",
      "imported",
      "convert",
      "contacts",
      "since",
      "removed",
      "returned",
      "integrations",
      "scheduled",
      "regularly",
      "data.",
      "example",
      "procedure",
      "below",
      "set",
      "ad.",
      "cases",
      "described",
      "document",
      "serve"
    ],
    "language": "en",
    "word_count": 110,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—microsoft azure active directory (ad) integration",
    "contentLower": "you can integrate service management automation with azure active directory. this integration allows you to import users and groups (as well as user-group relationships) from azure ad into service management. the import is achieved by using the following integration scenarios. user import that initially imports all the users in service management.user import (delta) that initially imports all the users in service management. in any subsequent import, only new or modified users are considered.group import and user assignment that initially imports all the groups and assigns the users to the groups.group import and user assignment (delta) that initially imports all the groups and assigns the users to the groups. in any subsequent import, only new or modified groups are considered. comparison: options user import user import (delta) group import & user assignment group import & user assignment (delta) allows to filter records in azure ad yes, azure ad filter query parameters can be users ",
    "keywordsLower": [
      "studiomicrosoft",
      "payload.data",
      "https://graph.microsoft.com",
      "scenario.use",
      "management.user",
      "response.data",
      "token.run",
      "page.in",
      "integration.in",
      "group.read",
      "microsoft.com",
      "readbasic.all",
      "groupmember.read",
      "user.read",
      "2.0",
      "true.run",
      "readwrite.all",
      "user.log",
      "directory.read",
      "assignment.in",
      "type.open",
      "integration",
      "studio",
      "microsoft",
      "azure",
      "active",
      "directory",
      "ad",
      "prerequisites",
      "prepare",
      "user",
      "create",
      "endpoint",
      "configure",
      "scenarios",
      "delta",
      "import",
      "functionality",
      "integrate",
      "service",
      "management",
      "automation",
      "directory.",
      "allows",
      "users",
      "groups",
      "well",
      "user-group",
      "relationships",
      "management.",
      "achieved",
      "following",
      "scenarios.",
      "initially",
      "imports",
      "all",
      "any",
      "subsequent",
      "new",
      "modified",
      "considered.group",
      "assignment",
      "assigns",
      "groups.group",
      "groups.",
      "considered.",
      "comparison",
      "options",
      "group",
      "filter",
      "records",
      "query",
      "parameters",
      "limited",
      "object",
      "id",
      "supported",
      "changes",
      "data",
      "matching",
      "criteria",
      "imported",
      "convert",
      "contacts",
      "since",
      "removed",
      "returned",
      "integrations",
      "scheduled",
      "regularly",
      "data.",
      "example",
      "procedure",
      "below",
      "set",
      "ad.",
      "cases",
      "described",
      "document",
      "serve"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—Azure DevOps integration",
    "content": "OverviewThis topic presents an illustrated set of integration use cases between SMAX and Microsoft Azure DevOps:Portal-based Defect or Enhancement requestsIncident to Defect integrationThe use cases in a nutshell:The integration use cases described in this document serve as examples to inspire and guide your integration efforts. However, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. The examples may or may not be applicable for your license edition.Use case 1: Portal-based Defect or Enhancement requestsIn this sample use case, end users who subscribe to a specific Business Service Application can report defects or request enhancements to the application by posting them via the SMAX Service Portal.We will follow the steps below, which are performed by Eve, an employee who uses her company's eExpense application for expense tracking and reporting. She has been frustrated that expense reporting can only be done while logged on to the e",
    "url": "integratedevops",
    "filename": "integratedevops",
    "headings": [
      "Overview",
      "Use case 1: Portal-based Defect or Enhancement requests",
      "Step 1: Portal access",
      "Step 2:  Submitting an enhancement request or defect",
      "Step 3: Azure DevOps work item created and confirmation provided back to the user",
      "Step 4: Status confirmation",
      "Step 5: Behind the scenes - association of the Azure DevOps ticket with the SMAX Request ticket",
      "Use case 2: Change request to Enhancement",
      "Use case 3: Incident/Request/Problem to defect",
      "Use case 4: Keep exchanged records in sync",
      "Set up the integration",
      "Prepare integration user",
      "Export the certificate from the Azure DevOps instance",
      "Configure SMAX",
      "Create an endpoint",
      "Create an integration",
      "Configure the scenarios",
      "Use the scenarios",
      "Use case 1—Portal-based Defect or Enhancement requests",
      "Specify user options"
    ],
    "keywords": [
      "studioazure",
      "Eve.Step",
      "listed.She",
      "backlog.Step",
      "intervention.Task",
      "needed.As",
      "task.Task",
      "Studio.The",
      "Management.Both",
      "current_user.Upn",
      "https://dev.azure.comCredentialsSelect",
      "day.Step",
      "listener.In",
      "Task.Give",
      "record.Use",
      "system.Use",
      "defect.Step",
      "system.Note",
      "step.The",
      "Definition.We",
      "tracking.Save",
      "failure.Task",
      "code.Save",
      "Management.The",
      "section.Set",
      "section.Next",
      "page.In",
      "group.Save",
      "integration.In",
      "Portal.We",
      "https://<FQDN>UsernameEnter",
      "application.The",
      "scenario.When",
      "request.To",
      "sync.In",
      "interval.Set",
      "DevOps.Use",
      "ExpertGroup.Name",
      "Management.Pull",
      "plan.In",
      "method.Use",
      "DevOps.Pull",
      "Agent.If",
      "2.0",
      "user.Log",
      "https.Base",
      "X.509",
      "tracking.In",
      "steps.Go",
      "task.This",
      "scenario.In",
      "entity.Id",
      "edition.Use",
      "Studio.In",
      "value.Save",
      "fulfilled.If",
      "created.If",
      "accordingly.Make",
      "integration.It",
      "templates.Tip",
      "integration",
      "studio",
      "azure",
      "devops",
      "overview",
      "case",
      "portal-based",
      "defect",
      "enhancement",
      "requests",
      "step",
      "portal",
      "access",
      "submitting",
      "request",
      "work",
      "item",
      "created",
      "confirmation",
      "provided",
      "back",
      "user",
      "status",
      "behind",
      "scenes",
      "association",
      "ticket",
      "smax",
      "change",
      "incident",
      "problem",
      "keep",
      "exchanged",
      "records",
      "sync",
      "set",
      "prepare",
      "export",
      "certificate",
      "instance"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—azure devops integration",
    "contentLower": "overviewthis topic presents an illustrated set of integration use cases between smax and microsoft azure devops:portal-based defect or enhancement requestsincident to defect integrationthe use cases in a nutshell:the integration use cases described in this document serve as examples to inspire and guide your integration efforts. however, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. the examples may or may not be applicable for your license edition.use case 1: portal-based defect or enhancement requestsin this sample use case, end users who subscribe to a specific business service application can report defects or request enhancements to the application by posting them via the smax service portal.we will follow the steps below, which are performed by eve, an employee who uses her company's eexpense application for expense tracking and reporting. she has been frustrated that expense reporting can only be done while logged on to the e",
    "keywordsLower": [
      "studioazure",
      "eve.step",
      "listed.she",
      "backlog.step",
      "intervention.task",
      "needed.as",
      "task.task",
      "studio.the",
      "management.both",
      "current_user.upn",
      "https://dev.azure.comcredentialsselect",
      "day.step",
      "listener.in",
      "task.give",
      "record.use",
      "system.use",
      "defect.step",
      "system.note",
      "step.the",
      "definition.we",
      "tracking.save",
      "failure.task",
      "code.save",
      "management.the",
      "section.set",
      "section.next",
      "page.in",
      "group.save",
      "integration.in",
      "portal.we",
      "https://<fqdn>usernameenter",
      "application.the",
      "scenario.when",
      "request.to",
      "sync.in",
      "interval.set",
      "devops.use",
      "expertgroup.name",
      "management.pull",
      "plan.in",
      "method.use",
      "devops.pull",
      "agent.if",
      "2.0",
      "user.log",
      "https.base",
      "x.509",
      "tracking.in",
      "steps.go",
      "task.this",
      "scenario.in",
      "entity.id",
      "edition.use",
      "studio.in",
      "value.save",
      "fulfilled.if",
      "created.if",
      "accordingly.make",
      "integration.it",
      "templates.tip",
      "integration",
      "studio",
      "azure",
      "devops",
      "overview",
      "case",
      "portal-based",
      "defect",
      "enhancement",
      "requests",
      "step",
      "portal",
      "access",
      "submitting",
      "request",
      "work",
      "item",
      "created",
      "confirmation",
      "provided",
      "back",
      "user",
      "status",
      "behind",
      "scenes",
      "association",
      "ticket",
      "smax",
      "change",
      "incident",
      "problem",
      "keep",
      "exchanged",
      "records",
      "sync",
      "set",
      "prepare",
      "export",
      "certificate",
      "instance"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—Atlassian Jira integration",
    "content": "Overview This topic presents an illustrated set of integration use cases between Service Management and Atlassian Jira: Portal-based Defect or Enhancement requests Incident to Defect integration Incident exchange The use cases in a nutshell: The integration use cases described in this document serve as examples to inspire and guide your integration efforts. However, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. The examples may or may not be applicable for your license edition. The integration templates for defect and enhancement tracking are designed to work with Bug tracking projects in Jira. Use case 1: Portal-based Defect or Enhancement requests In this sample use case, end users who subscribe to a specific Business Service Application can report defects or request enhancements to the application by posting them via the Service Portal. We will follow the steps below, which are performed by Eve, an employee who uses her company's",
    "url": "integratejira",
    "filename": "integratejira",
    "headings": [
      "Overview",
      "Use case 1: Portal-based Defect or Enhancement requests",
      "Step 1: Portal access",
      "Step 2:  Submitting an enhancement request or defect",
      "Step 3: Jira enhancement created and confirmation provided back to the user",
      "Step 4: Status confirmation",
      "Step 5: Behind the scenes - association of the Jira ticket from the Request ticket",
      "Use case 2: Change request to enhancement",
      "Use case 3: Incident/Request/Problem to defect",
      "Use case 4: Incident to Incident",
      "Use case 5: Get status update",
      "Set up the integration",
      "Prepare an integration user",
      "Export the certificate from the Jira instance",
      "Configure an app in Jira (if you use OAuth 2.0 authentication)",
      "Configure Service Management",
      "Create an endpoint",
      "Create an integration",
      "Configure the scenarios",
      "Use the scenarios"
    ],
    "keywords": [
      "studioatlassian",
      "X.509",
      "atlassian.com",
      "https://auth.atlassian.com/oauth/token",
      "current_user.Upn",
      "https://<FQDN",
      "entity.Id",
      "https://auth.atlassian.com/authorize?prompt=consent",
      "https://<external_access_host>/rest/<tenant_id>/opb/oauth/callback",
      "https://api.atlassian.com/ex/jira/<cloudid",
      "2.0",
      "ExpertGroup.Name",
      "integration",
      "studio",
      "atlassian",
      "jira",
      "overview",
      "case",
      "portal-based",
      "defect",
      "enhancement",
      "requests",
      "step",
      "portal",
      "access",
      "submitting",
      "request",
      "created",
      "confirmation",
      "provided",
      "back",
      "user",
      "status",
      "behind",
      "scenes",
      "association",
      "ticket",
      "change",
      "incident",
      "problem",
      "get",
      "update",
      "set",
      "prepare",
      "export",
      "certificate",
      "instance",
      "configure",
      "app",
      "oauth",
      "authentication",
      "service",
      "management",
      "create",
      "endpoint",
      "scenarios",
      "specify",
      "options",
      "define",
      "task",
      "plan",
      "feature",
      "pull",
      "updates",
      "topic",
      "presents",
      "illustrated",
      "cases",
      "between",
      "exchange",
      "nutshell",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "edition.",
      "templates",
      "tracking",
      "designed",
      "work",
      "bug",
      "projects",
      "jira.",
      "sample",
      "end",
      "users",
      "subscribe",
      "specific"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—atlassian jira integration",
    "contentLower": "overview this topic presents an illustrated set of integration use cases between service management and atlassian jira: portal-based defect or enhancement requests incident to defect integration incident exchange the use cases in a nutshell: the integration use cases described in this document serve as examples to inspire and guide your integration efforts. however, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. the examples may or may not be applicable for your license edition. the integration templates for defect and enhancement tracking are designed to work with bug tracking projects in jira. use case 1: portal-based defect or enhancement requests in this sample use case, end users who subscribe to a specific business service application can report defects or request enhancements to the application by posting them via the service portal. we will follow the steps below, which are performed by eve, an employee who uses her company's",
    "keywordsLower": [
      "studioatlassian",
      "x.509",
      "atlassian.com",
      "https://auth.atlassian.com/oauth/token",
      "current_user.upn",
      "https://<fqdn",
      "entity.id",
      "https://auth.atlassian.com/authorize?prompt=consent",
      "https://<external_access_host>/rest/<tenant_id>/opb/oauth/callback",
      "https://api.atlassian.com/ex/jira/<cloudid",
      "2.0",
      "expertgroup.name",
      "integration",
      "studio",
      "atlassian",
      "jira",
      "overview",
      "case",
      "portal-based",
      "defect",
      "enhancement",
      "requests",
      "step",
      "portal",
      "access",
      "submitting",
      "request",
      "created",
      "confirmation",
      "provided",
      "back",
      "user",
      "status",
      "behind",
      "scenes",
      "association",
      "ticket",
      "change",
      "incident",
      "problem",
      "get",
      "update",
      "set",
      "prepare",
      "export",
      "certificate",
      "instance",
      "configure",
      "app",
      "oauth",
      "authentication",
      "service",
      "management",
      "create",
      "endpoint",
      "scenarios",
      "specify",
      "options",
      "define",
      "task",
      "plan",
      "feature",
      "pull",
      "updates",
      "topic",
      "presents",
      "illustrated",
      "cases",
      "between",
      "exchange",
      "nutshell",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license",
      "edition.",
      "templates",
      "tracking",
      "designed",
      "work",
      "bug",
      "projects",
      "jira.",
      "sample",
      "end",
      "users",
      "subscribe",
      "specific"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—Salesforce Slack integration",
    "content": "You can integrate Service Management with Slack through the Integration Studio. This integration allows your agent users to collaborate in Slack while following the governance processes you have put in place.Use caseYou can use the Slack integration to send messages to Slack to handle, for example, a Major Incident use case.The integration notifies the key stakeholders in Slack immediately when a major incident occurs, to directly connect the right people. By viewing information about the incident that occurred, the users can collaborate on its resolution.Example of the message: In addition, a comment is added to the record where the integration was triggered. The integration use cases described in this document serve as examples to inspire and guide your integration efforts. However, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. The examples may or may not be applicable for your license edition. Complete prerequisite tasks in Slack",
    "url": "integrateslack",
    "filename": "integrateslack",
    "headings": [
      "Use case",
      "Complete prerequisite tasks in Slack",
      "Generate token",
      "Create channel",
      "Set up the integration",
      "Prepare an integration user",
      "Configure an endpoint",
      "Create an Integration",
      "Configure the scenario",
      "Understand the scenario",
      "Configure business rule to trigger the integration"
    ],
    "keywords": [
      "studiosalesforce",
      "Slack.In",
      "endpoint.Go",
      "channel.In",
      "needed.Sets",
      "page.In",
      "integration.In",
      "scenario.Note",
      "section.Gets",
      "URLhttps://slack.com/apiTokenThe",
      "Incident.Go",
      "channel.Adds",
      "case.The",
      "place.Use",
      "2.0",
      "sent.The",
      "Platform.Note",
      "slack.com",
      "user.Log",
      "integration.For",
      "integration",
      "studio",
      "salesforce",
      "slack",
      "case",
      "complete",
      "prerequisite",
      "tasks",
      "generate",
      "token",
      "create",
      "channel",
      "set",
      "prepare",
      "user",
      "configure",
      "endpoint",
      "scenario",
      "understand",
      "business",
      "rule",
      "trigger",
      "integrate",
      "service",
      "management",
      "through",
      "studio.",
      "allows",
      "agent",
      "users",
      "collaborate",
      "while",
      "following",
      "governance",
      "processes",
      "put",
      "caseyou",
      "send",
      "messages",
      "handle",
      "example",
      "major",
      "incident",
      "notifies",
      "key",
      "stakeholders",
      "immediately",
      "occurs",
      "directly",
      "connect",
      "right",
      "people.",
      "viewing",
      "information",
      "about",
      "occurred",
      "resolution.example",
      "message",
      "addition",
      "comment",
      "added",
      "record",
      "triggered.",
      "cases",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license"
    ],
    "language": "en",
    "word_count": 88,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—salesforce slack integration",
    "contentLower": "you can integrate service management with slack through the integration studio. this integration allows your agent users to collaborate in slack while following the governance processes you have put in place.use caseyou can use the slack integration to send messages to slack to handle, for example, a major incident use case.the integration notifies the key stakeholders in slack immediately when a major incident occurs, to directly connect the right people. by viewing information about the incident that occurred, the users can collaborate on its resolution.example of the message: in addition, a comment is added to the record where the integration was triggered. the integration use cases described in this document serve as examples to inspire and guide your integration efforts. however, they should not be considered as prescriptive or guaranteed solutions for your particular scenario. the examples may or may not be applicable for your license edition. complete prerequisite tasks in slack",
    "keywordsLower": [
      "studiosalesforce",
      "slack.in",
      "endpoint.go",
      "channel.in",
      "needed.sets",
      "page.in",
      "integration.in",
      "scenario.note",
      "section.gets",
      "urlhttps://slack.com/apitokenthe",
      "incident.go",
      "channel.adds",
      "case.the",
      "place.use",
      "2.0",
      "sent.the",
      "platform.note",
      "slack.com",
      "user.log",
      "integration.for",
      "integration",
      "studio",
      "salesforce",
      "slack",
      "case",
      "complete",
      "prerequisite",
      "tasks",
      "generate",
      "token",
      "create",
      "channel",
      "set",
      "prepare",
      "user",
      "configure",
      "endpoint",
      "scenario",
      "understand",
      "business",
      "rule",
      "trigger",
      "integrate",
      "service",
      "management",
      "through",
      "studio.",
      "allows",
      "agent",
      "users",
      "collaborate",
      "while",
      "following",
      "governance",
      "processes",
      "put",
      "caseyou",
      "send",
      "messages",
      "handle",
      "example",
      "major",
      "incident",
      "notifies",
      "key",
      "stakeholders",
      "immediately",
      "occurs",
      "directly",
      "connect",
      "right",
      "people.",
      "viewing",
      "information",
      "about",
      "occurred",
      "resolution.example",
      "message",
      "addition",
      "comment",
      "added",
      "record",
      "triggered.",
      "cases",
      "described",
      "document",
      "serve",
      "examples",
      "inspire",
      "guide",
      "efforts.",
      "however",
      "considered",
      "prescriptive",
      "guaranteed",
      "solutions",
      "particular",
      "scenario.",
      "applicable",
      "license"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio—Microsoft Teams integration",
    "content": "Microsoft Teams is a messaging app for your organization—a workspace for real-time collaboration and communication, meetings, file and app sharing! It's a central place to collaborate with everyone in your organization. SMAX integrates with Microsoft Teams in several ways. By integrating Microsoft Teams with business apps like Service Management,you can break down silos between departments and increase cross-functional communication and collaboration. For example, integrating Microsoft Teams with SMAX can help ensure that everyone is on the same page and up-to-date on their requests and ongoing IT activities. You can automate notifications and alerts for specific events or changes in Service Management. This can help ensure that everyone is notified of important updates in a timely manner and reduce the need for manual communication. This can help reduce miscommunication and misunderstandings and ensure that everyone is working towards the same goals. While you have many choices for ho",
    "url": "integrateteams",
    "filename": "integrateteams",
    "headings": [
      "Bot-based interactions",
      "Notifications into private / group chats",
      "Push notifications into teams and channels"
    ],
    "keywords": [
      "studiomicrosoft",
      "integration",
      "studio",
      "microsoft",
      "teams",
      "bot-based",
      "interactions",
      "notifications",
      "private",
      "group",
      "chats",
      "push",
      "channels",
      "messaging",
      "app",
      "organization",
      "workspace",
      "real-time",
      "collaboration",
      "communication",
      "meetings",
      "file",
      "sharing",
      "central",
      "place",
      "collaborate",
      "everyone",
      "organization.",
      "smax",
      "integrates",
      "several",
      "ways.",
      "integrating",
      "business",
      "apps",
      "like",
      "service",
      "management",
      "break",
      "silos",
      "between",
      "departments",
      "increase",
      "cross-functional",
      "collaboration.",
      "example",
      "help",
      "ensure",
      "same",
      "page",
      "up-to-date",
      "requests",
      "ongoing",
      "activities.",
      "automate",
      "alerts",
      "specific",
      "events",
      "changes",
      "management.",
      "notified",
      "important",
      "updates",
      "timely",
      "manner",
      "reduce",
      "need",
      "manual",
      "communication.",
      "miscommunication",
      "misunderstandings",
      "working",
      "towards",
      "goals.",
      "while",
      "many",
      "choices",
      "integrate",
      "let",
      "highlight",
      "options",
      "supported",
      "bot",
      "provides",
      "most",
      "comprehensive",
      "solution",
      "facilitate",
      "integration.",
      "offers",
      "users",
      "friendly",
      "intuitive",
      "way",
      "ask",
      "answering",
      "simple",
      "questions",
      "drawing",
      "existing"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio—microsoft teams integration",
    "contentLower": "microsoft teams is a messaging app for your organization—a workspace for real-time collaboration and communication, meetings, file and app sharing! it's a central place to collaborate with everyone in your organization. smax integrates with microsoft teams in several ways. by integrating microsoft teams with business apps like service management,you can break down silos between departments and increase cross-functional communication and collaboration. for example, integrating microsoft teams with smax can help ensure that everyone is on the same page and up-to-date on their requests and ongoing it activities. you can automate notifications and alerts for specific events or changes in service management. this can help ensure that everyone is notified of important updates in a timely manner and reduce the need for manual communication. this can help reduce miscommunication and misunderstandings and ensure that everyone is working towards the same goals. while you have many choices for ho",
    "keywordsLower": [
      "studiomicrosoft",
      "integration",
      "studio",
      "microsoft",
      "teams",
      "bot-based",
      "interactions",
      "notifications",
      "private",
      "group",
      "chats",
      "push",
      "channels",
      "messaging",
      "app",
      "organization",
      "workspace",
      "real-time",
      "collaboration",
      "communication",
      "meetings",
      "file",
      "sharing",
      "central",
      "place",
      "collaborate",
      "everyone",
      "organization.",
      "smax",
      "integrates",
      "several",
      "ways.",
      "integrating",
      "business",
      "apps",
      "like",
      "service",
      "management",
      "break",
      "silos",
      "between",
      "departments",
      "increase",
      "cross-functional",
      "collaboration.",
      "example",
      "help",
      "ensure",
      "same",
      "page",
      "up-to-date",
      "requests",
      "ongoing",
      "activities.",
      "automate",
      "alerts",
      "specific",
      "events",
      "changes",
      "management.",
      "notified",
      "important",
      "updates",
      "timely",
      "manner",
      "reduce",
      "need",
      "manual",
      "communication.",
      "miscommunication",
      "misunderstandings",
      "working",
      "towards",
      "goals.",
      "while",
      "many",
      "choices",
      "integrate",
      "let",
      "highlight",
      "options",
      "supported",
      "bot",
      "provides",
      "most",
      "comprehensive",
      "solution",
      "facilitate",
      "integration.",
      "offers",
      "users",
      "friendly",
      "intuitive",
      "way",
      "ask",
      "answering",
      "simple",
      "questions",
      "drawing",
      "existing"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Index external knowledge using IDOL connectors",
    "content": "You can use OpenText™ IDOL, a market-leading knowledge discovery and analytics platform, to index knowledge articles from various external sources to Service Management. Examples for external knowledge sources include a website, a SharePoint site, or an OpenText™ Extended ECM repository. The indexed knowledge articles can then be used by Service Portal users and Agent Interface users in the following ways: When users perform global search, the system includes the external knowledge articles (identified by the \"External Article\" or \"External Knowledge\"  badge) in the search results. The search results contain a brief summary of the knowledge article. The users can click a button next to the search result to open the article in its source system. When users interact with the Aviator or the virtual agent, the system uses the external knowledge articles (as well as other data stored in Service Management) to answer the user's questions or provide suggestions. Depending on the version of th",
    "url": "indexknowledgefromidol",
    "filename": "indexknowledgefromidol",
    "headings": [
      "Required components",
      "Related topics"
    ],
    "keywords": [
      "index",
      "external",
      "knowledge",
      "idol",
      "connectors",
      "required",
      "components",
      "related",
      "topics",
      "opentext",
      "market-leading",
      "discovery",
      "analytics",
      "platform",
      "articles",
      "various",
      "sources",
      "service",
      "management.",
      "examples",
      "include",
      "website",
      "sharepoint",
      "site",
      "extended",
      "ecm",
      "repository.",
      "indexed",
      "portal",
      "users",
      "agent",
      "interface",
      "following",
      "ways",
      "perform",
      "global",
      "search",
      "system",
      "includes",
      "identified",
      "article",
      "badge",
      "results.",
      "results",
      "contain",
      "brief",
      "summary",
      "article.",
      "click",
      "button",
      "next",
      "result",
      "open",
      "source",
      "system.",
      "interact",
      "aviator",
      "virtual",
      "uses",
      "well",
      "data",
      "stored",
      "management",
      "answer",
      "user",
      "questions",
      "provide",
      "suggestions.",
      "depending",
      "version",
      "displays",
      "suggestions",
      "references",
      "answer.",
      "link",
      "gather",
      "different",
      "indexing",
      "idol.",
      "connector",
      "indexes",
      "one",
      "type",
      "example",
      "remote",
      "retrieves",
      "sharepoint.",
      "multiple",
      "same",
      "environment.",
      "currently",
      "supports",
      "confluence",
      "rest",
      "web",
      "core",
      "content",
      "connector.",
      "framework",
      "server"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "index external knowledge using idol connectors",
    "contentLower": "you can use opentext™ idol, a market-leading knowledge discovery and analytics platform, to index knowledge articles from various external sources to service management. examples for external knowledge sources include a website, a sharepoint site, or an opentext™ extended ecm repository. the indexed knowledge articles can then be used by service portal users and agent interface users in the following ways: when users perform global search, the system includes the external knowledge articles (identified by the \"external article\" or \"external knowledge\"  badge) in the search results. the search results contain a brief summary of the knowledge article. the users can click a button next to the search result to open the article in its source system. when users interact with the aviator or the virtual agent, the system uses the external knowledge articles (as well as other data stored in service management) to answer the user's questions or provide suggestions. depending on the version of th",
    "keywordsLower": [
      "index",
      "external",
      "knowledge",
      "idol",
      "connectors",
      "required",
      "components",
      "related",
      "topics",
      "opentext",
      "market-leading",
      "discovery",
      "analytics",
      "platform",
      "articles",
      "various",
      "sources",
      "service",
      "management.",
      "examples",
      "include",
      "website",
      "sharepoint",
      "site",
      "extended",
      "ecm",
      "repository.",
      "indexed",
      "portal",
      "users",
      "agent",
      "interface",
      "following",
      "ways",
      "perform",
      "global",
      "search",
      "system",
      "includes",
      "identified",
      "article",
      "badge",
      "results.",
      "results",
      "contain",
      "brief",
      "summary",
      "article.",
      "click",
      "button",
      "next",
      "result",
      "open",
      "source",
      "system.",
      "interact",
      "aviator",
      "virtual",
      "uses",
      "well",
      "data",
      "stored",
      "management",
      "answer",
      "user",
      "questions",
      "provide",
      "suggestions.",
      "depending",
      "version",
      "displays",
      "suggestions",
      "references",
      "answer.",
      "link",
      "gather",
      "different",
      "indexing",
      "idol.",
      "connector",
      "indexes",
      "one",
      "type",
      "example",
      "remote",
      "retrieves",
      "sharepoint.",
      "multiple",
      "same",
      "environment.",
      "currently",
      "supports",
      "confluence",
      "rest",
      "web",
      "core",
      "content",
      "connector.",
      "framework",
      "server"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Index knowledge from web pages",
    "content": "You can use the IDOL Web Connector to index knowledge from web pages. The connector can crawl a website by retrieving the resources listed in a site map, or following the links that exist on each page. See Web Connector Features and Capabilities for the connector's capabilities and the authentication methods that it supports. Before indexing knowledge from a website, check the terms of use for the website. It's your sole responsibility to comply with the website's terms of use when you use the IDOL knowledge indexing solution to retrieve knowledge from websites. Set up the Web Connector To set up knowledge indexing against a website, perform the following steps. If this is the first IDOL connector that you set up, set up the shared components. Otherwise, skip this step. Download the Web Connector package from the ITOM Marketplace to the knowledge indexing server. You can also find the download link from the agent interface UI: Go to Administration > Utilities > Integration > Endpoints ",
    "url": "indexwebpages",
    "filename": "indexwebpages",
    "headings": [
      "Set up the Web Connector",
      "How do I...?",
      "How to remove irrelevant information from the web pages",
      "How to remove certain web pages from Service Management",
      "Related topics"
    ],
    "keywords": [
      "webconnector.cfg",
      "robots.txt",
      "versionkey.dat",
      "http://<website",
      "config.ini",
      "licensekey.dat",
      "insertSourceField.lua",
      "index",
      "knowledge",
      "web",
      "pages",
      "set",
      "connector",
      "i...",
      "remove",
      "irrelevant",
      "information",
      "certain",
      "service",
      "management",
      "related",
      "topics",
      "idol",
      "pages.",
      "crawl",
      "website",
      "retrieving",
      "resources",
      "listed",
      "site",
      "map",
      "following",
      "links",
      "exist",
      "page.",
      "see",
      "features",
      "capabilities",
      "authentication",
      "methods",
      "supports.",
      "before",
      "indexing",
      "check",
      "terms",
      "website.",
      "sole",
      "responsibility",
      "comply",
      "solution",
      "retrieve",
      "websites.",
      "against",
      "perform",
      "steps.",
      "first",
      "shared",
      "components.",
      "otherwise",
      "skip",
      "step.",
      "download",
      "package",
      "itom",
      "marketplace",
      "server.",
      "find",
      "link",
      "agent",
      "interface",
      "ui",
      "go",
      "administration",
      "utilities",
      "integration",
      "endpoints",
      "center",
      "right.",
      "click",
      "under",
      "connector.",
      "extract",
      "package.",
      "folder",
      "content",
      "extracted",
      "referred",
      "remaining",
      "copy",
      "files",
      "folder.",
      "already",
      "downloaded",
      "cfs.",
      "oem",
      "license",
      "lua",
      "scripts",
      "open",
      "file"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "index knowledge from web pages",
    "contentLower": "you can use the idol web connector to index knowledge from web pages. the connector can crawl a website by retrieving the resources listed in a site map, or following the links that exist on each page. see web connector features and capabilities for the connector's capabilities and the authentication methods that it supports. before indexing knowledge from a website, check the terms of use for the website. it's your sole responsibility to comply with the website's terms of use when you use the idol knowledge indexing solution to retrieve knowledge from websites. set up the web connector to set up knowledge indexing against a website, perform the following steps. if this is the first idol connector that you set up, set up the shared components. otherwise, skip this step. download the web connector package from the itom marketplace to the knowledge indexing server. you can also find the download link from the agent interface ui: go to administration > utilities > integration > endpoints ",
    "keywordsLower": [
      "webconnector.cfg",
      "robots.txt",
      "versionkey.dat",
      "http://<website",
      "config.ini",
      "licensekey.dat",
      "insertsourcefield.lua",
      "index",
      "knowledge",
      "web",
      "pages",
      "set",
      "connector",
      "i...",
      "remove",
      "irrelevant",
      "information",
      "certain",
      "service",
      "management",
      "related",
      "topics",
      "idol",
      "pages.",
      "crawl",
      "website",
      "retrieving",
      "resources",
      "listed",
      "site",
      "map",
      "following",
      "links",
      "exist",
      "page.",
      "see",
      "features",
      "capabilities",
      "authentication",
      "methods",
      "supports.",
      "before",
      "indexing",
      "check",
      "terms",
      "website.",
      "sole",
      "responsibility",
      "comply",
      "solution",
      "retrieve",
      "websites.",
      "against",
      "perform",
      "steps.",
      "first",
      "shared",
      "components.",
      "otherwise",
      "skip",
      "step.",
      "download",
      "package",
      "itom",
      "marketplace",
      "server.",
      "find",
      "link",
      "agent",
      "interface",
      "ui",
      "go",
      "administration",
      "utilities",
      "integration",
      "endpoints",
      "center",
      "right.",
      "click",
      "under",
      "connector.",
      "extract",
      "package.",
      "folder",
      "content",
      "extracted",
      "referred",
      "remaining",
      "copy",
      "files",
      "folder.",
      "already",
      "downloaded",
      "cfs.",
      "oem",
      "license",
      "lua",
      "scripts",
      "open",
      "file"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Index knowledge from Confluence",
    "content": "You can use the IDOL Confluence Connector to index Confluence pages. See Confluence Connector Features and Capabilities for the connector's capabilities and the supported versions and editions. Prerequisites You must meet the following prerequisites: Prepare a Confluence user account with enough permissions (Read permissions) to access the pages to index. If you index documents from Confluence Cloud, visit https://id.atlassian.com/manage-profile/security/api-tokens as the user mentioned above and create an API token. Use this API token as the password in the connector configuration file. Set up the Confluence Connector To set up knowledge indexing against a Confluence repository, perform the following steps. If this is the first IDOL connector that you set up, set up the shared components. Otherwise, skip this step. Download the Confluence Connector package from the ITOM Marketplace to the knowledge indexing server. You can also find the download link from the agent interface UI: go to",
    "url": "indexcloudconfluence",
    "filename": "indexcloudconfluence",
    "headings": [
      "Prerequisites",
      "Set up the Confluence Connector",
      "Related topics"
    ],
    "keywords": [
      "confluencerestconnector.cfg",
      "atlassian.com",
      "versionkey.dat",
      "config.ini",
      "https://id.atlassian.com/manage-profile/security/api-tokens",
      "licensekey.dat",
      "insertSourceField.lua",
      "index",
      "knowledge",
      "confluence",
      "prerequisites",
      "set",
      "connector",
      "related",
      "topics",
      "idol",
      "pages.",
      "see",
      "features",
      "capabilities",
      "supported",
      "versions",
      "editions.",
      "meet",
      "following",
      "prepare",
      "user",
      "account",
      "enough",
      "permissions",
      "read",
      "access",
      "pages",
      "index.",
      "documents",
      "cloud",
      "visit",
      "https",
      "id.atlassian.com",
      "manage-profile",
      "security",
      "api-tokens",
      "mentioned",
      "above",
      "create",
      "api",
      "token.",
      "token",
      "password",
      "configuration",
      "file.",
      "indexing",
      "against",
      "repository",
      "perform",
      "steps.",
      "first",
      "shared",
      "components.",
      "otherwise",
      "skip",
      "step.",
      "download",
      "package",
      "itom",
      "marketplace",
      "server.",
      "find",
      "link",
      "agent",
      "interface",
      "ui",
      "go",
      "administration",
      "utilities",
      "integration",
      "endpoints",
      "center",
      "right",
      "click",
      "under",
      "connector.",
      "extract",
      "package.",
      "folder",
      "content",
      "extracted",
      "referred",
      "remaining",
      "copy",
      "files",
      "folder.",
      "already",
      "downloaded",
      "cfs.",
      "oem",
      "license",
      "lua",
      "scripts",
      "open"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "index knowledge from confluence",
    "contentLower": "you can use the idol confluence connector to index confluence pages. see confluence connector features and capabilities for the connector's capabilities and the supported versions and editions. prerequisites you must meet the following prerequisites: prepare a confluence user account with enough permissions (read permissions) to access the pages to index. if you index documents from confluence cloud, visit https://id.atlassian.com/manage-profile/security/api-tokens as the user mentioned above and create an api token. use this api token as the password in the connector configuration file. set up the confluence connector to set up knowledge indexing against a confluence repository, perform the following steps. if this is the first idol connector that you set up, set up the shared components. otherwise, skip this step. download the confluence connector package from the itom marketplace to the knowledge indexing server. you can also find the download link from the agent interface ui: go to",
    "keywordsLower": [
      "confluencerestconnector.cfg",
      "atlassian.com",
      "versionkey.dat",
      "config.ini",
      "https://id.atlassian.com/manage-profile/security/api-tokens",
      "licensekey.dat",
      "insertsourcefield.lua",
      "index",
      "knowledge",
      "confluence",
      "prerequisites",
      "set",
      "connector",
      "related",
      "topics",
      "idol",
      "pages.",
      "see",
      "features",
      "capabilities",
      "supported",
      "versions",
      "editions.",
      "meet",
      "following",
      "prepare",
      "user",
      "account",
      "enough",
      "permissions",
      "read",
      "access",
      "pages",
      "index.",
      "documents",
      "cloud",
      "visit",
      "https",
      "id.atlassian.com",
      "manage-profile",
      "security",
      "api-tokens",
      "mentioned",
      "above",
      "create",
      "api",
      "token.",
      "token",
      "password",
      "configuration",
      "file.",
      "indexing",
      "against",
      "repository",
      "perform",
      "steps.",
      "first",
      "shared",
      "components.",
      "otherwise",
      "skip",
      "step.",
      "download",
      "package",
      "itom",
      "marketplace",
      "server.",
      "find",
      "link",
      "agent",
      "interface",
      "ui",
      "go",
      "administration",
      "utilities",
      "integration",
      "endpoints",
      "center",
      "right",
      "click",
      "under",
      "connector.",
      "extract",
      "package.",
      "folder",
      "content",
      "extracted",
      "referred",
      "remaining",
      "copy",
      "files",
      "folder.",
      "already",
      "downloaded",
      "cfs.",
      "oem",
      "license",
      "lua",
      "scripts",
      "open"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Index knowledge from SharePoint",
    "content": "You can use the IDOL SharePoint Remote Connector to index documents from SharePoint. See SharePoint Connector Features and Capabilities for the connector's capabilities and the supported versions and editions. The SharePoint Connector supports indexing only these file formats: text files (*.txt, *.xml, *.json, *html, *csv), Microsoft Office files (*.doc, *.docx, *.ppt, *.pptx, *.xls, *.xlsx), PDF files; it does not support images, videos, and zip files. Prerequisites Before you can index knowledge from SharePoint, you must prepare a SharePoint user account with enough permissions (Read permissions) to access the documents to index. Set up the SharePoint Connector To set up knowledge indexing against a SharePoint repository, perform the following steps. If this is the first IDOL connector that you set up, set up the shared components. Otherwise, skip this step. Download the SharePoint Remote Connector package from the ITOM Marketplace to the knowledge indexing server. You can also find ",
    "url": "indexknowledgeonpremsharepoint",
    "filename": "indexknowledgeonpremsharepoint",
    "headings": [
      "Prerequisites",
      "Set up the SharePoint Connector",
      "Related topics"
    ],
    "keywords": [
      "versionkey.dat",
      "config.ini",
      "sharepointremoteconnector.cfg",
      "licensekey.dat",
      "insertSourceField.lua",
      "index",
      "knowledge",
      "sharepoint",
      "prerequisites",
      "set",
      "connector",
      "related",
      "topics",
      "idol",
      "remote",
      "documents",
      "sharepoint.",
      "see",
      "features",
      "capabilities",
      "supported",
      "versions",
      "editions.",
      "supports",
      "indexing",
      "file",
      "formats",
      "text",
      "files",
      ".txt",
      ".xml",
      ".json",
      "html",
      "csv",
      "microsoft",
      "office",
      ".doc",
      ".docx",
      ".ppt",
      ".pptx",
      ".xls",
      ".xlsx",
      "pdf",
      "support",
      "images",
      "videos",
      "zip",
      "files.",
      "before",
      "prepare",
      "user",
      "account",
      "enough",
      "permissions",
      "read",
      "access",
      "index.",
      "against",
      "repository",
      "perform",
      "following",
      "steps.",
      "first",
      "shared",
      "components.",
      "otherwise",
      "skip",
      "step.",
      "download",
      "package",
      "itom",
      "marketplace",
      "server.",
      "find",
      "link",
      "agent",
      "interface",
      "ui",
      "go",
      "administration",
      "utilities",
      "integration",
      "endpoints",
      "center",
      "right",
      "click",
      "under",
      "connector.",
      "extract",
      "package.",
      "folder",
      "content",
      "extracted",
      "referred",
      "remaining",
      "copy",
      "folder.",
      "already",
      "downloaded",
      "cfs."
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "index knowledge from sharepoint",
    "contentLower": "you can use the idol sharepoint remote connector to index documents from sharepoint. see sharepoint connector features and capabilities for the connector's capabilities and the supported versions and editions. the sharepoint connector supports indexing only these file formats: text files (*.txt, *.xml, *.json, *html, *csv), microsoft office files (*.doc, *.docx, *.ppt, *.pptx, *.xls, *.xlsx), pdf files; it does not support images, videos, and zip files. prerequisites before you can index knowledge from sharepoint, you must prepare a sharepoint user account with enough permissions (read permissions) to access the documents to index. set up the sharepoint connector to set up knowledge indexing against a sharepoint repository, perform the following steps. if this is the first idol connector that you set up, set up the shared components. otherwise, skip this step. download the sharepoint remote connector package from the itom marketplace to the knowledge indexing server. you can also find ",
    "keywordsLower": [
      "versionkey.dat",
      "config.ini",
      "sharepointremoteconnector.cfg",
      "licensekey.dat",
      "insertsourcefield.lua",
      "index",
      "knowledge",
      "sharepoint",
      "prerequisites",
      "set",
      "connector",
      "related",
      "topics",
      "idol",
      "remote",
      "documents",
      "sharepoint.",
      "see",
      "features",
      "capabilities",
      "supported",
      "versions",
      "editions.",
      "supports",
      "indexing",
      "file",
      "formats",
      "text",
      "files",
      ".txt",
      ".xml",
      ".json",
      "html",
      "csv",
      "microsoft",
      "office",
      ".doc",
      ".docx",
      ".ppt",
      ".pptx",
      ".xls",
      ".xlsx",
      "pdf",
      "support",
      "images",
      "videos",
      "zip",
      "files.",
      "before",
      "prepare",
      "user",
      "account",
      "enough",
      "permissions",
      "read",
      "access",
      "index.",
      "against",
      "repository",
      "perform",
      "following",
      "steps.",
      "first",
      "shared",
      "components.",
      "otherwise",
      "skip",
      "step.",
      "download",
      "package",
      "itom",
      "marketplace",
      "server.",
      "find",
      "link",
      "agent",
      "interface",
      "ui",
      "go",
      "administration",
      "utilities",
      "integration",
      "endpoints",
      "center",
      "right",
      "click",
      "under",
      "connector.",
      "extract",
      "package.",
      "folder",
      "content",
      "extracted",
      "referred",
      "remaining",
      "copy",
      "folder.",
      "already",
      "downloaded",
      "cfs."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Index knowledge from OpenText Core Content",
    "content": "You can use the IDOL Core Content Connector to index documents from OpenText™ Core Content. The Core Content Connector supports indexing only these file formats: text files (*.txt, *.xml, *.json, *html, *csv), Microsoft Office files (*.doc, *.docx, *.ppt, *.pptx, *.xls, *.xlsx), PDF files; it does not support images, videos, and zip files. Set up the Core Content Connector To set up knowledge indexing against a Core Content repository, perform the following steps. If this is the first IDOL connector that you set up, set up the shared components. Otherwise, skip this step. Download the Core Content Connector package from the ITOM Marketplace to the knowledge indexing server. You can also find the download link from the agent interface UI: go to Administration > Utilities > Integration > Endpoints, and then in the Download Center on the right, click the Download link under IDOL Connector. Extract the package. The folder to which the package content is extracted is referred to as the Core",
    "url": "indexcorecontent",
    "filename": "indexcorecontent",
    "headings": [
      "Set up the Core Content Connector",
      "Related topics"
    ],
    "keywords": [
      "https://<Core",
      "oauth_tool.cfg",
      "corecontentconnector.cfg",
      "versionkey.dat",
      "oauth2_sites.bin",
      "config.ini",
      "licensekey.dat",
      "insertSourceField.lua",
      "index",
      "knowledge",
      "opentext",
      "core",
      "content",
      "set",
      "connector",
      "related",
      "topics",
      "idol",
      "documents",
      "content.",
      "supports",
      "indexing",
      "file",
      "formats",
      "text",
      "files",
      ".txt",
      ".xml",
      ".json",
      "html",
      "csv",
      "microsoft",
      "office",
      ".doc",
      ".docx",
      ".ppt",
      ".pptx",
      ".xls",
      ".xlsx",
      "pdf",
      "support",
      "images",
      "videos",
      "zip",
      "files.",
      "against",
      "repository",
      "perform",
      "following",
      "steps.",
      "first",
      "shared",
      "components.",
      "otherwise",
      "skip",
      "step.",
      "download",
      "package",
      "itom",
      "marketplace",
      "server.",
      "find",
      "link",
      "agent",
      "interface",
      "ui",
      "go",
      "administration",
      "utilities",
      "integration",
      "endpoints",
      "center",
      "right",
      "click",
      "under",
      "connector.",
      "extract",
      "package.",
      "folder",
      "extracted",
      "referred",
      "remaining",
      "copy",
      "folder.",
      "already",
      "downloaded",
      "cfs.",
      "oem",
      "license",
      "lua",
      "scripts",
      "open",
      "cfs",
      "editor",
      "configure",
      "parameters",
      "section",
      "type",
      "keep",
      "default"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "index knowledge from opentext core content",
    "contentLower": "you can use the idol core content connector to index documents from opentext™ core content. the core content connector supports indexing only these file formats: text files (*.txt, *.xml, *.json, *html, *csv), microsoft office files (*.doc, *.docx, *.ppt, *.pptx, *.xls, *.xlsx), pdf files; it does not support images, videos, and zip files. set up the core content connector to set up knowledge indexing against a core content repository, perform the following steps. if this is the first idol connector that you set up, set up the shared components. otherwise, skip this step. download the core content connector package from the itom marketplace to the knowledge indexing server. you can also find the download link from the agent interface ui: go to administration > utilities > integration > endpoints, and then in the download center on the right, click the download link under idol connector. extract the package. the folder to which the package content is extracted is referred to as the core",
    "keywordsLower": [
      "https://<core",
      "oauth_tool.cfg",
      "corecontentconnector.cfg",
      "versionkey.dat",
      "oauth2_sites.bin",
      "config.ini",
      "licensekey.dat",
      "insertsourcefield.lua",
      "index",
      "knowledge",
      "opentext",
      "core",
      "content",
      "set",
      "connector",
      "related",
      "topics",
      "idol",
      "documents",
      "content.",
      "supports",
      "indexing",
      "file",
      "formats",
      "text",
      "files",
      ".txt",
      ".xml",
      ".json",
      "html",
      "csv",
      "microsoft",
      "office",
      ".doc",
      ".docx",
      ".ppt",
      ".pptx",
      ".xls",
      ".xlsx",
      "pdf",
      "support",
      "images",
      "videos",
      "zip",
      "files.",
      "against",
      "repository",
      "perform",
      "following",
      "steps.",
      "first",
      "shared",
      "components.",
      "otherwise",
      "skip",
      "step.",
      "download",
      "package",
      "itom",
      "marketplace",
      "server.",
      "find",
      "link",
      "agent",
      "interface",
      "ui",
      "go",
      "administration",
      "utilities",
      "integration",
      "endpoints",
      "center",
      "right",
      "click",
      "under",
      "connector.",
      "extract",
      "package.",
      "folder",
      "extracted",
      "referred",
      "remaining",
      "copy",
      "folder.",
      "already",
      "downloaded",
      "cfs.",
      "oem",
      "license",
      "lua",
      "scripts",
      "open",
      "cfs",
      "editor",
      "configure",
      "parameters",
      "section",
      "type",
      "keep",
      "default"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Index knowledge from OpenText Extended ECM",
    "content": "You can use the IDOL OpenText Connector to index documents from OpenText™ Extended ECM (xECM). The OpenText Connector supports indexing only these file formats: text files (*.txt, *.xml, *.json, *html, *csv), Microsoft Office files (*.doc, *.docx, *.ppt, *.pptx, *.xls, *.xlsx), PDF files; it does not support images, videos, and zip files. Set up the OpenText Connector To set up knowledge indexing against an xECM repository, perform the following steps. If this is the first IDOL connector that you set up, set up the shared components. Otherwise, skip this step. Download the OpenText Connector package from the ITOM Marketplace to the knowledge indexing server. You can also find the download link from the agent interface UI: go to Administration > Utilities > Integration > Endpoints, and then in the Download Center on the right, click the Download link under IDOL Connector. Extract the package. The folder to which the package content is extracted is referred to as the OpenText Connector f",
    "url": "indexxecm",
    "filename": "indexxecm",
    "headings": [
      "Set up the OpenText Connector",
      "Related topics"
    ],
    "keywords": [
      "https://example.com/OTCS/cs.exe",
      "versionkey.dat",
      "llisapi.dll",
      "opentextconnector.cfg",
      "example.com",
      "config.ini",
      "cs.exe",
      "licensekey.dat",
      "insertSourceField.lua",
      "https://example.com/intranet/llisapi.dll",
      "index",
      "knowledge",
      "opentext",
      "extended",
      "ecm",
      "set",
      "connector",
      "related",
      "topics",
      "idol",
      "documents",
      "xecm",
      "supports",
      "indexing",
      "file",
      "formats",
      "text",
      "files",
      ".txt",
      ".xml",
      ".json",
      "html",
      "csv",
      "microsoft",
      "office",
      ".doc",
      ".docx",
      ".ppt",
      ".pptx",
      ".xls",
      ".xlsx",
      "pdf",
      "support",
      "images",
      "videos",
      "zip",
      "files.",
      "against",
      "repository",
      "perform",
      "following",
      "steps.",
      "first",
      "shared",
      "components.",
      "otherwise",
      "skip",
      "step.",
      "download",
      "package",
      "itom",
      "marketplace",
      "server.",
      "find",
      "link",
      "agent",
      "interface",
      "ui",
      "go",
      "administration",
      "utilities",
      "integration",
      "endpoints",
      "center",
      "right",
      "click",
      "under",
      "connector.",
      "extract",
      "package.",
      "folder",
      "content",
      "extracted",
      "referred",
      "remaining",
      "copy",
      "folder.",
      "already",
      "downloaded",
      "cfs.",
      "oem",
      "license",
      "lua",
      "scripts",
      "open",
      "cfs",
      "editor",
      "configure",
      "parameters",
      "section"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "index knowledge from opentext extended ecm",
    "contentLower": "you can use the idol opentext connector to index documents from opentext™ extended ecm (xecm). the opentext connector supports indexing only these file formats: text files (*.txt, *.xml, *.json, *html, *csv), microsoft office files (*.doc, *.docx, *.ppt, *.pptx, *.xls, *.xlsx), pdf files; it does not support images, videos, and zip files. set up the opentext connector to set up knowledge indexing against an xecm repository, perform the following steps. if this is the first idol connector that you set up, set up the shared components. otherwise, skip this step. download the opentext connector package from the itom marketplace to the knowledge indexing server. you can also find the download link from the agent interface ui: go to administration > utilities > integration > endpoints, and then in the download center on the right, click the download link under idol connector. extract the package. the folder to which the package content is extracted is referred to as the opentext connector f",
    "keywordsLower": [
      "https://example.com/otcs/cs.exe",
      "versionkey.dat",
      "llisapi.dll",
      "opentextconnector.cfg",
      "example.com",
      "config.ini",
      "cs.exe",
      "licensekey.dat",
      "insertsourcefield.lua",
      "https://example.com/intranet/llisapi.dll",
      "index",
      "knowledge",
      "opentext",
      "extended",
      "ecm",
      "set",
      "connector",
      "related",
      "topics",
      "idol",
      "documents",
      "xecm",
      "supports",
      "indexing",
      "file",
      "formats",
      "text",
      "files",
      ".txt",
      ".xml",
      ".json",
      "html",
      "csv",
      "microsoft",
      "office",
      ".doc",
      ".docx",
      ".ppt",
      ".pptx",
      ".xls",
      ".xlsx",
      "pdf",
      "support",
      "images",
      "videos",
      "zip",
      "files.",
      "against",
      "repository",
      "perform",
      "following",
      "steps.",
      "first",
      "shared",
      "components.",
      "otherwise",
      "skip",
      "step.",
      "download",
      "package",
      "itom",
      "marketplace",
      "server.",
      "find",
      "link",
      "agent",
      "interface",
      "ui",
      "go",
      "administration",
      "utilities",
      "integration",
      "endpoints",
      "center",
      "right",
      "click",
      "under",
      "connector.",
      "extract",
      "package.",
      "folder",
      "content",
      "extracted",
      "referred",
      "remaining",
      "copy",
      "folder.",
      "already",
      "downloaded",
      "cfs.",
      "oem",
      "license",
      "lua",
      "scripts",
      "open",
      "cfs",
      "editor",
      "configure",
      "parameters",
      "section"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with SAP Solution Manager",
    "content": "This topic provides complete information for integrating SMAX with SAP Solution Manager. This integration can implement bidirectional Incident Case Exchange through seamless integration between SMAX and an external Service Provider that uses SAP Solution Manager as a ticketing tool. Benefits of integrating The integration of SMAX with SAP Solution Manager adds the following capabilities to your current deployment:: Integrate external service providers seamlessly and remove inefficiencies due to disconnected Service Desks with the following benefits: Clear information exchange and E2E view about incidents and incident status Consistent information Coherent tracking and reporting of incidents Close collaboration when working on incidents Improved insight into interrelationships between SAP solutions and other IT components Bring new value to IT and the business due to increased ability to proactively reduce risk of disruptions, by Uniquely fast resolution of incidents increased availabil",
    "url": "integratesap",
    "filename": "integratesap",
    "headings": [
      "Benefits of integrating",
      "Use case - Incident Exchange between SMAX and SAP Solution Manager",
      "Step 1: End user asks for SAP Payroll support",
      "Portal access",
      "Submitting the Enhancement Request with help of the Virtual Agent",
      "Step 2: The system is handling the incident in the backend",
      "Step 3: End user confirms or rejects the solution",
      "Step 4: Incident is closed",
      "Set up the integration",
      "Meet prerequisites",
      "Configure SAP",
      "Configure OO",
      "Step 1: Install OO",
      "Step 2: Disable CSRF protection",
      "Step 3: Deploy the content packs",
      "Step 4: Configure the OO flows",
      "Configure SMAX",
      "Step 1: Set up an On-Premises Bridge agent",
      "Step 2: Add a REST execution endpoint for invoking OO REST APIs",
      "Step 3: Set up an external system"
    ],
    "keywords": [
      "FioriLaunchpad.html",
      "dd.mm",
      "system.html",
      "7.2",
      "support.sap",
      "entity.Id",
      "10.80",
      "1.12.3",
      "1.12",
      "https://support.sap.com/en/alm/demo-systems/internet-demo-system.html",
      "2.0",
      "http://<oo",
      "integrate",
      "sap",
      "solution",
      "manager",
      "benefits",
      "integrating",
      "case",
      "incident",
      "exchange",
      "between",
      "smax",
      "step",
      "end",
      "user",
      "asks",
      "payroll",
      "support",
      "portal",
      "access",
      "submitting",
      "enhancement",
      "request",
      "help",
      "virtual",
      "agent",
      "system",
      "handling",
      "backend",
      "confirms",
      "rejects",
      "closed",
      "set",
      "integration",
      "meet",
      "prerequisites",
      "configure",
      "oo",
      "install",
      "disable",
      "csrf",
      "protection",
      "deploy",
      "content",
      "packs",
      "flows",
      "on-premises",
      "bridge",
      "add",
      "rest",
      "execution",
      "endpoint",
      "invoking",
      "apis",
      "external",
      "assignment",
      "group",
      "workflow",
      "create",
      "custom",
      "action",
      "rules",
      "see",
      "following",
      "task",
      "details.",
      "new",
      "fields",
      "service",
      "offering",
      "optional",
      "verify",
      "limitations",
      "topic",
      "provides",
      "complete",
      "information",
      "manager.",
      "implement",
      "bidirectional",
      "through",
      "seamless",
      "provider",
      "uses",
      "ticketing",
      "tool.",
      "adds",
      "capabilities",
      "current"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with sap solution manager",
    "contentLower": "this topic provides complete information for integrating smax with sap solution manager. this integration can implement bidirectional incident case exchange through seamless integration between smax and an external service provider that uses sap solution manager as a ticketing tool. benefits of integrating the integration of smax with sap solution manager adds the following capabilities to your current deployment:: integrate external service providers seamlessly and remove inefficiencies due to disconnected service desks with the following benefits: clear information exchange and e2e view about incidents and incident status consistent information coherent tracking and reporting of incidents close collaboration when working on incidents improved insight into interrelationships between sap solutions and other it components bring new value to it and the business due to increased ability to proactively reduce risk of disruptions, by uniquely fast resolution of incidents increased availabil",
    "keywordsLower": [
      "fiorilaunchpad.html",
      "dd.mm",
      "system.html",
      "7.2",
      "support.sap",
      "entity.id",
      "10.80",
      "1.12.3",
      "1.12",
      "https://support.sap.com/en/alm/demo-systems/internet-demo-system.html",
      "2.0",
      "http://<oo",
      "integrate",
      "sap",
      "solution",
      "manager",
      "benefits",
      "integrating",
      "case",
      "incident",
      "exchange",
      "between",
      "smax",
      "step",
      "end",
      "user",
      "asks",
      "payroll",
      "support",
      "portal",
      "access",
      "submitting",
      "enhancement",
      "request",
      "help",
      "virtual",
      "agent",
      "system",
      "handling",
      "backend",
      "confirms",
      "rejects",
      "closed",
      "set",
      "integration",
      "meet",
      "prerequisites",
      "configure",
      "oo",
      "install",
      "disable",
      "csrf",
      "protection",
      "deploy",
      "content",
      "packs",
      "flows",
      "on-premises",
      "bridge",
      "add",
      "rest",
      "execution",
      "endpoint",
      "invoking",
      "apis",
      "external",
      "assignment",
      "group",
      "workflow",
      "create",
      "custom",
      "action",
      "rules",
      "see",
      "following",
      "task",
      "details.",
      "new",
      "fields",
      "service",
      "offering",
      "optional",
      "verify",
      "limitations",
      "topic",
      "provides",
      "complete",
      "information",
      "manager.",
      "implement",
      "bidirectional",
      "through",
      "seamless",
      "provider",
      "uses",
      "ticketing",
      "tool.",
      "adds",
      "capabilities",
      "current"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio based Microsoft 365 email integration",
    "content": "Any mention of the term SMAX in this topic refers to Service Management, which was formerly known as Service Management Automation X (SMAX).OverviewThe MS365 Email Integration solution leverages the Microsoft Graph API to seamlessly integrate Microsoft 365 email with your service management system. This integration enables automatic creation and updating of records from emails, enhancing efficiency and ensuring timely responses to user requests. The integration supports two main use cases:If you configure the solution to create records via email, email senders will also receive updates on the record, including solutions to their requests, through email notifications. It remains your sole responsibility to configure the solution properly to make sure that sensitive information is not exposed.Create Records from Emails: Automatically creates various types of records (requests, incidents, problems, changes) from emails sent to a specific Microsoft 365 mailbox.Update Records by Emails: Upd",
    "url": "integratems365",
    "filename": "integratems365",
    "headings": [
      "Overview",
      "Create records from emails",
      "Overview",
      "Prerequisites",
      "Set up the integration",
      "Configure mailbox",
      "Create folders in mailbox",
      "Configure mailbox rules",
      "Prepare integration user",
      "Create an endpoint",
      "Create an integration",
      "Configure the scenario",
      "Validate the configuration",
      "Use the integration",
      "Customize the integration",
      "Customize external contact creation",
      "Customize request creation",
      "Customize the scenario to create incidents, changes, or problems",
      "Troubleshooting",
      "Update records by emails"
    ],
    "keywords": [
      "records.Here",
      "https://login.microsoftonline.com/<Directory_Id>/oauth2/v2.0/authorize?prompt=login.Redirect",
      "hourly.The",
      "pattern.To",
      "true.To",
      "microsoftonline.com",
      "promptly.The",
      "Management.The",
      "expires.In",
      "Management.When",
      "record.Via",
      "ID.Go",
      "ID.The",
      "mailbox.If",
      "page.In",
      "integration.In",
      "3.Send",
      "it.You",
      "activated.Name",
      "https://graph.microsoft.com/.default",
      "scenario.When",
      "microsoft.com",
      "creation.When",
      "API.You",
      "incorrect.Go",
      "https://<Service",
      "Mail.Read",
      "rule.Go",
      "task.The",
      "User.Read",
      "info.From",
      "request.Rule",
      "v2.0",
      "creation.Here",
      "documentation.Set",
      "Next.In",
      "email.The",
      "record.For",
      "true.Make",
      "365.The",
      "satisfied.You",
      "2.0",
      "order.Move",
      "page.Keep",
      "day.Once",
      "title.The",
      "earlier.For",
      "user.Log",
      "mail.The",
      "https.Base",
      "mailbox.The",
      "Active.In",
      "scenario.In",
      "CompanyA.com",
      "entity.Id",
      "https://graph.microsoft.com.Certificate",
      "mail.Here",
      "entity.Type",
      "emails.In",
      "25.1.1",
      "integration",
      "studio",
      "based",
      "microsoft",
      "365",
      "email",
      "overview",
      "create",
      "records",
      "emails",
      "prerequisites",
      "set",
      "configure",
      "mailbox",
      "folders",
      "rules",
      "prepare",
      "user",
      "endpoint",
      "scenario",
      "validate",
      "configuration",
      "customize",
      "external",
      "contact",
      "creation",
      "request",
      "incidents",
      "changes",
      "problems",
      "troubleshooting",
      "update",
      "activate",
      "replies",
      "folder",
      "settings",
      "reply",
      "any",
      "mention",
      "term"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio based microsoft 365 email integration",
    "contentLower": "any mention of the term smax in this topic refers to service management, which was formerly known as service management automation x (smax).overviewthe ms365 email integration solution leverages the microsoft graph api to seamlessly integrate microsoft 365 email with your service management system. this integration enables automatic creation and updating of records from emails, enhancing efficiency and ensuring timely responses to user requests. the integration supports two main use cases:if you configure the solution to create records via email, email senders will also receive updates on the record, including solutions to their requests, through email notifications. it remains your sole responsibility to configure the solution properly to make sure that sensitive information is not exposed.create records from emails: automatically creates various types of records (requests, incidents, problems, changes) from emails sent to a specific microsoft 365 mailbox.update records by emails: upd",
    "keywordsLower": [
      "records.here",
      "https://login.microsoftonline.com/<directory_id>/oauth2/v2.0/authorize?prompt=login.redirect",
      "hourly.the",
      "pattern.to",
      "true.to",
      "microsoftonline.com",
      "promptly.the",
      "management.the",
      "expires.in",
      "management.when",
      "record.via",
      "id.go",
      "id.the",
      "mailbox.if",
      "page.in",
      "integration.in",
      "3.send",
      "it.you",
      "activated.name",
      "https://graph.microsoft.com/.default",
      "scenario.when",
      "microsoft.com",
      "creation.when",
      "api.you",
      "incorrect.go",
      "https://<service",
      "mail.read",
      "rule.go",
      "task.the",
      "user.read",
      "info.from",
      "request.rule",
      "v2.0",
      "creation.here",
      "documentation.set",
      "next.in",
      "email.the",
      "record.for",
      "true.make",
      "365.the",
      "satisfied.you",
      "2.0",
      "order.move",
      "page.keep",
      "day.once",
      "title.the",
      "earlier.for",
      "user.log",
      "mail.the",
      "https.base",
      "mailbox.the",
      "active.in",
      "scenario.in",
      "companya.com",
      "entity.id",
      "https://graph.microsoft.com.certificate",
      "mail.here",
      "entity.type",
      "emails.in",
      "25.1.1",
      "integration",
      "studio",
      "based",
      "microsoft",
      "365",
      "email",
      "overview",
      "create",
      "records",
      "emails",
      "prerequisites",
      "set",
      "configure",
      "mailbox",
      "folders",
      "rules",
      "prepare",
      "user",
      "endpoint",
      "scenario",
      "validate",
      "configuration",
      "customize",
      "external",
      "contact",
      "creation",
      "request",
      "incidents",
      "changes",
      "problems",
      "troubleshooting",
      "update",
      "activate",
      "replies",
      "folder",
      "settings",
      "reply",
      "any",
      "mention",
      "term"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio based Gmail integration",
    "content": "Overview The Gmail integration feature enables you to automatically create requests from emails sent to a specific Gmail account. You can customize it to create incidents, problems, and changes. You can also allow external users to raise records by configuring the scenario. If you configure Gmail integration for automatic record creation, remember that the email senders will also receive notifications about updates to their records, including solutions to their requests. Make sure that your configure does not expose sensitive information. The Gmail integration feature is supported on versions 24.4.1 and above. This email integration uses Gmail API and OAuth 2.0 Authorization Grant Type for authentication. Prerequisites Before you set up Gmail integration, make sure the following requirements are fulfilled. Gmail Account: Make sure you have created a Gmail account.Service Management System: Verify that your service management system can send and receive messages from Google Cloud Platfo",
    "url": "integrationstudiogmailintegration",
    "filename": "integrationstudiogmailintegration",
    "headings": [
      "Overview",
      "Prerequisites",
      "Enable OAuth 2.0 on Google Cloud Platform",
      "Set up the integration",
      "Create integration user",
      "Create an endpoint",
      "Create an integration"
    ],
    "keywords": [
      "CONTINUE.On",
      "https://<SERVICE_MANAGEMENT_FQDN>/rest/<SMAX_TENANT_ID>/opb/oauth/callback",
      "user.Log",
      "https://accounts.google.com/o/oauth2/auth?access_type=offline&prompt=consent",
      "page.In",
      "https://console.cloud.google.com/apis",
      "integration.In",
      "https://gmail.googleapis.com",
      "Save.Make",
      "googleapis.com",
      "google.com",
      "https://oauth2.googleapis.com/token",
      "2.0",
      "https://mail.google.com",
      "process.If",
      "24.4.1",
      "integration",
      "studio",
      "based",
      "gmail",
      "overview",
      "prerequisites",
      "enable",
      "oauth",
      "google",
      "cloud",
      "platform",
      "set",
      "create",
      "user",
      "endpoint",
      "feature",
      "enables",
      "automatically",
      "requests",
      "emails",
      "sent",
      "specific",
      "account.",
      "customize",
      "incidents",
      "problems",
      "changes.",
      "allow",
      "external",
      "users",
      "raise",
      "records",
      "configuring",
      "scenario.",
      "configure",
      "automatic",
      "record",
      "creation",
      "remember",
      "email",
      "senders",
      "receive",
      "notifications",
      "about",
      "updates",
      "including",
      "solutions",
      "requests.",
      "make",
      "sure",
      "expose",
      "sensitive",
      "information.",
      "supported",
      "versions",
      "above.",
      "uses",
      "api",
      "authorization",
      "grant",
      "type",
      "authentication.",
      "before",
      "following",
      "requirements",
      "fulfilled.",
      "account",
      "created",
      "account.service",
      "management",
      "system",
      "verify",
      "service",
      "send",
      "messages",
      "platform.",
      "complete",
      "tasks",
      "go",
      "https",
      "console.cloud.google.com",
      "apis",
      "url",
      "open"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio based gmail integration",
    "contentLower": "overview the gmail integration feature enables you to automatically create requests from emails sent to a specific gmail account. you can customize it to create incidents, problems, and changes. you can also allow external users to raise records by configuring the scenario. if you configure gmail integration for automatic record creation, remember that the email senders will also receive notifications about updates to their records, including solutions to their requests. make sure that your configure does not expose sensitive information. the gmail integration feature is supported on versions 24.4.1 and above. this email integration uses gmail api and oauth 2.0 authorization grant type for authentication. prerequisites before you set up gmail integration, make sure the following requirements are fulfilled. gmail account: make sure you have created a gmail account.service management system: verify that your service management system can send and receive messages from google cloud platfo",
    "keywordsLower": [
      "continue.on",
      "https://<service_management_fqdn>/rest/<smax_tenant_id>/opb/oauth/callback",
      "user.log",
      "https://accounts.google.com/o/oauth2/auth?access_type=offline&prompt=consent",
      "page.in",
      "https://console.cloud.google.com/apis",
      "integration.in",
      "https://gmail.googleapis.com",
      "save.make",
      "googleapis.com",
      "google.com",
      "https://oauth2.googleapis.com/token",
      "2.0",
      "https://mail.google.com",
      "process.if",
      "24.4.1",
      "integration",
      "studio",
      "based",
      "gmail",
      "overview",
      "prerequisites",
      "enable",
      "oauth",
      "google",
      "cloud",
      "platform",
      "set",
      "create",
      "user",
      "endpoint",
      "feature",
      "enables",
      "automatically",
      "requests",
      "emails",
      "sent",
      "specific",
      "account.",
      "customize",
      "incidents",
      "problems",
      "changes.",
      "allow",
      "external",
      "users",
      "raise",
      "records",
      "configuring",
      "scenario.",
      "configure",
      "automatic",
      "record",
      "creation",
      "remember",
      "email",
      "senders",
      "receive",
      "notifications",
      "about",
      "updates",
      "including",
      "solutions",
      "requests.",
      "make",
      "sure",
      "expose",
      "sensitive",
      "information.",
      "supported",
      "versions",
      "above.",
      "uses",
      "api",
      "authorization",
      "grant",
      "type",
      "authentication.",
      "before",
      "following",
      "requirements",
      "fulfilled.",
      "account",
      "created",
      "account.service",
      "management",
      "system",
      "verify",
      "service",
      "send",
      "messages",
      "platform.",
      "complete",
      "tasks",
      "go",
      "https",
      "console.cloud.google.com",
      "apis",
      "url",
      "open"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with LDAP",
    "content": "Person and group records can be synchronized from an LDAP compatible server to Service Management through the On-Premises Bridge (OPB) Agent. When configuring the endpoint for the integration, field mappings between the LDAP compatible server and record fields in Service Management can be defined. Sample use case Import contacts from Active Directory This use case explains how to use the LDAP integration to import contacts into Service Management. Company ABC is using Microsoft Active Directory to manage their users and wants to use the LDAP integration to import some of their users as contacts into Service Management. Only Active Directory members who are currently active users and are in the MyCompany Organization Unit (OU) need to be imported; all other Active Directory members will not be imported. While importing the contacts, the company wants to import the following fields: First and Last name Email Address UID (unique identifier) User Status Location Manager Hire Date Home Phon",
    "url": "integldap",
    "filename": "integldap",
    "headings": [
      "Sample use case",
      "Import contacts from Active Directory",
      "Set up the integration",
      "Prerequisites",
      "Confirm the version of Active Directory supported",
      "Confirm the credentials of an account capable of viewing the active directory",
      "Determine which fields in Active Directory are needed and match them in Service Management",
      "Add the Active Directory connection credentials to OPB",
      "Add LDAP endpoint in the OPB agent",
      "Configure LDAP endpoint in Service Management",
      "Connection configuration",
      "Integration configuration",
      "Configure Record to Record mapping",
      "Map record to record for Person records:",
      "Map record to record for Group records:",
      "Target field configuration",
      "How to use",
      "Limitations",
      "Related topics"
    ],
    "keywords": [
      "source.city",
      "456.789",
      "integrate",
      "ldap",
      "sample",
      "case",
      "import",
      "contacts",
      "active",
      "directory",
      "set",
      "integration",
      "prerequisites",
      "confirm",
      "version",
      "supported",
      "credentials",
      "account",
      "capable",
      "viewing",
      "determine",
      "fields",
      "needed",
      "match",
      "service",
      "management",
      "add",
      "connection",
      "opb",
      "endpoint",
      "agent",
      "configure",
      "configuration",
      "record",
      "mapping",
      "map",
      "person",
      "records",
      "group",
      "target",
      "field",
      "limitations",
      "related",
      "topics",
      "synchronized",
      "compatible",
      "server",
      "through",
      "on-premises",
      "bridge",
      "agent.",
      "configuring",
      "mappings",
      "between",
      "defined.",
      "explains",
      "management.",
      "company",
      "abc",
      "microsoft",
      "manage",
      "users",
      "wants",
      "members",
      "currently",
      "mycompany",
      "organization",
      "unit",
      "ou",
      "need",
      "imported",
      "all",
      "imported.",
      "while",
      "importing",
      "following",
      "first",
      "last",
      "name",
      "email",
      "address",
      "uid",
      "unique",
      "identifier",
      "user",
      "status",
      "location",
      "manager",
      "hire",
      "date",
      "home",
      "phone",
      "number",
      "mobile",
      "distinguished",
      "ui",
      "connect",
      "server.",
      "create",
      "new"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with ldap",
    "contentLower": "person and group records can be synchronized from an ldap compatible server to service management through the on-premises bridge (opb) agent. when configuring the endpoint for the integration, field mappings between the ldap compatible server and record fields in service management can be defined. sample use case import contacts from active directory this use case explains how to use the ldap integration to import contacts into service management. company abc is using microsoft active directory to manage their users and wants to use the ldap integration to import some of their users as contacts into service management. only active directory members who are currently active users and are in the mycompany organization unit (ou) need to be imported; all other active directory members will not be imported. while importing the contacts, the company wants to import the following fields: first and last name email address uid (unique identifier) user status location manager hire date home phon",
    "keywordsLower": [
      "source.city",
      "456.789",
      "integrate",
      "ldap",
      "sample",
      "case",
      "import",
      "contacts",
      "active",
      "directory",
      "set",
      "integration",
      "prerequisites",
      "confirm",
      "version",
      "supported",
      "credentials",
      "account",
      "capable",
      "viewing",
      "determine",
      "fields",
      "needed",
      "match",
      "service",
      "management",
      "add",
      "connection",
      "opb",
      "endpoint",
      "agent",
      "configure",
      "configuration",
      "record",
      "mapping",
      "map",
      "person",
      "records",
      "group",
      "target",
      "field",
      "limitations",
      "related",
      "topics",
      "synchronized",
      "compatible",
      "server",
      "through",
      "on-premises",
      "bridge",
      "agent.",
      "configuring",
      "mappings",
      "between",
      "defined.",
      "explains",
      "management.",
      "company",
      "abc",
      "microsoft",
      "manage",
      "users",
      "wants",
      "members",
      "currently",
      "mycompany",
      "organization",
      "unit",
      "ou",
      "need",
      "imported",
      "all",
      "imported.",
      "while",
      "importing",
      "following",
      "first",
      "last",
      "name",
      "email",
      "address",
      "uid",
      "unique",
      "identifier",
      "user",
      "status",
      "location",
      "manager",
      "hire",
      "date",
      "home",
      "phone",
      "number",
      "mobile",
      "distinguished",
      "ui",
      "connect",
      "server.",
      "create",
      "new"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Power BI to create Cloud Cost Reporting reports",
    "content": "This topic describes how to integrate with Microsoft Power BI and then use it to create Cloud Cost Reporting reports based on the views provided out of the box. Before you begin designing reports: Ensure you have a fair idea of how the report will look like to address the use case you have from the end user(s). If possible, design the report on a whiteboard or paper beforehand.If you are new to Power BI, learn the basics of the tool. Some common constructs that would be useful while designing are input filters, parameters, date/time filters, drilling options, online data aggregation, date and time transformation functions, and some common visualizations like line charts, and bar charts.It's best to design your reports in a test environment. If you do this on your production setup, make a copy of the tables you are working with, and limit the amount of data in them during the design phase. This way your report loading after every edit is fast enough to not slow you down. Architecture ov",
    "url": "byobi",
    "filename": "byobi",
    "headings": [
      "Architecture overview",
      "Set up and configure Power BI Gateway",
      "Create a VM and install Power BI Gateway",
      "Harden the communication between Vertica and Power BI Gateway",
      "Create a read-only user in Vertica",
      "Configure the resource pool",
      "Use Power BI Service to create dataflows",
      "Create reports using Power BI Desktop",
      "Cloud Cost Reporting sample reports",
      "OOTB views",
      "AWS views",
      "Key columns in AWS views",
      "Azure views",
      "Key columns in Azure views",
      "Related topics"
    ],
    "keywords": [
      "ca.crt",
      "here.Go",
      "vertica_ca.crt",
      "workspace.In",
      "website.Run",
      "xxx.com",
      "Finish.Use",
      "saas.ca",
      "charts.It",
      "aggregation.1q",
      "server.Go",
      "beforehand.If",
      "powerbi.com",
      "node.Run",
      "network.The",
      "server.Log",
      "installation.Once",
      "it.Go",
      "aggregation.1m",
      "aggregation.1y",
      "integrate",
      "power",
      "bi",
      "create",
      "cloud",
      "cost",
      "reporting",
      "reports",
      "architecture",
      "overview",
      "set",
      "configure",
      "gateway",
      "vm",
      "install",
      "harden",
      "communication",
      "between",
      "vertica",
      "read-only",
      "user",
      "resource",
      "pool",
      "service",
      "dataflows",
      "desktop",
      "sample",
      "ootb",
      "views",
      "aws",
      "key",
      "columns",
      "azure",
      "related",
      "topics",
      "topic",
      "describes",
      "microsoft",
      "based",
      "provided",
      "out",
      "box.",
      "before",
      "begin",
      "designing",
      "ensure",
      "fair",
      "idea",
      "report",
      "look",
      "like",
      "address",
      "case",
      "end",
      "possible",
      "design",
      "whiteboard",
      "paper",
      "new",
      "learn",
      "basics",
      "tool.",
      "common",
      "constructs",
      "useful",
      "while",
      "input",
      "filters",
      "parameters",
      "date",
      "time",
      "drilling",
      "options",
      "online",
      "data",
      "aggregation",
      "transformation",
      "functions",
      "visualizations",
      "line"
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with power bi to create cloud cost reporting reports",
    "contentLower": "this topic describes how to integrate with microsoft power bi and then use it to create cloud cost reporting reports based on the views provided out of the box. before you begin designing reports: ensure you have a fair idea of how the report will look like to address the use case you have from the end user(s). if possible, design the report on a whiteboard or paper beforehand.if you are new to power bi, learn the basics of the tool. some common constructs that would be useful while designing are input filters, parameters, date/time filters, drilling options, online data aggregation, date and time transformation functions, and some common visualizations like line charts, and bar charts.it's best to design your reports in a test environment. if you do this on your production setup, make a copy of the tables you are working with, and limit the amount of data in them during the design phase. this way your report loading after every edit is fast enough to not slow you down. architecture ov",
    "keywordsLower": [
      "ca.crt",
      "here.go",
      "vertica_ca.crt",
      "workspace.in",
      "website.run",
      "xxx.com",
      "finish.use",
      "saas.ca",
      "charts.it",
      "aggregation.1q",
      "server.go",
      "beforehand.if",
      "powerbi.com",
      "node.run",
      "network.the",
      "server.log",
      "installation.once",
      "it.go",
      "aggregation.1m",
      "aggregation.1y",
      "integrate",
      "power",
      "bi",
      "create",
      "cloud",
      "cost",
      "reporting",
      "reports",
      "architecture",
      "overview",
      "set",
      "configure",
      "gateway",
      "vm",
      "install",
      "harden",
      "communication",
      "between",
      "vertica",
      "read-only",
      "user",
      "resource",
      "pool",
      "service",
      "dataflows",
      "desktop",
      "sample",
      "ootb",
      "views",
      "aws",
      "key",
      "columns",
      "azure",
      "related",
      "topics",
      "topic",
      "describes",
      "microsoft",
      "based",
      "provided",
      "out",
      "box.",
      "before",
      "begin",
      "designing",
      "ensure",
      "fair",
      "idea",
      "report",
      "look",
      "like",
      "address",
      "case",
      "end",
      "possible",
      "design",
      "whiteboard",
      "paper",
      "new",
      "learn",
      "basics",
      "tool.",
      "common",
      "constructs",
      "useful",
      "while",
      "input",
      "filters",
      "parameters",
      "date",
      "time",
      "drilling",
      "options",
      "online",
      "data",
      "aggregation",
      "transformation",
      "functions",
      "visualizations",
      "line"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate SAM with Raynet",
    "content": "This page describes how to integrate SAM with Raynet catalog using UCMDB to strengthen compliance, cost optimization, and software asset lifecycle governance. Integration benefits By integrating UCMDB with RayVentory Catalog, SAM gains access to significantly improved software inventory data. Key benefits include: Enriched Software Metadata: Release Date: Tracks when a software version was launched, which is referred to as General availability in SAM. End of Life Date: Indicates when vendor support ends. Obsolete Date: Indicates when software becomes non-viable. Lifecycle-Aware License Management: Enables creation of a license compatibility matrix  based on software release timelines. Supports informed decisions around renewals, upgrades, and decommissioning. Enhances accuracy in dashboards, compliance reports, and license tracking. Prerequisites Before you begin, complete the following prerequisites: UCMDB is integrated with Raynet and actively receiving normalized inventory. For more",
    "url": "integratesamwithraynet",
    "filename": "integratesamwithraynet",
    "headings": [
      "Integration benefits",
      "Prerequisites",
      "Use case",
      "Enriched data consumption from UCMDB",
      "Auto-Build compatibility matrix based on GA date",
      "New lifecycle reports in Service Management dashboard",
      "SAM reporting enhancements"
    ],
    "keywords": [
      "25.4",
      "2023.R2",
      "integrate",
      "sam",
      "raynet",
      "integration",
      "benefits",
      "prerequisites",
      "case",
      "enriched",
      "data",
      "consumption",
      "ucmdb",
      "auto-build",
      "compatibility",
      "matrix",
      "based",
      "ga",
      "date",
      "new",
      "lifecycle",
      "reports",
      "service",
      "management",
      "dashboard",
      "reporting",
      "enhancements",
      "page",
      "describes",
      "catalog",
      "strengthen",
      "compliance",
      "cost",
      "optimization",
      "software",
      "asset",
      "governance.",
      "integrating",
      "rayventory",
      "gains",
      "access",
      "significantly",
      "improved",
      "inventory",
      "data.",
      "key",
      "include",
      "metadata",
      "release",
      "tracks",
      "version",
      "launched",
      "referred",
      "general",
      "availability",
      "sam.",
      "end",
      "life",
      "indicates",
      "vendor",
      "support",
      "ends.",
      "obsolete",
      "becomes",
      "non-viable.",
      "lifecycle-aware",
      "license",
      "enables",
      "creation",
      "timelines.",
      "supports",
      "informed",
      "decisions",
      "around",
      "renewals",
      "upgrades",
      "decommissioning.",
      "enhances",
      "accuracy",
      "dashboards",
      "tracking.",
      "before",
      "begin",
      "complete",
      "following",
      "integrated",
      "actively",
      "receiving",
      "normalized",
      "inventory.",
      "information",
      "see",
      "ud",
      "one.",
      "ucmdb.",
      "enable",
      "capability",
      "management.",
      "configure",
      "consume"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate sam with raynet",
    "contentLower": "this page describes how to integrate sam with raynet catalog using ucmdb to strengthen compliance, cost optimization, and software asset lifecycle governance. integration benefits by integrating ucmdb with rayventory catalog, sam gains access to significantly improved software inventory data. key benefits include: enriched software metadata: release date: tracks when a software version was launched, which is referred to as general availability in sam. end of life date: indicates when vendor support ends. obsolete date: indicates when software becomes non-viable. lifecycle-aware license management: enables creation of a license compatibility matrix  based on software release timelines. supports informed decisions around renewals, upgrades, and decommissioning. enhances accuracy in dashboards, compliance reports, and license tracking. prerequisites before you begin, complete the following prerequisites: ucmdb is integrated with raynet and actively receiving normalized inventory. for more",
    "keywordsLower": [
      "25.4",
      "2023.r2",
      "integrate",
      "sam",
      "raynet",
      "integration",
      "benefits",
      "prerequisites",
      "case",
      "enriched",
      "data",
      "consumption",
      "ucmdb",
      "auto-build",
      "compatibility",
      "matrix",
      "based",
      "ga",
      "date",
      "new",
      "lifecycle",
      "reports",
      "service",
      "management",
      "dashboard",
      "reporting",
      "enhancements",
      "page",
      "describes",
      "catalog",
      "strengthen",
      "compliance",
      "cost",
      "optimization",
      "software",
      "asset",
      "governance.",
      "integrating",
      "rayventory",
      "gains",
      "access",
      "significantly",
      "improved",
      "inventory",
      "data.",
      "key",
      "include",
      "metadata",
      "release",
      "tracks",
      "version",
      "launched",
      "referred",
      "general",
      "availability",
      "sam.",
      "end",
      "life",
      "indicates",
      "vendor",
      "support",
      "ends.",
      "obsolete",
      "becomes",
      "non-viable.",
      "lifecycle-aware",
      "license",
      "enables",
      "creation",
      "timelines.",
      "supports",
      "informed",
      "decisions",
      "around",
      "renewals",
      "upgrades",
      "decommissioning.",
      "enhances",
      "accuracy",
      "dashboards",
      "tracking.",
      "before",
      "begin",
      "complete",
      "following",
      "integrated",
      "actively",
      "receiving",
      "normalized",
      "inventory.",
      "information",
      "see",
      "ud",
      "one.",
      "ucmdb.",
      "enable",
      "capability",
      "management.",
      "configure",
      "consume"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import CA certificate into the suite",
    "content": "The following procedure describes how to import an external CA certificate into the suite. Get your CA certificate. For Audit, see Export the Audit service engine certificate. For OO, see Get OO CA certificate. For UCMDB, see Get the root CA certificate of UCMDB Gateway. If you wish to integrate other products, you'll need to get the certificates for those products. Run the following command to get your my-values.yaml file: helm get values <ESM_RELEASE_NAME> -n <ESM_NAMESPACE> > my-values.yaml Where: <ESM_NAMESPACE> is the unique namespace of the suite deployment. <ESM_RELEASE_NAME> is the release name, which you can obtain by using the following command helm list -n <ESM_NAMESPACE> | awk '{print $1}' Open your my-values.yaml file, and paste the content of the root CA certificate in parallel with the global settings(there is no indent before the caCertificates line): caCertificates: <ca-cert>: <cert-file-base64-encoded> or PEM Where <ca_cert> certificate name you chose. For example cms",
    "url": "importcertificatehelm",
    "filename": "importcertificatehelm",
    "headings": [],
    "keywords": [
      "1.0.0",
      "cms_ca.crt",
      "audit_ca.crt",
      "xxx.tgz",
      "values.yaml",
      "oo_ca.crt",
      "import",
      "ca",
      "certificate",
      "suite",
      "following",
      "procedure",
      "describes",
      "external",
      "suite.",
      "get",
      "certificate.",
      "audit",
      "see",
      "export",
      "service",
      "engine",
      "oo",
      "ucmdb",
      "root",
      "gateway.",
      "wish",
      "integrate",
      "products",
      "ll",
      "need",
      "certificates",
      "products.",
      "run",
      "command",
      "my-values.yaml",
      "file",
      "helm",
      "values",
      "-n",
      "unique",
      "namespace",
      "deployment.",
      "release",
      "name",
      "obtain",
      "list",
      "awk",
      "print",
      "open",
      "paste",
      "content",
      "parallel",
      "global",
      "settings",
      "there",
      "indent",
      "before",
      "cacertificates",
      "line",
      "pem",
      "chose.",
      "example",
      "make",
      "update",
      "take",
      "effect",
      "upgrade",
      "-f",
      "absolute",
      "path",
      "package.",
      "sma",
      "esm-1.0.0",
      "2x.x-xxx",
      "charts",
      "2x.x-xxx.tgz",
      "itsma-xxx"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import ca certificate into the suite",
    "contentLower": "the following procedure describes how to import an external ca certificate into the suite. get your ca certificate. for audit, see export the audit service engine certificate. for oo, see get oo ca certificate. for ucmdb, see get the root ca certificate of ucmdb gateway. if you wish to integrate other products, you'll need to get the certificates for those products. run the following command to get your my-values.yaml file: helm get values <esm_release_name> -n <esm_namespace> > my-values.yaml where: <esm_namespace> is the unique namespace of the suite deployment. <esm_release_name> is the release name, which you can obtain by using the following command helm list -n <esm_namespace> | awk '{print $1}' open your my-values.yaml file, and paste the content of the root ca certificate in parallel with the global settings(there is no indent before the cacertificates line): cacertificates: <ca-cert>: <cert-file-base64-encoded> or pem where <ca_cert> certificate name you chose. for example cms",
    "keywordsLower": [
      "1.0.0",
      "cms_ca.crt",
      "audit_ca.crt",
      "xxx.tgz",
      "values.yaml",
      "oo_ca.crt",
      "import",
      "ca",
      "certificate",
      "suite",
      "following",
      "procedure",
      "describes",
      "external",
      "suite.",
      "get",
      "certificate.",
      "audit",
      "see",
      "export",
      "service",
      "engine",
      "oo",
      "ucmdb",
      "root",
      "gateway.",
      "wish",
      "integrate",
      "products",
      "ll",
      "need",
      "certificates",
      "products.",
      "run",
      "command",
      "my-values.yaml",
      "file",
      "helm",
      "values",
      "-n",
      "unique",
      "namespace",
      "deployment.",
      "release",
      "name",
      "obtain",
      "list",
      "awk",
      "print",
      "open",
      "paste",
      "content",
      "parallel",
      "global",
      "settings",
      "there",
      "indent",
      "before",
      "cacertificates",
      "line",
      "pem",
      "chose.",
      "example",
      "make",
      "update",
      "take",
      "effect",
      "upgrade",
      "-f",
      "absolute",
      "path",
      "package.",
      "sma",
      "esm-1.0.0",
      "2x.x-xxx",
      "charts",
      "2x.x-xxx.tgz",
      "itsma-xxx"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Load balance optimization among online platform pods",
    "content": "A load balance mechanism is embedded in the online platform pod's readiness probe, which runs every 10 seconds. A script constantly monitors the CPU usage of this pod, if the CPU usage exceeds the threshold (2.5 CPU cores by default) for longer than 30 seconds, the system will set the readiness status to \"Unavailable.\" Whenever the probe detects that the CPU usage is lower than the threshold, the system resumes the readiness status to \"Available.\" This mechanism operates optimally when sufficient online pods are available within the deployment. As the incoming online workload is evenly distributed across all pods, in scenarios where some pods remain idle while others are busy, overall performance may be affected. Enabling this mechanism allows busy pods to rest while idle ones manage the workload, thereby reducing response time. Conversely, if resources are limited (with only a few pods), enabling this mechanism might further degrade performance by reducing already scarce resources. En",
    "url": "balanceonlineworkload",
    "filename": "balanceonlineworkload",
    "headings": [
      "Enable the load balance mechanism"
    ],
    "keywords": [
      "1.5",
      "2.5",
      "load",
      "balance",
      "optimization",
      "among",
      "online",
      "platform",
      "pods",
      "enable",
      "mechanism",
      "embedded",
      "pod",
      "readiness",
      "probe",
      "runs",
      "every",
      "10",
      "seconds.",
      "script",
      "constantly",
      "monitors",
      "cpu",
      "usage",
      "exceeds",
      "threshold",
      "cores",
      "default",
      "longer",
      "30",
      "seconds",
      "system",
      "set",
      "status",
      "unavailable.",
      "whenever",
      "detects",
      "lower",
      "resumes",
      "available.",
      "operates",
      "optimally",
      "sufficient",
      "available",
      "deployment.",
      "incoming",
      "workload",
      "evenly",
      "distributed",
      "across",
      "all",
      "scenarios",
      "remain",
      "idle",
      "while",
      "others",
      "busy",
      "overall",
      "performance",
      "affected.",
      "enabling",
      "allows",
      "rest",
      "ones",
      "manage",
      "thereby",
      "reducing",
      "response",
      "time.",
      "conversely",
      "resources",
      "limited",
      "few",
      "further",
      "degrade",
      "already",
      "scarce",
      "resources.",
      "disabled",
      "turned",
      "parameter",
      "itom-xruntime-infra-config",
      "config",
      "map.",
      "run",
      "following",
      "command",
      "edit",
      "kubectl",
      "configmap",
      "-n",
      "change",
      "false",
      "true.",
      "meanwhile",
      "itom-xruntime-platform",
      "deployment",
      "limit",
      "customized",
      "consider"
    ],
    "language": "en",
    "word_count": 111,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "load balance optimization among online platform pods",
    "contentLower": "a load balance mechanism is embedded in the online platform pod's readiness probe, which runs every 10 seconds. a script constantly monitors the cpu usage of this pod, if the cpu usage exceeds the threshold (2.5 cpu cores by default) for longer than 30 seconds, the system will set the readiness status to \"unavailable.\" whenever the probe detects that the cpu usage is lower than the threshold, the system resumes the readiness status to \"available.\" this mechanism operates optimally when sufficient online pods are available within the deployment. as the incoming online workload is evenly distributed across all pods, in scenarios where some pods remain idle while others are busy, overall performance may be affected. enabling this mechanism allows busy pods to rest while idle ones manage the workload, thereby reducing response time. conversely, if resources are limited (with only a few pods), enabling this mechanism might further degrade performance by reducing already scarce resources. en",
    "keywordsLower": [
      "1.5",
      "2.5",
      "load",
      "balance",
      "optimization",
      "among",
      "online",
      "platform",
      "pods",
      "enable",
      "mechanism",
      "embedded",
      "pod",
      "readiness",
      "probe",
      "runs",
      "every",
      "10",
      "seconds.",
      "script",
      "constantly",
      "monitors",
      "cpu",
      "usage",
      "exceeds",
      "threshold",
      "cores",
      "default",
      "longer",
      "30",
      "seconds",
      "system",
      "set",
      "status",
      "unavailable.",
      "whenever",
      "detects",
      "lower",
      "resumes",
      "available.",
      "operates",
      "optimally",
      "sufficient",
      "available",
      "deployment.",
      "incoming",
      "workload",
      "evenly",
      "distributed",
      "across",
      "all",
      "scenarios",
      "remain",
      "idle",
      "while",
      "others",
      "busy",
      "overall",
      "performance",
      "affected.",
      "enabling",
      "allows",
      "rest",
      "ones",
      "manage",
      "thereby",
      "reducing",
      "response",
      "time.",
      "conversely",
      "resources",
      "limited",
      "few",
      "further",
      "degrade",
      "already",
      "scarce",
      "resources.",
      "disabled",
      "turned",
      "parameter",
      "itom-xruntime-infra-config",
      "config",
      "map.",
      "run",
      "following",
      "command",
      "edit",
      "kubectl",
      "configmap",
      "-n",
      "change",
      "false",
      "true.",
      "meanwhile",
      "itom-xruntime-platform",
      "deployment",
      "limit",
      "customized",
      "consider"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Log management",
    "content": "This section provides information that can help you manage logs for OPTIC Management Toolkit (OMT) and the suite. How to collect suite logs Where to find suite logs (Classic) or Where to find suite logs (Helm) Clean up log files Change log levels for suite logs",
    "url": "suitelogmanagement",
    "filename": "suitelogmanagement",
    "headings": [],
    "keywords": [
      "log",
      "management",
      "section",
      "provides",
      "information",
      "help",
      "manage",
      "logs",
      "optic",
      "toolkit",
      "omt",
      "suite.",
      "collect",
      "suite",
      "find",
      "classic",
      "helm",
      "clean",
      "files",
      "change",
      "levels"
    ],
    "language": "en",
    "word_count": 32,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "log management",
    "contentLower": "this section provides information that can help you manage logs for optic management toolkit (omt) and the suite. how to collect suite logs where to find suite logs (classic) or where to find suite logs (helm) clean up log files change log levels for suite logs",
    "keywordsLower": [
      "log",
      "management",
      "section",
      "provides",
      "information",
      "help",
      "manage",
      "logs",
      "optic",
      "toolkit",
      "omt",
      "suite.",
      "collect",
      "suite",
      "find",
      "classic",
      "helm",
      "clean",
      "files",
      "change",
      "levels"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License pools",
    "content": "Licenses pools provides a centralized management of license capacity and assignments. Licenses are added to license pools, then you can assign a combination of multiple licenses to the tenants. You can create license pools by business function, organization, or license mode. Note: You don't need a license to deploy the Service Manager tenant, so the Service Manager tenant isn't listed for selection. Related topics How to create and edit a license pool Licenses",
    "url": "licensepools",
    "filename": "licensepools",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "license",
      "pools",
      "related",
      "topics",
      "licenses",
      "provides",
      "centralized",
      "management",
      "capacity",
      "assignments.",
      "added",
      "assign",
      "combination",
      "multiple",
      "tenants.",
      "create",
      "business",
      "function",
      "organization",
      "mode.",
      "note",
      "don",
      "need",
      "deploy",
      "service",
      "manager",
      "tenant",
      "isn",
      "listed",
      "selection.",
      "edit",
      "pool"
    ],
    "language": "en",
    "word_count": 50,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license pools",
    "contentLower": "licenses pools provides a centralized management of license capacity and assignments. licenses are added to license pools, then you can assign a combination of multiple licenses to the tenants. you can create license pools by business function, organization, or license mode. note: you don't need a license to deploy the service manager tenant, so the service manager tenant isn't listed for selection. related topics how to create and edit a license pool licenses",
    "keywordsLower": [
      "license",
      "pools",
      "related",
      "topics",
      "licenses",
      "provides",
      "centralized",
      "management",
      "capacity",
      "assignments.",
      "added",
      "assign",
      "combination",
      "multiple",
      "tenants.",
      "create",
      "business",
      "function",
      "organization",
      "mode.",
      "note",
      "don",
      "need",
      "deploy",
      "service",
      "manager",
      "tenant",
      "isn",
      "listed",
      "selection.",
      "edit",
      "pool"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Licenses",
    "content": "License management enables you to manage license records, for example, creating a trial license file and uploading encrypted license key. The following license editions are supported: Service Management Express Service Management Premium Cloud Management Asset Management For more information on how to manage these licenses, see Service Management licenses, Cloud Management license, and Asset Management license. Modes of License There are four modes of license: Production: The official license key is issued by OpenText, redeemed and installed by AutoPass License Server (APLS). Suite Administration syncs the license information from APLS. Only this mode of license can be assigned to a production tenant. Trial: This license can be created manually in Suite Administration, the duration and capacity are limited. Only this mode of license can be assigned to a trial tenant. This mode of license can also be assigned to a dev tenant. Evaluation: The official license key is issued by OpenText, r",
    "url": "licensesmgmt",
    "filename": "licensesmgmt",
    "headings": [
      "Modes of License"
    ],
    "keywords": [
      "licenses",
      "modes",
      "license",
      "management",
      "enables",
      "manage",
      "records",
      "example",
      "creating",
      "trial",
      "file",
      "uploading",
      "encrypted",
      "key.",
      "following",
      "editions",
      "supported",
      "service",
      "express",
      "premium",
      "cloud",
      "asset",
      "information",
      "see",
      "license.",
      "there",
      "four",
      "production",
      "official",
      "key",
      "issued",
      "opentext",
      "redeemed",
      "installed",
      "autopass",
      "server",
      "apls",
      "suite",
      "administration",
      "syncs",
      "apls.",
      "mode",
      "assigned",
      "tenant.",
      "created",
      "manually",
      "duration",
      "capacity",
      "limited.",
      "dev",
      "evaluation",
      "internal",
      "non-production",
      "lock",
      "code",
      "needed",
      "applying",
      "device",
      "type",
      "portal",
      "main",
      "menu",
      "go",
      "click",
      "get",
      "instance.",
      "make",
      "sure",
      "instance",
      "environment",
      "apply",
      "follow",
      "rehost",
      "process",
      "need",
      "change",
      "deploy",
      "manager",
      "tenants."
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "licenses",
    "contentLower": "license management enables you to manage license records, for example, creating a trial license file and uploading encrypted license key. the following license editions are supported: service management express service management premium cloud management asset management for more information on how to manage these licenses, see service management licenses, cloud management license, and asset management license. modes of license there are four modes of license: production: the official license key is issued by opentext, redeemed and installed by autopass license server (apls). suite administration syncs the license information from apls. only this mode of license can be assigned to a production tenant. trial: this license can be created manually in suite administration, the duration and capacity are limited. only this mode of license can be assigned to a trial tenant. this mode of license can also be assigned to a dev tenant. evaluation: the official license key is issued by opentext, r",
    "keywordsLower": [
      "licenses",
      "modes",
      "license",
      "management",
      "enables",
      "manage",
      "records",
      "example",
      "creating",
      "trial",
      "file",
      "uploading",
      "encrypted",
      "key.",
      "following",
      "editions",
      "supported",
      "service",
      "express",
      "premium",
      "cloud",
      "asset",
      "information",
      "see",
      "license.",
      "there",
      "four",
      "production",
      "official",
      "key",
      "issued",
      "opentext",
      "redeemed",
      "installed",
      "autopass",
      "server",
      "apls",
      "suite",
      "administration",
      "syncs",
      "apls.",
      "mode",
      "assigned",
      "tenant.",
      "created",
      "manually",
      "duration",
      "capacity",
      "limited.",
      "dev",
      "evaluation",
      "internal",
      "non-production",
      "lock",
      "code",
      "needed",
      "applying",
      "device",
      "type",
      "portal",
      "main",
      "menu",
      "go",
      "click",
      "get",
      "instance.",
      "make",
      "sure",
      "instance",
      "environment",
      "apply",
      "follow",
      "rehost",
      "process",
      "need",
      "change",
      "deploy",
      "manager",
      "tenants."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Localize Service Management by using Openl10n",
    "content": "The Openl10n community site enables you to localize Service Management into certain languages that aren't officially supported by Service Management. You can also use this approach to update the existing translation for the officially supported languages. Pay attention to the following: Currently, the Openl10n tool supports only the localization of the Service Portal, agent portal, and Service Catalog data. After you upgrade the suite to a new version, you need to download the language pack of the new version from the Openl10n website, then re-implement the translation and redeploy it. If you want to modify the strings when the language setting in your profile is English (U.S.), you need to download the language pack for English. Don't overwrite a language pack with another language that has a different language code, even for those similar languages. For example, if you want to add Portuguese (Portugal) or Chinese (Traditional / Taiwan) languages, you must download pt-PT language pack",
    "url": "localizeusingopenl10n",
    "filename": "localizeusingopenl10n",
    "headings": [],
    "keywords": [
      "localize",
      "service",
      "management",
      "openl10n",
      "community",
      "site",
      "enables",
      "certain",
      "languages",
      "aren",
      "officially",
      "supported",
      "management.",
      "approach",
      "update",
      "existing",
      "translation",
      "languages.",
      "pay",
      "attention",
      "following",
      "currently",
      "tool",
      "supports",
      "localization",
      "portal",
      "agent",
      "catalog",
      "data.",
      "after",
      "upgrade",
      "suite",
      "new",
      "version",
      "need",
      "download",
      "language",
      "pack",
      "website",
      "re-implement",
      "redeploy",
      "it.",
      "want",
      "modify",
      "strings",
      "setting",
      "profile",
      "english",
      "u.s.",
      "english.",
      "don",
      "overwrite",
      "another",
      "different",
      "code",
      "even",
      "similar",
      "example",
      "add",
      "portuguese",
      "portugal",
      "chinese",
      "traditional",
      "taiwan",
      "pt-pt",
      "zh-tw",
      "rather",
      "brazil",
      "pt-br",
      "simplified",
      "china",
      "zh-cn",
      "directly",
      "modifications",
      "wording.",
      "otherwise",
      "system",
      "won",
      "user",
      "list",
      "thus",
      "able",
      "select",
      "switch",
      "language.",
      "details",
      "see",
      "openl10n."
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "localize service management by using openl10n",
    "contentLower": "the openl10n community site enables you to localize service management into certain languages that aren't officially supported by service management. you can also use this approach to update the existing translation for the officially supported languages. pay attention to the following: currently, the openl10n tool supports only the localization of the service portal, agent portal, and service catalog data. after you upgrade the suite to a new version, you need to download the language pack of the new version from the openl10n website, then re-implement the translation and redeploy it. if you want to modify the strings when the language setting in your profile is english (u.s.), you need to download the language pack for english. don't overwrite a language pack with another language that has a different language code, even for those similar languages. for example, if you want to add portuguese (portugal) or chinese (traditional / taiwan) languages, you must download pt-pt language pack",
    "keywordsLower": [
      "localize",
      "service",
      "management",
      "openl10n",
      "community",
      "site",
      "enables",
      "certain",
      "languages",
      "aren",
      "officially",
      "supported",
      "management.",
      "approach",
      "update",
      "existing",
      "translation",
      "languages.",
      "pay",
      "attention",
      "following",
      "currently",
      "tool",
      "supports",
      "localization",
      "portal",
      "agent",
      "catalog",
      "data.",
      "after",
      "upgrade",
      "suite",
      "new",
      "version",
      "need",
      "download",
      "language",
      "pack",
      "website",
      "re-implement",
      "redeploy",
      "it.",
      "want",
      "modify",
      "strings",
      "setting",
      "profile",
      "english",
      "u.s.",
      "english.",
      "don",
      "overwrite",
      "another",
      "different",
      "code",
      "even",
      "similar",
      "example",
      "add",
      "portuguese",
      "portugal",
      "chinese",
      "traditional",
      "taiwan",
      "pt-pt",
      "zh-tw",
      "rather",
      "brazil",
      "pt-br",
      "simplified",
      "china",
      "zh-cn",
      "directly",
      "modifications",
      "wording.",
      "otherwise",
      "system",
      "won",
      "user",
      "list",
      "thus",
      "able",
      "select",
      "switch",
      "language.",
      "details",
      "see",
      "openl10n."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import data",
    "content": "The Import Data tab enables you to import record data for the selected record. The imported data must be in CSV format. For more information, see Import Data file format. Import record data for most record types From the main menu, select Administration > Configuration > Studio. Select the required record type and click the Import Data tab. In the text box, enter the record data manually, in CSV format. The first row should contain the field names delimited by comma. The following rows should contain the corresponding field values delimited by comma. Alternatively, click the Browse button and select a .csv file to upload. When the file is uploaded, the data appears in the text box. You can edit the record if necessary. However, the User Principal Name (UPN) cannot be modified for Person record type data. If the CSV file contains a field name that doesn't exist for the current record, an error is generated when you try to import. Additionally, the maximum size of a single CVS file is ei",
    "url": "importdata",
    "filename": "importdata",
    "headings": [
      "Import record data for most record types",
      "Import record data for selected record types",
      "Advanced import identification",
      "Advanced import limitations",
      "Related topics"
    ],
    "keywords": [
      "format.If",
      "failure.If",
      "examples.If",
      "empty.For",
      "imports.The",
      "import",
      "data",
      "record",
      "most",
      "types",
      "selected",
      "advanced",
      "identification",
      "limitations",
      "related",
      "topics",
      "tab",
      "enables",
      "record.",
      "imported",
      "csv",
      "format.",
      "information",
      "see",
      "file",
      "main",
      "menu",
      "select",
      "administration",
      "configuration",
      "studio.",
      "required",
      "type",
      "click",
      "tab.",
      "text",
      "box",
      "enter",
      "manually",
      "first",
      "row",
      "contain",
      "field",
      "names",
      "delimited",
      "comma.",
      "following",
      "rows",
      "corresponding",
      "values",
      "alternatively",
      "browse",
      "button",
      ".csv",
      "upload.",
      "uploaded",
      "appears",
      "box.",
      "edit",
      "necessary.",
      "however",
      "user",
      "principal",
      "name",
      "upn",
      "cannot",
      "modified",
      "person",
      "data.",
      "contains",
      "doesn",
      "exist",
      "current",
      "error",
      "generated",
      "try",
      "import.",
      "additionally",
      "maximum",
      "size",
      "single",
      "cvs",
      "either",
      "10",
      "mb",
      "max",
      "tenant",
      "setting",
      "defined",
      "suite",
      "whichever",
      "smaller",
      "ready",
      "start",
      "process.",
      "dialog",
      "displaying",
      "time",
      "elapsed",
      "since"
    ],
    "language": "en",
    "word_count": 113,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import data",
    "contentLower": "the import data tab enables you to import record data for the selected record. the imported data must be in csv format. for more information, see import data file format. import record data for most record types from the main menu, select administration > configuration > studio. select the required record type and click the import data tab. in the text box, enter the record data manually, in csv format. the first row should contain the field names delimited by comma. the following rows should contain the corresponding field values delimited by comma. alternatively, click the browse button and select a .csv file to upload. when the file is uploaded, the data appears in the text box. you can edit the record if necessary. however, the user principal name (upn) cannot be modified for person record type data. if the csv file contains a field name that doesn't exist for the current record, an error is generated when you try to import. additionally, the maximum size of a single cvs file is ei",
    "keywordsLower": [
      "format.if",
      "failure.if",
      "examples.if",
      "empty.for",
      "imports.the",
      "import",
      "data",
      "record",
      "most",
      "types",
      "selected",
      "advanced",
      "identification",
      "limitations",
      "related",
      "topics",
      "tab",
      "enables",
      "record.",
      "imported",
      "csv",
      "format.",
      "information",
      "see",
      "file",
      "main",
      "menu",
      "select",
      "administration",
      "configuration",
      "studio.",
      "required",
      "type",
      "click",
      "tab.",
      "text",
      "box",
      "enter",
      "manually",
      "first",
      "row",
      "contain",
      "field",
      "names",
      "delimited",
      "comma.",
      "following",
      "rows",
      "corresponding",
      "values",
      "alternatively",
      "browse",
      "button",
      ".csv",
      "upload.",
      "uploaded",
      "appears",
      "box.",
      "edit",
      "necessary.",
      "however",
      "user",
      "principal",
      "name",
      "upn",
      "cannot",
      "modified",
      "person",
      "data.",
      "contains",
      "doesn",
      "exist",
      "current",
      "error",
      "generated",
      "try",
      "import.",
      "additionally",
      "maximum",
      "size",
      "single",
      "cvs",
      "either",
      "10",
      "mb",
      "max",
      "tenant",
      "setting",
      "defined",
      "suite",
      "whichever",
      "smaller",
      "ready",
      "start",
      "process.",
      "dialog",
      "displaying",
      "time",
      "elapsed",
      "since"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import Data file format",
    "content": "When importing data using the Import Data tab, the imported data must be in CSV format. Right-click here and then select Open link in new tab to download or open a spreadsheet with sample CSV files for a series of records. The files should be imported sequentially according to their number. Note The CSV format must use UTF-8 encoding to retain special characters such as language accents. For more information, see How to create a CSV file with UTF-8 encoding from an Excel file. The file must include the fields marked as Required in the Administration > Configuration > Studio > Fields tab for the selected record type, with the exception of fields that are auto-populated by the system (such as the Create time field). You may select any additional fields to include in the file as necessary for each record. The imported records must adhere to any restrictions defined in the workflow and the business rules for that record. For more information about the fields of a specific record type, refe",
    "url": "importdataformat",
    "filename": "importdataformat",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "OwnedByPerson.Upn",
      "import",
      "data",
      "file",
      "format",
      "related",
      "topics",
      "importing",
      "tab",
      "imported",
      "csv",
      "format.",
      "right-click",
      "here",
      "select",
      "open",
      "link",
      "new",
      "download",
      "spreadsheet",
      "sample",
      "files",
      "series",
      "records.",
      "sequentially",
      "according",
      "number.",
      "note",
      "utf-8",
      "encoding",
      "retain",
      "special",
      "characters",
      "such",
      "language",
      "accents.",
      "information",
      "see",
      "create",
      "excel",
      "file.",
      "include",
      "fields",
      "marked",
      "required",
      "administration",
      "configuration",
      "studio",
      "selected",
      "record",
      "type",
      "exception",
      "auto-populated",
      "system",
      "time",
      "field",
      "any",
      "additional",
      "necessary",
      "record.",
      "records",
      "adhere",
      "restrictions",
      "defined",
      "workflow",
      "business",
      "rules",
      "about",
      "specific",
      "refer",
      "section",
      "documentation",
      "type.",
      "logical",
      "order",
      "based",
      "dependencies",
      "between",
      "types.",
      "includes",
      "references",
      "first.",
      "example",
      "device",
      "ownedbyperson",
      "contained",
      "contact",
      "isn",
      "first",
      "fail.",
      "notes",
      "selecting",
      "attribute",
      "reference",
      "recommended",
      "unique",
      "value.",
      "identifier",
      "owner.",
      "datetime"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import data file format",
    "contentLower": "when importing data using the import data tab, the imported data must be in csv format. right-click here and then select open link in new tab to download or open a spreadsheet with sample csv files for a series of records. the files should be imported sequentially according to their number. note the csv format must use utf-8 encoding to retain special characters such as language accents. for more information, see how to create a csv file with utf-8 encoding from an excel file. the file must include the fields marked as required in the administration > configuration > studio > fields tab for the selected record type, with the exception of fields that are auto-populated by the system (such as the create time field). you may select any additional fields to include in the file as necessary for each record. the imported records must adhere to any restrictions defined in the workflow and the business rules for that record. for more information about the fields of a specific record type, refe",
    "keywordsLower": [
      "ownedbyperson.upn",
      "import",
      "data",
      "file",
      "format",
      "related",
      "topics",
      "importing",
      "tab",
      "imported",
      "csv",
      "format.",
      "right-click",
      "here",
      "select",
      "open",
      "link",
      "new",
      "download",
      "spreadsheet",
      "sample",
      "files",
      "series",
      "records.",
      "sequentially",
      "according",
      "number.",
      "note",
      "utf-8",
      "encoding",
      "retain",
      "special",
      "characters",
      "such",
      "language",
      "accents.",
      "information",
      "see",
      "create",
      "excel",
      "file.",
      "include",
      "fields",
      "marked",
      "required",
      "administration",
      "configuration",
      "studio",
      "selected",
      "record",
      "type",
      "exception",
      "auto-populated",
      "system",
      "time",
      "field",
      "any",
      "additional",
      "necessary",
      "record.",
      "records",
      "adhere",
      "restrictions",
      "defined",
      "workflow",
      "business",
      "rules",
      "about",
      "specific",
      "refer",
      "section",
      "documentation",
      "type.",
      "logical",
      "order",
      "based",
      "dependencies",
      "between",
      "types.",
      "includes",
      "references",
      "first.",
      "example",
      "device",
      "ownedbyperson",
      "contained",
      "contact",
      "isn",
      "first",
      "fail.",
      "notes",
      "selecting",
      "attribute",
      "reference",
      "recommended",
      "unique",
      "value.",
      "identifier",
      "owner.",
      "datetime"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import translations",
    "content": "Note This tab is available for the article, category, service definition, offering, and survey record types only. You can import localized data files (translated records) into Service Management. You use this to import localized: Service Catalog data. For more information, see How to import translated Service Catalog definitions. Articles. For more information, see How to import translated articles. Related topics Import data",
    "url": "importtrans",
    "filename": "importtrans",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "import",
      "translations",
      "related",
      "topics",
      "note",
      "tab",
      "available",
      "article",
      "category",
      "service",
      "definition",
      "offering",
      "survey",
      "record",
      "types",
      "only.",
      "localized",
      "data",
      "files",
      "translated",
      "records",
      "management.",
      "catalog",
      "data.",
      "information",
      "see",
      "definitions.",
      "articles."
    ],
    "language": "en",
    "word_count": 44,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import translations",
    "contentLower": "note this tab is available for the article, category, service definition, offering, and survey record types only. you can import localized data files (translated records) into service management. you use this to import localized: service catalog data. for more information, see how to import translated service catalog definitions. articles. for more information, see how to import translated articles. related topics import data",
    "keywordsLower": [
      "import",
      "translations",
      "related",
      "topics",
      "note",
      "tab",
      "available",
      "article",
      "category",
      "service",
      "definition",
      "offering",
      "survey",
      "record",
      "types",
      "only.",
      "localized",
      "data",
      "files",
      "translated",
      "records",
      "management.",
      "catalog",
      "data.",
      "information",
      "see",
      "definitions.",
      "articles."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import translated Service Catalog definitions",
    "content": "If you are an administrator, you can import translated Service Catalog definitions into Service Management. The translations are contained in CSV files exported from the Service Catalog. For details, see Localize the Service Catalog.",
    "url": "importcatalog",
    "filename": "importcatalog",
    "headings": [],
    "keywords": [
      "import",
      "translated",
      "service",
      "catalog",
      "definitions",
      "administrator",
      "management.",
      "translations",
      "contained",
      "csv",
      "files",
      "exported",
      "catalog.",
      "details",
      "see",
      "localize"
    ],
    "language": "en",
    "word_count": 25,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import translated service catalog definitions",
    "contentLower": "if you are an administrator, you can import translated service catalog definitions into service management. the translations are contained in csv files exported from the service catalog. for details, see localize the service catalog.",
    "keywordsLower": [
      "import",
      "translated",
      "service",
      "catalog",
      "definitions",
      "administrator",
      "management.",
      "translations",
      "contained",
      "csv",
      "files",
      "exported",
      "catalog.",
      "details",
      "see",
      "localize"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import translated articles",
    "content": "If you are an administrator, you can import translated articles into Service Management. The translations are contained in CSV files that were exported from Knowledge Management. Note To export and prepare the files that are to be imported, see How to localize articles. From the main menu, select Administration > Configuration > Studio. In the drop-down list at the top of the page, select Article. In the Import translations tab, browse to the translated CSV or ZIP files, and click Open. Click Import. A dialog box displays the progress of the import. At the end of the operation, the dialog box displays the results, including the number of records successfully imported and the number of failed records. Click the Details link to download a report of the failed records. The report displays the error for each failed record in the operation. After a successful import, translated articles are displayed in the Service Portal in the language of the user's locale, as long as the article has been",
    "url": "importtranslatedarticles",
    "filename": "importtranslatedarticles",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "import",
      "translated",
      "articles",
      "related",
      "topics",
      "administrator",
      "service",
      "management.",
      "translations",
      "contained",
      "csv",
      "files",
      "exported",
      "knowledge",
      "note",
      "export",
      "prepare",
      "imported",
      "see",
      "localize",
      "articles.",
      "main",
      "menu",
      "select",
      "administration",
      "configuration",
      "studio.",
      "drop-down",
      "list",
      "top",
      "page",
      "article.",
      "tab",
      "browse",
      "zip",
      "click",
      "open.",
      "import.",
      "dialog",
      "box",
      "displays",
      "progress",
      "end",
      "operation",
      "results",
      "including",
      "number",
      "records",
      "successfully",
      "failed",
      "records.",
      "details",
      "link",
      "download",
      "report",
      "error",
      "record",
      "operation.",
      "after",
      "successful",
      "displayed",
      "portal",
      "language",
      "user",
      "locale",
      "long",
      "article",
      "language.",
      "moreover",
      "users",
      "perform",
      "searches",
      "localized",
      "example",
      "french",
      "chinese",
      "locales",
      "search",
      "respectively.",
      "entry",
      "original",
      "portal.",
      "above",
      "german",
      "non-translated",
      "exporting",
      "file",
      "rich",
      "text",
      "fields",
      "saving",
      "excel",
      "formatting",
      "become",
      "corrupted",
      "re-imported",
      "display",
      "correctly.",
      "continue",
      "management"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import translated articles",
    "contentLower": "if you are an administrator, you can import translated articles into service management. the translations are contained in csv files that were exported from knowledge management. note to export and prepare the files that are to be imported, see how to localize articles. from the main menu, select administration > configuration > studio. in the drop-down list at the top of the page, select article. in the import translations tab, browse to the translated csv or zip files, and click open. click import. a dialog box displays the progress of the import. at the end of the operation, the dialog box displays the results, including the number of records successfully imported and the number of failed records. click the details link to download a report of the failed records. the report displays the error for each failed record in the operation. after a successful import, translated articles are displayed in the service portal in the language of the user's locale, as long as the article has been",
    "keywordsLower": [
      "import",
      "translated",
      "articles",
      "related",
      "topics",
      "administrator",
      "service",
      "management.",
      "translations",
      "contained",
      "csv",
      "files",
      "exported",
      "knowledge",
      "note",
      "export",
      "prepare",
      "imported",
      "see",
      "localize",
      "articles.",
      "main",
      "menu",
      "select",
      "administration",
      "configuration",
      "studio.",
      "drop-down",
      "list",
      "top",
      "page",
      "article.",
      "tab",
      "browse",
      "zip",
      "click",
      "open.",
      "import.",
      "dialog",
      "box",
      "displays",
      "progress",
      "end",
      "operation",
      "results",
      "including",
      "number",
      "records",
      "successfully",
      "failed",
      "records.",
      "details",
      "link",
      "download",
      "report",
      "error",
      "record",
      "operation.",
      "after",
      "successful",
      "displayed",
      "portal",
      "language",
      "user",
      "locale",
      "long",
      "article",
      "language.",
      "moreover",
      "users",
      "perform",
      "searches",
      "localized",
      "example",
      "french",
      "chinese",
      "locales",
      "search",
      "respectively.",
      "entry",
      "original",
      "portal.",
      "above",
      "german",
      "non-translated",
      "exporting",
      "file",
      "rich",
      "text",
      "fields",
      "saving",
      "excel",
      "formatting",
      "become",
      "corrupted",
      "re-imported",
      "display",
      "correctly.",
      "continue",
      "management"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Locations",
    "content": "You manage locations in the Location Management module. Note To manage locations, you must have the Tenant Admin role. Add a location to the list of locations From the main menu, select Administration > Master Data > Locations. Click New. Enter the details of the location. See the Location details section below. Note When adding a vendor site, under Type, select Site. When adding a stockroom, under Type, select Stockroom. Save the location. To enter other details, such as the location code and the location's address details, select the location and click Details. Update the details and save your changes. Update a location's details From the main menu, select Administration > Master Data > Locations. Select the location you want to update. To filter the record list, click the Add filter button. For more information, see Filters. Click the record identifier in the ID column to display the selected record. Update the location's general and address details. See the Location details section",
    "url": "locationmgmt",
    "filename": "locationmgmt",
    "headings": [
      "Add a location to the list of locations",
      "Update a location's details",
      "Location details",
      "Related topics"
    ],
    "keywords": [
      "locations",
      "add",
      "location",
      "list",
      "update",
      "details",
      "related",
      "topics",
      "manage",
      "management",
      "module.",
      "note",
      "tenant",
      "admin",
      "role.",
      "main",
      "menu",
      "select",
      "administration",
      "master",
      "data",
      "locations.",
      "click",
      "new.",
      "enter",
      "location.",
      "see",
      "section",
      "below.",
      "adding",
      "vendor",
      "site",
      "under",
      "type",
      "site.",
      "stockroom",
      "stockroom.",
      "save",
      "such",
      "code",
      "address",
      "details.",
      "changes.",
      "want",
      "update.",
      "filter",
      "record",
      "button.",
      "information",
      "filters.",
      "identifier",
      "id",
      "column",
      "display",
      "selected",
      "record.",
      "general",
      "view",
      "changes",
      "updates",
      "made",
      "history",
      "tab.",
      "history.",
      "edit",
      "multiple",
      "records",
      "simultaneously",
      "selecting",
      "grid",
      "updating",
      "preview",
      "pane",
      "right.",
      "mass",
      "tap",
      "person",
      "mobile",
      "app",
      "agent",
      "mode",
      "system",
      "open",
      "map",
      "search",
      "case",
      "full",
      "street",
      "first",
      "priority.",
      "addresss",
      "doesn",
      "exist",
      "name",
      "field",
      "description",
      "examples",
      "country",
      "city",
      "building"
    ],
    "language": "en",
    "word_count": 109,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "locations",
    "contentLower": "you manage locations in the location management module. note to manage locations, you must have the tenant admin role. add a location to the list of locations from the main menu, select administration > master data > locations. click new. enter the details of the location. see the location details section below. note when adding a vendor site, under type, select site. when adding a stockroom, under type, select stockroom. save the location. to enter other details, such as the location code and the location's address details, select the location and click details. update the details and save your changes. update a location's details from the main menu, select administration > master data > locations. select the location you want to update. to filter the record list, click the add filter button. for more information, see filters. click the record identifier in the id column to display the selected record. update the location's general and address details. see the location details section",
    "keywordsLower": [
      "locations",
      "add",
      "location",
      "list",
      "update",
      "details",
      "related",
      "topics",
      "manage",
      "management",
      "module.",
      "note",
      "tenant",
      "admin",
      "role.",
      "main",
      "menu",
      "select",
      "administration",
      "master",
      "data",
      "locations.",
      "click",
      "new.",
      "enter",
      "location.",
      "see",
      "section",
      "below.",
      "adding",
      "vendor",
      "site",
      "under",
      "type",
      "site.",
      "stockroom",
      "stockroom.",
      "save",
      "such",
      "code",
      "address",
      "details.",
      "changes.",
      "want",
      "update.",
      "filter",
      "record",
      "button.",
      "information",
      "filters.",
      "identifier",
      "id",
      "column",
      "display",
      "selected",
      "record.",
      "general",
      "view",
      "changes",
      "updates",
      "made",
      "history",
      "tab.",
      "history.",
      "edit",
      "multiple",
      "records",
      "simultaneously",
      "selecting",
      "grid",
      "updating",
      "preview",
      "pane",
      "right.",
      "mass",
      "tap",
      "person",
      "mobile",
      "app",
      "agent",
      "mode",
      "system",
      "open",
      "map",
      "search",
      "case",
      "full",
      "street",
      "first",
      "priority.",
      "addresss",
      "doesn",
      "exist",
      "name",
      "field",
      "description",
      "examples",
      "country",
      "city",
      "building"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Lists",
    "content": "Service Management uses predefined lists in the application forms. You can add more items and build new customized lists to meet the requirements of your environment. Lists are categorized into system-defined and user-defined. System-defined lists are locked. You cannot add, edit or delete any of the list items.User-defined lists are unlocked by default allowing you to add and edit the list items but you cannot delete them. Although system-defined lists are locked, they can still be customized to align with specific customer needs and preferences. The following table summarizes the actions you can perform on these lists: Action System-defined/locked lists User-defined/unlocked lists Edit list name (display name) No Yes Delete list No Yes Edit list item name Yes Yes Add list item No Yes Delete list item You can delete any item while adding it to the list if you no longer need it. After you save the list, you cannot delete its items. No No User-defined lists are categorized into volatile",
    "url": "listsmgmt",
    "filename": "listsmgmt",
    "headings": [
      "Create list",
      "Edit list",
      "Create or edit list in User Options tab",
      "Search for list"
    ],
    "keywords": [
      "lists.Type",
      "Lists.The",
      "items.User",
      "deleted.User",
      "lists",
      "create",
      "list",
      "edit",
      "user",
      "options",
      "tab",
      "search",
      "service",
      "management",
      "uses",
      "predefined",
      "application",
      "forms.",
      "add",
      "items",
      "build",
      "new",
      "customized",
      "meet",
      "requirements",
      "environment.",
      "categorized",
      "system-defined",
      "user-defined.",
      "locked.",
      "cannot",
      "delete",
      "any",
      "items.user-defined",
      "unlocked",
      "default",
      "allowing",
      "them.",
      "although",
      "locked",
      "still",
      "align",
      "specific",
      "customer",
      "needs",
      "preferences.",
      "following",
      "table",
      "summarizes",
      "actions",
      "perform",
      "action",
      "user-defined",
      "name",
      "display",
      "item",
      "while",
      "adding",
      "longer",
      "need",
      "it.",
      "after",
      "save",
      "items.",
      "volatile",
      "non-volatile.",
      "both",
      "types",
      "tailored",
      "preferences",
      "share",
      "characteristics",
      "lists.",
      "highlights",
      "key",
      "differences",
      "between",
      "two",
      "non-volatile",
      "created",
      "defining",
      "option",
      "inside",
      "offering",
      "model.",
      "administration",
      "configuration",
      "icon",
      "appears",
      "next",
      "want",
      "go",
      "plan",
      "catalog",
      "offerings",
      "tab.",
      "select",
      "existing",
      "one",
      "click"
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "lists",
    "contentLower": "service management uses predefined lists in the application forms. you can add more items and build new customized lists to meet the requirements of your environment. lists are categorized into system-defined and user-defined. system-defined lists are locked. you cannot add, edit or delete any of the list items.user-defined lists are unlocked by default allowing you to add and edit the list items but you cannot delete them. although system-defined lists are locked, they can still be customized to align with specific customer needs and preferences. the following table summarizes the actions you can perform on these lists: action system-defined/locked lists user-defined/unlocked lists edit list name (display name) no yes delete list no yes edit list item name yes yes add list item no yes delete list item you can delete any item while adding it to the list if you no longer need it. after you save the list, you cannot delete its items. no no user-defined lists are categorized into volatile",
    "keywordsLower": [
      "lists.type",
      "lists.the",
      "items.user",
      "deleted.user",
      "lists",
      "create",
      "list",
      "edit",
      "user",
      "options",
      "tab",
      "search",
      "service",
      "management",
      "uses",
      "predefined",
      "application",
      "forms.",
      "add",
      "items",
      "build",
      "new",
      "customized",
      "meet",
      "requirements",
      "environment.",
      "categorized",
      "system-defined",
      "user-defined.",
      "locked.",
      "cannot",
      "delete",
      "any",
      "items.user-defined",
      "unlocked",
      "default",
      "allowing",
      "them.",
      "although",
      "locked",
      "still",
      "align",
      "specific",
      "customer",
      "needs",
      "preferences.",
      "following",
      "table",
      "summarizes",
      "actions",
      "perform",
      "action",
      "user-defined",
      "name",
      "display",
      "item",
      "while",
      "adding",
      "longer",
      "need",
      "it.",
      "after",
      "save",
      "items.",
      "volatile",
      "non-volatile.",
      "both",
      "types",
      "tailored",
      "preferences",
      "share",
      "characteristics",
      "lists.",
      "highlights",
      "key",
      "differences",
      "between",
      "two",
      "non-volatile",
      "created",
      "defining",
      "option",
      "inside",
      "offering",
      "model.",
      "administration",
      "configuration",
      "icon",
      "appears",
      "next",
      "want",
      "go",
      "plan",
      "catalog",
      "offerings",
      "tab.",
      "select",
      "existing",
      "one",
      "click"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident transition rules",
    "content": "Built-in business rules define conditions for automatic transitions from one phase to another. If the condition evaluates as true, the automatic transition occurs. Some conditions are simple and some are complex. If the condition isn't true, the record remains in its original phase. The following table identifies each supported transition in the Incident Management workflow. Notice that it's possible for a record to return to an earlier phase in the workflow if the status changes to a certain value. You can return to an earlier phase when any of the following circumstances occur: The identified solution doesn't solve the reported issue. The incident was categorized incorrectly. The incident needs further investigation. The Incident Coordinator reassigns the incident to a different support analyst. The Action or Condition column in the following tables show fields that contain data either entered by the end user or automatically provided by Service Management. At the start of each phase",
    "url": "incidenttransitionrules",
    "filename": "incidenttransitionrules",
    "headings": [
      "Log phase",
      "Classify phase",
      "Initial support phase",
      "Review phase",
      "Escalate phase",
      "Close phase",
      "Related topics"
    ],
    "keywords": [
      "incident",
      "transition",
      "rules",
      "log",
      "phase",
      "classify",
      "initial",
      "support",
      "review",
      "escalate",
      "close",
      "related",
      "topics",
      "built-in",
      "business",
      "define",
      "conditions",
      "automatic",
      "transitions",
      "one",
      "another.",
      "condition",
      "evaluates",
      "true",
      "occurs.",
      "simple",
      "complex.",
      "isn",
      "record",
      "remains",
      "original",
      "phase.",
      "following",
      "table",
      "identifies",
      "supported",
      "management",
      "workflow.",
      "notice",
      "possible",
      "return",
      "earlier",
      "workflow",
      "status",
      "changes",
      "certain",
      "value.",
      "any",
      "circumstances",
      "occur",
      "identified",
      "solution",
      "doesn",
      "solve",
      "reported",
      "issue.",
      "categorized",
      "incorrectly.",
      "needs",
      "further",
      "investigation.",
      "coordinator",
      "reassigns",
      "different",
      "analyst.",
      "action",
      "column",
      "tables",
      "show",
      "fields",
      "contain",
      "data",
      "either",
      "entered",
      "end",
      "user",
      "automatically",
      "provided",
      "service",
      "management.",
      "start",
      "verifies",
      "user-defined",
      "values",
      "set",
      "last",
      "unchanged.",
      "there",
      "change",
      "prior",
      "repeat",
      "action.",
      "example",
      "re-starts",
      "level",
      "target",
      "calculations.",
      "all",
      "except",
      "final"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident transition rules",
    "contentLower": "built-in business rules define conditions for automatic transitions from one phase to another. if the condition evaluates as true, the automatic transition occurs. some conditions are simple and some are complex. if the condition isn't true, the record remains in its original phase. the following table identifies each supported transition in the incident management workflow. notice that it's possible for a record to return to an earlier phase in the workflow if the status changes to a certain value. you can return to an earlier phase when any of the following circumstances occur: the identified solution doesn't solve the reported issue. the incident was categorized incorrectly. the incident needs further investigation. the incident coordinator reassigns the incident to a different support analyst. the action or condition column in the following tables show fields that contain data either entered by the end user or automatically provided by service management. at the start of each phase",
    "keywordsLower": [
      "incident",
      "transition",
      "rules",
      "log",
      "phase",
      "classify",
      "initial",
      "support",
      "review",
      "escalate",
      "close",
      "related",
      "topics",
      "built-in",
      "business",
      "define",
      "conditions",
      "automatic",
      "transitions",
      "one",
      "another.",
      "condition",
      "evaluates",
      "true",
      "occurs.",
      "simple",
      "complex.",
      "isn",
      "record",
      "remains",
      "original",
      "phase.",
      "following",
      "table",
      "identifies",
      "supported",
      "management",
      "workflow.",
      "notice",
      "possible",
      "return",
      "earlier",
      "workflow",
      "status",
      "changes",
      "certain",
      "value.",
      "any",
      "circumstances",
      "occur",
      "identified",
      "solution",
      "doesn",
      "solve",
      "reported",
      "issue.",
      "categorized",
      "incorrectly.",
      "needs",
      "further",
      "investigation.",
      "coordinator",
      "reassigns",
      "different",
      "analyst.",
      "action",
      "column",
      "tables",
      "show",
      "fields",
      "contain",
      "data",
      "either",
      "entered",
      "end",
      "user",
      "automatically",
      "provided",
      "service",
      "management.",
      "start",
      "verifies",
      "user-defined",
      "values",
      "set",
      "last",
      "unchanged.",
      "there",
      "change",
      "prior",
      "repeat",
      "action.",
      "example",
      "re-starts",
      "level",
      "target",
      "calculations.",
      "all",
      "except",
      "final"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Idea process - Business rules",
    "content": "In the out-of-the-box idea workflow, the following business rules apply to the indicated metaphase or phase. Idea process - all phases Condition Service Management action After change Related proposal is changed and isn't empty In the proposal, set Created from to Idea Review decision is changed to one of the following: Reject Need more information Create a proposal Create a change Send an email to the person listed in the Created by field notifying the review decision Phase Id is changed to Review Send an email notification to the user who created the idea using the default template definition None Validate: Review decision is Create a proposal, and Related change isn't empty Display the idea workflow error message in case of a failure None Validate: Review decision is Create a change, and Related proposal isn't empty Display the idea workflow error message in case of a failure None Validate: Review decision is empty, or Review decision and Review feedback aren't empty Display the ide",
    "url": "processideas",
    "filename": "processideas",
    "headings": [
      "Idea process - all phases",
      "Draft phase",
      "Refine phase",
      "Review phase",
      "Refinement metaphase",
      "Related topics"
    ],
    "keywords": [
      "idea",
      "process",
      "business",
      "rules",
      "all",
      "phases",
      "draft",
      "phase",
      "refine",
      "review",
      "refinement",
      "metaphase",
      "related",
      "topics",
      "out-of-the-box",
      "workflow",
      "following",
      "apply",
      "indicated",
      "phase.",
      "condition",
      "service",
      "management",
      "action",
      "after",
      "change",
      "proposal",
      "changed",
      "isn",
      "empty",
      "set",
      "created",
      "decision",
      "one",
      "reject",
      "need",
      "information",
      "create",
      "send",
      "email",
      "person",
      "listed",
      "field",
      "notifying",
      "id",
      "notification",
      "user",
      "default",
      "template",
      "definition",
      "none",
      "validate",
      "display",
      "error",
      "message",
      "case",
      "failure",
      "feedback",
      "aren",
      "task",
      "plan",
      "rejected",
      "close",
      "creator",
      "rendering",
      "forms",
      "assessment",
      "disable",
      "fields",
      "suggested",
      "values",
      "unit",
      "organizational",
      "group",
      "type",
      "compose",
      "target",
      "operate",
      "log",
      "evaluate",
      "event",
      "entering",
      "current",
      "number",
      "likes",
      "priority",
      "medium",
      "transitioning",
      "abandon",
      "running",
      "expand",
      "details",
      "null",
      "applying",
      "changes",
      "notify",
      "users",
      "new",
      "working",
      "processes"
    ],
    "language": "en",
    "word_count": 118,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "idea process - business rules",
    "contentLower": "in the out-of-the-box idea workflow, the following business rules apply to the indicated metaphase or phase. idea process - all phases condition service management action after change related proposal is changed and isn't empty in the proposal, set created from to idea review decision is changed to one of the following: reject need more information create a proposal create a change send an email to the person listed in the created by field notifying the review decision phase id is changed to review send an email notification to the user who created the idea using the default template definition none validate: review decision is create a proposal, and related change isn't empty display the idea workflow error message in case of a failure none validate: review decision is create a change, and related proposal isn't empty display the idea workflow error message in case of a failure none validate: review decision is empty, or review decision and review feedback aren't empty display the ide",
    "keywordsLower": [
      "idea",
      "process",
      "business",
      "rules",
      "all",
      "phases",
      "draft",
      "phase",
      "refine",
      "review",
      "refinement",
      "metaphase",
      "related",
      "topics",
      "out-of-the-box",
      "workflow",
      "following",
      "apply",
      "indicated",
      "phase.",
      "condition",
      "service",
      "management",
      "action",
      "after",
      "change",
      "proposal",
      "changed",
      "isn",
      "empty",
      "set",
      "created",
      "decision",
      "one",
      "reject",
      "need",
      "information",
      "create",
      "send",
      "email",
      "person",
      "listed",
      "field",
      "notifying",
      "id",
      "notification",
      "user",
      "default",
      "template",
      "definition",
      "none",
      "validate",
      "display",
      "error",
      "message",
      "case",
      "failure",
      "feedback",
      "aren",
      "task",
      "plan",
      "rejected",
      "close",
      "creator",
      "rendering",
      "forms",
      "assessment",
      "disable",
      "fields",
      "suggested",
      "values",
      "unit",
      "organizational",
      "group",
      "type",
      "compose",
      "target",
      "operate",
      "log",
      "evaluate",
      "event",
      "entering",
      "current",
      "number",
      "likes",
      "priority",
      "medium",
      "transitioning",
      "abandon",
      "running",
      "expand",
      "details",
      "null",
      "applying",
      "changes",
      "notify",
      "users",
      "new",
      "working",
      "processes"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Infrastructure &amp; peripheral process - Business rules",
    "content": "The infrastructure & peripheral asset workflow relies on a few simple business rules. Rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. All processes In the out-of-the-box infrastructure & peripheral asset workflow, the rules detailed for each of the following events apply to all processes. Rendering Condition Service Management actions Phase ID isn't In Stock Stockroom entry date = hidden Reserved = hidden Stockroom = hidden Phase ID isn't End, or Close code isn't Retired Retirement date = hidden Retirement reason = hidden Located in stock isn't null Location = read-only Subtype isn't Monitor Screen size = hidden Subtype isn't Mass storage Storage size = hidden Acquisition type is Rental or Lease or Loan In the Finance tab: Total amount = hidden Currency = hidden Tax rate = hidden Payment terms = hidden Payment date = hidden External PO Number = hidden Invoice number = hidden Resale section = hidden Fix",
    "url": "processip",
    "filename": "processip",
    "headings": [
      "Rendering",
      "After change",
      "After applying change",
      "Unavailable metaphase",
      "Available metaphase",
      "In stock phase",
      "In use metaphase",
      "In use phase",
      "Retire phase",
      "Ended",
      "Canceled",
      "Related topics"
    ],
    "keywords": [
      "infrastructure",
      "amp",
      "peripheral",
      "process",
      "business",
      "rules",
      "rendering",
      "after",
      "change",
      "applying",
      "unavailable",
      "metaphase",
      "available",
      "stock",
      "phase",
      "retire",
      "ended",
      "canceled",
      "related",
      "topics",
      "asset",
      "workflow",
      "relies",
      "few",
      "simple",
      "rules.",
      "repeat",
      "one",
      "another",
      "end",
      "user",
      "make",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "all",
      "processes",
      "out-of-the-box",
      "detailed",
      "following",
      "events",
      "apply",
      "processes.",
      "condition",
      "service",
      "management",
      "actions",
      "id",
      "isn",
      "stockroom",
      "entry",
      "date",
      "hidden",
      "reserved",
      "close",
      "code",
      "retired",
      "retirement",
      "reason",
      "located",
      "null",
      "location",
      "read-only",
      "subtype",
      "monitor",
      "screen",
      "size",
      "mass",
      "storage",
      "acquisition",
      "type",
      "rental",
      "lease",
      "loan",
      "finance",
      "tab",
      "total",
      "amount",
      "currency",
      "tax",
      "rate",
      "payment",
      "terms",
      "external",
      "po",
      "number",
      "invoice",
      "resale",
      "section",
      "fixed",
      "assets",
      "empty",
      "show",
      "model",
      "list",
      "filter",
      "category",
      "false"
    ],
    "language": "en",
    "word_count": 132,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "infrastructure &amp; peripheral process - business rules",
    "contentLower": "the infrastructure & peripheral asset workflow relies on a few simple business rules. rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. all processes in the out-of-the-box infrastructure & peripheral asset workflow, the rules detailed for each of the following events apply to all processes. rendering condition service management actions phase id isn't in stock stockroom entry date = hidden reserved = hidden stockroom = hidden phase id isn't end, or close code isn't retired retirement date = hidden retirement reason = hidden located in stock isn't null location = read-only subtype isn't monitor screen size = hidden subtype isn't mass storage storage size = hidden acquisition type is rental or lease or loan in the finance tab: total amount = hidden currency = hidden tax rate = hidden payment terms = hidden payment date = hidden external po number = hidden invoice number = hidden resale section = hidden fix",
    "keywordsLower": [
      "infrastructure",
      "amp",
      "peripheral",
      "process",
      "business",
      "rules",
      "rendering",
      "after",
      "change",
      "applying",
      "unavailable",
      "metaphase",
      "available",
      "stock",
      "phase",
      "retire",
      "ended",
      "canceled",
      "related",
      "topics",
      "asset",
      "workflow",
      "relies",
      "few",
      "simple",
      "rules.",
      "repeat",
      "one",
      "another",
      "end",
      "user",
      "make",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "all",
      "processes",
      "out-of-the-box",
      "detailed",
      "following",
      "events",
      "apply",
      "processes.",
      "condition",
      "service",
      "management",
      "actions",
      "id",
      "isn",
      "stockroom",
      "entry",
      "date",
      "hidden",
      "reserved",
      "close",
      "code",
      "retired",
      "retirement",
      "reason",
      "located",
      "null",
      "location",
      "read-only",
      "subtype",
      "monitor",
      "screen",
      "size",
      "mass",
      "storage",
      "acquisition",
      "type",
      "rental",
      "lease",
      "loan",
      "finance",
      "tab",
      "total",
      "amount",
      "currency",
      "tax",
      "rate",
      "payment",
      "terms",
      "external",
      "po",
      "number",
      "invoice",
      "resale",
      "section",
      "fixed",
      "assets",
      "empty",
      "show",
      "model",
      "list",
      "filter",
      "category",
      "false"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License process - Business rules",
    "content": "The license workflow relies on a few simple business rules. Rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. All processes In the out-of-the-box license workflow, the rules detailed for each of the following events apply to all processes. Rendering Condition Service Management actions Phase isn't Ended Close code = hidden If Perpetual = true Start date = hidden End date = hidden If Acquisition type is Rental or Lease or Loan Total amount = hidden Currency = hidden Tax rate = hidden Payment terms = hidden Payment date = hidden External PO Number = hidden Invoice number = hidden Fixed assets section (in Finance tab) = hidden None Show License contract list with filter: Contract type = License contract If Asset model isn't empty and Brand of Asset model isn't empty Show Publisher list with filter: Owner of Brand of Asset model Publisher isn't empty and License type isn't empty Show License model list with ",
    "url": "processlicense",
    "filename": "processlicense",
    "headings": [
      "All processes",
      "Rendering",
      "After change",
      "After applying change",
      "End phase",
      "After change",
      "Related topics"
    ],
    "keywords": [
      "25.4",
      "license",
      "process",
      "business",
      "rules",
      "all",
      "processes",
      "rendering",
      "after",
      "change",
      "applying",
      "end",
      "phase",
      "related",
      "topics",
      "workflow",
      "relies",
      "few",
      "simple",
      "rules.",
      "repeat",
      "one",
      "another",
      "user",
      "make",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "out-of-the-box",
      "detailed",
      "following",
      "events",
      "apply",
      "processes.",
      "condition",
      "service",
      "management",
      "actions",
      "isn",
      "ended",
      "close",
      "code",
      "hidden",
      "perpetual",
      "true",
      "start",
      "date",
      "acquisition",
      "type",
      "rental",
      "lease",
      "loan",
      "total",
      "amount",
      "currency",
      "tax",
      "rate",
      "payment",
      "terms",
      "external",
      "po",
      "number",
      "invoice",
      "fixed",
      "assets",
      "section",
      "finance",
      "tab",
      "none",
      "show",
      "contract",
      "list",
      "filter",
      "asset",
      "model",
      "empty",
      "brand",
      "publisher",
      "owner",
      "filters",
      "category",
      "false",
      "active",
      "default",
      "maintenance",
      "covers",
      "cost",
      "centers",
      "center",
      "drop-down",
      "types",
      "display",
      "label",
      "tag",
      "unit",
      "rights",
      "quantity",
      "entitlements"
    ],
    "language": "en",
    "word_count": 132,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license process - business rules",
    "contentLower": "the license workflow relies on a few simple business rules. rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. all processes in the out-of-the-box license workflow, the rules detailed for each of the following events apply to all processes. rendering condition service management actions phase isn't ended close code = hidden if perpetual = true start date = hidden end date = hidden if acquisition type is rental or lease or loan total amount = hidden currency = hidden tax rate = hidden payment terms = hidden payment date = hidden external po number = hidden invoice number = hidden fixed assets section (in finance tab) = hidden none show license contract list with filter: contract type = license contract if asset model isn't empty and brand of asset model isn't empty show publisher list with filter: owner of brand of asset model publisher isn't empty and license type isn't empty show license model list with ",
    "keywordsLower": [
      "25.4",
      "license",
      "process",
      "business",
      "rules",
      "all",
      "processes",
      "rendering",
      "after",
      "change",
      "applying",
      "end",
      "phase",
      "related",
      "topics",
      "workflow",
      "relies",
      "few",
      "simple",
      "rules.",
      "repeat",
      "one",
      "another",
      "user",
      "make",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "out-of-the-box",
      "detailed",
      "following",
      "events",
      "apply",
      "processes.",
      "condition",
      "service",
      "management",
      "actions",
      "isn",
      "ended",
      "close",
      "code",
      "hidden",
      "perpetual",
      "true",
      "start",
      "date",
      "acquisition",
      "type",
      "rental",
      "lease",
      "loan",
      "total",
      "amount",
      "currency",
      "tax",
      "rate",
      "payment",
      "terms",
      "external",
      "po",
      "number",
      "invoice",
      "fixed",
      "assets",
      "section",
      "finance",
      "tab",
      "none",
      "show",
      "contract",
      "list",
      "filter",
      "asset",
      "model",
      "empty",
      "brand",
      "publisher",
      "owner",
      "filters",
      "category",
      "false",
      "active",
      "default",
      "maintenance",
      "covers",
      "cost",
      "centers",
      "center",
      "drop-down",
      "types",
      "display",
      "label",
      "tag",
      "unit",
      "rights",
      "quantity",
      "entitlements"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Impact of data domain segmentation",
    "content": "Data domain assignment impacts the following functionalities. \"Disappearing\" records When you create or edit records, you might make a change that results in an update to the data domain assignment and removes your authorization to view the records (for example, changing the owner of a record assigns the owner's data domains to the record, or changing the location assigns a different group due to a business rule running in the background). This can occur when you are updating or creating records. Updating records If you are updating a single record and you change a field that affects the data domain assignment, the Health Indicator (the light bulb icon ) displays a number (or increments an existing number), indicating that there is a new message–in this case, an information message. This occurs in both the record page itself and in the Preview pane. The relevant message for this action indicates that saving the record will remove your authorization to view that changed record. Note Mes",
    "url": "impact",
    "filename": "impact",
    "headings": [
      "\"Disappearing\" records",
      "Updating records",
      "Creating records",
      "Search",
      "History",
      "Comments",
      "Entity Link fields and MANY2MANY fields",
      "Knowledge and news articles",
      "Tasks and task plans",
      "Approvals",
      "Reports",
      "Service Portal",
      "Hot Topic Analytics",
      "Related topics"
    ],
    "keywords": [
      "impact",
      "data",
      "domain",
      "segmentation",
      "disappearing",
      "records",
      "updating",
      "creating",
      "search",
      "history",
      "comments",
      "entity",
      "link",
      "fields",
      "many2many",
      "knowledge",
      "news",
      "articles",
      "tasks",
      "task",
      "plans",
      "approvals",
      "reports",
      "service",
      "portal",
      "hot",
      "topic",
      "analytics",
      "related",
      "topics",
      "assignment",
      "impacts",
      "following",
      "functionalities.",
      "create",
      "edit",
      "make",
      "change",
      "results",
      "update",
      "removes",
      "authorization",
      "view",
      "example",
      "changing",
      "owner",
      "record",
      "assigns",
      "domains",
      "location",
      "different",
      "group",
      "due",
      "business",
      "rule",
      "running",
      "background",
      "occur",
      "records.",
      "single",
      "field",
      "affects",
      "health",
      "indicator",
      "light",
      "bulb",
      "icon",
      "displays",
      "number",
      "increments",
      "existing",
      "indicating",
      "there",
      "new",
      "message",
      "case",
      "information",
      "message.",
      "occurs",
      "both",
      "page",
      "itself",
      "preview",
      "pane.",
      "relevant",
      "action",
      "indicates",
      "saving",
      "remove",
      "changed",
      "record.",
      "note",
      "messages",
      "appear",
      "order",
      "severity--first",
      "error",
      "warning",
      "messages.",
      "multiple"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "impact of data domain segmentation",
    "contentLower": "data domain assignment impacts the following functionalities. \"disappearing\" records when you create or edit records, you might make a change that results in an update to the data domain assignment and removes your authorization to view the records (for example, changing the owner of a record assigns the owner's data domains to the record, or changing the location assigns a different group due to a business rule running in the background). this can occur when you are updating or creating records. updating records if you are updating a single record and you change a field that affects the data domain assignment, the health indicator (the light bulb icon ) displays a number (or increments an existing number), indicating that there is a new message–in this case, an information message. this occurs in both the record page itself and in the preview pane. the relevant message for this action indicates that saving the record will remove your authorization to view that changed record. note mes",
    "keywordsLower": [
      "impact",
      "data",
      "domain",
      "segmentation",
      "disappearing",
      "records",
      "updating",
      "creating",
      "search",
      "history",
      "comments",
      "entity",
      "link",
      "fields",
      "many2many",
      "knowledge",
      "news",
      "articles",
      "tasks",
      "task",
      "plans",
      "approvals",
      "reports",
      "service",
      "portal",
      "hot",
      "topic",
      "analytics",
      "related",
      "topics",
      "assignment",
      "impacts",
      "following",
      "functionalities.",
      "create",
      "edit",
      "make",
      "change",
      "results",
      "update",
      "removes",
      "authorization",
      "view",
      "example",
      "changing",
      "owner",
      "record",
      "assigns",
      "domains",
      "location",
      "different",
      "group",
      "due",
      "business",
      "rule",
      "running",
      "background",
      "occur",
      "records.",
      "single",
      "field",
      "affects",
      "health",
      "indicator",
      "light",
      "bulb",
      "icon",
      "displays",
      "number",
      "increments",
      "existing",
      "indicating",
      "there",
      "new",
      "message",
      "case",
      "information",
      "message.",
      "occurs",
      "both",
      "page",
      "itself",
      "preview",
      "pane.",
      "relevant",
      "action",
      "indicates",
      "saving",
      "remove",
      "changed",
      "record.",
      "note",
      "messages",
      "appear",
      "order",
      "severity--first",
      "error",
      "warning",
      "messages.",
      "multiple"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Image Aggregation Providers",
    "content": "To see the consolidated/aggregated images, you need to configure the image aggregation. This will generate events that will communicate with cloud providers to offer all the images. The Image Aggregation provider screen offers a “Quick Create Fulfillment Provider” capability to automatically create a Deployment Resource Provider for use in fulfilling aggregated images. (see ‘Quick Create Fulfillment Provider’ section below). Optional tasks The below tasks are optional if you want to provide proxy details at the time of configuring a provider. If you do not want to provide the same proxy details every time during provider configuration, you can perform the following tasks. Configure proxy settings in the Suite Administration To validate the user credentials and collect the images from AWS, Azure, vCenter, and Google Cloud Platform (GCP) servers, configure the proxy settings as follows: Log in to Suite Administration at https://<EXTERNAL_ACCESS_HOST>/bo as a suite admin. On the Configura",
    "url": "iaproviders",
    "filename": "iaproviders",
    "headings": [
      "Optional tasks",
      "Configure proxy settings in the Suite Administration",
      "Restart IA pods",
      "Configure an image aggregation provider",
      "Edit an image aggregation provider",
      "Delete an image aggregation provider"
    ],
    "keywords": [
      "amazon.com",
      "https://aws.amazon.com",
      "https://<EXTERNAL_ACCESS_HOST>/bo",
      "image",
      "aggregation",
      "providers",
      "optional",
      "tasks",
      "configure",
      "proxy",
      "settings",
      "suite",
      "administration",
      "restart",
      "ia",
      "pods",
      "provider",
      "edit",
      "delete",
      "see",
      "consolidated",
      "aggregated",
      "images",
      "need",
      "aggregation.",
      "generate",
      "events",
      "communicate",
      "cloud",
      "offer",
      "all",
      "images.",
      "screen",
      "offers",
      "quick",
      "create",
      "fulfillment",
      "capability",
      "automatically",
      "deployment",
      "resource",
      "fulfilling",
      "section",
      "below",
      "want",
      "provide",
      "details",
      "time",
      "configuring",
      "provider.",
      "same",
      "every",
      "during",
      "configuration",
      "perform",
      "following",
      "tasks.",
      "validate",
      "user",
      "credentials",
      "collect",
      "aws",
      "azure",
      "vcenter",
      "google",
      "platform",
      "gcp",
      "servers",
      "follows",
      "log",
      "https",
      "bo",
      "admin.",
      "configurations",
      "page",
      "click",
      "settings.",
      "specify",
      "save",
      "changes.",
      "host",
      "port",
      "username",
      "password",
      "one",
      "control",
      "plane",
      "nodes",
      "get",
      "namespace",
      "running",
      "command",
      "kubectl",
      "ns",
      "pods.",
      "refer",
      "example",
      "itom-cmp-integration-gateway",
      "itom-cmp-accounts",
      "itom-dnd-image-catalog"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "image aggregation providers",
    "contentLower": "to see the consolidated/aggregated images, you need to configure the image aggregation. this will generate events that will communicate with cloud providers to offer all the images. the image aggregation provider screen offers a “quick create fulfillment provider” capability to automatically create a deployment resource provider for use in fulfilling aggregated images. (see ‘quick create fulfillment provider’ section below). optional tasks the below tasks are optional if you want to provide proxy details at the time of configuring a provider. if you do not want to provide the same proxy details every time during provider configuration, you can perform the following tasks. configure proxy settings in the suite administration to validate the user credentials and collect the images from aws, azure, vcenter, and google cloud platform (gcp) servers, configure the proxy settings as follows: log in to suite administration at https://<external_access_host>/bo as a suite admin. on the configura",
    "keywordsLower": [
      "amazon.com",
      "https://aws.amazon.com",
      "https://<external_access_host>/bo",
      "image",
      "aggregation",
      "providers",
      "optional",
      "tasks",
      "configure",
      "proxy",
      "settings",
      "suite",
      "administration",
      "restart",
      "ia",
      "pods",
      "provider",
      "edit",
      "delete",
      "see",
      "consolidated",
      "aggregated",
      "images",
      "need",
      "aggregation.",
      "generate",
      "events",
      "communicate",
      "cloud",
      "offer",
      "all",
      "images.",
      "screen",
      "offers",
      "quick",
      "create",
      "fulfillment",
      "capability",
      "automatically",
      "deployment",
      "resource",
      "fulfilling",
      "section",
      "below",
      "want",
      "provide",
      "details",
      "time",
      "configuring",
      "provider.",
      "same",
      "every",
      "during",
      "configuration",
      "perform",
      "following",
      "tasks.",
      "validate",
      "user",
      "credentials",
      "collect",
      "aws",
      "azure",
      "vcenter",
      "google",
      "platform",
      "gcp",
      "servers",
      "follows",
      "log",
      "https",
      "bo",
      "admin.",
      "configurations",
      "page",
      "click",
      "settings.",
      "specify",
      "save",
      "changes.",
      "host",
      "port",
      "username",
      "password",
      "one",
      "control",
      "plane",
      "nodes",
      "get",
      "namespace",
      "running",
      "command",
      "kubectl",
      "ns",
      "pods.",
      "refer",
      "example",
      "itom-cmp-integration-gateway",
      "itom-cmp-accounts",
      "itom-dnd-image-catalog"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Live Support",
    "content": "Service Management supports integration with your CTI system and includes the option of location-based Live Support. Related topics Live Support and chat Location-based Live Support",
    "url": "livelanding",
    "filename": "livelanding",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "live",
      "support",
      "related",
      "topics",
      "service",
      "management",
      "supports",
      "integration",
      "cti",
      "system",
      "includes",
      "option",
      "location-based",
      "support.",
      "chat"
    ],
    "language": "en",
    "word_count": 21,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "live support",
    "contentLower": "service management supports integration with your cti system and includes the option of location-based live support. related topics live support and chat location-based live support",
    "keywordsLower": [
      "live",
      "support",
      "related",
      "topics",
      "service",
      "management",
      "supports",
      "integration",
      "cti",
      "system",
      "includes",
      "option",
      "location-based",
      "support.",
      "chat"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Live Support and chat",
    "content": "Service Management supports integration with your Computer Telephony Integration (CTI) system, enabling automatic display of relevant information. Service Management also allows you to enable chat capability for use with service requests, support requests, and case requests. Related topics Chat capability for the Service Portal How to configure Live Support with CTI How to enable chat capability for the Service Portal How to configure support agent anonymity",
    "url": "livepluschat",
    "filename": "livepluschat",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "live",
      "support",
      "chat",
      "related",
      "topics",
      "service",
      "management",
      "supports",
      "integration",
      "computer",
      "telephony",
      "cti",
      "system",
      "enabling",
      "automatic",
      "display",
      "relevant",
      "information.",
      "allows",
      "enable",
      "capability",
      "requests",
      "case",
      "requests.",
      "portal",
      "configure",
      "agent",
      "anonymity"
    ],
    "language": "en",
    "word_count": 48,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "live support and chat",
    "contentLower": "service management supports integration with your computer telephony integration (cti) system, enabling automatic display of relevant information. service management also allows you to enable chat capability for use with service requests, support requests, and case requests. related topics chat capability for the service portal how to configure live support with cti how to enable chat capability for the service portal how to configure support agent anonymity",
    "keywordsLower": [
      "live",
      "support",
      "chat",
      "related",
      "topics",
      "service",
      "management",
      "supports",
      "integration",
      "computer",
      "telephony",
      "cti",
      "system",
      "enabling",
      "automatic",
      "display",
      "relevant",
      "information.",
      "allows",
      "enable",
      "capability",
      "requests",
      "case",
      "requests.",
      "portal",
      "configure",
      "agent",
      "anonymity"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Location-based Live Support",
    "content": "The location-based Live Support feature adds functionality to the Live Support page. By default, location-based Live Support is enabled for a new tenant, and disabled for an upgraded tenant. To enable location-based Live Support, use the following PUT command with the REST API: PUT../rest/100000001/feature-flags/cti/enableCtiLocationMode/true For more information about the REST API, see Develop. Related topics How to deal with a Live Support call with location-based support enabled",
    "url": "livepluslocation",
    "filename": "livepluslocation",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "location-based",
      "live",
      "support",
      "related",
      "topics",
      "feature",
      "adds",
      "functionality",
      "page.",
      "default",
      "enabled",
      "new",
      "tenant",
      "disabled",
      "upgraded",
      "tenant.",
      "enable",
      "following",
      "put",
      "command",
      "rest",
      "api",
      "put..",
      "100000001",
      "feature-flags",
      "cti",
      "enablectilocationmode",
      "true",
      "information",
      "about",
      "see",
      "develop.",
      "deal",
      "call"
    ],
    "language": "en",
    "word_count": 53,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "location-based live support",
    "contentLower": "the location-based live support feature adds functionality to the live support page. by default, location-based live support is enabled for a new tenant, and disabled for an upgraded tenant. to enable location-based live support, use the following put command with the rest api: put../rest/100000001/feature-flags/cti/enablectilocationmode/true for more information about the rest api, see develop. related topics how to deal with a live support call with location-based support enabled",
    "keywordsLower": [
      "location-based",
      "live",
      "support",
      "related",
      "topics",
      "feature",
      "adds",
      "functionality",
      "page.",
      "default",
      "enabled",
      "new",
      "tenant",
      "disabled",
      "upgraded",
      "tenant.",
      "enable",
      "following",
      "put",
      "command",
      "rest",
      "api",
      "put..",
      "100000001",
      "feature-flags",
      "cti",
      "enablectilocationmode",
      "true",
      "information",
      "about",
      "see",
      "develop.",
      "deal",
      "call"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Kubernetes Pod Security Admission",
    "content": "The Kubernetes Pod Security Standards define different isolation levels for Pods. These standards let you define how you want to restrict the behavior of pods in a clear, consistent fashion. Kubernetes offers a built-in Pod Security admission controller to enforce the Pod Security Standards. Pod security restrictions are applied at the namespace level when pods are created. Pod Security levels Pod Security admission places requirements on a Pod's Security Context and other related fields according to the three levels defined by the Pod Security Standards: privileged, baseline, and restricted. Refer to the Pod Security Standards page for an in-depth look at those requirements. Level Description Privileged Unrestricted level, providing the widest possible level of permissions. This level allows for known privilege escalations. Baseline Minimally restrictive level which prevents known privilege escalations. Allows the default (minimally specified) Pod configuration. Restricted Heavily res",
    "url": "kubernetespsa",
    "filename": "kubernetespsa",
    "headings": [
      "Pod Security levels",
      "How to set PSA Level for RAS",
      "Pod Security Admission labels for namespaces",
      "Workload resources and Pod templates",
      "Exemptions",
      "Metrics"
    ],
    "keywords": [
      "kubernetes.io",
      "security.beta",
      "v1.28",
      "kubernetes",
      "pod",
      "security",
      "admission",
      "levels",
      "set",
      "psa",
      "level",
      "ras",
      "labels",
      "namespaces",
      "workload",
      "resources",
      "templates",
      "exemptions",
      "metrics",
      "standards",
      "define",
      "different",
      "isolation",
      "pods.",
      "let",
      "want",
      "restrict",
      "behavior",
      "pods",
      "clear",
      "consistent",
      "fashion.",
      "offers",
      "built-in",
      "controller",
      "enforce",
      "standards.",
      "restrictions",
      "applied",
      "namespace",
      "created.",
      "places",
      "requirements",
      "context",
      "related",
      "fields",
      "according",
      "three",
      "defined",
      "privileged",
      "baseline",
      "restricted.",
      "refer",
      "page",
      "in-depth",
      "look",
      "requirements.",
      "description",
      "unrestricted",
      "providing",
      "widest",
      "possible",
      "permissions.",
      "allows",
      "known",
      "privilege",
      "escalations.",
      "minimally",
      "restrictive",
      "prevents",
      "default",
      "specified",
      "configuration.",
      "restricted",
      "heavily",
      "following",
      "current",
      "hardening",
      "best",
      "practices.",
      "oo",
      "supports",
      "profile.",
      "component",
      "responsible",
      "executing",
      "flows.",
      "limits",
      "functionality",
      "restricts",
      "bash",
      "operations",
      "like",
      "ping",
      "etc.",
      "user",
      "wants",
      "allow",
      "execute",
      "operation"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "kubernetes pod security admission",
    "contentLower": "the kubernetes pod security standards define different isolation levels for pods. these standards let you define how you want to restrict the behavior of pods in a clear, consistent fashion. kubernetes offers a built-in pod security admission controller to enforce the pod security standards. pod security restrictions are applied at the namespace level when pods are created. pod security levels pod security admission places requirements on a pod's security context and other related fields according to the three levels defined by the pod security standards: privileged, baseline, and restricted. refer to the pod security standards page for an in-depth look at those requirements. level description privileged unrestricted level, providing the widest possible level of permissions. this level allows for known privilege escalations. baseline minimally restrictive level which prevents known privilege escalations. allows the default (minimally specified) pod configuration. restricted heavily res",
    "keywordsLower": [
      "kubernetes.io",
      "security.beta",
      "v1.28",
      "kubernetes",
      "pod",
      "security",
      "admission",
      "levels",
      "set",
      "psa",
      "level",
      "ras",
      "labels",
      "namespaces",
      "workload",
      "resources",
      "templates",
      "exemptions",
      "metrics",
      "standards",
      "define",
      "different",
      "isolation",
      "pods.",
      "let",
      "want",
      "restrict",
      "behavior",
      "pods",
      "clear",
      "consistent",
      "fashion.",
      "offers",
      "built-in",
      "controller",
      "enforce",
      "standards.",
      "restrictions",
      "applied",
      "namespace",
      "created.",
      "places",
      "requirements",
      "context",
      "related",
      "fields",
      "according",
      "three",
      "defined",
      "privileged",
      "baseline",
      "restricted.",
      "refer",
      "page",
      "in-depth",
      "look",
      "requirements.",
      "description",
      "unrestricted",
      "providing",
      "widest",
      "possible",
      "permissions.",
      "allows",
      "known",
      "privilege",
      "escalations.",
      "minimally",
      "restrictive",
      "prevents",
      "default",
      "specified",
      "configuration.",
      "restricted",
      "heavily",
      "following",
      "current",
      "hardening",
      "best",
      "practices.",
      "oo",
      "supports",
      "profile.",
      "component",
      "responsible",
      "executing",
      "flows.",
      "limits",
      "functionality",
      "restricts",
      "bash",
      "operations",
      "like",
      "ping",
      "etc.",
      "user",
      "wants",
      "allow",
      "execute",
      "operation"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Initial administration tasks",
    "content": "This topic includes administration tasks that you will perform once while setting up Operations Orchestration (OO) Workflow Designer after installation. The following subtopics are covered in this topic: Administer IdM in OO Workflow Designer Manage user authentication in OO Workflow Designer",
    "url": "initialdesignertasks",
    "filename": "initialdesignertasks",
    "headings": [],
    "keywords": [
      "initial",
      "administration",
      "tasks",
      "topic",
      "includes",
      "perform",
      "once",
      "while",
      "setting",
      "operations",
      "orchestration",
      "oo",
      "workflow",
      "designer",
      "after",
      "installation.",
      "following",
      "subtopics",
      "covered",
      "administer",
      "idm",
      "manage",
      "user",
      "authentication"
    ],
    "language": "en",
    "word_count": 33,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "initial administration tasks",
    "contentLower": "this topic includes administration tasks that you will perform once while setting up operations orchestration (oo) workflow designer after installation. the following subtopics are covered in this topic: administer idm in oo workflow designer manage user authentication in oo workflow designer",
    "keywordsLower": [
      "initial",
      "administration",
      "tasks",
      "topic",
      "includes",
      "perform",
      "once",
      "while",
      "setting",
      "operations",
      "orchestration",
      "oo",
      "workflow",
      "designer",
      "after",
      "installation.",
      "following",
      "subtopics",
      "covered",
      "administer",
      "idm",
      "manage",
      "user",
      "authentication"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Installation wizard Other database option in OO Workflow Designer",
    "content": "This topic contains information about the Other database option in the OO Workflow Designer installation wizard. This option enables using specific JDBC driver and connection options. Use this option if: You want to use a different version of the JDBC driver, other than the ones provided in the OO Workflow Designer installation (see the note below). You want to provide a JDBC connection URL yourself, to include an option or options which aren't currently provided by the standard database connection options. Connection is restricted to the versions of the database described in the System requirements topic. The following JDBC drivers are supported: Usage of a JDBC driver not provided by the company may be a risky option. mssql-jdbc-7.4.1.jre8.jar–for SQL Server 2016 and above ojdbc8-19.3.0.0.jar postgresql-42.2.14.jar When you install OO Workflow Designer using the Other database option, the installation wizard doesn't create the database and/or related user or roles. You must create th",
    "url": "otherdbdesigner",
    "filename": "otherdbdesigner",
    "headings": [
      "Microsoft SQL server named instance example",
      "Oracle examples–with SID, Service Name and Oracle RAC",
      "PostgreSQL example"
    ],
    "keywords": [
      "0.jar",
      "RAC1.MY",
      "sqlserver://<DB_IP_OR_HOSTNAME>:<INSTANCE_",
      "1.jre8",
      "sqlserver://<DB_IP_OR_HOSTNAME>;instanceName=<INSTANCE_NAME>;databaseName=<DB_NAME>;sendStringParametersAsUnicode=true",
      "42.2.14",
      "oracle.jdbc",
      "19.3.0.0",
      "sqlserver.jdbc",
      "7.4.1",
      "postgresql://<HOST>:<PORT>/<DB_NAME>[?[propertyName=propertyValue",
      "2.14",
      "19.3.0",
      "installation",
      "wizard",
      "database",
      "option",
      "oo",
      "workflow",
      "designer",
      "microsoft",
      "sql",
      "server",
      "named",
      "instance",
      "example",
      "oracle",
      "examples",
      "sid",
      "service",
      "name",
      "rac",
      "postgresql",
      "topic",
      "contains",
      "information",
      "about",
      "wizard.",
      "enables",
      "specific",
      "jdbc",
      "driver",
      "connection",
      "options.",
      "want",
      "different",
      "version",
      "ones",
      "provided",
      "see",
      "note",
      "below",
      "provide",
      "url",
      "yourself",
      "include",
      "options",
      "aren",
      "currently",
      "standard",
      "restricted",
      "versions",
      "described",
      "system",
      "requirements",
      "topic.",
      "following",
      "drivers",
      "supported",
      "usage",
      "company",
      "risky",
      "option.",
      "mssql-jdbc-7.4.1.jre8.jar",
      "2016",
      "above",
      "ojdbc8-19.3.0.0.jar",
      "postgresql-42.2.14.jar",
      "install",
      "doesn",
      "create",
      "related",
      "user",
      "roles.",
      "beforehand.",
      "detailed",
      "instructions",
      "role",
      "found",
      "under",
      "manually",
      "creating",
      "section",
      "complete",
      "database.",
      "idm",
      "scenario",
      "enter",
      "credentials.",
      "shows"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "installation wizard other database option in oo workflow designer",
    "contentLower": "this topic contains information about the other database option in the oo workflow designer installation wizard. this option enables using specific jdbc driver and connection options. use this option if: you want to use a different version of the jdbc driver, other than the ones provided in the oo workflow designer installation (see the note below). you want to provide a jdbc connection url yourself, to include an option or options which aren't currently provided by the standard database connection options. connection is restricted to the versions of the database described in the system requirements topic. the following jdbc drivers are supported: usage of a jdbc driver not provided by the company may be a risky option. mssql-jdbc-7.4.1.jre8.jar–for sql server 2016 and above ojdbc8-19.3.0.0.jar postgresql-42.2.14.jar when you install oo workflow designer using the other database option, the installation wizard doesn't create the database and/or related user or roles. you must create th",
    "keywordsLower": [
      "0.jar",
      "rac1.my",
      "sqlserver://<db_ip_or_hostname>:<instance_",
      "1.jre8",
      "sqlserver://<db_ip_or_hostname>;instancename=<instance_name>;databasename=<db_name>;sendstringparametersasunicode=true",
      "42.2.14",
      "oracle.jdbc",
      "19.3.0.0",
      "sqlserver.jdbc",
      "7.4.1",
      "postgresql://<host>:<port>/<db_name>[?[propertyname=propertyvalue",
      "2.14",
      "19.3.0",
      "installation",
      "wizard",
      "database",
      "option",
      "oo",
      "workflow",
      "designer",
      "microsoft",
      "sql",
      "server",
      "named",
      "instance",
      "example",
      "oracle",
      "examples",
      "sid",
      "service",
      "name",
      "rac",
      "postgresql",
      "topic",
      "contains",
      "information",
      "about",
      "wizard.",
      "enables",
      "specific",
      "jdbc",
      "driver",
      "connection",
      "options.",
      "want",
      "different",
      "version",
      "ones",
      "provided",
      "see",
      "note",
      "below",
      "provide",
      "url",
      "yourself",
      "include",
      "options",
      "aren",
      "currently",
      "standard",
      "restricted",
      "versions",
      "described",
      "system",
      "requirements",
      "topic.",
      "following",
      "drivers",
      "supported",
      "usage",
      "company",
      "risky",
      "option.",
      "mssql-jdbc-7.4.1.jre8.jar",
      "2016",
      "above",
      "ojdbc8-19.3.0.0.jar",
      "postgresql-42.2.14.jar",
      "install",
      "doesn",
      "create",
      "related",
      "user",
      "roles.",
      "beforehand.",
      "detailed",
      "instructions",
      "role",
      "found",
      "under",
      "manually",
      "creating",
      "section",
      "complete",
      "database.",
      "idm",
      "scenario",
      "enter",
      "credentials.",
      "shows"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Log files",
    "content": "Logs let you trace errors, warnings, information, and debugging messages. The logs are saved in the file server, in the following location: <installation>/designer/var/logs It must be noted that no sensitive data is kept in the log files in OO Workflow Designer.",
    "url": "logfilesdesigner",
    "filename": "logfilesdesigner",
    "headings": [],
    "keywords": [
      "log",
      "files",
      "logs",
      "let",
      "trace",
      "errors",
      "warnings",
      "information",
      "debugging",
      "messages.",
      "saved",
      "file",
      "server",
      "following",
      "location",
      "designer",
      "var",
      "noted",
      "sensitive",
      "data",
      "kept",
      "oo",
      "workflow",
      "designer."
    ],
    "language": "en",
    "word_count": 28,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "log files",
    "contentLower": "logs let you trace errors, warnings, information, and debugging messages. the logs are saved in the file server, in the following location: <installation>/designer/var/logs it must be noted that no sensitive data is kept in the log files in oo workflow designer.",
    "keywordsLower": [
      "log",
      "files",
      "logs",
      "let",
      "trace",
      "errors",
      "warnings",
      "information",
      "debugging",
      "messages.",
      "saved",
      "file",
      "server",
      "following",
      "location",
      "designer",
      "var",
      "noted",
      "sensitive",
      "data",
      "kept",
      "oo",
      "workflow",
      "designer."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Log in to the suite",
    "content": "The Service Management Automation suite has the following user interfaces for different user roles: suite administrators, IT agents, and self-service users. These interfaces use different URLs, as described in the following table. Note If you use SAML authentication and you want to do Single Sign On (SSO) between Service Management and other applications in your organization, you can append the \"AUTH=SAML\" parameter to the Service Management URL: https://<EXTERNAL_ACCESS_HOST>/saw/ess?TENANTID=xxxxxxxxx&AUTH=SAML. Then, you are redirected to your corporate IdP and you don't need to enter your user name and password for Service Management if you are already logged in. User interface Login URL Log in as Suite Administration https://<EXTERNAL_ACCESS_HOST>/bo Suite administration users The out-of-tbox suite administration user is suite-admin, who can create more suite administration users in the Suite Administration interface. Suite Administration https://<EXTERNAL_ACCESS_HOST>/bo?ACCOUNTI",
    "url": "logtosuite",
    "filename": "logtosuite",
    "headings": [],
    "keywords": [
      "https://<External_Access_Host>/idm-admin",
      "https://<EXTERNAL_ACCESS_HOST>/dashboard?TENANTID=xxxxxxxxxSHARED=TRUE",
      "https://<EXTERNAL_ACCESS_HOST>/bo",
      "https://<EXTERNAL_ACCESS_HOST>/saw/ess?TENANTID=xxxxxxxxx&AUTH=SAML",
      "https://<EXTERNAL_ACCESS_HOST>/saw/ess?TENANTID=xxxxxxxxx",
      "https://<EXTERNAL_ACCESS_HOST>/bo?ACCOUNTID=xxxxxxxxx",
      "https://<EXTERNAL_ACCESS_HOST>/dashboard?TENANTID=xxxxxxxxx",
      "log",
      "suite",
      "service",
      "management",
      "automation",
      "following",
      "user",
      "interfaces",
      "different",
      "roles",
      "administrators",
      "agents",
      "self-service",
      "users.",
      "urls",
      "described",
      "table.",
      "note",
      "saml",
      "authentication",
      "want",
      "single",
      "sign",
      "sso",
      "between",
      "applications",
      "organization",
      "append",
      "auth",
      "parameter",
      "url",
      "https",
      "saw",
      "ess",
      "tenantid",
      "xxxxxxxxx",
      "saml.",
      "redirected",
      "corporate",
      "idp",
      "don",
      "need",
      "enter",
      "name",
      "password",
      "already",
      "logged",
      "in.",
      "interface",
      "login",
      "administration",
      "bo",
      "users",
      "out-of-tbox",
      "suite-admin",
      "create",
      "interface.",
      "accountid",
      "shared",
      "admin",
      "agent",
      "idm",
      "portal",
      "idm-admin",
      "out-of-box",
      "suite-admin.",
      "notes",
      "perform",
      "tasks",
      "indicated",
      "documents",
      "such",
      "configuring",
      "authorization",
      "dnd",
      "adding",
      "configurations",
      "policies.",
      "operations",
      "creation",
      "lightweight",
      "settings",
      "signing",
      "key",
      "configure",
      "instead.",
      "supports",
      "release.",
      "however",
      "consistent",
      "logout",
      "session",
      "timeout"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "log in to the suite",
    "contentLower": "the service management automation suite has the following user interfaces for different user roles: suite administrators, it agents, and self-service users. these interfaces use different urls, as described in the following table. note if you use saml authentication and you want to do single sign on (sso) between service management and other applications in your organization, you can append the \"auth=saml\" parameter to the service management url: https://<external_access_host>/saw/ess?tenantid=xxxxxxxxx&auth=saml. then, you are redirected to your corporate idp and you don't need to enter your user name and password for service management if you are already logged in. user interface login url log in as suite administration https://<external_access_host>/bo suite administration users the out-of-tbox suite administration user is suite-admin, who can create more suite administration users in the suite administration interface. suite administration https://<external_access_host>/bo?accounti",
    "keywordsLower": [
      "https://<external_access_host>/idm-admin",
      "https://<external_access_host>/dashboard?tenantid=xxxxxxxxxshared=true",
      "https://<external_access_host>/bo",
      "https://<external_access_host>/saw/ess?tenantid=xxxxxxxxx&auth=saml",
      "https://<external_access_host>/saw/ess?tenantid=xxxxxxxxx",
      "https://<external_access_host>/bo?accountid=xxxxxxxxx",
      "https://<external_access_host>/dashboard?tenantid=xxxxxxxxx",
      "log",
      "suite",
      "service",
      "management",
      "automation",
      "following",
      "user",
      "interfaces",
      "different",
      "roles",
      "administrators",
      "agents",
      "self-service",
      "users.",
      "urls",
      "described",
      "table.",
      "note",
      "saml",
      "authentication",
      "want",
      "single",
      "sign",
      "sso",
      "between",
      "applications",
      "organization",
      "append",
      "auth",
      "parameter",
      "url",
      "https",
      "saw",
      "ess",
      "tenantid",
      "xxxxxxxxx",
      "saml.",
      "redirected",
      "corporate",
      "idp",
      "don",
      "need",
      "enter",
      "name",
      "password",
      "already",
      "logged",
      "in.",
      "interface",
      "login",
      "administration",
      "bo",
      "users",
      "out-of-tbox",
      "suite-admin",
      "create",
      "interface.",
      "accountid",
      "shared",
      "admin",
      "agent",
      "idm",
      "portal",
      "idm-admin",
      "out-of-box",
      "suite-admin.",
      "notes",
      "perform",
      "tasks",
      "indicated",
      "documents",
      "such",
      "configuring",
      "authorization",
      "dnd",
      "adding",
      "configurations",
      "policies.",
      "operations",
      "creation",
      "lightweight",
      "settings",
      "signing",
      "key",
      "configure",
      "instead.",
      "supports",
      "release.",
      "however",
      "consistent",
      "logout",
      "session",
      "timeout"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Logging in to the agent interface",
    "content": "At the login page, enter your username and password to log in to the agent interface. The URL of the login page includes a TENANTID. Note Make sure that cookies are enabled on your machine before you log in. You must enter a username that belongs to the tenant designated in the URL. If you enter a username that doesn't belong to the specified tenant, an error is generated and you can't log in. When working in the agent interface, you can click the arrow in the upper right and select About to open a dialog box, which displays the following details: The suite version you are running The TENANTID The tenant type (Production, DEV, or Trial) You can also view the TENANTID and tenant type at the bottom of the main menu. Related topics Service Management overview Glossary Roles and permissions Filters Views Service Management user interface",
    "url": "loggingin",
    "filename": "loggingin",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "logging",
      "agent",
      "interface",
      "related",
      "topics",
      "login",
      "page",
      "enter",
      "username",
      "password",
      "log",
      "interface.",
      "url",
      "includes",
      "tenantid.",
      "note",
      "make",
      "sure",
      "cookies",
      "enabled",
      "machine",
      "before",
      "in.",
      "belongs",
      "tenant",
      "designated",
      "url.",
      "doesn",
      "belong",
      "specified",
      "error",
      "generated",
      "working",
      "click",
      "arrow",
      "upper",
      "right",
      "select",
      "about",
      "open",
      "dialog",
      "box",
      "displays",
      "following",
      "details",
      "suite",
      "version",
      "running",
      "tenantid",
      "type",
      "production",
      "dev",
      "trial",
      "view",
      "bottom",
      "main",
      "menu.",
      "service",
      "management",
      "overview",
      "glossary",
      "roles",
      "permissions",
      "filters",
      "views",
      "user"
    ],
    "language": "en",
    "word_count": 88,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "logging in to the agent interface",
    "contentLower": "at the login page, enter your username and password to log in to the agent interface. the url of the login page includes a tenantid. note make sure that cookies are enabled on your machine before you log in. you must enter a username that belongs to the tenant designated in the url. if you enter a username that doesn't belong to the specified tenant, an error is generated and you can't log in. when working in the agent interface, you can click the arrow in the upper right and select about to open a dialog box, which displays the following details: the suite version you are running the tenantid the tenant type (production, dev, or trial) you can also view the tenantid and tenant type at the bottom of the main menu. related topics service management overview glossary roles and permissions filters views service management user interface",
    "keywordsLower": [
      "logging",
      "agent",
      "interface",
      "related",
      "topics",
      "login",
      "page",
      "enter",
      "username",
      "password",
      "log",
      "interface.",
      "url",
      "includes",
      "tenantid.",
      "note",
      "make",
      "sure",
      "cookies",
      "enabled",
      "machine",
      "before",
      "in.",
      "belongs",
      "tenant",
      "designated",
      "url.",
      "doesn",
      "belong",
      "specified",
      "error",
      "generated",
      "working",
      "click",
      "arrow",
      "upper",
      "right",
      "select",
      "about",
      "open",
      "dialog",
      "box",
      "displays",
      "following",
      "details",
      "suite",
      "version",
      "running",
      "tenantid",
      "type",
      "production",
      "dev",
      "trial",
      "view",
      "bottom",
      "main",
      "menu.",
      "service",
      "management",
      "overview",
      "glossary",
      "roles",
      "permissions",
      "filters",
      "views",
      "user"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "List view columns",
    "content": "The Columns feature on the list view toolbar enables you to customize your view of the available fields in each record. To customize: Click Columns. Service Management displays a list of fields that are displayed in the view. To add a column to the view: Select a field from the combo box at the top of the list. Click Add. If the field you select to add is a record type itself, you can click the +Related link to select a property of that field. For example, if you selected Owner as the column to add, you can select Email as the secondary field. Then the column added to the grid is Owner.Email. You can click the +Related link next to fields already selected in the list of columns to change the column to display a property of the field. You can't add more than 50 entity link fields. To remove a column from the view, select the column name in the list, and click X. To format the columns, click Format columns. Select the Force fit columns check box to display all selected columns on the scr",
    "url": "listviewcolumns",
    "filename": "listviewcolumns",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "list",
      "view",
      "columns",
      "related",
      "topics",
      "feature",
      "toolbar",
      "enables",
      "customize",
      "available",
      "fields",
      "record.",
      "click",
      "columns.",
      "service",
      "management",
      "displays",
      "displayed",
      "view.",
      "add",
      "column",
      "select",
      "field",
      "combo",
      "box",
      "top",
      "list.",
      "add.",
      "record",
      "type",
      "itself",
      "link",
      "property",
      "field.",
      "example",
      "selected",
      "owner",
      "email",
      "secondary",
      "added",
      "grid",
      "owner.email.",
      "next",
      "already",
      "change",
      "display",
      "50",
      "entity",
      "fields.",
      "remove",
      "name",
      "x.",
      "format",
      "force",
      "fit",
      "check",
      "all",
      "screen.",
      "clear",
      "restore",
      "default",
      "size.",
      "sort",
      "header",
      "perform",
      "ascending",
      "column.",
      "again",
      "descending",
      "based",
      "press",
      "shift",
      "button",
      "second",
      "header.",
      "additional",
      "level",
      "search",
      "another",
      "order",
      "follows",
      "select.",
      "ctrl",
      "one",
      "sort.",
      "filters",
      "views",
      "discussions",
      "task",
      "plans",
      "edit",
      "records",
      "preview",
      "pane",
      "history",
      "date",
      "time"
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "list view columns",
    "contentLower": "the columns feature on the list view toolbar enables you to customize your view of the available fields in each record. to customize: click columns. service management displays a list of fields that are displayed in the view. to add a column to the view: select a field from the combo box at the top of the list. click add. if the field you select to add is a record type itself, you can click the +related link to select a property of that field. for example, if you selected owner as the column to add, you can select email as the secondary field. then the column added to the grid is owner.email. you can click the +related link next to fields already selected in the list of columns to change the column to display a property of the field. you can't add more than 50 entity link fields. to remove a column from the view, select the column name in the list, and click x. to format the columns, click format columns. select the force fit columns check box to display all selected columns on the scr",
    "keywordsLower": [
      "list",
      "view",
      "columns",
      "related",
      "topics",
      "feature",
      "toolbar",
      "enables",
      "customize",
      "available",
      "fields",
      "record.",
      "click",
      "columns.",
      "service",
      "management",
      "displays",
      "displayed",
      "view.",
      "add",
      "column",
      "select",
      "field",
      "combo",
      "box",
      "top",
      "list.",
      "add.",
      "record",
      "type",
      "itself",
      "link",
      "property",
      "field.",
      "example",
      "selected",
      "owner",
      "email",
      "secondary",
      "added",
      "grid",
      "owner.email.",
      "next",
      "already",
      "change",
      "display",
      "50",
      "entity",
      "fields.",
      "remove",
      "name",
      "x.",
      "format",
      "force",
      "fit",
      "check",
      "all",
      "screen.",
      "clear",
      "restore",
      "default",
      "size.",
      "sort",
      "header",
      "perform",
      "ascending",
      "column.",
      "again",
      "descending",
      "based",
      "press",
      "shift",
      "button",
      "second",
      "header.",
      "additional",
      "level",
      "search",
      "another",
      "order",
      "follows",
      "select.",
      "ctrl",
      "one",
      "sort.",
      "filters",
      "views",
      "discussions",
      "task",
      "plans",
      "edit",
      "records",
      "preview",
      "pane",
      "history",
      "date",
      "time"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "ID numbers for records",
    "content": "Each record in Service Management has a unique ID number. You can click on a number to display the details for that record. These ID numbers may not appear to be continuous because of the high availability setup of the application. In particular, each server has its own list of ID numbers to allocate. So, if consecutive records are processed by the same server, they are in sequence. However, if they are processed by different servers, they aren't in sequence. Tip Sort your record list by clicking the header of the Creation Time column to see the latest record (most recently created) at the top of the list. If the Creation Time field isn't already displayed, click Columns, and select the field. Related topics Service Management user interface Task plans Roles and permissions",
    "url": "idnumbers",
    "filename": "idnumbers",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "id",
      "numbers",
      "records",
      "related",
      "topics",
      "record",
      "service",
      "management",
      "unique",
      "number.",
      "click",
      "number",
      "display",
      "details",
      "record.",
      "appear",
      "continuous",
      "because",
      "high",
      "availability",
      "setup",
      "application.",
      "particular",
      "server",
      "own",
      "list",
      "allocate.",
      "consecutive",
      "processed",
      "same",
      "sequence.",
      "however",
      "different",
      "servers",
      "aren",
      "tip",
      "sort",
      "clicking",
      "header",
      "creation",
      "time",
      "column",
      "see",
      "latest",
      "most",
      "recently",
      "created",
      "top",
      "list.",
      "field",
      "isn",
      "already",
      "displayed",
      "columns",
      "select",
      "field.",
      "user",
      "interface",
      "task",
      "plans",
      "roles",
      "permissions"
    ],
    "language": "en",
    "word_count": 81,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "id numbers for records",
    "contentLower": "each record in service management has a unique id number. you can click on a number to display the details for that record. these id numbers may not appear to be continuous because of the high availability setup of the application. in particular, each server has its own list of id numbers to allocate. so, if consecutive records are processed by the same server, they are in sequence. however, if they are processed by different servers, they aren't in sequence. tip sort your record list by clicking the header of the creation time column to see the latest record (most recently created) at the top of the list. if the creation time field isn't already displayed, click columns, and select the field. related topics service management user interface task plans roles and permissions",
    "keywordsLower": [
      "id",
      "numbers",
      "records",
      "related",
      "topics",
      "record",
      "service",
      "management",
      "unique",
      "number.",
      "click",
      "number",
      "display",
      "details",
      "record.",
      "appear",
      "continuous",
      "because",
      "high",
      "availability",
      "setup",
      "application.",
      "particular",
      "server",
      "own",
      "list",
      "allocate.",
      "consecutive",
      "processed",
      "same",
      "sequence.",
      "however",
      "different",
      "servers",
      "aren",
      "tip",
      "sort",
      "clicking",
      "header",
      "creation",
      "time",
      "column",
      "see",
      "latest",
      "most",
      "recently",
      "created",
      "top",
      "list.",
      "field",
      "isn",
      "already",
      "displayed",
      "columns",
      "select",
      "field.",
      "user",
      "interface",
      "task",
      "plans",
      "roles",
      "permissions"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "In-line translations",
    "content": "Service Management can provide in-line translations of certain blocks of text in requests and incidents. The sections of text that can be translated are: Descriptions Agents' comments Knowledge articles Q&A Searches on the Service Portal Example: A Chinese user of the Service Portal opens a request and inputs a description in Chinese. An agent in the United States is assigned to handle this request. When the agent opens the request record, all UI text is displayed in English (the default language for that agent), except for the description. The agent can use this feature to translate the description. Note  At least one section in a record must be expanded for the in-line translation to display properly. Enable in-line translation In-line translation is disabled by default. You can enable this function as needed: From the main menu, select Administration > Configuration > Application Settings. In the In-line translation tool field, select On. Select the external tool that you want to us",
    "url": "inlinetrans",
    "filename": "inlinetrans",
    "headings": [
      "Enable in-line translation",
      "Translate text"
    ],
    "keywords": [
      "in-line",
      "translations",
      "enable",
      "translation",
      "translate",
      "text",
      "service",
      "management",
      "provide",
      "certain",
      "blocks",
      "requests",
      "incidents.",
      "sections",
      "translated",
      "descriptions",
      "agents",
      "comments",
      "knowledge",
      "articles",
      "searches",
      "portal",
      "example",
      "chinese",
      "user",
      "opens",
      "request",
      "inputs",
      "description",
      "chinese.",
      "agent",
      "united",
      "states",
      "assigned",
      "handle",
      "request.",
      "record",
      "all",
      "ui",
      "displayed",
      "english",
      "default",
      "language",
      "except",
      "description.",
      "feature",
      "note",
      "least",
      "one",
      "section",
      "expanded",
      "display",
      "properly.",
      "disabled",
      "default.",
      "function",
      "needed",
      "main",
      "menu",
      "select",
      "administration",
      "configuration",
      "application",
      "settings.",
      "tool",
      "field",
      "on.",
      "external",
      "want",
      "perform",
      "translation.",
      "currently",
      "google",
      "microsoft",
      "translator",
      "supported.",
      "purchase",
      "license",
      "services",
      "functionality",
      "management.",
      "need",
      "enter",
      "region",
      "id",
      "consume",
      "regional",
      "resource",
      "e.g.",
      "westus",
      "westus2.",
      "leave",
      "blank",
      "global",
      "resource.",
      "click",
      "save.",
      "following",
      "general",
      "tab"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "in-line translations",
    "contentLower": "service management can provide in-line translations of certain blocks of text in requests and incidents. the sections of text that can be translated are: descriptions agents' comments knowledge articles q&a searches on the service portal example: a chinese user of the service portal opens a request and inputs a description in chinese. an agent in the united states is assigned to handle this request. when the agent opens the request record, all ui text is displayed in english (the default language for that agent), except for the description. the agent can use this feature to translate the description. note  at least one section in a record must be expanded for the in-line translation to display properly. enable in-line translation in-line translation is disabled by default. you can enable this function as needed: from the main menu, select administration > configuration > application settings. in the in-line translation tool field, select on. select the external tool that you want to us",
    "keywordsLower": [
      "in-line",
      "translations",
      "enable",
      "translation",
      "translate",
      "text",
      "service",
      "management",
      "provide",
      "certain",
      "blocks",
      "requests",
      "incidents.",
      "sections",
      "translated",
      "descriptions",
      "agents",
      "comments",
      "knowledge",
      "articles",
      "searches",
      "portal",
      "example",
      "chinese",
      "user",
      "opens",
      "request",
      "inputs",
      "description",
      "chinese.",
      "agent",
      "united",
      "states",
      "assigned",
      "handle",
      "request.",
      "record",
      "all",
      "ui",
      "displayed",
      "english",
      "default",
      "language",
      "except",
      "description.",
      "feature",
      "note",
      "least",
      "one",
      "section",
      "expanded",
      "display",
      "properly.",
      "disabled",
      "default.",
      "function",
      "needed",
      "main",
      "menu",
      "select",
      "administration",
      "configuration",
      "application",
      "settings.",
      "tool",
      "field",
      "on.",
      "external",
      "want",
      "perform",
      "translation.",
      "currently",
      "google",
      "microsoft",
      "translator",
      "supported.",
      "purchase",
      "license",
      "services",
      "functionality",
      "management.",
      "need",
      "enter",
      "region",
      "id",
      "consume",
      "regional",
      "resource",
      "e.g.",
      "westus",
      "westus2.",
      "leave",
      "blank",
      "global",
      "resource.",
      "click",
      "save.",
      "following",
      "general",
      "tab"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Limitations for aggregated offerings",
    "content": "This release supports aggregated offerings based on service design and deployment with the following limitations and known issues: Default value validation defined in service design and deployment might not take effect on the offering definition page. In the default value panel of the offering definition page, the value of a user option can't be retrieved from a dynamic list defined in DND. User option value dependency doesn't work in the default value panel of the offering definition page. The offering type is predefined as “Service Offering” in an aggregated offering. As a task plan is also auto-created in the aggregated offering, it might cause unpredictable issues if the offering type is manually changed to another value. The prefix (\"Request: \", \"Modify: \", and \"Cancel: \") in the title of a request based on an aggregated offering from a cloud service doesn't support localization. If the icon of an option set is in the SVG format in service design and deployment, the system ignores",
    "url": "dndlimitations",
    "filename": "dndlimitations",
    "headings": [],
    "keywords": [
      "limitations",
      "aggregated",
      "offerings",
      "release",
      "supports",
      "based",
      "service",
      "design",
      "deployment",
      "following",
      "known",
      "issues",
      "default",
      "value",
      "validation",
      "defined",
      "take",
      "effect",
      "offering",
      "definition",
      "page.",
      "panel",
      "page",
      "user",
      "option",
      "retrieved",
      "dynamic",
      "list",
      "dnd.",
      "dependency",
      "doesn",
      "work",
      "type",
      "predefined",
      "offering.",
      "task",
      "plan",
      "auto-created",
      "cause",
      "unpredictable",
      "manually",
      "changed",
      "another",
      "value.",
      "prefix",
      "request",
      "modify",
      "cancel",
      "title",
      "cloud",
      "support",
      "localization.",
      "icon",
      "set",
      "svg",
      "format",
      "system",
      "ignores",
      "during",
      "aggregation",
      "process.",
      "custom",
      "regular",
      "expression",
      "dnd",
      "isn",
      "supported",
      "offerings.",
      "tokens",
      "aren",
      "such",
      "token",
      "on.",
      "display",
      "name",
      "returned",
      "script",
      "includes",
      "special",
      "characters",
      "content",
      "between",
      "treated",
      "html",
      "tags",
      "rendered",
      "accordingly.",
      "therefore",
      "valid",
      "found",
      "nothing",
      "displayed",
      "ui.",
      "bundle",
      "itself",
      "create",
      "subscription",
      "action.",
      "otherwise",
      "child"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "limitations for aggregated offerings",
    "contentLower": "this release supports aggregated offerings based on service design and deployment with the following limitations and known issues: default value validation defined in service design and deployment might not take effect on the offering definition page. in the default value panel of the offering definition page, the value of a user option can't be retrieved from a dynamic list defined in dnd. user option value dependency doesn't work in the default value panel of the offering definition page. the offering type is predefined as “service offering” in an aggregated offering. as a task plan is also auto-created in the aggregated offering, it might cause unpredictable issues if the offering type is manually changed to another value. the prefix (\"request: \", \"modify: \", and \"cancel: \") in the title of a request based on an aggregated offering from a cloud service doesn't support localization. if the icon of an option set is in the svg format in service design and deployment, the system ignores",
    "keywordsLower": [
      "limitations",
      "aggregated",
      "offerings",
      "release",
      "supports",
      "based",
      "service",
      "design",
      "deployment",
      "following",
      "known",
      "issues",
      "default",
      "value",
      "validation",
      "defined",
      "take",
      "effect",
      "offering",
      "definition",
      "page.",
      "panel",
      "page",
      "user",
      "option",
      "retrieved",
      "dynamic",
      "list",
      "dnd.",
      "dependency",
      "doesn",
      "work",
      "type",
      "predefined",
      "offering.",
      "task",
      "plan",
      "auto-created",
      "cause",
      "unpredictable",
      "manually",
      "changed",
      "another",
      "value.",
      "prefix",
      "request",
      "modify",
      "cancel",
      "title",
      "cloud",
      "support",
      "localization.",
      "icon",
      "set",
      "svg",
      "format",
      "system",
      "ignores",
      "during",
      "aggregation",
      "process.",
      "custom",
      "regular",
      "expression",
      "dnd",
      "isn",
      "supported",
      "offerings.",
      "tokens",
      "aren",
      "such",
      "token",
      "on.",
      "display",
      "name",
      "returned",
      "script",
      "includes",
      "special",
      "characters",
      "content",
      "between",
      "treated",
      "html",
      "tags",
      "rendered",
      "accordingly.",
      "therefore",
      "valid",
      "found",
      "nothing",
      "displayed",
      "ui.",
      "bundle",
      "itself",
      "create",
      "subscription",
      "action.",
      "otherwise",
      "child"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Localize the Service Catalog",
    "content": "If your Service Portal users use one or more languages other than English, you can localize the Service Catalog (including categories, offerings, and service definitions) into one or more of the languages supported by Service Management. If the catalog records are localized into a user's language, Service Portal displays categories, offerings, and service definitions and search results in the user's language. For example, if the Service Catalog has been translated into French and Chinese, Service Portal users on the French and Chinese locales see the catalog records and search results in French and Chinese, respectively. Note If the catalog records aren't translated into the user's language or if an entry isn't translated, Service Portal displays the non-translated values to the user. Service Catalog localization process The Service Catalog localization process is as follows: Export your Service Catalog to a CSV file for each target language, which must be a language supported by Servi",
    "url": "localizecatalog",
    "filename": "localizecatalog",
    "headings": [
      "Service Catalog localization process",
      "Export the Service Catalog for localization",
      "Translate the exported CSV files",
      "Import the translated CSV files",
      "Verify the localization in Service Portal",
      "Related topics"
    ],
    "keywords": [
      "ServiceDefinitions.zip",
      "TranslatedServiceDefinitions.zip",
      "Categories.zip",
      "CN.csv",
      "ServiceDefinitions_fr.csv",
      "Offerings.zip",
      "TranslatedCategories.zip",
      "Categories_fr.csv",
      "Offerings_fr.csv",
      "TranslatedOfferings.zip",
      "localize",
      "service",
      "catalog",
      "localization",
      "process",
      "export",
      "translate",
      "exported",
      "csv",
      "files",
      "import",
      "translated",
      "verify",
      "portal",
      "related",
      "topics",
      "users",
      "one",
      "languages",
      "english",
      "including",
      "categories",
      "offerings",
      "definitions",
      "supported",
      "management.",
      "records",
      "localized",
      "user",
      "language",
      "displays",
      "search",
      "results",
      "language.",
      "example",
      "french",
      "chinese",
      "locales",
      "see",
      "respectively.",
      "note",
      "aren",
      "entry",
      "isn",
      "non-translated",
      "values",
      "user.",
      "follows",
      "file",
      "target",
      "rich",
      "text",
      "fields",
      "save",
      "excel",
      "formatting",
      "become",
      "corrupted",
      "re-imported",
      "displayed",
      "incorrectly.",
      "files.",
      "back",
      "portal.",
      "main",
      "menu",
      "select",
      "plan",
      "catalog.",
      "click",
      "submenu.",
      "toolbar",
      "button",
      "localization.",
      "export.",
      "following",
      "table",
      "lists",
      "definitions.",
      "zip",
      "contains",
      "corresponding",
      "selected.",
      "number",
      "selected",
      "both",
      "openl10n",
      "community",
      "site",
      "offers"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "localize the service catalog",
    "contentLower": "if your service portal users use one or more languages other than english, you can localize the service catalog (including categories, offerings, and service definitions) into one or more of the languages supported by service management. if the catalog records are localized into a user's language, service portal displays categories, offerings, and service definitions and search results in the user's language. for example, if the service catalog has been translated into french and chinese, service portal users on the french and chinese locales see the catalog records and search results in french and chinese, respectively. note if the catalog records aren't translated into the user's language or if an entry isn't translated, service portal displays the non-translated values to the user. service catalog localization process the service catalog localization process is as follows: export your service catalog to a csv file for each target language, which must be a language supported by servi",
    "keywordsLower": [
      "servicedefinitions.zip",
      "translatedservicedefinitions.zip",
      "categories.zip",
      "cn.csv",
      "servicedefinitions_fr.csv",
      "offerings.zip",
      "translatedcategories.zip",
      "categories_fr.csv",
      "offerings_fr.csv",
      "translatedofferings.zip",
      "localize",
      "service",
      "catalog",
      "localization",
      "process",
      "export",
      "translate",
      "exported",
      "csv",
      "files",
      "import",
      "translated",
      "verify",
      "portal",
      "related",
      "topics",
      "users",
      "one",
      "languages",
      "english",
      "including",
      "categories",
      "offerings",
      "definitions",
      "supported",
      "management.",
      "records",
      "localized",
      "user",
      "language",
      "displays",
      "search",
      "results",
      "language.",
      "example",
      "french",
      "chinese",
      "locales",
      "see",
      "respectively.",
      "note",
      "aren",
      "entry",
      "isn",
      "non-translated",
      "values",
      "user.",
      "follows",
      "file",
      "target",
      "rich",
      "text",
      "fields",
      "save",
      "excel",
      "formatting",
      "become",
      "corrupted",
      "re-imported",
      "displayed",
      "incorrectly.",
      "files.",
      "back",
      "portal.",
      "main",
      "menu",
      "select",
      "plan",
      "catalog.",
      "click",
      "submenu.",
      "toolbar",
      "button",
      "localization.",
      "export.",
      "following",
      "table",
      "lists",
      "definitions.",
      "zip",
      "contains",
      "corresponding",
      "selected.",
      "number",
      "selected",
      "both",
      "openl10n",
      "community",
      "site",
      "offers"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Learn more about Service Level Management",
    "content": "It's helpful to understand the ITIL 3 view of Service Level Management. It's mainly focused on these tasks: Gather service requirements. Monitor service levels. Report the results of monitoring activities. When you have a role and responsibilities in Service Level Management, you need to understand the typical workflow, best practice recommendations, the process inputs and outputs, and key performance indicators. Read the information in the links below to learn more about these concepts and the data that Service Level Management uses. Related topics Service Level Management ITIL process Service Level Target calculations Service Level Target duration Service Level Target status and history",
    "url": "slmlearnmore",
    "filename": "slmlearnmore",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "learn",
      "about",
      "service",
      "level",
      "management",
      "related",
      "topics",
      "helpful",
      "understand",
      "itil",
      "view",
      "management.",
      "mainly",
      "focused",
      "tasks",
      "gather",
      "requirements.",
      "monitor",
      "levels.",
      "report",
      "results",
      "monitoring",
      "activities.",
      "role",
      "responsibilities",
      "need",
      "typical",
      "workflow",
      "best",
      "practice",
      "recommendations",
      "process",
      "inputs",
      "outputs",
      "key",
      "performance",
      "indicators.",
      "read",
      "information",
      "links",
      "below",
      "concepts",
      "data",
      "uses.",
      "target",
      "calculations",
      "duration",
      "status",
      "history"
    ],
    "language": "en",
    "word_count": 78,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "learn more about service level management",
    "contentLower": "it's helpful to understand the itil 3 view of service level management. it's mainly focused on these tasks: gather service requirements. monitor service levels. report the results of monitoring activities. when you have a role and responsibilities in service level management, you need to understand the typical workflow, best practice recommendations, the process inputs and outputs, and key performance indicators. read the information in the links below to learn more about these concepts and the data that service level management uses. related topics service level management itil process service level target calculations service level target duration service level target status and history",
    "keywordsLower": [
      "learn",
      "about",
      "service",
      "level",
      "management",
      "related",
      "topics",
      "helpful",
      "understand",
      "itil",
      "view",
      "management.",
      "mainly",
      "focused",
      "tasks",
      "gather",
      "requirements.",
      "monitor",
      "levels.",
      "report",
      "results",
      "monitoring",
      "activities.",
      "role",
      "responsibilities",
      "need",
      "typical",
      "workflow",
      "best",
      "practice",
      "recommendations",
      "process",
      "inputs",
      "outputs",
      "key",
      "performance",
      "indicators.",
      "read",
      "information",
      "links",
      "below",
      "concepts",
      "data",
      "uses.",
      "target",
      "calculations",
      "duration",
      "status",
      "history"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Idea and Proposal Management",
    "content": "Service Management users may come up with innovative ideas every now and then. No matter whether the ideas are aimed at improving the IT infrastructure, services, processes, or tools, you can always submit them through the Service Portal or the Idea and Proposal Management module in Service Management. Ideation is the creative process of generating, developing, and communicating new ideas. The Idea and Proposal Management module enables a company to directly collect feedback from the service consumer, which ensures continuous improvement of the IT service and drives innovation. With social collaboration, ideas can evolve to proposals or changes after being categorized, reviewed, and approved. When you have an approved idea in place, you can create a proposal or change record in Service Management by simply a few clicks. When the proposal is created, you officially enter the project and portfolio management domain. In the Idea and Proposal Management module, the following features are p",
    "url": "ideaproposaloverview",
    "filename": "ideaproposaloverview",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "idea",
      "proposal",
      "management",
      "related",
      "topics",
      "service",
      "users",
      "come",
      "innovative",
      "ideas",
      "every",
      "now",
      "then.",
      "matter",
      "whether",
      "aimed",
      "improving",
      "infrastructure",
      "services",
      "processes",
      "tools",
      "always",
      "submit",
      "through",
      "portal",
      "module",
      "management.",
      "ideation",
      "creative",
      "process",
      "generating",
      "developing",
      "communicating",
      "new",
      "ideas.",
      "enables",
      "company",
      "directly",
      "collect",
      "feedback",
      "consumer",
      "ensures",
      "continuous",
      "improvement",
      "drives",
      "innovation.",
      "social",
      "collaboration",
      "evolve",
      "proposals",
      "changes",
      "after",
      "categorized",
      "reviewed",
      "approved.",
      "approved",
      "place",
      "create",
      "change",
      "record",
      "simply",
      "few",
      "clicks.",
      "created",
      "officially",
      "enter",
      "project",
      "portfolio",
      "domain.",
      "following",
      "features",
      "provided",
      "edit",
      "business",
      "owners",
      "review",
      "board",
      "members.",
      "feature",
      "idea.",
      "administrators",
      "customize",
      "manage",
      "life",
      "cycle",
      "refinement",
      "approval.",
      "managers",
      "leverage",
      "analytics",
      "analyze",
      "evaluate",
      "values",
      "select",
      "candidate",
      "projects.",
      "later",
      "implemented",
      "applications.",
      "resource"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "idea and proposal management",
    "contentLower": "service management users may come up with innovative ideas every now and then. no matter whether the ideas are aimed at improving the it infrastructure, services, processes, or tools, you can always submit them through the service portal or the idea and proposal management module in service management. ideation is the creative process of generating, developing, and communicating new ideas. the idea and proposal management module enables a company to directly collect feedback from the service consumer, which ensures continuous improvement of the it service and drives innovation. with social collaboration, ideas can evolve to proposals or changes after being categorized, reviewed, and approved. when you have an approved idea in place, you can create a proposal or change record in service management by simply a few clicks. when the proposal is created, you officially enter the project and portfolio management domain. in the idea and proposal management module, the following features are p",
    "keywordsLower": [
      "idea",
      "proposal",
      "management",
      "related",
      "topics",
      "service",
      "users",
      "come",
      "innovative",
      "ideas",
      "every",
      "now",
      "then.",
      "matter",
      "whether",
      "aimed",
      "improving",
      "infrastructure",
      "services",
      "processes",
      "tools",
      "always",
      "submit",
      "through",
      "portal",
      "module",
      "management.",
      "ideation",
      "creative",
      "process",
      "generating",
      "developing",
      "communicating",
      "new",
      "ideas.",
      "enables",
      "company",
      "directly",
      "collect",
      "feedback",
      "consumer",
      "ensures",
      "continuous",
      "improvement",
      "drives",
      "innovation.",
      "social",
      "collaboration",
      "evolve",
      "proposals",
      "changes",
      "after",
      "categorized",
      "reviewed",
      "approved.",
      "approved",
      "place",
      "create",
      "change",
      "record",
      "simply",
      "few",
      "clicks.",
      "created",
      "officially",
      "enter",
      "project",
      "portfolio",
      "domain.",
      "following",
      "features",
      "provided",
      "edit",
      "business",
      "owners",
      "review",
      "board",
      "members.",
      "feature",
      "idea.",
      "administrators",
      "customize",
      "manage",
      "life",
      "cycle",
      "refinement",
      "approval.",
      "managers",
      "leverage",
      "analytics",
      "analyze",
      "evaluate",
      "values",
      "select",
      "candidate",
      "projects.",
      "later",
      "implemented",
      "applications.",
      "resource"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Ideas",
    "content": "Ideation is the creative process of generating, developing, and communicating new ideas. The Ideas feature offers you a simple and effective way to create, edit, and review ideas. Business owners or review board members can then categorize, review, and approve these ideas. With this feature, you can also create a change or proposal record from an approved idea. Related topics Idea roles and permissions Idea workflow How to create an idea How to edit an idea How to create a change from an idea How to create a proposal from an idea",
    "url": "ideamgmt",
    "filename": "ideamgmt",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "ideas",
      "related",
      "topics",
      "ideation",
      "creative",
      "process",
      "generating",
      "developing",
      "communicating",
      "new",
      "ideas.",
      "feature",
      "offers",
      "simple",
      "effective",
      "way",
      "create",
      "edit",
      "review",
      "business",
      "owners",
      "board",
      "members",
      "categorize",
      "approve",
      "change",
      "proposal",
      "record",
      "approved",
      "idea.",
      "idea",
      "roles",
      "permissions",
      "workflow"
    ],
    "language": "en",
    "word_count": 52,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "ideas",
    "contentLower": "ideation is the creative process of generating, developing, and communicating new ideas. the ideas feature offers you a simple and effective way to create, edit, and review ideas. business owners or review board members can then categorize, review, and approve these ideas. with this feature, you can also create a change or proposal record from an approved idea. related topics idea roles and permissions idea workflow how to create an idea how to edit an idea how to create a change from an idea how to create a proposal from an idea",
    "keywordsLower": [
      "ideas",
      "related",
      "topics",
      "ideation",
      "creative",
      "process",
      "generating",
      "developing",
      "communicating",
      "new",
      "ideas.",
      "feature",
      "offers",
      "simple",
      "effective",
      "way",
      "create",
      "edit",
      "review",
      "business",
      "owners",
      "board",
      "members",
      "categorize",
      "approve",
      "change",
      "proposal",
      "record",
      "approved",
      "idea.",
      "idea",
      "roles",
      "permissions",
      "workflow"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Idea roles and permissions",
    "content": "There are specific roles associated with ideas. Service Management uses permissions based on roles for task alignment with your assigned role. The tenant administrator manages and assigns these permissions. By default, the following roles are related to an idea lifecycle. These roles support idea creation and process ownership. Role Responsibilities Service Portal user Creates ideas from the Service Portal Edits or abandons ideas before publication Publishes ideas when they're ready Idea reviewer Evaluates ideas and moves them to the Review phase when needed Reviews existing ideas and provides review feedback Categorizes and prioritizes ideas Approves or rejects ideas after review Idea administrator Assigns roles and permissions related to idea management to Service Management users Defines and configures the workflow process for ideas Review the individual permission assignments for each role in Administration > Master Data > People > Roles. Related topics Roles",
    "url": "idearoles",
    "filename": "idearoles",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "idea",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "ideas.",
      "service",
      "management",
      "uses",
      "based",
      "task",
      "alignment",
      "assigned",
      "role.",
      "tenant",
      "administrator",
      "manages",
      "assigns",
      "permissions.",
      "default",
      "following",
      "lifecycle.",
      "support",
      "creation",
      "process",
      "ownership.",
      "role",
      "responsibilities",
      "portal",
      "user",
      "creates",
      "ideas",
      "edits",
      "abandons",
      "before",
      "publication",
      "publishes",
      "re",
      "ready",
      "reviewer",
      "evaluates",
      "moves",
      "review",
      "phase",
      "needed",
      "reviews",
      "existing",
      "provides",
      "feedback",
      "categorizes",
      "prioritizes",
      "approves",
      "rejects",
      "after",
      "users",
      "defines",
      "configures",
      "workflow",
      "individual",
      "permission",
      "assignments",
      "administration",
      "master",
      "data",
      "people",
      "roles."
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "idea roles and permissions",
    "contentLower": "there are specific roles associated with ideas. service management uses permissions based on roles for task alignment with your assigned role. the tenant administrator manages and assigns these permissions. by default, the following roles are related to an idea lifecycle. these roles support idea creation and process ownership. role responsibilities service portal user creates ideas from the service portal edits or abandons ideas before publication publishes ideas when they're ready idea reviewer evaluates ideas and moves them to the review phase when needed reviews existing ideas and provides review feedback categorizes and prioritizes ideas approves or rejects ideas after review idea administrator assigns roles and permissions related to idea management to service management users defines and configures the workflow process for ideas review the individual permission assignments for each role in administration > master data > people > roles. related topics roles",
    "keywordsLower": [
      "idea",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "ideas.",
      "service",
      "management",
      "uses",
      "based",
      "task",
      "alignment",
      "assigned",
      "role.",
      "tenant",
      "administrator",
      "manages",
      "assigns",
      "permissions.",
      "default",
      "following",
      "lifecycle.",
      "support",
      "creation",
      "process",
      "ownership.",
      "role",
      "responsibilities",
      "portal",
      "user",
      "creates",
      "ideas",
      "edits",
      "abandons",
      "before",
      "publication",
      "publishes",
      "re",
      "ready",
      "reviewer",
      "evaluates",
      "moves",
      "review",
      "phase",
      "needed",
      "reviews",
      "existing",
      "provides",
      "feedback",
      "categorizes",
      "prioritizes",
      "approves",
      "rejects",
      "after",
      "users",
      "defines",
      "configures",
      "workflow",
      "individual",
      "permission",
      "assignments",
      "administration",
      "master",
      "data",
      "people",
      "roles."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Idea workflow",
    "content": "This section describes the metaphases and subordinate phases in the life cycle of an idea. The idea workflow relies on business rules. Rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. For more information about the out-of-the-box business rules defined for the idea workflow, see Idea process - Business rules. Metaphase: Refinement The first metaphase in the idea's lifecycle, where the idea enters into the Draft or Refine phase. Phase Transition Description Draft Manual In Service Portal, if the idea creator clicks the Save as Draft button while creating an idea, the idea enters into the Draft phase. In this phase, the idea can't be publicly viewed. When the idea is ready to publish, the user who is creating this idea can manually transition the idea to the Refine phase so that the idea can be engaged in social activities. The user who is creating this idea can manually transition the idea to the Abandon",
    "url": "ideawflw",
    "filename": "ideawflw",
    "headings": [
      "Metaphase: Refinement",
      "Metaphase: Assessment",
      "Metaphase: Done (End)",
      "Related topics"
    ],
    "keywords": [
      "idea",
      "workflow",
      "metaphase",
      "refinement",
      "assessment",
      "done",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "idea.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "out-of-the-box",
      "defined",
      "see",
      "process",
      "first",
      "lifecycle",
      "enters",
      "draft",
      "refine",
      "transition",
      "description",
      "manual",
      "service",
      "portal",
      "creator",
      "clicks",
      "save",
      "button",
      "while",
      "creating",
      "publicly",
      "viewed.",
      "ready",
      "publish",
      "manually",
      "engaged",
      "social",
      "activities.",
      "abandon",
      "isn",
      "valid.",
      "next",
      "submitted",
      "result",
      "successful",
      "publication",
      "continue",
      "work",
      "users",
      "view",
      "comment",
      "reviewer",
      "review",
      "worth",
      "reviewing.",
      "perform",
      "following",
      "actions",
      "categorize",
      "ideas",
      "prioritize",
      "approve",
      "decide",
      "subsequent",
      "create",
      "proposal",
      "record",
      "needs",
      "proceed",
      "ways",
      "reject",
      "back",
      "provide",
      "automatic",
      "decision"
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "idea workflow",
    "contentLower": "this section describes the metaphases and subordinate phases in the life cycle of an idea. the idea workflow relies on business rules. rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. for more information about the out-of-the-box business rules defined for the idea workflow, see idea process - business rules. metaphase: refinement the first metaphase in the idea's lifecycle, where the idea enters into the draft or refine phase. phase transition description draft manual in service portal, if the idea creator clicks the save as draft button while creating an idea, the idea enters into the draft phase. in this phase, the idea can't be publicly viewed. when the idea is ready to publish, the user who is creating this idea can manually transition the idea to the refine phase so that the idea can be engaged in social activities. the user who is creating this idea can manually transition the idea to the abandon",
    "keywordsLower": [
      "idea",
      "workflow",
      "metaphase",
      "refinement",
      "assessment",
      "done",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "idea.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "out-of-the-box",
      "defined",
      "see",
      "process",
      "first",
      "lifecycle",
      "enters",
      "draft",
      "refine",
      "transition",
      "description",
      "manual",
      "service",
      "portal",
      "creator",
      "clicks",
      "save",
      "button",
      "while",
      "creating",
      "publicly",
      "viewed.",
      "ready",
      "publish",
      "manually",
      "engaged",
      "social",
      "activities.",
      "abandon",
      "isn",
      "valid.",
      "next",
      "submitted",
      "result",
      "successful",
      "publication",
      "continue",
      "work",
      "users",
      "view",
      "comment",
      "reviewer",
      "review",
      "worth",
      "reviewing.",
      "perform",
      "following",
      "actions",
      "categorize",
      "ideas",
      "prioritize",
      "approve",
      "decide",
      "subsequent",
      "create",
      "proposal",
      "record",
      "needs",
      "proceed",
      "ways",
      "reject",
      "back",
      "provide",
      "automatic",
      "decision"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Learn more about changes",
    "content": "Change Management includes the activities necessary for you to control changes to service assets and configuration items across the entire service life cycle. It provides standard methods and procedures to use when implementing all changes. Service Management's Change Management module helps ensure that your organization: Follows a predefined change process. Notifies stakeholders at checkpoints in that process. Monitors the progress of each change request. Notifies stakeholders if deadlines are missed. Tracks each change through the appropriate life cycle: simple or complex. Adheres to ITIL version 3 recommendations for standard, normal, and emergency change requests. Related topics ITIL and Change Management Change input and output Emergency Changes Change Management KPIs Change Management and other modules Scheduling changes",
    "url": "changemgmtreference",
    "filename": "changemgmtreference",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "learn",
      "about",
      "changes",
      "related",
      "topics",
      "change",
      "management",
      "includes",
      "activities",
      "necessary",
      "control",
      "service",
      "assets",
      "configuration",
      "items",
      "across",
      "entire",
      "life",
      "cycle.",
      "provides",
      "standard",
      "methods",
      "procedures",
      "implementing",
      "all",
      "changes.",
      "module",
      "helps",
      "ensure",
      "organization",
      "follows",
      "predefined",
      "process.",
      "notifies",
      "stakeholders",
      "checkpoints",
      "monitors",
      "progress",
      "request.",
      "deadlines",
      "missed.",
      "tracks",
      "through",
      "appropriate",
      "cycle",
      "simple",
      "complex.",
      "adheres",
      "itil",
      "version",
      "recommendations",
      "normal",
      "emergency",
      "requests.",
      "input",
      "output",
      "kpis",
      "modules",
      "scheduling"
    ],
    "language": "en",
    "word_count": 87,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "learn more about changes",
    "contentLower": "change management includes the activities necessary for you to control changes to service assets and configuration items across the entire service life cycle. it provides standard methods and procedures to use when implementing all changes. service management's change management module helps ensure that your organization: follows a predefined change process. notifies stakeholders at checkpoints in that process. monitors the progress of each change request. notifies stakeholders if deadlines are missed. tracks each change through the appropriate life cycle: simple or complex. adheres to itil version 3 recommendations for standard, normal, and emergency change requests. related topics itil and change management change input and output emergency changes change management kpis change management and other modules scheduling changes",
    "keywordsLower": [
      "learn",
      "about",
      "changes",
      "related",
      "topics",
      "change",
      "management",
      "includes",
      "activities",
      "necessary",
      "control",
      "service",
      "assets",
      "configuration",
      "items",
      "across",
      "entire",
      "life",
      "cycle.",
      "provides",
      "standard",
      "methods",
      "procedures",
      "implementing",
      "all",
      "changes.",
      "module",
      "helps",
      "ensure",
      "organization",
      "follows",
      "predefined",
      "process.",
      "notifies",
      "stakeholders",
      "checkpoints",
      "monitors",
      "progress",
      "request.",
      "deadlines",
      "missed.",
      "tracks",
      "through",
      "appropriate",
      "cycle",
      "simple",
      "complex.",
      "adheres",
      "itil",
      "version",
      "recommendations",
      "normal",
      "emergency",
      "requests.",
      "input",
      "output",
      "kpis",
      "modules",
      "scheduling"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "ITIL and Change Management",
    "content": "Change Management is the process responsible for ensuring that changes are recorded, evaluated, planned, tested, implemented, and reviewed in a controlled manner. Change Management enables you to meet the following business objectives: Use standardized methods and procedures to ensure efficient and prompt handling of all changes. Record all changes to service assets and configuration items (CIs) in the Universal Discovery and CMDB (UD/UCMDB). Optimize overall business risk. Respond to customers’ changing business requirements maximizes value and reduces incidents, disruptions, and rework. Respond to business and IT requests for changes aligns services with business needs. The ITIL Change Management process model includes: The steps to initiate a change. The order to take those steps. Who has responsibility for each part of the process. Scheduling and planning. When (and how) to escalate a change. Related topics Change input and output Emergency Changes Change Management KPIs Change Man",
    "url": "cmitil",
    "filename": "cmitil",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "itil",
      "change",
      "management",
      "related",
      "topics",
      "process",
      "responsible",
      "ensuring",
      "changes",
      "recorded",
      "evaluated",
      "planned",
      "tested",
      "implemented",
      "reviewed",
      "controlled",
      "manner.",
      "enables",
      "meet",
      "following",
      "business",
      "objectives",
      "standardized",
      "methods",
      "procedures",
      "ensure",
      "efficient",
      "prompt",
      "handling",
      "all",
      "changes.",
      "record",
      "service",
      "assets",
      "configuration",
      "items",
      "cis",
      "universal",
      "discovery",
      "cmdb",
      "ud",
      "ucmdb",
      "optimize",
      "overall",
      "risk.",
      "respond",
      "customers",
      "changing",
      "requirements",
      "maximizes",
      "value",
      "reduces",
      "incidents",
      "disruptions",
      "rework.",
      "requests",
      "aligns",
      "services",
      "needs.",
      "model",
      "includes",
      "steps",
      "initiate",
      "change.",
      "order",
      "take",
      "steps.",
      "responsibility",
      "part",
      "process.",
      "scheduling",
      "planning.",
      "escalate",
      "input",
      "output",
      "emergency",
      "kpis",
      "modules"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "itil and change management",
    "contentLower": "change management is the process responsible for ensuring that changes are recorded, evaluated, planned, tested, implemented, and reviewed in a controlled manner. change management enables you to meet the following business objectives: use standardized methods and procedures to ensure efficient and prompt handling of all changes. record all changes to service assets and configuration items (cis) in the universal discovery and cmdb (ud/ucmdb). optimize overall business risk. respond to customers’ changing business requirements maximizes value and reduces incidents, disruptions, and rework. respond to business and it requests for changes aligns services with business needs. the itil change management process model includes: the steps to initiate a change. the order to take those steps. who has responsibility for each part of the process. scheduling and planning. when (and how) to escalate a change. related topics change input and output emergency changes change management kpis change man",
    "keywordsLower": [
      "itil",
      "change",
      "management",
      "related",
      "topics",
      "process",
      "responsible",
      "ensuring",
      "changes",
      "recorded",
      "evaluated",
      "planned",
      "tested",
      "implemented",
      "reviewed",
      "controlled",
      "manner.",
      "enables",
      "meet",
      "following",
      "business",
      "objectives",
      "standardized",
      "methods",
      "procedures",
      "ensure",
      "efficient",
      "prompt",
      "handling",
      "all",
      "changes.",
      "record",
      "service",
      "assets",
      "configuration",
      "items",
      "cis",
      "universal",
      "discovery",
      "cmdb",
      "ud",
      "ucmdb",
      "optimize",
      "overall",
      "risk.",
      "respond",
      "customers",
      "changing",
      "requirements",
      "maximizes",
      "value",
      "reduces",
      "incidents",
      "disruptions",
      "rework.",
      "requests",
      "aligns",
      "services",
      "needs.",
      "model",
      "includes",
      "steps",
      "initiate",
      "change.",
      "order",
      "take",
      "steps.",
      "responsibility",
      "part",
      "process.",
      "scheduling",
      "planning.",
      "escalate",
      "input",
      "output",
      "emergency",
      "kpis",
      "modules"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Knowledge Management",
    "content": "Knowledge Management enables you to manage the knowledge articles that help Service Portal users solve problems, post news articles on the Service Portal, and moderate questions and answers. The Knowledge Management module is made up of the following areas: Articles. Enables you to add, modify, and delete knowledge articles. News. Enables you to add, modify, and delete news articles. News articles can be alerts for the Service Portal users about critical issues, and other important or general alerts. They are displayed in the widgets panel of the Service Portal. Models. Enables you to add, modify, activate, and retire article models. For more information, see Article models. Q&A. Enables you to moderate questions and answers from Service Portal users. For more information, see How to moderate Q&A. Hot Topic Analytics. Enables you to analyze questions and requests submitted by the Service Portal users, as well as the types of information users are looking for. For more information, see ",
    "url": "knowledgemgmt",
    "filename": "knowledgemgmt",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "knowledge",
      "management",
      "related",
      "topics",
      "enables",
      "manage",
      "articles",
      "help",
      "service",
      "portal",
      "users",
      "solve",
      "problems",
      "post",
      "news",
      "moderate",
      "questions",
      "answers.",
      "module",
      "made",
      "following",
      "areas",
      "articles.",
      "add",
      "modify",
      "delete",
      "news.",
      "alerts",
      "about",
      "critical",
      "issues",
      "important",
      "general",
      "alerts.",
      "displayed",
      "widgets",
      "panel",
      "portal.",
      "models.",
      "activate",
      "retire",
      "article",
      "information",
      "see",
      "a.",
      "answers",
      "users.",
      "hot",
      "topic",
      "analytics.",
      "analyze",
      "requests",
      "submitted",
      "well",
      "types",
      "looking",
      "for.",
      "analytics",
      "note",
      "recommended",
      "assign",
      "data",
      "domains",
      "contain",
      "sensitive",
      "targeted",
      "permissions",
      "enable",
      "strong",
      "security.",
      "record",
      "domain.",
      "don",
      "cases",
      "view",
      "published",
      "internally",
      "rest",
      "apis.",
      "domain",
      "segmentation.",
      "category",
      "links",
      "get",
      "started",
      "administer",
      "fields",
      "forms",
      "roles",
      "segmentation",
      "current",
      "page",
      "packaging",
      "importing",
      "workflow",
      "procedures",
      "configure",
      "create",
      "update",
      "searches"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "knowledge management",
    "contentLower": "knowledge management enables you to manage the knowledge articles that help service portal users solve problems, post news articles on the service portal, and moderate questions and answers. the knowledge management module is made up of the following areas: articles. enables you to add, modify, and delete knowledge articles. news. enables you to add, modify, and delete news articles. news articles can be alerts for the service portal users about critical issues, and other important or general alerts. they are displayed in the widgets panel of the service portal. models. enables you to add, modify, activate, and retire article models. for more information, see article models. q&a. enables you to moderate questions and answers from service portal users. for more information, see how to moderate q&a. hot topic analytics. enables you to analyze questions and requests submitted by the service portal users, as well as the types of information users are looking for. for more information, see ",
    "keywordsLower": [
      "knowledge",
      "management",
      "related",
      "topics",
      "enables",
      "manage",
      "articles",
      "help",
      "service",
      "portal",
      "users",
      "solve",
      "problems",
      "post",
      "news",
      "moderate",
      "questions",
      "answers.",
      "module",
      "made",
      "following",
      "areas",
      "articles.",
      "add",
      "modify",
      "delete",
      "news.",
      "alerts",
      "about",
      "critical",
      "issues",
      "important",
      "general",
      "alerts.",
      "displayed",
      "widgets",
      "panel",
      "portal.",
      "models.",
      "activate",
      "retire",
      "article",
      "information",
      "see",
      "a.",
      "answers",
      "users.",
      "hot",
      "topic",
      "analytics.",
      "analyze",
      "requests",
      "submitted",
      "well",
      "types",
      "looking",
      "for.",
      "analytics",
      "note",
      "recommended",
      "assign",
      "data",
      "domains",
      "contain",
      "sensitive",
      "targeted",
      "permissions",
      "enable",
      "strong",
      "security.",
      "record",
      "domain.",
      "don",
      "cases",
      "view",
      "published",
      "internally",
      "rest",
      "apis.",
      "domain",
      "segmentation.",
      "category",
      "links",
      "get",
      "started",
      "administer",
      "fields",
      "forms",
      "roles",
      "segmentation",
      "current",
      "page",
      "packaging",
      "importing",
      "workflow",
      "procedures",
      "configure",
      "create",
      "update",
      "searches"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Knowledge Management roles and permissions",
    "content": "Different phases of knowledge management require different responsibilities. Depending on the size of the organization, some roles might be combined but the responsibilities remain the same. Service Management uses role-based permissions to enable you to complete a task that's appropriate to your role. The tenant administrator manages and assigns these permissions. By default, the roles assigned to Knowledge Management support creation, review, and publishing of knowledge articles, as well as the moderation of user questions: Role Responsibilities Knowledge Contributor Creates and updates new knowledge articles, and moves articles through certain phases of the workflow process Imports knowledge articles Moderates questions and answers Defines audiences for articles Knowledge Publisher Performs all of the responsibilities of a Knowledge Contributor Publishes articles to the Service Portal Removes articles from the Service Portal Updates articles in the Service Portal Defines new entitle",
    "url": "kmroles",
    "filename": "kmroles",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "knowledge",
      "management",
      "roles",
      "permissions",
      "related",
      "topics",
      "different",
      "phases",
      "require",
      "responsibilities.",
      "depending",
      "size",
      "organization",
      "combined",
      "responsibilities",
      "remain",
      "same.",
      "service",
      "uses",
      "role-based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "tenant",
      "administrator",
      "manages",
      "assigns",
      "permissions.",
      "default",
      "assigned",
      "support",
      "creation",
      "review",
      "publishing",
      "articles",
      "well",
      "moderation",
      "user",
      "questions",
      "role",
      "contributor",
      "creates",
      "updates",
      "new",
      "moves",
      "through",
      "certain",
      "workflow",
      "process",
      "imports",
      "moderates",
      "answers",
      "defines",
      "audiences",
      "publisher",
      "performs",
      "all",
      "publishes",
      "portal",
      "removes",
      "entitlement",
      "rules",
      "system",
      "define",
      "configures",
      "on-premises",
      "bridge",
      "perform",
      "external",
      "indexing",
      "create",
      "additional",
      "relevant",
      "administration",
      "module.",
      "information",
      "see",
      "edit",
      "individual",
      "permission",
      "assignments",
      "master",
      "data",
      "people",
      "roles.",
      "specific",
      "available",
      "import",
      "publish",
      "update",
      "currently",
      "published",
      "hide",
      "moderate",
      "hot",
      "topic",
      "analytics",
      "authorize"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "knowledge management roles and permissions",
    "contentLower": "different phases of knowledge management require different responsibilities. depending on the size of the organization, some roles might be combined but the responsibilities remain the same. service management uses role-based permissions to enable you to complete a task that's appropriate to your role. the tenant administrator manages and assigns these permissions. by default, the roles assigned to knowledge management support creation, review, and publishing of knowledge articles, as well as the moderation of user questions: role responsibilities knowledge contributor creates and updates new knowledge articles, and moves articles through certain phases of the workflow process imports knowledge articles moderates questions and answers defines audiences for articles knowledge publisher performs all of the responsibilities of a knowledge contributor publishes articles to the service portal removes articles from the service portal updates articles in the service portal defines new entitle",
    "keywordsLower": [
      "knowledge",
      "management",
      "roles",
      "permissions",
      "related",
      "topics",
      "different",
      "phases",
      "require",
      "responsibilities.",
      "depending",
      "size",
      "organization",
      "combined",
      "responsibilities",
      "remain",
      "same.",
      "service",
      "uses",
      "role-based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "tenant",
      "administrator",
      "manages",
      "assigns",
      "permissions.",
      "default",
      "assigned",
      "support",
      "creation",
      "review",
      "publishing",
      "articles",
      "well",
      "moderation",
      "user",
      "questions",
      "role",
      "contributor",
      "creates",
      "updates",
      "new",
      "moves",
      "through",
      "certain",
      "workflow",
      "process",
      "imports",
      "moderates",
      "answers",
      "defines",
      "audiences",
      "publisher",
      "performs",
      "all",
      "publishes",
      "portal",
      "removes",
      "entitlement",
      "rules",
      "system",
      "define",
      "configures",
      "on-premises",
      "bridge",
      "perform",
      "external",
      "indexing",
      "create",
      "additional",
      "relevant",
      "administration",
      "module.",
      "information",
      "see",
      "edit",
      "individual",
      "permission",
      "assignments",
      "master",
      "data",
      "people",
      "roles.",
      "specific",
      "available",
      "import",
      "publish",
      "update",
      "currently",
      "published",
      "hide",
      "moderate",
      "hot",
      "topic",
      "analytics",
      "authorize"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Knowledge/News article workflow",
    "content": "This section describes the out-of-the-box metaphases and subordinate phases in the life cycle of a knowledge article or news article, including all necessary phases to create and publish an article. The Service Management article workflow reflects ITILv3 process recommendations. Metaphase: Creation The first metaphase in the article's lifecycle, where the article enters into the Draft phase. Phase Transition Description Draft Automatic or Manual If you have a Knowledge Management role, you can create or import an article. When you import (as opposed to create) articles, you can specify the phase at which to import them. The result of the phase is the successful creation of an article. When the article is ready for internal stakeholders’ review, the Knowledge Management user can manually transition the article to the Review phase. If a task plan is defined and is run successfully, the workflow automatically transitions to the Review phase. Next phase: Review Metaphase: Publication The a",
    "url": "kmwflw",
    "filename": "kmwflw",
    "headings": [
      "Metaphase: Creation",
      "Metaphase: Publication",
      "Metaphase: Consumption",
      "Metaphase: Retirement (End)",
      "Related topics"
    ],
    "keywords": [
      "knowledgenews",
      "knowledge",
      "news",
      "article",
      "workflow",
      "metaphase",
      "creation",
      "publication",
      "consumption",
      "retirement",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "out-of-the-box",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "including",
      "all",
      "necessary",
      "create",
      "publish",
      "article.",
      "service",
      "management",
      "reflects",
      "itilv3",
      "process",
      "recommendations.",
      "first",
      "lifecycle",
      "enters",
      "draft",
      "phase.",
      "phase",
      "transition",
      "description",
      "automatic",
      "manual",
      "role",
      "import",
      "opposed",
      "articles",
      "specify",
      "them.",
      "result",
      "successful",
      "ready",
      "internal",
      "stakeholders",
      "review",
      "user",
      "manually",
      "task",
      "plan",
      "defined",
      "run",
      "successfully",
      "automatically",
      "transitions",
      "next",
      "undergoes",
      "updating.",
      "decide",
      "whether",
      "send",
      "back",
      "further",
      "editing.",
      "approved",
      "return",
      "audience.",
      "external",
      "audience",
      "publisher",
      "administrator",
      "roles.",
      "wish",
      "transitioned",
      "released",
      "intended",
      "either",
      "externally",
      "visible",
      "both",
      "managementand",
      "through",
      "portal",
      "internally",
      "users",
      "only.",
      "here",
      "returned",
      "updating",
      "archived",
      "longer"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "knowledge/news article workflow",
    "contentLower": "this section describes the out-of-the-box metaphases and subordinate phases in the life cycle of a knowledge article or news article, including all necessary phases to create and publish an article. the service management article workflow reflects itilv3 process recommendations. metaphase: creation the first metaphase in the article's lifecycle, where the article enters into the draft phase. phase transition description draft automatic or manual if you have a knowledge management role, you can create or import an article. when you import (as opposed to create) articles, you can specify the phase at which to import them. the result of the phase is the successful creation of an article. when the article is ready for internal stakeholders’ review, the knowledge management user can manually transition the article to the review phase. if a task plan is defined and is run successfully, the workflow automatically transitions to the review phase. next phase: review metaphase: publication the a",
    "keywordsLower": [
      "knowledgenews",
      "knowledge",
      "news",
      "article",
      "workflow",
      "metaphase",
      "creation",
      "publication",
      "consumption",
      "retirement",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "out-of-the-box",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "including",
      "all",
      "necessary",
      "create",
      "publish",
      "article.",
      "service",
      "management",
      "reflects",
      "itilv3",
      "process",
      "recommendations.",
      "first",
      "lifecycle",
      "enters",
      "draft",
      "phase.",
      "phase",
      "transition",
      "description",
      "automatic",
      "manual",
      "role",
      "import",
      "opposed",
      "articles",
      "specify",
      "them.",
      "result",
      "successful",
      "ready",
      "internal",
      "stakeholders",
      "review",
      "user",
      "manually",
      "task",
      "plan",
      "defined",
      "run",
      "successfully",
      "automatically",
      "transitions",
      "next",
      "undergoes",
      "updating.",
      "decide",
      "whether",
      "send",
      "back",
      "further",
      "editing.",
      "approved",
      "return",
      "audience.",
      "external",
      "audience",
      "publisher",
      "administrator",
      "roles.",
      "wish",
      "transitioned",
      "released",
      "intended",
      "either",
      "externally",
      "visible",
      "both",
      "managementand",
      "through",
      "portal",
      "internally",
      "users",
      "only.",
      "here",
      "returned",
      "updating",
      "archived",
      "longer"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Knowledge Management procedures",
    "content": "Knowledge Management procedures help you deal with the complete lifecycle of knowledge articles. Related topics How to create or update a knowledge/news article How to import knowledge article packages Index external knowledge using IDOL connectors Quickstart - How to package knowledge articles using the Knowledge Packaging Tool How to moderate Q&A Hot Topic Analytics for knowledge articles Move knowledge articles",
    "url": "kmtasks",
    "filename": "kmtasks",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "knowledge",
      "management",
      "procedures",
      "related",
      "topics",
      "help",
      "deal",
      "complete",
      "lifecycle",
      "articles.",
      "create",
      "update",
      "news",
      "article",
      "import",
      "packages",
      "index",
      "external",
      "idol",
      "connectors",
      "quickstart",
      "package",
      "articles",
      "packaging",
      "tool",
      "moderate",
      "hot",
      "topic",
      "analytics",
      "move"
    ],
    "language": "en",
    "word_count": 45,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "knowledge management procedures",
    "contentLower": "knowledge management procedures help you deal with the complete lifecycle of knowledge articles. related topics how to create or update a knowledge/news article how to import knowledge article packages index external knowledge using idol connectors quickstart - how to package knowledge articles using the knowledge packaging tool how to moderate q&a hot topic analytics for knowledge articles move knowledge articles",
    "keywordsLower": [
      "knowledge",
      "management",
      "procedures",
      "related",
      "topics",
      "help",
      "deal",
      "complete",
      "lifecycle",
      "articles.",
      "create",
      "update",
      "news",
      "article",
      "import",
      "packages",
      "index",
      "external",
      "idol",
      "connectors",
      "quickstart",
      "package",
      "articles",
      "packaging",
      "tool",
      "moderate",
      "hot",
      "topic",
      "analytics",
      "move"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Localize articles",
    "content": "You can export your articles to CSV files which can be sent off for localization. You can translate articles into one or more of the languages supported by Service Management. The values in the exported files must be translated, after which the files are imported back into Service Management. After a successful import, the localized article is displayed in the Service Portal in the language of the user's locale, as long as the article has been translated into that language. Moreover, when Service Portal users perform searches, the results are displayed in the localized language. For example, if articles were translated into French and Chinese, Service Portal users on French and Chinese locales see the article search results in French and Chinese respectively. Note If an article was not translated into the language of the user's locale, or if an entry was not translated, the original article is displayed in the Service Portal. In the example above, users on a German locale see the origi",
    "url": "localizearticles",
    "filename": "localizearticles",
    "headings": [
      "Export articles for localization",
      "Translate the values in the exported article",
      "Import the translated files",
      "Related topics"
    ],
    "keywords": [
      "Article_translations_fr.csv",
      "CN.csv",
      "Article_translations.zip",
      "localize",
      "articles",
      "export",
      "localization",
      "translate",
      "values",
      "exported",
      "article",
      "import",
      "translated",
      "files",
      "related",
      "topics",
      "csv",
      "sent",
      "off",
      "localization.",
      "one",
      "languages",
      "supported",
      "service",
      "management.",
      "after",
      "imported",
      "back",
      "successful",
      "localized",
      "displayed",
      "portal",
      "language",
      "user",
      "locale",
      "long",
      "language.",
      "moreover",
      "users",
      "perform",
      "searches",
      "results",
      "example",
      "french",
      "chinese",
      "locales",
      "see",
      "search",
      "respectively.",
      "note",
      "entry",
      "original",
      "portal.",
      "above",
      "german",
      "non-translated",
      "article.",
      "exporting",
      "file",
      "rich",
      "text",
      "fields",
      "saving",
      "excel",
      "formatting",
      "become",
      "corrupted",
      "re-imported",
      "display",
      "correctly.",
      "main",
      "menu",
      "select",
      "build",
      "knowledge.",
      "articles.",
      "record",
      "list",
      "click",
      "button",
      "export.",
      "following",
      "generated",
      "number",
      "selected",
      "zip",
      "contains",
      "corresponding",
      "open",
      "file.",
      "value",
      "source",
      "field",
      "enter",
      "translation",
      "last",
      "position",
      "row",
      "field.",
      "delete"
    ],
    "language": "en",
    "word_count": 91,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "localize articles",
    "contentLower": "you can export your articles to csv files which can be sent off for localization. you can translate articles into one or more of the languages supported by service management. the values in the exported files must be translated, after which the files are imported back into service management. after a successful import, the localized article is displayed in the service portal in the language of the user's locale, as long as the article has been translated into that language. moreover, when service portal users perform searches, the results are displayed in the localized language. for example, if articles were translated into french and chinese, service portal users on french and chinese locales see the article search results in french and chinese respectively. note if an article was not translated into the language of the user's locale, or if an entry was not translated, the original article is displayed in the service portal. in the example above, users on a german locale see the origi",
    "keywordsLower": [
      "article_translations_fr.csv",
      "cn.csv",
      "article_translations.zip",
      "localize",
      "articles",
      "export",
      "localization",
      "translate",
      "values",
      "exported",
      "article",
      "import",
      "translated",
      "files",
      "related",
      "topics",
      "csv",
      "sent",
      "off",
      "localization.",
      "one",
      "languages",
      "supported",
      "service",
      "management.",
      "after",
      "imported",
      "back",
      "successful",
      "localized",
      "displayed",
      "portal",
      "language",
      "user",
      "locale",
      "long",
      "language.",
      "moreover",
      "users",
      "perform",
      "searches",
      "results",
      "example",
      "french",
      "chinese",
      "locales",
      "see",
      "search",
      "respectively.",
      "note",
      "entry",
      "original",
      "portal.",
      "above",
      "german",
      "non-translated",
      "article.",
      "exporting",
      "file",
      "rich",
      "text",
      "fields",
      "saving",
      "excel",
      "formatting",
      "become",
      "corrupted",
      "re-imported",
      "display",
      "correctly.",
      "main",
      "menu",
      "select",
      "build",
      "knowledge.",
      "articles.",
      "record",
      "list",
      "click",
      "button",
      "export.",
      "following",
      "generated",
      "number",
      "selected",
      "zip",
      "contains",
      "corresponding",
      "open",
      "file.",
      "value",
      "source",
      "field",
      "enter",
      "translation",
      "last",
      "position",
      "row",
      "field.",
      "delete"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Implement an in-house article retriever plugin",
    "content": "To package articles from a custom knowledge base, you need to implement an article retriever. An article retriever is a Java class that implements the com.hp.saw.upload.ArticleRetriever interface. The retriever reads the contents of the articles from the knowledge base and formats them as com.hp.saw.upload.Article objects. For an example, see the default java implementations, found in the <Packaging_tool_deployment>/saw-article-packaging-tool/src folder: com.hp.saw.upload.folder.FolderArticleRetriever and com.hp.saw.upload.folder.FileSystemArticle. The knowledge packaging tool source code is found in the same folder in the /upload folder. To implement the retriever plugin and package the retrieved files: Implement the Java interface com.hp.saw.upload.ArticleRetriever (provided in the JAR file). This interface requires implementing the getArticles() method, which returns Article objects that represent your knowledge articles. ArticleRetriever interface source code Note Located in <Packa",
    "url": "implementinhousearticleretrieverplugin",
    "filename": "implementinhousearticleretrieverplugin",
    "headings": [],
    "keywords": [
      "metadata.xml",
      "knowledgearticles.zip",
      "info.log",
      "summary.log",
      "file1.html",
      "error.log",
      "com.hp",
      "articles.zip",
      "article_packager.bat",
      "file2.html",
      "implement",
      "in-house",
      "article",
      "retriever",
      "plugin",
      "package",
      "articles",
      "custom",
      "knowledge",
      "base",
      "need",
      "retriever.",
      "java",
      "class",
      "implements",
      "com.hp.saw.upload.articleretriever",
      "interface.",
      "reads",
      "contents",
      "formats",
      "com.hp.saw.upload.article",
      "objects.",
      "example",
      "see",
      "default",
      "implementations",
      "found",
      "saw-article-packaging-tool",
      "src",
      "folder",
      "com.hp.saw.upload.folder.folderarticleretriever",
      "com.hp.saw.upload.folder.filesystemarticle.",
      "packaging",
      "tool",
      "source",
      "code",
      "same",
      "upload",
      "folder.",
      "retrieved",
      "files",
      "interface",
      "provided",
      "jar",
      "file",
      "requires",
      "implementing",
      "getarticles",
      "method",
      "returns",
      "objects",
      "represent",
      "articles.",
      "articleretriever",
      "note",
      "located",
      "src.",
      "public",
      "initializes",
      "arguments",
      "packager",
      "command",
      "line",
      "enables",
      "implementation",
      "get",
      "external",
      "parameters",
      "identifying",
      "such",
      "database",
      "connections",
      "path",
      "etc",
      "param",
      "retrieverargs",
      "list",
      "params",
      "void",
      "init",
      "return",
      "represents",
      "document",
      "ess",
      "system.",
      "having",
      "id",
      "title",
      "content.",
      "single"
    ],
    "language": "en",
    "word_count": 82,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "implement an in-house article retriever plugin",
    "contentLower": "to package articles from a custom knowledge base, you need to implement an article retriever. an article retriever is a java class that implements the com.hp.saw.upload.articleretriever interface. the retriever reads the contents of the articles from the knowledge base and formats them as com.hp.saw.upload.article objects. for an example, see the default java implementations, found in the <packaging_tool_deployment>/saw-article-packaging-tool/src folder: com.hp.saw.upload.folder.folderarticleretriever and com.hp.saw.upload.folder.filesystemarticle. the knowledge packaging tool source code is found in the same folder in the /upload folder. to implement the retriever plugin and package the retrieved files: implement the java interface com.hp.saw.upload.articleretriever (provided in the jar file). this interface requires implementing the getarticles() method, which returns article objects that represent your knowledge articles. articleretriever interface source code note located in <packa",
    "keywordsLower": [
      "metadata.xml",
      "knowledgearticles.zip",
      "info.log",
      "summary.log",
      "file1.html",
      "error.log",
      "com.hp",
      "articles.zip",
      "article_packager.bat",
      "file2.html",
      "implement",
      "in-house",
      "article",
      "retriever",
      "plugin",
      "package",
      "articles",
      "custom",
      "knowledge",
      "base",
      "need",
      "retriever.",
      "java",
      "class",
      "implements",
      "com.hp.saw.upload.articleretriever",
      "interface.",
      "reads",
      "contents",
      "formats",
      "com.hp.saw.upload.article",
      "objects.",
      "example",
      "see",
      "default",
      "implementations",
      "found",
      "saw-article-packaging-tool",
      "src",
      "folder",
      "com.hp.saw.upload.folder.folderarticleretriever",
      "com.hp.saw.upload.folder.filesystemarticle.",
      "packaging",
      "tool",
      "source",
      "code",
      "same",
      "upload",
      "folder.",
      "retrieved",
      "files",
      "interface",
      "provided",
      "jar",
      "file",
      "requires",
      "implementing",
      "getarticles",
      "method",
      "returns",
      "objects",
      "represent",
      "articles.",
      "articleretriever",
      "note",
      "located",
      "src.",
      "public",
      "initializes",
      "arguments",
      "packager",
      "command",
      "line",
      "enables",
      "implementation",
      "get",
      "external",
      "parameters",
      "identifying",
      "such",
      "database",
      "connections",
      "path",
      "etc",
      "param",
      "retrieverargs",
      "list",
      "params",
      "void",
      "init",
      "return",
      "represents",
      "document",
      "ess",
      "system.",
      "having",
      "id",
      "title",
      "content.",
      "single"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import knowledge article packages",
    "content": "Service Management enables you to import knowledge articles into the Knowledge Management module. The Knowledge Packaging tool packages your articles into a ZIP file. For details, see Packaging and importing knowledge articles. Prerequisite: Ensure that you have the ZIP file containing the packaged articles. From the main menu, select Build > Knowledge > Articles. Click Import. The Import articles dialog box opens. Click Select package and browse to the location of the ZIP file that contains the prepared articles. Select a workflow phase for the articles. All articles in the package will be uploaded to Service Management at the selected workflow phase. For example: For pre-published phases, select Review or Publish For published articles, select Internal or External If desired, select the Override phase check box to force Service Management to override any existing phase when an article is imported. Click Import. Note The process may take some time, depending on the number of articles ",
    "url": "importarticles",
    "filename": "importarticles",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "import",
      "knowledge",
      "article",
      "packages",
      "related",
      "topics",
      "service",
      "management",
      "enables",
      "articles",
      "module.",
      "packaging",
      "tool",
      "zip",
      "file.",
      "details",
      "see",
      "importing",
      "articles.",
      "prerequisite",
      "ensure",
      "file",
      "containing",
      "packaged",
      "main",
      "menu",
      "select",
      "build",
      "click",
      "import.",
      "dialog",
      "box",
      "opens.",
      "package",
      "browse",
      "location",
      "contains",
      "prepared",
      "workflow",
      "phase",
      "all",
      "uploaded",
      "selected",
      "phase.",
      "example",
      "pre-published",
      "phases",
      "review",
      "publish",
      "published",
      "internal",
      "external",
      "desired",
      "override",
      "check",
      "force",
      "any",
      "existing",
      "imported.",
      "note",
      "process",
      "take",
      "time",
      "depending",
      "number",
      "author",
      "field",
      "blank.",
      "assign",
      "imported",
      "update",
      "manually",
      "information",
      "create",
      "news",
      "article.",
      "same",
      "id",
      "new",
      "content",
      "overrides",
      "content.",
      "maximum",
      "size",
      "100",
      "mb.",
      "index",
      "idol",
      "connectors",
      "quickstart",
      "export",
      "manager"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import knowledge article packages",
    "contentLower": "service management enables you to import knowledge articles into the knowledge management module. the knowledge packaging tool packages your articles into a zip file. for details, see packaging and importing knowledge articles. prerequisite: ensure that you have the zip file containing the packaged articles. from the main menu, select build > knowledge > articles. click import. the import articles dialog box opens. click select package and browse to the location of the zip file that contains the prepared articles. select a workflow phase for the articles. all articles in the package will be uploaded to service management at the selected workflow phase. for example: for pre-published phases, select review or publish for published articles, select internal or external if desired, select the override phase check box to force service management to override any existing phase when an article is imported. click import. note the process may take some time, depending on the number of articles ",
    "keywordsLower": [
      "import",
      "knowledge",
      "article",
      "packages",
      "related",
      "topics",
      "service",
      "management",
      "enables",
      "articles",
      "module.",
      "packaging",
      "tool",
      "zip",
      "file.",
      "details",
      "see",
      "importing",
      "articles.",
      "prerequisite",
      "ensure",
      "file",
      "containing",
      "packaged",
      "main",
      "menu",
      "select",
      "build",
      "click",
      "import.",
      "dialog",
      "box",
      "opens.",
      "package",
      "browse",
      "location",
      "contains",
      "prepared",
      "workflow",
      "phase",
      "all",
      "uploaded",
      "selected",
      "phase.",
      "example",
      "pre-published",
      "phases",
      "review",
      "publish",
      "published",
      "internal",
      "external",
      "desired",
      "override",
      "check",
      "force",
      "any",
      "existing",
      "imported.",
      "note",
      "process",
      "take",
      "time",
      "depending",
      "number",
      "author",
      "field",
      "blank.",
      "assign",
      "imported",
      "update",
      "manually",
      "information",
      "create",
      "news",
      "article.",
      "same",
      "id",
      "new",
      "content",
      "overrides",
      "content.",
      "maximum",
      "size",
      "100",
      "mb.",
      "index",
      "idol",
      "connectors",
      "quickstart",
      "export",
      "manager"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Infrastructure &amp; peripheral assets",
    "content": "Infrastructure & peripheral assets include, but aren't limited to items such as projectors, power supplies, fire protection systems, air-conditioning systems, RFID systems, and so on. You can expand the available subtypes to be managed. For example, a parking system. Related topics How to create an infrastructure & peripheral asset record How to edit an infrastructure & peripheral asset record How to delete an infrastructure & peripheral asset record Infrastructure & peripheral asset workflow",
    "url": "infraperiphassets",
    "filename": "infraperiphassets",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "infrastructure",
      "amp",
      "peripheral",
      "assets",
      "related",
      "topics",
      "include",
      "aren",
      "limited",
      "items",
      "such",
      "projectors",
      "power",
      "supplies",
      "fire",
      "protection",
      "systems",
      "air-conditioning",
      "rfid",
      "on.",
      "expand",
      "available",
      "subtypes",
      "managed.",
      "example",
      "parking",
      "system.",
      "create",
      "asset",
      "record",
      "edit",
      "delete",
      "workflow"
    ],
    "language": "en",
    "word_count": 52,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "infrastructure &amp; peripheral assets",
    "contentLower": "infrastructure & peripheral assets include, but aren't limited to items such as projectors, power supplies, fire protection systems, air-conditioning systems, rfid systems, and so on. you can expand the available subtypes to be managed. for example, a parking system. related topics how to create an infrastructure & peripheral asset record how to edit an infrastructure & peripheral asset record how to delete an infrastructure & peripheral asset record infrastructure & peripheral asset workflow",
    "keywordsLower": [
      "infrastructure",
      "amp",
      "peripheral",
      "assets",
      "related",
      "topics",
      "include",
      "aren",
      "limited",
      "items",
      "such",
      "projectors",
      "power",
      "supplies",
      "fire",
      "protection",
      "systems",
      "air-conditioning",
      "rfid",
      "on.",
      "expand",
      "available",
      "subtypes",
      "managed.",
      "example",
      "parking",
      "system.",
      "create",
      "asset",
      "record",
      "edit",
      "delete",
      "workflow"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Infrastructure &amp; peripheral asset workflow",
    "content": "This section describes the metaphases and subordinate phases in the life cycle of infrastructure and peripheral assets. The workflow relies on business rules. Rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. For more information about the out-of-the-box business rules defined for the contract workflow, see Infrastructure & peripheral process - Business rules. Metaphase: Unavailable The asset is entered into the system and still awaiting delivery. Phase Transition Description New Automatic New is a starting point. Next phase: Awaiting delivery Awaiting delivery Manual The infrastructure & peripheral asset is in transit from a vendor or other provider. Next phase: Receive or Canceled Example: the organization is completing a new demonstration facility. It needs a projector. The appropriate manager requests a new projector to be acquired. A procurement team member creates a record for the new projector whe",
    "url": "infraperiphwflw",
    "filename": "infraperiphwflw",
    "headings": [
      "Metaphase: Unavailable",
      "Metaphase: Available",
      "Metaphase: In use",
      "Metaphase: Retiring",
      "Metaphase: End of life (End)",
      "Related topics"
    ],
    "keywords": [
      "infrastructure",
      "amp",
      "peripheral",
      "asset",
      "workflow",
      "metaphase",
      "unavailable",
      "available",
      "retiring",
      "end",
      "life",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "cycle",
      "assets.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "out-of-the-box",
      "defined",
      "contract",
      "see",
      "process",
      "entered",
      "system",
      "still",
      "awaiting",
      "delivery.",
      "transition",
      "description",
      "new",
      "automatic",
      "starting",
      "point.",
      "next",
      "delivery",
      "manual",
      "transit",
      "vendor",
      "provider.",
      "receive",
      "canceled",
      "example",
      "organization",
      "completing",
      "demonstration",
      "facility.",
      "needs",
      "projector.",
      "appropriate",
      "manager",
      "requests",
      "projector",
      "acquired.",
      "procurement",
      "team",
      "member",
      "creates",
      "record",
      "places",
      "order",
      "external",
      "vendor.",
      "moves",
      "because",
      "isn",
      "yet.",
      "complete",
      "move",
      "manually",
      "final",
      "canceled.",
      "arrived",
      "use.",
      "delivered",
      "yet",
      "fulfill",
      "request.",
      "stock",
      "request"
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "infrastructure &amp; peripheral asset workflow",
    "contentLower": "this section describes the metaphases and subordinate phases in the life cycle of infrastructure and peripheral assets. the workflow relies on business rules. rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. for more information about the out-of-the-box business rules defined for the contract workflow, see infrastructure & peripheral process - business rules. metaphase: unavailable the asset is entered into the system and still awaiting delivery. phase transition description new automatic new is a starting point. next phase: awaiting delivery awaiting delivery manual the infrastructure & peripheral asset is in transit from a vendor or other provider. next phase: receive or canceled example: the organization is completing a new demonstration facility. it needs a projector. the appropriate manager requests a new projector to be acquired. a procurement team member creates a record for the new projector whe",
    "keywordsLower": [
      "infrastructure",
      "amp",
      "peripheral",
      "asset",
      "workflow",
      "metaphase",
      "unavailable",
      "available",
      "retiring",
      "end",
      "life",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "cycle",
      "assets.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "out-of-the-box",
      "defined",
      "contract",
      "see",
      "process",
      "entered",
      "system",
      "still",
      "awaiting",
      "delivery.",
      "transition",
      "description",
      "new",
      "automatic",
      "starting",
      "point.",
      "next",
      "delivery",
      "manual",
      "transit",
      "vendor",
      "provider.",
      "receive",
      "canceled",
      "example",
      "organization",
      "completing",
      "demonstration",
      "facility.",
      "needs",
      "projector.",
      "appropriate",
      "manager",
      "requests",
      "projector",
      "acquired.",
      "procurement",
      "team",
      "member",
      "creates",
      "record",
      "places",
      "order",
      "external",
      "vendor.",
      "moves",
      "because",
      "isn",
      "yet.",
      "complete",
      "move",
      "manually",
      "final",
      "canceled.",
      "arrived",
      "use.",
      "delivered",
      "yet",
      "fulfill",
      "request.",
      "stock",
      "request"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Identification and synchronization",
    "content": "Service Management makes web services available to identify network-attached devices. Service Management uses matching algorithms to classify new components according to the SACM model. Synchronization updates Service Management with information gathered from your remote asset management system, such as UCMDB, using the On-Premises Bridge. For more information about identification and synchronization, see the following sections in the UCMDB integration documentation: Identification Synchronization",
    "url": "identificationsync",
    "filename": "identificationsync",
    "headings": [],
    "keywords": [
      "identification",
      "synchronization",
      "service",
      "management",
      "makes",
      "web",
      "services",
      "available",
      "identify",
      "network-attached",
      "devices.",
      "uses",
      "matching",
      "algorithms",
      "classify",
      "new",
      "components",
      "according",
      "sacm",
      "model.",
      "updates",
      "information",
      "gathered",
      "remote",
      "asset",
      "system",
      "such",
      "ucmdb",
      "on-premises",
      "bridge.",
      "about",
      "see",
      "following",
      "sections",
      "integration",
      "documentation"
    ],
    "language": "en",
    "word_count": 48,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "identification and synchronization",
    "contentLower": "service management makes web services available to identify network-attached devices. service management uses matching algorithms to classify new components according to the sacm model. synchronization updates service management with information gathered from your remote asset management system, such as ucmdb, using the on-premises bridge. for more information about identification and synchronization, see the following sections in the ucmdb integration documentation: identification synchronization",
    "keywordsLower": [
      "identification",
      "synchronization",
      "service",
      "management",
      "makes",
      "web",
      "services",
      "available",
      "identify",
      "network-attached",
      "devices.",
      "uses",
      "matching",
      "algorithms",
      "classify",
      "new",
      "components",
      "according",
      "sacm",
      "model.",
      "updates",
      "information",
      "gathered",
      "remote",
      "asset",
      "system",
      "such",
      "ucmdb",
      "on-premises",
      "bridge.",
      "about",
      "see",
      "following",
      "sections",
      "integration",
      "documentation"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Live Support",
    "content": "Service Management's Service Request Management module includes Live Support for dealing with telephone support calls. With the appropriate configuration and an integrated CTI system, Live Support provides you with key information and tools that can help to efficiently process new requests and inquiries received by telephone. For example, if your system is appropriately configured, when you receive a call from a user, Service Management can automatically identify the caller's details and recent requests. For more information about Live Support, see How to configure Live Support with CTI. Related topics How to deal with a Live Support call",
    "url": "livesupport",
    "filename": "livesupport",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "live",
      "support",
      "related",
      "topics",
      "service",
      "management",
      "request",
      "module",
      "includes",
      "dealing",
      "telephone",
      "calls.",
      "appropriate",
      "configuration",
      "integrated",
      "cti",
      "system",
      "provides",
      "key",
      "information",
      "tools",
      "help",
      "efficiently",
      "process",
      "new",
      "requests",
      "inquiries",
      "received",
      "telephone.",
      "example",
      "appropriately",
      "configured",
      "receive",
      "call",
      "user",
      "automatically",
      "identify",
      "caller",
      "details",
      "recent",
      "requests.",
      "about",
      "see",
      "configure",
      "cti.",
      "deal"
    ],
    "language": "en",
    "word_count": 66,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "live support",
    "contentLower": "service management's service request management module includes live support for dealing with telephone support calls. with the appropriate configuration and an integrated cti system, live support provides you with key information and tools that can help to efficiently process new requests and inquiries received by telephone. for example, if your system is appropriately configured, when you receive a call from a user, service management can automatically identify the caller's details and recent requests. for more information about live support, see how to configure live support with cti. related topics how to deal with a live support call",
    "keywordsLower": [
      "live",
      "support",
      "related",
      "topics",
      "service",
      "management",
      "request",
      "module",
      "includes",
      "dealing",
      "telephone",
      "calls.",
      "appropriate",
      "configuration",
      "integrated",
      "cti",
      "system",
      "provides",
      "key",
      "information",
      "tools",
      "help",
      "efficiently",
      "process",
      "new",
      "requests",
      "inquiries",
      "received",
      "telephone.",
      "example",
      "appropriately",
      "configured",
      "receive",
      "call",
      "user",
      "automatically",
      "identify",
      "caller",
      "details",
      "recent",
      "requests.",
      "about",
      "see",
      "configure",
      "cti.",
      "deal"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Keyboard shortcuts in Live Support",
    "content": "The following keyboard shortcuts are available in Live Support. When you create a service request: Task Shortcut Log request Alt+N After selecting an offering from search results, log a new offering request Alt+S After selecting an article from search results, log a new request and propose the article as a solution to the newly logged request Alt+S After selecting an incident from search results, log a new request and relate the incident to the newly logged request Alt+S Save new service request Ctrl+Enter When you work on the Recent requests section: Task Shortcut After selecting a request, to work on it Alt+W When you work on an existing request: Task Shortcut Display full request details Alt+D After selecting an article or question from search results, propose it as a solution C, S After selecting an incident from search results, copy the incident solution to the request solution C, S After selecting an incident from search results, link the incident to the request L, R Save Ctrl+S ",
    "url": "srmkeyboardshortcuts",
    "filename": "srmkeyboardshortcuts",
    "headings": [
      "When you create a service request:",
      "When you work on the Recent requests section:",
      "When you work on an existing request:",
      "General:",
      "Related topics"
    ],
    "keywords": [
      "keyboard",
      "shortcuts",
      "live",
      "support",
      "create",
      "service",
      "request",
      "work",
      "recent",
      "requests",
      "section",
      "existing",
      "general",
      "related",
      "topics",
      "following",
      "available",
      "support.",
      "task",
      "shortcut",
      "log",
      "alt",
      "after",
      "selecting",
      "offering",
      "search",
      "results",
      "new",
      "article",
      "propose",
      "solution",
      "newly",
      "logged",
      "incident",
      "relate",
      "save",
      "ctrl",
      "enter",
      "display",
      "full",
      "details",
      "question",
      "copy",
      "link",
      "next",
      "hide",
      "show",
      "select",
      "previous",
      "result",
      "list",
      "key",
      "view",
      "selected",
      "add",
      "favorites",
      "shift",
      "remove",
      "open",
      "caller",
      "chat",
      "capability"
    ],
    "language": "en",
    "word_count": 125,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "keyboard shortcuts in live support",
    "contentLower": "the following keyboard shortcuts are available in live support. when you create a service request: task shortcut log request alt+n after selecting an offering from search results, log a new offering request alt+s after selecting an article from search results, log a new request and propose the article as a solution to the newly logged request alt+s after selecting an incident from search results, log a new request and relate the incident to the newly logged request alt+s save new service request ctrl+enter when you work on the recent requests section: task shortcut after selecting a request, to work on it alt+w when you work on an existing request: task shortcut display full request details alt+d after selecting an article or question from search results, propose it as a solution c, s after selecting an incident from search results, copy the incident solution to the request solution c, s after selecting an incident from search results, link the incident to the request l, r save ctrl+s ",
    "keywordsLower": [
      "keyboard",
      "shortcuts",
      "live",
      "support",
      "create",
      "service",
      "request",
      "work",
      "recent",
      "requests",
      "section",
      "existing",
      "general",
      "related",
      "topics",
      "following",
      "available",
      "support.",
      "task",
      "shortcut",
      "log",
      "alt",
      "after",
      "selecting",
      "offering",
      "search",
      "results",
      "new",
      "article",
      "propose",
      "solution",
      "newly",
      "logged",
      "incident",
      "relate",
      "save",
      "ctrl",
      "enter",
      "display",
      "full",
      "details",
      "question",
      "copy",
      "link",
      "next",
      "hide",
      "show",
      "select",
      "previous",
      "result",
      "list",
      "key",
      "view",
      "selected",
      "add",
      "favorites",
      "shift",
      "remove",
      "open",
      "caller",
      "chat",
      "capability"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Live Support request form details",
    "content": "This page explains the fields on the Live support request form. Field Description Title Type the title of the request. Description Enter a description for the request. Service Management may automatically recognize CIs whose details are entered here. See CI Detection - Automatic recognition of CIs. Requested by Select the name of the user who contacted the service desk about opening the request. Select a name from the drop-down list or type a few characters to search for the name. Requested for Select the name of the user for whom you are opening the request. Select a name from the drop-down list or type a few characters to search for the name. Impact Select the scope of impact for the request. Urgency Select how severely the problem for which you are requesting assistance impacts your job. Preferred contact method Select a value from the drop-down. Offering Select an offering. At the top of the list of offerings Service Management displays smart suggestions for suitable offerings, bas",
    "url": "livesupportrequestformdetails",
    "filename": "livesupportrequestformdetails",
    "headings": [],
    "keywords": [
      "live",
      "support",
      "request",
      "form",
      "details",
      "page",
      "explains",
      "fields",
      "form.",
      "field",
      "description",
      "title",
      "type",
      "request.",
      "enter",
      "service",
      "management",
      "automatically",
      "recognize",
      "cis",
      "whose",
      "entered",
      "here.",
      "see",
      "ci",
      "detection",
      "automatic",
      "recognition",
      "cis.",
      "requested",
      "select",
      "name",
      "user",
      "contacted",
      "desk",
      "about",
      "opening",
      "drop-down",
      "list",
      "few",
      "characters",
      "search",
      "name.",
      "whom",
      "impact",
      "scope",
      "urgency",
      "severely",
      "problem",
      "requesting",
      "assistance",
      "impacts",
      "job.",
      "preferred",
      "contact",
      "method",
      "value",
      "drop-down.",
      "offering",
      "offering.",
      "top",
      "offerings",
      "displays",
      "smart",
      "suggestions",
      "suitable",
      "based",
      "text",
      "fields.",
      "information",
      "catalog",
      "management.",
      "isn",
      "selected",
      "created",
      "later",
      "changes",
      "hr",
      "case",
      "options",
      "defined",
      "appear",
      "section.",
      "all",
      "business",
      "infrastructure",
      "services.",
      "filter",
      "entire",
      "individual",
      "columns",
      "narrow",
      "records.",
      "point",
      "popup",
      "window",
      "displayed",
      "including",
      "brief",
      "link"
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "live support request form details",
    "contentLower": "this page explains the fields on the live support request form. field description title type the title of the request. description enter a description for the request. service management may automatically recognize cis whose details are entered here. see ci detection - automatic recognition of cis. requested by select the name of the user who contacted the service desk about opening the request. select a name from the drop-down list or type a few characters to search for the name. requested for select the name of the user for whom you are opening the request. select a name from the drop-down list or type a few characters to search for the name. impact select the scope of impact for the request. urgency select how severely the problem for which you are requesting assistance impacts your job. preferred contact method select a value from the drop-down. offering select an offering. at the top of the list of offerings service management displays smart suggestions for suitable offerings, bas",
    "keywordsLower": [
      "live",
      "support",
      "request",
      "form",
      "details",
      "page",
      "explains",
      "fields",
      "form.",
      "field",
      "description",
      "title",
      "type",
      "request.",
      "enter",
      "service",
      "management",
      "automatically",
      "recognize",
      "cis",
      "whose",
      "entered",
      "here.",
      "see",
      "ci",
      "detection",
      "automatic",
      "recognition",
      "cis.",
      "requested",
      "select",
      "name",
      "user",
      "contacted",
      "desk",
      "about",
      "opening",
      "drop-down",
      "list",
      "few",
      "characters",
      "search",
      "name.",
      "whom",
      "impact",
      "scope",
      "urgency",
      "severely",
      "problem",
      "requesting",
      "assistance",
      "impacts",
      "job.",
      "preferred",
      "contact",
      "method",
      "value",
      "drop-down.",
      "offering",
      "offering.",
      "top",
      "offerings",
      "displays",
      "smart",
      "suggestions",
      "suitable",
      "based",
      "text",
      "fields.",
      "information",
      "catalog",
      "management.",
      "isn",
      "selected",
      "created",
      "later",
      "changes",
      "hr",
      "case",
      "options",
      "defined",
      "appear",
      "section.",
      "all",
      "business",
      "infrastructure",
      "services.",
      "filter",
      "entire",
      "individual",
      "columns",
      "narrow",
      "records.",
      "point",
      "popup",
      "window",
      "displayed",
      "including",
      "brief",
      "link"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Live Support call with location based support enabled",
    "content": "If the location-based support is enabled for Live Support, the user interface displays as detailed on this page. Note The main differences from the default user interface are: Ability to select a location in the Information Ribbon. Different modes. Options in: Recent requests list (depending on mode). Related CIs part of the Suggestions Bar. To go to the Live Support page, select Run > Service Request > Live Support. The Live Support page contains the following: Information Ribbon UI element Description REQUEST ID Allows you to find a request by its ID. Click the Request ID link. A search box is displayed. Type the ID in the search box, and press Enter. The request is displayed. PERSON Displays the caller's name. If the caller's name isn't automatically displayed, you can find it in one of the following ways: Click the list icon to display a simple list of eligible values. When the list of values is long, you can type a few characters from the start of the caller's name into the search",
    "url": "livesupportlocationbased",
    "filename": "livesupportlocationbased",
    "headings": [
      "Information Ribbon",
      "Recent requests list",
      "Suggestions Bar, List Pane, and Preview Pane",
      "Search box",
      "Save & next caller/Save & next request",
      "New request form",
      "Related topics"
    ],
    "keywords": [
      "live",
      "support",
      "call",
      "location",
      "based",
      "enabled",
      "information",
      "ribbon",
      "recent",
      "requests",
      "list",
      "suggestions",
      "bar",
      "pane",
      "preview",
      "search",
      "box",
      "save",
      "next",
      "caller",
      "request",
      "new",
      "form",
      "related",
      "topics",
      "location-based",
      "user",
      "interface",
      "displays",
      "detailed",
      "page.",
      "note",
      "main",
      "differences",
      "default",
      "ability",
      "select",
      "ribbon.",
      "different",
      "modes.",
      "options",
      "depending",
      "mode",
      "cis",
      "part",
      "bar.",
      "go",
      "page",
      "run",
      "service",
      "support.",
      "contains",
      "following",
      "ui",
      "element",
      "description",
      "id",
      "allows",
      "find",
      "id.",
      "click",
      "link.",
      "displayed.",
      "type",
      "press",
      "enter.",
      "person",
      "name.",
      "name",
      "isn",
      "automatically",
      "displayed",
      "one",
      "ways",
      "icon",
      "display",
      "simple",
      "eligible",
      "values.",
      "values",
      "long",
      "few",
      "characters",
      "start",
      "field",
      "limit",
      "results.",
      "expanded",
      "record",
      "format.",
      "appear",
      "view",
      "useful",
      "information.",
      "after",
      "finding",
      "info",
      "enter",
      "mode.",
      "see"
    ],
    "language": "en",
    "word_count": 117,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "live support call with location based support enabled",
    "contentLower": "if the location-based support is enabled for live support, the user interface displays as detailed on this page. note the main differences from the default user interface are: ability to select a location in the information ribbon. different modes. options in: recent requests list (depending on mode). related cis part of the suggestions bar. to go to the live support page, select run > service request > live support. the live support page contains the following: information ribbon ui element description request id allows you to find a request by its id. click the request id link. a search box is displayed. type the id in the search box, and press enter. the request is displayed. person displays the caller's name. if the caller's name isn't automatically displayed, you can find it in one of the following ways: click the list icon to display a simple list of eligible values. when the list of values is long, you can type a few characters from the start of the caller's name into the search",
    "keywordsLower": [
      "live",
      "support",
      "call",
      "location",
      "based",
      "enabled",
      "information",
      "ribbon",
      "recent",
      "requests",
      "list",
      "suggestions",
      "bar",
      "pane",
      "preview",
      "search",
      "box",
      "save",
      "next",
      "caller",
      "request",
      "new",
      "form",
      "related",
      "topics",
      "location-based",
      "user",
      "interface",
      "displays",
      "detailed",
      "page.",
      "note",
      "main",
      "differences",
      "default",
      "ability",
      "select",
      "ribbon.",
      "different",
      "modes.",
      "options",
      "depending",
      "mode",
      "cis",
      "part",
      "bar.",
      "go",
      "page",
      "run",
      "service",
      "support.",
      "contains",
      "following",
      "ui",
      "element",
      "description",
      "id",
      "allows",
      "find",
      "id.",
      "click",
      "link.",
      "displayed.",
      "type",
      "press",
      "enter.",
      "person",
      "name.",
      "name",
      "isn",
      "automatically",
      "displayed",
      "one",
      "ways",
      "icon",
      "display",
      "simple",
      "eligible",
      "values.",
      "values",
      "long",
      "few",
      "characters",
      "start",
      "field",
      "limit",
      "results.",
      "expanded",
      "record",
      "format.",
      "appear",
      "view",
      "useful",
      "information.",
      "after",
      "finding",
      "info",
      "enter",
      "mode.",
      "see"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident Management",
    "content": "Incident Management enables you to restore normal service operation as quickly as possible and minimize the adverse impact on business operations. You can access incidents from the Incident landing page enabling you to easily view the events and incidents of most interest. For more information, see Incident Management Landing Page. Incident Management does the following: Helps you categorize and track different types of service interruptions, such as service unavailability, performance issues, hardware, or software failures. Ensures that incidents are resolved within agreed on service level targets. This approach ensures that the best possible levels of service quality and availability are maintained. Incident Analysts can escalate and reassign incidents. Incident Management can also automatically issue alerts or escalate an incident automatically to meet the agreed-upon terms of the service contract. For example, if a network printer is disabled, an Analyst can escalate the incident t",
    "url": "incidentmgmt",
    "filename": "incidentmgmt",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "incident",
      "management",
      "related",
      "topics",
      "enables",
      "restore",
      "normal",
      "service",
      "operation",
      "quickly",
      "possible",
      "minimize",
      "adverse",
      "impact",
      "business",
      "operations.",
      "access",
      "incidents",
      "landing",
      "page",
      "enabling",
      "easily",
      "view",
      "events",
      "most",
      "interest.",
      "information",
      "see",
      "page.",
      "following",
      "helps",
      "categorize",
      "track",
      "different",
      "types",
      "interruptions",
      "such",
      "unavailability",
      "performance",
      "issues",
      "hardware",
      "software",
      "failures.",
      "ensures",
      "resolved",
      "agreed",
      "level",
      "targets.",
      "approach",
      "best",
      "levels",
      "quality",
      "availability",
      "maintained.",
      "analysts",
      "escalate",
      "reassign",
      "incidents.",
      "automatically",
      "issue",
      "alerts",
      "meet",
      "agreed-upon",
      "terms",
      "contract.",
      "example",
      "network",
      "printer",
      "disabled",
      "analyst",
      "higher",
      "priority",
      "ensure",
      "quickly.",
      "organization",
      "maintains",
      "availability.",
      "includes",
      "communicated",
      "directly",
      "end",
      "users",
      "either",
      "through",
      "desk",
      "employee",
      "self",
      "interface.",
      "automate",
      "reporting",
      "tracking",
      "single",
      "group",
      "achieve",
      "results",
      "keep",
      "resolution.",
      "require",
      "records",
      "follow"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident management",
    "contentLower": "incident management enables you to restore normal service operation as quickly as possible and minimize the adverse impact on business operations. you can access incidents from the incident landing page enabling you to easily view the events and incidents of most interest. for more information, see incident management landing page. incident management does the following: helps you categorize and track different types of service interruptions, such as service unavailability, performance issues, hardware, or software failures. ensures that incidents are resolved within agreed on service level targets. this approach ensures that the best possible levels of service quality and availability are maintained. incident analysts can escalate and reassign incidents. incident management can also automatically issue alerts or escalate an incident automatically to meet the agreed-upon terms of the service contract. for example, if a network printer is disabled, an analyst can escalate the incident t",
    "keywordsLower": [
      "incident",
      "management",
      "related",
      "topics",
      "enables",
      "restore",
      "normal",
      "service",
      "operation",
      "quickly",
      "possible",
      "minimize",
      "adverse",
      "impact",
      "business",
      "operations.",
      "access",
      "incidents",
      "landing",
      "page",
      "enabling",
      "easily",
      "view",
      "events",
      "most",
      "interest.",
      "information",
      "see",
      "page.",
      "following",
      "helps",
      "categorize",
      "track",
      "different",
      "types",
      "interruptions",
      "such",
      "unavailability",
      "performance",
      "issues",
      "hardware",
      "software",
      "failures.",
      "ensures",
      "resolved",
      "agreed",
      "level",
      "targets.",
      "approach",
      "best",
      "levels",
      "quality",
      "availability",
      "maintained.",
      "analysts",
      "escalate",
      "reassign",
      "incidents.",
      "automatically",
      "issue",
      "alerts",
      "meet",
      "agreed-upon",
      "terms",
      "contract.",
      "example",
      "network",
      "printer",
      "disabled",
      "analyst",
      "higher",
      "priority",
      "ensure",
      "quickly.",
      "organization",
      "maintains",
      "availability.",
      "includes",
      "communicated",
      "directly",
      "end",
      "users",
      "either",
      "through",
      "desk",
      "employee",
      "self",
      "interface.",
      "automate",
      "reporting",
      "tracking",
      "single",
      "group",
      "achieve",
      "results",
      "keep",
      "resolution.",
      "require",
      "records",
      "follow"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident Management landing page",
    "content": "The landing page for Incident Management includes the following features that give you easier access to the incidents you are most interested in seeing. Full text and integer search Enables you to search for an incident based on its incident number, title, or description. Direct widget links to favorite filters The following filters can be accessed from the corresponding widgets on the landing page. When selected, these take you directly to the same favorite filters on the incidents page. Active incidents Owned by me Assigned to me Ready to assign You can also click the New Incident widget to define a new incident. For more information, see How to create an incident record. Related topics How to create an incident record How to edit an incident record Incident templates Incident Management roles and permissions Incident workflow Learn more about Incident Management",
    "url": "incidentlanding",
    "filename": "incidentlanding",
    "headings": [
      "Full text and integer search",
      "Direct widget links to favorite filters",
      "Related topics"
    ],
    "keywords": [
      "incident",
      "management",
      "landing",
      "page",
      "full",
      "text",
      "integer",
      "search",
      "direct",
      "widget",
      "links",
      "favorite",
      "filters",
      "related",
      "topics",
      "includes",
      "following",
      "features",
      "give",
      "easier",
      "access",
      "incidents",
      "most",
      "interested",
      "seeing.",
      "enables",
      "based",
      "number",
      "title",
      "description.",
      "accessed",
      "corresponding",
      "widgets",
      "page.",
      "selected",
      "take",
      "directly",
      "same",
      "active",
      "owned",
      "assigned",
      "ready",
      "assign",
      "click",
      "new",
      "define",
      "incident.",
      "information",
      "see",
      "create",
      "record.",
      "record",
      "edit",
      "templates",
      "roles",
      "permissions",
      "workflow",
      "learn",
      "about"
    ],
    "language": "en",
    "word_count": 88,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident management landing page",
    "contentLower": "the landing page for incident management includes the following features that give you easier access to the incidents you are most interested in seeing. full text and integer search enables you to search for an incident based on its incident number, title, or description. direct widget links to favorite filters the following filters can be accessed from the corresponding widgets on the landing page. when selected, these take you directly to the same favorite filters on the incidents page. active incidents owned by me assigned to me ready to assign you can also click the new incident widget to define a new incident. for more information, see how to create an incident record. related topics how to create an incident record how to edit an incident record incident templates incident management roles and permissions incident workflow learn more about incident management",
    "keywordsLower": [
      "incident",
      "management",
      "landing",
      "page",
      "full",
      "text",
      "integer",
      "search",
      "direct",
      "widget",
      "links",
      "favorite",
      "filters",
      "related",
      "topics",
      "includes",
      "following",
      "features",
      "give",
      "easier",
      "access",
      "incidents",
      "most",
      "interested",
      "seeing.",
      "enables",
      "based",
      "number",
      "title",
      "description.",
      "accessed",
      "corresponding",
      "widgets",
      "page.",
      "selected",
      "take",
      "directly",
      "same",
      "active",
      "owned",
      "assigned",
      "ready",
      "assign",
      "click",
      "new",
      "define",
      "incident.",
      "information",
      "see",
      "create",
      "record.",
      "record",
      "edit",
      "templates",
      "roles",
      "permissions",
      "workflow",
      "learn",
      "about"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident Management roles and permissions",
    "content": "There are specific roles associated with Incident Management. Service Management uses permissions based on roles to enable you to complete a task that's appropriate to your role. System Administrators manage and assign these permissions. By default, there are the following roles assigned to Incident Management. These roles support incident tracking and process ownership. Role Responsibilities Incident Analyst Reviews and accepts or rejects assigned incidents. Investigates and diagnoses incidents. Documents incident resolutions or workarounds. Implements incident resolutions. Verifies that incidents are resolved. Closes incidents. Incident Coordinator Reviews and accepts or rejects incidents assigned to the support group. Manages incidents escalated by an Incident Analyst. Monitors Operational Level Agreements (OLA) and Underpinning Contracts (UC). Incident Manager Manages incidents escalated by an Incident Coordinator or by a Service Desk Agent. Determines and performs the appropriate ",
    "url": "imroles",
    "filename": "imroles",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "incident",
      "management",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "management.",
      "service",
      "uses",
      "based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "system",
      "administrators",
      "manage",
      "assign",
      "permissions.",
      "default",
      "following",
      "assigned",
      "support",
      "tracking",
      "process",
      "ownership.",
      "role",
      "responsibilities",
      "analyst",
      "reviews",
      "accepts",
      "rejects",
      "incidents.",
      "investigates",
      "diagnoses",
      "documents",
      "resolutions",
      "workarounds.",
      "implements",
      "resolutions.",
      "verifies",
      "incidents",
      "resolved.",
      "closes",
      "coordinator",
      "group.",
      "manages",
      "escalated",
      "analyst.",
      "monitors",
      "operational",
      "level",
      "agreements",
      "ola",
      "underpinning",
      "contracts",
      "uc",
      "manager",
      "desk",
      "agent.",
      "determines",
      "performs",
      "escalation",
      "actions.",
      "requests",
      "emergency",
      "changes.",
      "owner",
      "sponsors",
      "designs",
      "process.",
      "ensures",
      "compliance",
      "enterprise",
      "policies.",
      "identifies",
      "key",
      "performance",
      "indicators",
      "metrics.",
      "integrates",
      "processes.",
      "champions",
      "continual",
      "improvement",
      "review",
      "individual",
      "permission",
      "assignments",
      "administration",
      "master",
      "data",
      "people",
      "roles."
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident management roles and permissions",
    "contentLower": "there are specific roles associated with incident management. service management uses permissions based on roles to enable you to complete a task that's appropriate to your role. system administrators manage and assign these permissions. by default, there are the following roles assigned to incident management. these roles support incident tracking and process ownership. role responsibilities incident analyst reviews and accepts or rejects assigned incidents. investigates and diagnoses incidents. documents incident resolutions or workarounds. implements incident resolutions. verifies that incidents are resolved. closes incidents. incident coordinator reviews and accepts or rejects incidents assigned to the support group. manages incidents escalated by an incident analyst. monitors operational level agreements (ola) and underpinning contracts (uc). incident manager manages incidents escalated by an incident coordinator or by a service desk agent. determines and performs the appropriate ",
    "keywordsLower": [
      "incident",
      "management",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "management.",
      "service",
      "uses",
      "based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "system",
      "administrators",
      "manage",
      "assign",
      "permissions.",
      "default",
      "following",
      "assigned",
      "support",
      "tracking",
      "process",
      "ownership.",
      "role",
      "responsibilities",
      "analyst",
      "reviews",
      "accepts",
      "rejects",
      "incidents.",
      "investigates",
      "diagnoses",
      "documents",
      "resolutions",
      "workarounds.",
      "implements",
      "resolutions.",
      "verifies",
      "incidents",
      "resolved.",
      "closes",
      "coordinator",
      "group.",
      "manages",
      "escalated",
      "analyst.",
      "monitors",
      "operational",
      "level",
      "agreements",
      "ola",
      "underpinning",
      "contracts",
      "uc",
      "manager",
      "desk",
      "agent.",
      "determines",
      "performs",
      "escalation",
      "actions.",
      "requests",
      "emergency",
      "changes.",
      "owner",
      "sponsors",
      "designs",
      "process.",
      "ensures",
      "compliance",
      "enterprise",
      "policies.",
      "identifies",
      "key",
      "performance",
      "indicators",
      "metrics.",
      "integrates",
      "processes.",
      "champions",
      "continual",
      "improvement",
      "review",
      "individual",
      "permission",
      "assignments",
      "administration",
      "master",
      "data",
      "people",
      "roles."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident workflow",
    "content": "A workflow is the end-to-end process from the incident creation to the incident closure. The building blocks of the workflow are metaphases, phases and transitions. Service Management displays a graphic view of the workflow where you can see the current phase and the transitions that connect the current phase to all other phases. The Incident Management process workflow includes all necessary steps to log and resolve an incident, including any necessary escalations or reassignments. If you are the Incident Coordinator, you can assign an incident to an Incident Analyst for investigation and diagnosis. If the incident is escalated, you might need to reassign the incident to an escalation assignment group. The typical Incident Management workflow contains four metaphases and six phases that lead to closure. When you update a record or assign a new status, the record can transition from one process phase to the next automatically. The Service Management incident workflow reflects ITILv3 pr",
    "url": "imwflw",
    "filename": "imwflw",
    "headings": [
      "Metaphase: Classification",
      "Metaphase: Resolution",
      "Metaphase: Validation",
      "Metaphase: Done (End)",
      "Related topics"
    ],
    "keywords": [
      "incident",
      "workflow",
      "metaphase",
      "classification",
      "resolution",
      "validation",
      "done",
      "end",
      "related",
      "topics",
      "end-to-end",
      "process",
      "creation",
      "closure.",
      "building",
      "blocks",
      "metaphases",
      "phases",
      "transitions.",
      "service",
      "management",
      "displays",
      "graphic",
      "view",
      "see",
      "current",
      "phase",
      "transitions",
      "connect",
      "all",
      "phases.",
      "includes",
      "necessary",
      "steps",
      "log",
      "resolve",
      "including",
      "any",
      "escalations",
      "reassignments.",
      "coordinator",
      "assign",
      "analyst",
      "investigation",
      "diagnosis.",
      "escalated",
      "need",
      "reassign",
      "escalation",
      "assignment",
      "group.",
      "typical",
      "contains",
      "four",
      "six",
      "lead",
      "update",
      "record",
      "new",
      "status",
      "transition",
      "one",
      "next",
      "automatically.",
      "reflects",
      "itilv3",
      "recommendations.",
      "example",
      "depend",
      "value",
      "field",
      "record.",
      "review",
      "closure",
      "occurs",
      "desk",
      "changes",
      "closed.",
      "description",
      "automatic",
      "receives",
      "information",
      "user",
      "about",
      "outage",
      "business",
      "failure",
      "organization",
      "able",
      "fulfill",
      "request.",
      "gathers",
      "required",
      "creates",
      "classify",
      "step",
      "add",
      "enough",
      "issue",
      "hardware"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident workflow",
    "contentLower": "a workflow is the end-to-end process from the incident creation to the incident closure. the building blocks of the workflow are metaphases, phases and transitions. service management displays a graphic view of the workflow where you can see the current phase and the transitions that connect the current phase to all other phases. the incident management process workflow includes all necessary steps to log and resolve an incident, including any necessary escalations or reassignments. if you are the incident coordinator, you can assign an incident to an incident analyst for investigation and diagnosis. if the incident is escalated, you might need to reassign the incident to an escalation assignment group. the typical incident management workflow contains four metaphases and six phases that lead to closure. when you update a record or assign a new status, the record can transition from one process phase to the next automatically. the service management incident workflow reflects itilv3 pr",
    "keywordsLower": [
      "incident",
      "workflow",
      "metaphase",
      "classification",
      "resolution",
      "validation",
      "done",
      "end",
      "related",
      "topics",
      "end-to-end",
      "process",
      "creation",
      "closure.",
      "building",
      "blocks",
      "metaphases",
      "phases",
      "transitions.",
      "service",
      "management",
      "displays",
      "graphic",
      "view",
      "see",
      "current",
      "phase",
      "transitions",
      "connect",
      "all",
      "phases.",
      "includes",
      "necessary",
      "steps",
      "log",
      "resolve",
      "including",
      "any",
      "escalations",
      "reassignments.",
      "coordinator",
      "assign",
      "analyst",
      "investigation",
      "diagnosis.",
      "escalated",
      "need",
      "reassign",
      "escalation",
      "assignment",
      "group.",
      "typical",
      "contains",
      "four",
      "six",
      "lead",
      "update",
      "record",
      "new",
      "status",
      "transition",
      "one",
      "next",
      "automatically.",
      "reflects",
      "itilv3",
      "recommendations.",
      "example",
      "depend",
      "value",
      "field",
      "record.",
      "review",
      "closure",
      "occurs",
      "desk",
      "changes",
      "closed.",
      "description",
      "automatic",
      "receives",
      "information",
      "user",
      "about",
      "outage",
      "business",
      "failure",
      "organization",
      "able",
      "fulfill",
      "request.",
      "gathers",
      "required",
      "creates",
      "classify",
      "step",
      "add",
      "enough",
      "issue",
      "hardware"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident notification rules",
    "content": "Service Management sends an email notification to designated users when a business rule triggers a notification event. The following table describes the event trigger, identifies the email recipient, and identifies the information contained in the notification. The default business rules define the recipients according to the user or group identified in the incident record, but an administrator must first do the following: Assign the appropriate Incident Management roles to the named users Populate groups with users who also have the appropriate roles to add, change, or update incidents Event Recipients Email contains this information Assignment Incident owner Service desk group Status, service, category, priority, description, last comment Close Incident owner Service desk group Status, service, category, priority, description, closure code, solution Escalation Incident owner Service desk group Assignee Assignment group Status, service, category, priority, description, last comment Pe",
    "url": "imnotifrules",
    "filename": "imnotifrules",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "incident",
      "notification",
      "rules",
      "related",
      "topics",
      "service",
      "management",
      "sends",
      "email",
      "designated",
      "users",
      "business",
      "rule",
      "triggers",
      "event.",
      "following",
      "table",
      "describes",
      "event",
      "trigger",
      "identifies",
      "recipient",
      "information",
      "contained",
      "notification.",
      "default",
      "define",
      "recipients",
      "according",
      "user",
      "group",
      "identified",
      "record",
      "administrator",
      "first",
      "assign",
      "appropriate",
      "roles",
      "named",
      "populate",
      "groups",
      "add",
      "change",
      "update",
      "incidents",
      "contains",
      "assignment",
      "owner",
      "desk",
      "status",
      "category",
      "priority",
      "description",
      "last",
      "comment",
      "close",
      "closure",
      "code",
      "solution",
      "escalation",
      "assignee",
      "pending",
      "requestor",
      "resolution",
      "slt",
      "breach",
      "50",
      "name",
      "type",
      "target",
      "time",
      "75",
      "90",
      "100",
      "about",
      "customizing",
      "templates",
      "see",
      "notifications.",
      "transition",
      "workflow"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident notification rules",
    "contentLower": "service management sends an email notification to designated users when a business rule triggers a notification event. the following table describes the event trigger, identifies the email recipient, and identifies the information contained in the notification. the default business rules define the recipients according to the user or group identified in the incident record, but an administrator must first do the following: assign the appropriate incident management roles to the named users populate groups with users who also have the appropriate roles to add, change, or update incidents event recipients email contains this information assignment incident owner service desk group status, service, category, priority, description, last comment close incident owner service desk group status, service, category, priority, description, closure code, solution escalation incident owner service desk group assignee assignment group status, service, category, priority, description, last comment pe",
    "keywordsLower": [
      "incident",
      "notification",
      "rules",
      "related",
      "topics",
      "service",
      "management",
      "sends",
      "email",
      "designated",
      "users",
      "business",
      "rule",
      "triggers",
      "event.",
      "following",
      "table",
      "describes",
      "event",
      "trigger",
      "identifies",
      "recipient",
      "information",
      "contained",
      "notification.",
      "default",
      "define",
      "recipients",
      "according",
      "user",
      "group",
      "identified",
      "record",
      "administrator",
      "first",
      "assign",
      "appropriate",
      "roles",
      "named",
      "populate",
      "groups",
      "add",
      "change",
      "update",
      "incidents",
      "contains",
      "assignment",
      "owner",
      "desk",
      "status",
      "category",
      "priority",
      "description",
      "last",
      "comment",
      "close",
      "closure",
      "code",
      "solution",
      "escalation",
      "assignee",
      "pending",
      "requestor",
      "resolution",
      "slt",
      "breach",
      "50",
      "name",
      "type",
      "target",
      "time",
      "75",
      "90",
      "100",
      "about",
      "customizing",
      "templates",
      "see",
      "notifications.",
      "transition",
      "workflow"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident Management procedures",
    "content": "When Service Desk personnel can't resolve a customer incident in time, they can escalate the incident. Service Desk personnel can also create and resolve an incident directly when they know what the solution is. Related topics How to create an incident record How to edit an incident record How to relate incident records How to create a record from an incident How to create a record from a problem How to create a record from a request How to relate change records",
    "url": "imtasks",
    "filename": "imtasks",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "incident",
      "management",
      "procedures",
      "related",
      "topics",
      "service",
      "desk",
      "personnel",
      "resolve",
      "customer",
      "time",
      "escalate",
      "incident.",
      "create",
      "directly",
      "know",
      "what",
      "solution",
      "is.",
      "record",
      "edit",
      "relate",
      "records",
      "problem",
      "request",
      "change"
    ],
    "language": "en",
    "word_count": 47,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident management procedures",
    "contentLower": "when service desk personnel can't resolve a customer incident in time, they can escalate the incident. service desk personnel can also create and resolve an incident directly when they know what the solution is. related topics how to create an incident record how to edit an incident record how to relate incident records how to create a record from an incident how to create a record from a problem how to create a record from a request how to relate change records",
    "keywordsLower": [
      "incident",
      "management",
      "procedures",
      "related",
      "topics",
      "service",
      "desk",
      "personnel",
      "resolve",
      "customer",
      "time",
      "escalate",
      "incident.",
      "create",
      "directly",
      "know",
      "what",
      "solution",
      "is.",
      "record",
      "edit",
      "relate",
      "records",
      "problem",
      "request",
      "change"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident templates",
    "content": "A Service Management template is a group of fields with default values already completed, providing a quick start to create a new record or apply to an existing record. You can use a template to achieve consistency when you process records that have the same workflow. You can create a template with either of these methods: Save an existing record (such as an incident, problem, or change) as a template. For example, you can save a change record as a change template. Create a new template, where you decide which fields should have default values already filled in. Related topics How to create an incident template How to edit an incident template How to delete an incident template How to apply an incident template",
    "url": "incidenttemplates",
    "filename": "incidenttemplates",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "incident",
      "templates",
      "related",
      "topics",
      "service",
      "management",
      "template",
      "group",
      "fields",
      "default",
      "values",
      "already",
      "completed",
      "providing",
      "quick",
      "start",
      "create",
      "new",
      "record",
      "apply",
      "existing",
      "record.",
      "achieve",
      "consistency",
      "process",
      "records",
      "same",
      "workflow.",
      "either",
      "methods",
      "save",
      "such",
      "problem",
      "change",
      "template.",
      "example",
      "decide",
      "filled",
      "in.",
      "edit",
      "delete"
    ],
    "language": "en",
    "word_count": 69,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident templates",
    "contentLower": "a service management template is a group of fields with default values already completed, providing a quick start to create a new record or apply to an existing record. you can use a template to achieve consistency when you process records that have the same workflow. you can create a template with either of these methods: save an existing record (such as an incident, problem, or change) as a template. for example, you can save a change record as a change template. create a new template, where you decide which fields should have default values already filled in. related topics how to create an incident template how to edit an incident template how to delete an incident template how to apply an incident template",
    "keywordsLower": [
      "incident",
      "templates",
      "related",
      "topics",
      "service",
      "management",
      "template",
      "group",
      "fields",
      "default",
      "values",
      "already",
      "completed",
      "providing",
      "quick",
      "start",
      "create",
      "new",
      "record",
      "apply",
      "existing",
      "record.",
      "achieve",
      "consistency",
      "process",
      "records",
      "same",
      "workflow.",
      "either",
      "methods",
      "save",
      "such",
      "problem",
      "change",
      "template.",
      "example",
      "decide",
      "filled",
      "in.",
      "edit",
      "delete"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident models",
    "content": "Incident models can help you standardize the end-to-end process of incident management, and maximize efficiency. Related topics How to create an incident model How to edit an incident model How to activate an incident model How to retire an incident model Incident model business rules",
    "url": "incidentmodels",
    "filename": "incidentmodels",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "incident",
      "models",
      "related",
      "topics",
      "help",
      "standardize",
      "end-to-end",
      "process",
      "management",
      "maximize",
      "efficiency.",
      "create",
      "model",
      "edit",
      "activate",
      "retire",
      "business",
      "rules"
    ],
    "language": "en",
    "word_count": 30,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident models",
    "contentLower": "incident models can help you standardize the end-to-end process of incident management, and maximize efficiency. related topics how to create an incident model how to edit an incident model how to activate an incident model how to retire an incident model incident model business rules",
    "keywordsLower": [
      "incident",
      "models",
      "related",
      "topics",
      "help",
      "standardize",
      "end-to-end",
      "process",
      "management",
      "maximize",
      "efficiency.",
      "create",
      "model",
      "edit",
      "activate",
      "retire",
      "business",
      "rules"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident model business rules",
    "content": "You can define business rules within a specific incident model. The rules run in the incident workflow for incident records based on the selected model. These business rules can be used to make a user option mandatory, hidden, or to control values displayed in the user option fields and any fields in the incident based on the model. You can define the rules to run in connection with one of the following process events: Process event Description Before change The rule is run before the data is updated. After change The rule is run after the data is updated. Rendering forms The rule is run when a form is opened. After applying changes The rule is run after the change is committed. SLT Event The rule is run when the Service Level target duration reaches the 0%, 15%, 30%, 50%, 75%, 90%, or 100% level of the target, as defined by the rule. For information on defining business rules under the SLT Event process event, see How to add Service Level Target event business rules. In each process e",
    "url": "incmodelbizrules",
    "filename": "incmodelbizrules",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "incident",
      "model",
      "business",
      "rules",
      "related",
      "topics",
      "define",
      "specific",
      "model.",
      "run",
      "workflow",
      "records",
      "based",
      "selected",
      "make",
      "user",
      "option",
      "mandatory",
      "hidden",
      "control",
      "values",
      "displayed",
      "fields",
      "any",
      "connection",
      "one",
      "following",
      "process",
      "events",
      "event",
      "description",
      "before",
      "change",
      "rule",
      "data",
      "updated.",
      "after",
      "rendering",
      "forms",
      "form",
      "opened.",
      "applying",
      "changes",
      "committed.",
      "slt",
      "service",
      "level",
      "target",
      "duration",
      "reaches",
      "15",
      "30",
      "50",
      "75",
      "90",
      "100",
      "defined",
      "rule.",
      "information",
      "defining",
      "under",
      "see",
      "add",
      "rules.",
      "either",
      "general",
      "record",
      "select",
      "required",
      "tab.",
      "click",
      "links",
      "button",
      "next",
      "event.",
      "template",
      "left",
      "margin",
      "display",
      "arrow",
      "buttons",
      "enable",
      "order",
      "move",
      "vice",
      "versa.",
      "alternatively",
      "drag",
      "drop",
      "desired",
      "location.",
      "save",
      "toolbar.",
      "note",
      "different",
      "basis",
      "new",
      "apply",
      "record.",
      "modify"
    ],
    "language": "en",
    "word_count": 115,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident model business rules",
    "contentLower": "you can define business rules within a specific incident model. the rules run in the incident workflow for incident records based on the selected model. these business rules can be used to make a user option mandatory, hidden, or to control values displayed in the user option fields and any fields in the incident based on the model. you can define the rules to run in connection with one of the following process events: process event description before change the rule is run before the data is updated. after change the rule is run after the data is updated. rendering forms the rule is run when a form is opened. after applying changes the rule is run after the change is committed. slt event the rule is run when the service level target duration reaches the 0%, 15%, 30%, 50%, 75%, 90%, or 100% level of the target, as defined by the rule. for information on defining business rules under the slt event process event, see how to add service level target event business rules. in each process e",
    "keywordsLower": [
      "incident",
      "model",
      "business",
      "rules",
      "related",
      "topics",
      "define",
      "specific",
      "model.",
      "run",
      "workflow",
      "records",
      "based",
      "selected",
      "make",
      "user",
      "option",
      "mandatory",
      "hidden",
      "control",
      "values",
      "displayed",
      "fields",
      "any",
      "connection",
      "one",
      "following",
      "process",
      "events",
      "event",
      "description",
      "before",
      "change",
      "rule",
      "data",
      "updated.",
      "after",
      "rendering",
      "forms",
      "form",
      "opened.",
      "applying",
      "changes",
      "committed.",
      "slt",
      "service",
      "level",
      "target",
      "duration",
      "reaches",
      "15",
      "30",
      "50",
      "75",
      "90",
      "100",
      "defined",
      "rule.",
      "information",
      "defining",
      "under",
      "see",
      "add",
      "rules.",
      "either",
      "general",
      "record",
      "select",
      "required",
      "tab.",
      "click",
      "links",
      "button",
      "next",
      "event.",
      "template",
      "left",
      "margin",
      "display",
      "arrow",
      "buttons",
      "enable",
      "order",
      "move",
      "vice",
      "versa.",
      "alternatively",
      "drag",
      "drop",
      "desired",
      "location.",
      "save",
      "toolbar.",
      "note",
      "different",
      "basis",
      "new",
      "apply",
      "record.",
      "modify"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Learn more about Incident Management",
    "content": "Incidents are unplanned service interruptions or a degradation in a service that affects normal business operations. Services are grouped into categories, such as hardware or software. Services usually support your business infrastructure, such as email or network access. When a service interruption occurs, it can impact revenue or productivity. The severity of the incident often depends on when the interruption occurs. In a global organization, the service needs to be available 24 hours a day and seven days a week. In other scenarios, an interruption during off-peak hours has a smaller impact. Service Management uses ITIL v3 best practice recommendations to manage the typical incident resolution process. The process depends on incident models, which describe typical incident scenarios or categories, related tasks, and predictable process phases. Related topics Incident Management ITIL process First touch solutions Incident solution matching Incident workflow",
    "url": "imlearnmore",
    "filename": "imlearnmore",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "learn",
      "about",
      "incident",
      "management",
      "related",
      "topics",
      "incidents",
      "unplanned",
      "service",
      "interruptions",
      "degradation",
      "affects",
      "normal",
      "business",
      "operations.",
      "services",
      "grouped",
      "categories",
      "such",
      "hardware",
      "software.",
      "usually",
      "support",
      "infrastructure",
      "email",
      "network",
      "access.",
      "interruption",
      "occurs",
      "impact",
      "revenue",
      "productivity.",
      "severity",
      "often",
      "depends",
      "occurs.",
      "global",
      "organization",
      "needs",
      "available",
      "24",
      "hours",
      "day",
      "seven",
      "days",
      "week.",
      "scenarios",
      "during",
      "off-peak",
      "smaller",
      "impact.",
      "uses",
      "itil",
      "v3",
      "best",
      "practice",
      "recommendations",
      "manage",
      "typical",
      "resolution",
      "process.",
      "process",
      "models",
      "describe",
      "tasks",
      "predictable",
      "phases.",
      "first",
      "touch",
      "solutions",
      "solution",
      "matching",
      "workflow"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "learn more about incident management",
    "contentLower": "incidents are unplanned service interruptions or a degradation in a service that affects normal business operations. services are grouped into categories, such as hardware or software. services usually support your business infrastructure, such as email or network access. when a service interruption occurs, it can impact revenue or productivity. the severity of the incident often depends on when the interruption occurs. in a global organization, the service needs to be available 24 hours a day and seven days a week. in other scenarios, an interruption during off-peak hours has a smaller impact. service management uses itil v3 best practice recommendations to manage the typical incident resolution process. the process depends on incident models, which describe typical incident scenarios or categories, related tasks, and predictable process phases. related topics incident management itil process first touch solutions incident solution matching incident workflow",
    "keywordsLower": [
      "learn",
      "about",
      "incident",
      "management",
      "related",
      "topics",
      "incidents",
      "unplanned",
      "service",
      "interruptions",
      "degradation",
      "affects",
      "normal",
      "business",
      "operations.",
      "services",
      "grouped",
      "categories",
      "such",
      "hardware",
      "software.",
      "usually",
      "support",
      "infrastructure",
      "email",
      "network",
      "access.",
      "interruption",
      "occurs",
      "impact",
      "revenue",
      "productivity.",
      "severity",
      "often",
      "depends",
      "occurs.",
      "global",
      "organization",
      "needs",
      "available",
      "24",
      "hours",
      "day",
      "seven",
      "days",
      "week.",
      "scenarios",
      "during",
      "off-peak",
      "smaller",
      "impact.",
      "uses",
      "itil",
      "v3",
      "best",
      "practice",
      "recommendations",
      "manage",
      "typical",
      "resolution",
      "process.",
      "process",
      "models",
      "describe",
      "tasks",
      "predictable",
      "phases.",
      "first",
      "touch",
      "solutions",
      "solution",
      "matching",
      "workflow"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incident Management ITIL process",
    "content": "The Service Management Incident Management module supports the incident management function of the Information Technology Infrastructure Library (ITIL) to detect and resolve incidents. Incident Management is described in the ITIL core publication \"Service Operation.\" The purpose of Incident Management is to restore normal service operation as quickly as possible. The result is reduced down time and increased service availability. Incident Management aligns IT activity to real-time business priorities. Incident Management metrics help you identify opportunities to improve existing services, add new services, or develop new training. Incident Management minimizes the adverse impact on business operations to ensure that the best possible levels of service quality and availability are maintained. Service Management standardizes the ITIL activities into discrete phases within a predefined process: Incident detection and recording (Logging) Classification and initial support (Categorization)",
    "url": "imitilprocess",
    "filename": "imitilprocess",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "incident",
      "management",
      "itil",
      "process",
      "related",
      "topics",
      "service",
      "module",
      "supports",
      "function",
      "information",
      "technology",
      "infrastructure",
      "library",
      "detect",
      "resolve",
      "incidents.",
      "described",
      "core",
      "publication",
      "operation.",
      "purpose",
      "restore",
      "normal",
      "operation",
      "quickly",
      "possible.",
      "result",
      "reduced",
      "time",
      "increased",
      "availability.",
      "aligns",
      "activity",
      "real-time",
      "business",
      "priorities.",
      "metrics",
      "help",
      "identify",
      "opportunities",
      "improve",
      "existing",
      "services",
      "add",
      "new",
      "develop",
      "training.",
      "minimizes",
      "adverse",
      "impact",
      "operations",
      "ensure",
      "best",
      "possible",
      "levels",
      "quality",
      "availability",
      "maintained.",
      "standardizes",
      "activities",
      "discrete",
      "phases",
      "predefined",
      "detection",
      "recording",
      "logging",
      "classification",
      "initial",
      "support",
      "categorization",
      "investigation",
      "diagnosis",
      "resolution",
      "recovery",
      "closure",
      "defines",
      "performance",
      "level",
      "required",
      "meet",
      "defined",
      "targets.",
      "practices",
      "first",
      "touch",
      "solutions",
      "input",
      "output",
      "key",
      "indicators",
      "learn",
      "about",
      "solution",
      "matching"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incident management itil process",
    "contentLower": "the service management incident management module supports the incident management function of the information technology infrastructure library (itil) to detect and resolve incidents. incident management is described in the itil core publication \"service operation.\" the purpose of incident management is to restore normal service operation as quickly as possible. the result is reduced down time and increased service availability. incident management aligns it activity to real-time business priorities. incident management metrics help you identify opportunities to improve existing services, add new services, or develop new training. incident management minimizes the adverse impact on business operations to ensure that the best possible levels of service quality and availability are maintained. service management standardizes the itil activities into discrete phases within a predefined process: incident detection and recording (logging) classification and initial support (categorization)",
    "keywordsLower": [
      "incident",
      "management",
      "itil",
      "process",
      "related",
      "topics",
      "service",
      "module",
      "supports",
      "function",
      "information",
      "technology",
      "infrastructure",
      "library",
      "detect",
      "resolve",
      "incidents.",
      "described",
      "core",
      "publication",
      "operation.",
      "purpose",
      "restore",
      "normal",
      "operation",
      "quickly",
      "possible.",
      "result",
      "reduced",
      "time",
      "increased",
      "availability.",
      "aligns",
      "activity",
      "real-time",
      "business",
      "priorities.",
      "metrics",
      "help",
      "identify",
      "opportunities",
      "improve",
      "existing",
      "services",
      "add",
      "new",
      "develop",
      "training.",
      "minimizes",
      "adverse",
      "impact",
      "operations",
      "ensure",
      "best",
      "possible",
      "levels",
      "quality",
      "availability",
      "maintained.",
      "standardizes",
      "activities",
      "discrete",
      "phases",
      "predefined",
      "detection",
      "recording",
      "logging",
      "classification",
      "initial",
      "support",
      "categorization",
      "investigation",
      "diagnosis",
      "resolution",
      "recovery",
      "closure",
      "defines",
      "performance",
      "level",
      "required",
      "meet",
      "defined",
      "targets.",
      "practices",
      "first",
      "touch",
      "solutions",
      "input",
      "output",
      "key",
      "indicators",
      "learn",
      "about",
      "solution",
      "matching"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Input and output",
    "content": "The following table describes how incidents are created and the result of incident resolution. Input Sources Outputs A Service Desk Agent can escalate a customer incident. Resolved or closed incidents Related incidents Requests for Change (RFC) Problems Resolution metrics Customer communication An event management tool can automatically open an incident. Support staff and others in Service Management roles can open an incident. Related topics Incident Management best practices Incident Management ITIL process Incident Management Key Performance Indicators",
    "url": "iminputoutput",
    "filename": "iminputoutput",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "input",
      "output",
      "related",
      "topics",
      "following",
      "table",
      "describes",
      "incidents",
      "created",
      "result",
      "incident",
      "resolution.",
      "sources",
      "outputs",
      "service",
      "desk",
      "agent",
      "escalate",
      "customer",
      "incident.",
      "resolved",
      "closed",
      "requests",
      "change",
      "rfc",
      "problems",
      "resolution",
      "metrics",
      "communication",
      "event",
      "management",
      "tool",
      "automatically",
      "open",
      "support",
      "staff",
      "others",
      "roles",
      "best",
      "practices",
      "itil",
      "process",
      "key",
      "performance",
      "indicators"
    ],
    "language": "en",
    "word_count": 61,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "input and output",
    "contentLower": "the following table describes how incidents are created and the result of incident resolution. input sources outputs a service desk agent can escalate a customer incident. resolved or closed incidents related incidents requests for change (rfc) problems resolution metrics customer communication an event management tool can automatically open an incident. support staff and others in service management roles can open an incident. related topics incident management best practices incident management itil process incident management key performance indicators",
    "keywordsLower": [
      "input",
      "output",
      "related",
      "topics",
      "following",
      "table",
      "describes",
      "incidents",
      "created",
      "result",
      "incident",
      "resolution.",
      "sources",
      "outputs",
      "service",
      "desk",
      "agent",
      "escalate",
      "customer",
      "incident.",
      "resolved",
      "closed",
      "requests",
      "change",
      "rfc",
      "problems",
      "resolution",
      "metrics",
      "communication",
      "event",
      "management",
      "tool",
      "automatically",
      "open",
      "support",
      "staff",
      "others",
      "roles",
      "best",
      "practices",
      "itil",
      "process",
      "key",
      "performance",
      "indicators"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Key performance indicators",
    "content": "Key Performance Indicators (KPIs) are useful for evaluating your Incident Management processes. To visualize trend information, it's useful to graph KPI data periodically. In addition to the data provided by Service Management, you may need additional tools to report on all of your KPI requirements. Metric Description Percentage of incidents closed within Service Level Target time The number of incidents that are closed within the Service Level Target time, relative to the number of all incidents closed, in a given time period. Percentage of reopened incidents The number of incidents closed that are reopened because the solution was not accepted by the customer, relative to the number of all incidents closed, in a given time period. Backlog of incidents The number of incidents that aren't yet closed, in a given time period. Total number of incidents Total number of new reported incidents, in a given time period. These are common metrics, however, each standards organization has their o",
    "url": "imkpis",
    "filename": "imkpis",
    "headings": [
      "ITIL V3 KPIs",
      "COBIT 4.1 KPIs",
      "RACI Matrix KPIs",
      "Useful incident fields",
      "Related topics"
    ],
    "keywords": [
      "2.9",
      "2.5",
      "2.6",
      "4.1",
      "2.1",
      "2.3",
      "2.2",
      "2.7",
      "2.4",
      "2.8",
      "key",
      "performance",
      "indicators",
      "itil",
      "v3",
      "kpis",
      "cobit",
      "raci",
      "matrix",
      "useful",
      "incident",
      "fields",
      "related",
      "topics",
      "evaluating",
      "management",
      "processes.",
      "visualize",
      "trend",
      "information",
      "graph",
      "kpi",
      "data",
      "periodically.",
      "addition",
      "provided",
      "service",
      "need",
      "additional",
      "tools",
      "report",
      "all",
      "requirements.",
      "metric",
      "description",
      "percentage",
      "incidents",
      "closed",
      "level",
      "target",
      "time",
      "number",
      "relative",
      "given",
      "period.",
      "reopened",
      "because",
      "solution",
      "accepted",
      "customer",
      "backlog",
      "aren",
      "yet",
      "total",
      "new",
      "reported",
      "common",
      "metrics",
      "however",
      "standards",
      "organization",
      "own",
      "recommendations.",
      "recommends",
      "measure",
      "control",
      "breakdown",
      "stage.",
      "example",
      "logged",
      "work",
      "progress",
      "closed.",
      "size",
      "current",
      "backlog.",
      "major",
      "incidents.",
      "mean",
      "elapsed",
      "achieve",
      "resolution",
      "workaround",
      "impact.",
      "handled",
      "response",
      "time.",
      "specify",
      "response-time",
      "targets"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "key performance indicators",
    "contentLower": "key performance indicators (kpis) are useful for evaluating your incident management processes. to visualize trend information, it's useful to graph kpi data periodically. in addition to the data provided by service management, you may need additional tools to report on all of your kpi requirements. metric description percentage of incidents closed within service level target time the number of incidents that are closed within the service level target time, relative to the number of all incidents closed, in a given time period. percentage of reopened incidents the number of incidents closed that are reopened because the solution was not accepted by the customer, relative to the number of all incidents closed, in a given time period. backlog of incidents the number of incidents that aren't yet closed, in a given time period. total number of incidents total number of new reported incidents, in a given time period. these are common metrics, however, each standards organization has their o",
    "keywordsLower": [
      "2.9",
      "2.5",
      "2.6",
      "4.1",
      "2.1",
      "2.3",
      "2.2",
      "2.7",
      "2.4",
      "2.8",
      "key",
      "performance",
      "indicators",
      "itil",
      "v3",
      "kpis",
      "cobit",
      "raci",
      "matrix",
      "useful",
      "incident",
      "fields",
      "related",
      "topics",
      "evaluating",
      "management",
      "processes.",
      "visualize",
      "trend",
      "information",
      "graph",
      "kpi",
      "data",
      "periodically.",
      "addition",
      "provided",
      "service",
      "need",
      "additional",
      "tools",
      "report",
      "all",
      "requirements.",
      "metric",
      "description",
      "percentage",
      "incidents",
      "closed",
      "level",
      "target",
      "time",
      "number",
      "relative",
      "given",
      "period.",
      "reopened",
      "because",
      "solution",
      "accepted",
      "customer",
      "backlog",
      "aren",
      "yet",
      "total",
      "new",
      "reported",
      "common",
      "metrics",
      "however",
      "standards",
      "organization",
      "own",
      "recommendations.",
      "recommends",
      "measure",
      "control",
      "breakdown",
      "stage.",
      "example",
      "logged",
      "work",
      "progress",
      "closed.",
      "size",
      "current",
      "backlog.",
      "major",
      "incidents.",
      "mean",
      "elapsed",
      "achieve",
      "resolution",
      "workaround",
      "impact.",
      "handled",
      "response",
      "time.",
      "specify",
      "response-time",
      "targets"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Learn more about problems",
    "content": "Problem Management includes the activities required to prevent the recurrence or replication of incidents or known errors. It enables you to form recommendations for improvement, maintain problem records, and review the status of corrective actions. Proactive Problem Management encompasses problem prevention, ranging from prevention of individual incidents, such as repeated difficulties with a particular system feature, to making higher level strategic decisions. The latter may require major expenditure to implement, such as investment in a better network. Problem prevention also includes proactively providing information to customers for future use. This communication reduces future requests for information and helps to prevent incidents caused by gaps in user knowledge or training. Related topics Problem Management and ITIL Problem Management input and output Problem Management KPIs",
    "url": "pmreference",
    "filename": "pmreference",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "learn",
      "about",
      "problems",
      "related",
      "topics",
      "problem",
      "management",
      "includes",
      "activities",
      "required",
      "prevent",
      "recurrence",
      "replication",
      "incidents",
      "known",
      "errors.",
      "enables",
      "form",
      "recommendations",
      "improvement",
      "maintain",
      "records",
      "review",
      "status",
      "corrective",
      "actions.",
      "proactive",
      "encompasses",
      "prevention",
      "ranging",
      "individual",
      "such",
      "repeated",
      "difficulties",
      "particular",
      "system",
      "feature",
      "making",
      "higher",
      "level",
      "strategic",
      "decisions.",
      "latter",
      "require",
      "major",
      "expenditure",
      "implement",
      "investment",
      "better",
      "network.",
      "proactively",
      "providing",
      "information",
      "customers",
      "future",
      "use.",
      "communication",
      "reduces",
      "requests",
      "helps",
      "caused",
      "gaps",
      "user",
      "knowledge",
      "training.",
      "itil",
      "input",
      "output",
      "kpis"
    ],
    "language": "en",
    "word_count": 89,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "learn more about problems",
    "contentLower": "problem management includes the activities required to prevent the recurrence or replication of incidents or known errors. it enables you to form recommendations for improvement, maintain problem records, and review the status of corrective actions. proactive problem management encompasses problem prevention, ranging from prevention of individual incidents, such as repeated difficulties with a particular system feature, to making higher level strategic decisions. the latter may require major expenditure to implement, such as investment in a better network. problem prevention also includes proactively providing information to customers for future use. this communication reduces future requests for information and helps to prevent incidents caused by gaps in user knowledge or training. related topics problem management and itil problem management input and output problem management kpis",
    "keywordsLower": [
      "learn",
      "about",
      "problems",
      "related",
      "topics",
      "problem",
      "management",
      "includes",
      "activities",
      "required",
      "prevent",
      "recurrence",
      "replication",
      "incidents",
      "known",
      "errors.",
      "enables",
      "form",
      "recommendations",
      "improvement",
      "maintain",
      "records",
      "review",
      "status",
      "corrective",
      "actions.",
      "proactive",
      "encompasses",
      "prevention",
      "ranging",
      "individual",
      "such",
      "repeated",
      "difficulties",
      "particular",
      "system",
      "feature",
      "making",
      "higher",
      "level",
      "strategic",
      "decisions.",
      "latter",
      "require",
      "major",
      "expenditure",
      "implement",
      "investment",
      "better",
      "network.",
      "proactively",
      "providing",
      "information",
      "customers",
      "future",
      "use.",
      "communication",
      "reduces",
      "requests",
      "helps",
      "caused",
      "gaps",
      "user",
      "knowledge",
      "training.",
      "itil",
      "input",
      "output",
      "kpis"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Licenses",
    "content": "The licenses home page displays details of your software licenses. Related topics Software Asset Management License workflow How to create a license record How to edit a license record",
    "url": "licenses",
    "filename": "licenses",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "licenses",
      "related",
      "topics",
      "home",
      "page",
      "displays",
      "details",
      "software",
      "licenses.",
      "asset",
      "management",
      "license",
      "workflow",
      "create",
      "record",
      "edit"
    ],
    "language": "en",
    "word_count": 21,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "licenses",
    "contentLower": "the licenses home page displays details of your software licenses. related topics software asset management license workflow how to create a license record how to edit a license record",
    "keywordsLower": [
      "licenses",
      "related",
      "topics",
      "home",
      "page",
      "displays",
      "details",
      "software",
      "licenses.",
      "asset",
      "management",
      "license",
      "workflow",
      "create",
      "record",
      "edit"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License workflow",
    "content": "This section describes the metaphases and subordinate phases in the life cycle of a software license. The license workflow relies on business rules. Rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. For more information about the out-of-the-box business rules defined for the license workflow, see License process - Business rules. Metaphase: Unavailable The software license is entered into the system and still awaiting delivery. Phase Transition Description New Manual New is a starting point. If the license is awaiting delivery from a vendor, you can manually transition the record to the Awaiting delivery phase. If the license is available and ready to use, you can manually transition the record to the Pool phase. Next phase: Pool or Awaiting delivery Awaiting delivery Manual The software license is in transit from a software publisher or other vendor. When it's received, you can manually transition the r",
    "url": "licensewflw",
    "filename": "licensewflw",
    "headings": [
      "Metaphase: Unavailable",
      "Metaphase: Available",
      "Metaphase: In use",
      "Metaphase: Retiring",
      "Metaphase: End of life (End)",
      "Related topics"
    ],
    "keywords": [
      "license",
      "workflow",
      "metaphase",
      "unavailable",
      "available",
      "retiring",
      "end",
      "life",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "cycle",
      "software",
      "license.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "out-of-the-box",
      "defined",
      "see",
      "process",
      "entered",
      "system",
      "still",
      "awaiting",
      "delivery.",
      "transition",
      "description",
      "new",
      "manual",
      "starting",
      "point.",
      "delivery",
      "vendor",
      "manually",
      "record",
      "ready",
      "pool",
      "next",
      "transit",
      "publisher",
      "vendor.",
      "received",
      "complete",
      "order",
      "canceled.",
      "receive",
      "canceled",
      "example",
      "organization",
      "starts",
      "off",
      "project",
      "improve",
      "response",
      "time",
      "critical",
      "service.",
      "database",
      "application",
      "introduced",
      "boost",
      "performance.",
      "procurement",
      "team",
      "member",
      "places",
      "purchase",
      "once",
      "issued",
      "creates",
      "sets",
      "received.",
      "yet",
      "use.",
      "prepare.",
      "status",
      "changes",
      "manager",
      "updates",
      "together"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license workflow",
    "contentLower": "this section describes the metaphases and subordinate phases in the life cycle of a software license. the license workflow relies on business rules. rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. for more information about the out-of-the-box business rules defined for the license workflow, see license process - business rules. metaphase: unavailable the software license is entered into the system and still awaiting delivery. phase transition description new manual new is a starting point. if the license is awaiting delivery from a vendor, you can manually transition the record to the awaiting delivery phase. if the license is available and ready to use, you can manually transition the record to the pool phase. next phase: pool or awaiting delivery awaiting delivery manual the software license is in transit from a software publisher or other vendor. when it's received, you can manually transition the r",
    "keywordsLower": [
      "license",
      "workflow",
      "metaphase",
      "unavailable",
      "available",
      "retiring",
      "end",
      "life",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "cycle",
      "software",
      "license.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "out-of-the-box",
      "defined",
      "see",
      "process",
      "entered",
      "system",
      "still",
      "awaiting",
      "delivery.",
      "transition",
      "description",
      "new",
      "manual",
      "starting",
      "point.",
      "delivery",
      "vendor",
      "manually",
      "record",
      "ready",
      "pool",
      "next",
      "transit",
      "publisher",
      "vendor.",
      "received",
      "complete",
      "order",
      "canceled.",
      "receive",
      "canceled",
      "example",
      "organization",
      "starts",
      "off",
      "project",
      "improve",
      "response",
      "time",
      "critical",
      "service.",
      "database",
      "application",
      "introduced",
      "boost",
      "performance.",
      "procurement",
      "team",
      "member",
      "places",
      "purchase",
      "once",
      "issued",
      "creates",
      "sets",
      "received.",
      "yet",
      "use.",
      "prepare.",
      "status",
      "changes",
      "manager",
      "updates",
      "together"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License metrics (original SAM)",
    "content": "In the Service Management Software Asset Management module, each license is associated with a particular license metric. This directs how the license is consumed, and how license compliance is calculated. The license metrics home page displays details of your license metrics. Related topics Software Asset Management License workflow How to create a license metric record How to edit a license metric record How to retire a license metric record",
    "url": "licensetypes",
    "filename": "licensetypes",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "license",
      "metrics",
      "original",
      "sam",
      "related",
      "topics",
      "service",
      "management",
      "software",
      "asset",
      "module",
      "associated",
      "particular",
      "metric.",
      "directs",
      "consumed",
      "compliance",
      "calculated.",
      "home",
      "page",
      "displays",
      "details",
      "metrics.",
      "workflow",
      "create",
      "metric",
      "record",
      "edit",
      "retire"
    ],
    "language": "en",
    "word_count": 48,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license metrics (original sam)",
    "contentLower": "in the service management software asset management module, each license is associated with a particular license metric. this directs how the license is consumed, and how license compliance is calculated. the license metrics home page displays details of your license metrics. related topics software asset management license workflow how to create a license metric record how to edit a license metric record how to retire a license metric record",
    "keywordsLower": [
      "license",
      "metrics",
      "original",
      "sam",
      "related",
      "topics",
      "service",
      "management",
      "software",
      "asset",
      "module",
      "associated",
      "particular",
      "metric.",
      "directs",
      "consumed",
      "compliance",
      "calculated.",
      "home",
      "page",
      "displays",
      "details",
      "metrics.",
      "workflow",
      "create",
      "metric",
      "record",
      "edit",
      "retire"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License models",
    "content": "In the Service Management Software Asset Management module, a license model is a classification applied to license assets. Normalizing each license to a particular license model is a prerequisite. The license models home page displays details of your license models. Related topics Software Asset Management How to create a license model How to edit a license model How to retire a license model",
    "url": "licensemodel",
    "filename": "licensemodel",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "license",
      "models",
      "related",
      "topics",
      "service",
      "management",
      "software",
      "asset",
      "module",
      "model",
      "classification",
      "applied",
      "assets.",
      "normalizing",
      "particular",
      "prerequisite.",
      "home",
      "page",
      "displays",
      "details",
      "models.",
      "create",
      "edit",
      "retire"
    ],
    "language": "en",
    "word_count": 42,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license models",
    "contentLower": "in the service management software asset management module, a license model is a classification applied to license assets. normalizing each license to a particular license model is a prerequisite. the license models home page displays details of your license models. related topics software asset management how to create a license model how to edit a license model how to retire a license model",
    "keywordsLower": [
      "license",
      "models",
      "related",
      "topics",
      "service",
      "management",
      "software",
      "asset",
      "module",
      "model",
      "classification",
      "applied",
      "assets.",
      "normalizing",
      "particular",
      "prerequisite.",
      "home",
      "page",
      "displays",
      "details",
      "models.",
      "create",
      "edit",
      "retire"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Licenses",
    "content": "The licenses home page displays details of your software licenses. Related topics Software Asset Management License workflow How to create a license record How to edit a license record",
    "url": "samlicenses",
    "filename": "samlicenses",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "licenses",
      "related",
      "topics",
      "home",
      "page",
      "displays",
      "details",
      "software",
      "licenses.",
      "asset",
      "management",
      "license",
      "workflow",
      "create",
      "record",
      "edit"
    ],
    "language": "en",
    "word_count": 21,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "licenses",
    "contentLower": "the licenses home page displays details of your software licenses. related topics software asset management license workflow how to create a license record how to edit a license record",
    "keywordsLower": [
      "licenses",
      "related",
      "topics",
      "home",
      "page",
      "displays",
      "details",
      "software",
      "licenses.",
      "asset",
      "management",
      "license",
      "workflow",
      "create",
      "record",
      "edit"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License workflow",
    "content": "This section describes the metaphases and subordinate phases in the life cycle of a software license. The license workflow relies on business rules. Rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. For more information about the out-of-the-box business rules defined for the license workflow, see License process - Business rules. Metaphase: Unavailable The software license is entered into the system and still awaiting delivery. Phase Transition Description New Manual New is a starting point. If the license is awaiting delivery from a vendor, you can manually transition the record to the Awaiting delivery phase. If the license is available and ready to use, you can manually transition the record to the Pool phase. Next phase: Pool or Awaiting delivery Awaiting delivery Manual The software license is in transit from a software publisher or other vendor. When it's received, you can manually transition the r",
    "url": "samlicensewflw",
    "filename": "samlicensewflw",
    "headings": [
      "Metaphase: Unavailable",
      "Metaphase: Available",
      "Metaphase: In use",
      "Metaphase: Retiring",
      "Metaphase: End of life (End)",
      "Related topics"
    ],
    "keywords": [
      "license",
      "workflow",
      "metaphase",
      "unavailable",
      "available",
      "retiring",
      "end",
      "life",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "cycle",
      "software",
      "license.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "out-of-the-box",
      "defined",
      "see",
      "process",
      "entered",
      "system",
      "still",
      "awaiting",
      "delivery.",
      "transition",
      "description",
      "new",
      "manual",
      "starting",
      "point.",
      "delivery",
      "vendor",
      "manually",
      "record",
      "ready",
      "pool",
      "next",
      "transit",
      "publisher",
      "vendor.",
      "received",
      "complete",
      "order",
      "canceled.",
      "receive",
      "canceled",
      "example",
      "organization",
      "starts",
      "off",
      "project",
      "improve",
      "response",
      "time",
      "critical",
      "service.",
      "database",
      "application",
      "introduced",
      "boost",
      "performance.",
      "procurement",
      "team",
      "member",
      "places",
      "purchase",
      "once",
      "issued",
      "creates",
      "sets",
      "received.",
      "yet",
      "use.",
      "prepare.",
      "status",
      "changes",
      "manager",
      "updates",
      "together"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license workflow",
    "contentLower": "this section describes the metaphases and subordinate phases in the life cycle of a software license. the license workflow relies on business rules. rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. for more information about the out-of-the-box business rules defined for the license workflow, see license process - business rules. metaphase: unavailable the software license is entered into the system and still awaiting delivery. phase transition description new manual new is a starting point. if the license is awaiting delivery from a vendor, you can manually transition the record to the awaiting delivery phase. if the license is available and ready to use, you can manually transition the record to the pool phase. next phase: pool or awaiting delivery awaiting delivery manual the software license is in transit from a software publisher or other vendor. when it's received, you can manually transition the r",
    "keywordsLower": [
      "license",
      "workflow",
      "metaphase",
      "unavailable",
      "available",
      "retiring",
      "end",
      "life",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "cycle",
      "software",
      "license.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "out-of-the-box",
      "defined",
      "see",
      "process",
      "entered",
      "system",
      "still",
      "awaiting",
      "delivery.",
      "transition",
      "description",
      "new",
      "manual",
      "starting",
      "point.",
      "delivery",
      "vendor",
      "manually",
      "record",
      "ready",
      "pool",
      "next",
      "transit",
      "publisher",
      "vendor.",
      "received",
      "complete",
      "order",
      "canceled.",
      "receive",
      "canceled",
      "example",
      "organization",
      "starts",
      "off",
      "project",
      "improve",
      "response",
      "time",
      "critical",
      "service.",
      "database",
      "application",
      "introduced",
      "boost",
      "performance.",
      "procurement",
      "team",
      "member",
      "places",
      "purchase",
      "once",
      "issued",
      "creates",
      "sets",
      "received.",
      "yet",
      "use.",
      "prepare.",
      "status",
      "changes",
      "manager",
      "updates",
      "together"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License models",
    "content": "In the Service Management Software Asset Management module, a license model is a classification applied to license assets. Each license is normalized to a particular license model. The license models home page displays details of your license models. Related topics New SAM How to create a license model How to edit a license model How to retire a license model",
    "url": "samlicensemodel",
    "filename": "samlicensemodel",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "license",
      "models",
      "related",
      "topics",
      "service",
      "management",
      "software",
      "asset",
      "module",
      "model",
      "classification",
      "applied",
      "assets.",
      "normalized",
      "particular",
      "model.",
      "home",
      "page",
      "displays",
      "details",
      "models.",
      "new",
      "sam",
      "create",
      "edit",
      "retire"
    ],
    "language": "en",
    "word_count": 40,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license models",
    "contentLower": "in the service management software asset management module, a license model is a classification applied to license assets. each license is normalized to a particular license model. the license models home page displays details of your license models. related topics new sam how to create a license model how to edit a license model how to retire a license model",
    "keywordsLower": [
      "license",
      "models",
      "related",
      "topics",
      "service",
      "management",
      "software",
      "asset",
      "module",
      "model",
      "classification",
      "applied",
      "assets.",
      "normalized",
      "particular",
      "model.",
      "home",
      "page",
      "displays",
      "details",
      "models.",
      "new",
      "sam",
      "create",
      "edit",
      "retire"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License usage",
    "content": "The License Usage page gives you a full list of license usage records per license model and product version. Create a license usage record Go to License Models. Select the required license model record to open its General tab. In the Software coverage section, select a license metric for the product, and then add the product version as needed. We recommend that you select the highest product version. For more information, see Edit a license model. Click Save. This creates a license usage record on the License Usage page. View a license usage record A new license usage record presents you with some basic information such as the record ID and the display name of the license model. To view more information about the latest license consumption result of the product, follow these steps: Go to the Products page, locate the product you configured, and then calculate license consumption for this product. You can either manually trigger a calculation task or define a scheduler to calculate prod",
    "url": "samlicenseusage",
    "filename": "samlicenseusage",
    "headings": [
      "Create a license usage record",
      "View a license usage record",
      "Delete a license usage record",
      "Related topics"
    ],
    "keywords": [
      "license",
      "usage",
      "create",
      "record",
      "view",
      "delete",
      "related",
      "topics",
      "page",
      "gives",
      "full",
      "list",
      "records",
      "per",
      "model",
      "product",
      "version.",
      "go",
      "models.",
      "select",
      "required",
      "open",
      "general",
      "tab.",
      "software",
      "coverage",
      "section",
      "metric",
      "add",
      "version",
      "needed.",
      "recommend",
      "highest",
      "information",
      "see",
      "edit",
      "model.",
      "click",
      "save.",
      "creates",
      "page.",
      "new",
      "presents",
      "basic",
      "such",
      "id",
      "display",
      "name",
      "about",
      "latest",
      "consumption",
      "result",
      "follow",
      "steps",
      "products",
      "locate",
      "configured",
      "calculate",
      "product.",
      "either",
      "manually",
      "trigger",
      "calculation",
      "task",
      "define",
      "scheduler",
      "compliance",
      "periodically.",
      "products.",
      "after",
      "complete",
      "refresh.",
      "ll",
      "fields",
      "populated",
      "values",
      "including",
      "available",
      "points",
      "consumed",
      "estimated",
      "cost",
      "purchasing",
      "extra",
      "licenses",
      "stay",
      "compliant.",
      "data",
      "displayed",
      "check",
      "disable",
      "field",
      "rule",
      "rather",
      "restrict",
      "table",
      "fields.",
      "details",
      "description",
      "detail"
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license usage",
    "contentLower": "the license usage page gives you a full list of license usage records per license model and product version. create a license usage record go to license models. select the required license model record to open its general tab. in the software coverage section, select a license metric for the product, and then add the product version as needed. we recommend that you select the highest product version. for more information, see edit a license model. click save. this creates a license usage record on the license usage page. view a license usage record a new license usage record presents you with some basic information such as the record id and the display name of the license model. to view more information about the latest license consumption result of the product, follow these steps: go to the products page, locate the product you configured, and then calculate license consumption for this product. you can either manually trigger a calculation task or define a scheduler to calculate prod",
    "keywordsLower": [
      "license",
      "usage",
      "create",
      "record",
      "view",
      "delete",
      "related",
      "topics",
      "page",
      "gives",
      "full",
      "list",
      "records",
      "per",
      "model",
      "product",
      "version.",
      "go",
      "models.",
      "select",
      "required",
      "open",
      "general",
      "tab.",
      "software",
      "coverage",
      "section",
      "metric",
      "add",
      "version",
      "needed.",
      "recommend",
      "highest",
      "information",
      "see",
      "edit",
      "model.",
      "click",
      "save.",
      "creates",
      "page.",
      "new",
      "presents",
      "basic",
      "such",
      "id",
      "display",
      "name",
      "about",
      "latest",
      "consumption",
      "result",
      "follow",
      "steps",
      "products",
      "locate",
      "configured",
      "calculate",
      "product.",
      "either",
      "manually",
      "trigger",
      "calculation",
      "task",
      "define",
      "scheduler",
      "compliance",
      "periodically.",
      "products.",
      "after",
      "complete",
      "refresh.",
      "ll",
      "fields",
      "populated",
      "values",
      "including",
      "available",
      "points",
      "consumed",
      "estimated",
      "cost",
      "purchasing",
      "extra",
      "licenses",
      "stay",
      "compliant.",
      "data",
      "displayed",
      "check",
      "disable",
      "field",
      "rule",
      "rather",
      "restrict",
      "table",
      "fields.",
      "details",
      "description",
      "detail"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License usage workflow",
    "content": "This section describes the metaphases and subordinate phases in the life cycle of a license usage record. The license usage workflow relies on business rules. Metaphase: Unavailable Phase Transition Description New Automatic New is a starting point. If the number of license points consumed is smaller than or equal to the number of license points available, the record will automatically transition to the Compliant phase. If the number of license points consumed exceeds the number of license points available, the record will automatically transition to the Noncompliant phase. Next phase: Compliant or Noncompliant Manual If you don't want to use the managed license usage record anymore, you can manually transition the record to the Retiring phase. Next phase: Retiring Metaphase: Active Phase Transition Description Compliant Automatic If the number of license points consumed exceeds the number of license points available, the record will automatically transition to the Noncompliant phase. ",
    "url": "licenseusagewfl",
    "filename": "licenseusagewfl",
    "headings": [
      "Metaphase: Unavailable",
      "Metaphase: Active",
      "Metaphase: Inactive (End)",
      "Related topics"
    ],
    "keywords": [
      "license",
      "usage",
      "workflow",
      "metaphase",
      "unavailable",
      "active",
      "inactive",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "record.",
      "relies",
      "business",
      "rules.",
      "phase",
      "transition",
      "description",
      "new",
      "automatic",
      "starting",
      "point.",
      "number",
      "points",
      "consumed",
      "smaller",
      "equal",
      "available",
      "record",
      "automatically",
      "compliant",
      "phase.",
      "exceeds",
      "noncompliant",
      "next",
      "manual",
      "don",
      "want",
      "managed",
      "anymore",
      "manually",
      "retiring",
      "ended",
      "stopped",
      "product",
      "goes",
      "none",
      "consumption",
      "result",
      "becomes",
      "zero",
      "continue"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license usage workflow",
    "contentLower": "this section describes the metaphases and subordinate phases in the life cycle of a license usage record. the license usage workflow relies on business rules. metaphase: unavailable phase transition description new automatic new is a starting point. if the number of license points consumed is smaller than or equal to the number of license points available, the record will automatically transition to the compliant phase. if the number of license points consumed exceeds the number of license points available, the record will automatically transition to the noncompliant phase. next phase: compliant or noncompliant manual if you don't want to use the managed license usage record anymore, you can manually transition the record to the retiring phase. next phase: retiring metaphase: active phase transition description compliant automatic if the number of license points consumed exceeds the number of license points available, the record will automatically transition to the noncompliant phase. ",
    "keywordsLower": [
      "license",
      "usage",
      "workflow",
      "metaphase",
      "unavailable",
      "active",
      "inactive",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "record.",
      "relies",
      "business",
      "rules.",
      "phase",
      "transition",
      "description",
      "new",
      "automatic",
      "starting",
      "point.",
      "number",
      "points",
      "consumed",
      "smaller",
      "equal",
      "available",
      "record",
      "automatically",
      "compliant",
      "phase.",
      "exceeds",
      "noncompliant",
      "next",
      "manual",
      "don",
      "want",
      "managed",
      "anymore",
      "manually",
      "retiring",
      "ended",
      "stopped",
      "product",
      "goes",
      "none",
      "consumption",
      "result",
      "becomes",
      "zero",
      "continue"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License metrics",
    "content": "The license metric assigned to a software application measures software usage and defines the consumption calculation algorithms. For a publisher, products are licensed under one or more metrics. Generally, you can measure the use of products licensed under the same metric with the same license rule. Before creating a license rule, you need to manage the license metrics on the Licence Metrics page. All publishers with license metrics are listed in the left panel. The area on the right displays the license metrics under a selected publisher. The System tag indicates that the license metric is out of the box (OOTB). You can't edit or delete an OOTB license metric. Create a new license metric On the Licence Metrics page, click New. In the New License Metric dialog box that appears, set all required fields. Parameter Description Publisher From the drop-down list, select a publisher for whom you want to create the new license metric. Name Specifies the metric name. You can refer to the publ",
    "url": "samlicensemetrics",
    "filename": "samlicensemetrics",
    "headings": [
      "Create a new license metric",
      "Edit a license metric",
      "Delete a license metric"
    ],
    "keywords": [
      "license",
      "metrics",
      "create",
      "new",
      "metric",
      "edit",
      "delete",
      "assigned",
      "software",
      "application",
      "measures",
      "usage",
      "defines",
      "consumption",
      "calculation",
      "algorithms.",
      "publisher",
      "products",
      "licensed",
      "under",
      "one",
      "metrics.",
      "generally",
      "measure",
      "same",
      "rule.",
      "before",
      "creating",
      "rule",
      "need",
      "manage",
      "licence",
      "page.",
      "all",
      "publishers",
      "listed",
      "left",
      "panel.",
      "area",
      "right",
      "displays",
      "selected",
      "publisher.",
      "system",
      "tag",
      "indicates",
      "out",
      "box",
      "ootb",
      "metric.",
      "page",
      "click",
      "new.",
      "dialog",
      "appears",
      "set",
      "required",
      "fields.",
      "parameter",
      "description",
      "drop-down",
      "list",
      "select",
      "whom",
      "want",
      "name",
      "specifies",
      "name.",
      "refer",
      "documentation",
      "field.",
      "code",
      "unique",
      "string",
      "identify",
      "recommend",
      "format.",
      "example",
      "core",
      "cal",
      "microsoft",
      "operating",
      "system.",
      "notes",
      "alphanumeric",
      "characters",
      "underscores",
      "allowed",
      "don",
      "sys",
      "prefix",
      "value",
      "unit",
      "object",
      "based",
      "compliance",
      "calculated.",
      "optional",
      "values",
      "include"
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license metrics",
    "contentLower": "the license metric assigned to a software application measures software usage and defines the consumption calculation algorithms. for a publisher, products are licensed under one or more metrics. generally, you can measure the use of products licensed under the same metric with the same license rule. before creating a license rule, you need to manage the license metrics on the licence metrics page. all publishers with license metrics are listed in the left panel. the area on the right displays the license metrics under a selected publisher. the system tag indicates that the license metric is out of the box (ootb). you can't edit or delete an ootb license metric. create a new license metric on the licence metrics page, click new. in the new license metric dialog box that appears, set all required fields. parameter description publisher from the drop-down list, select a publisher for whom you want to create the new license metric. name specifies the metric name. you can refer to the publ",
    "keywordsLower": [
      "license",
      "metrics",
      "create",
      "new",
      "metric",
      "edit",
      "delete",
      "assigned",
      "software",
      "application",
      "measures",
      "usage",
      "defines",
      "consumption",
      "calculation",
      "algorithms.",
      "publisher",
      "products",
      "licensed",
      "under",
      "one",
      "metrics.",
      "generally",
      "measure",
      "same",
      "rule.",
      "before",
      "creating",
      "rule",
      "need",
      "manage",
      "licence",
      "page.",
      "all",
      "publishers",
      "listed",
      "left",
      "panel.",
      "area",
      "right",
      "displays",
      "selected",
      "publisher.",
      "system",
      "tag",
      "indicates",
      "out",
      "box",
      "ootb",
      "metric.",
      "page",
      "click",
      "new.",
      "dialog",
      "appears",
      "set",
      "required",
      "fields.",
      "parameter",
      "description",
      "drop-down",
      "list",
      "select",
      "whom",
      "want",
      "name",
      "specifies",
      "name.",
      "refer",
      "documentation",
      "field.",
      "code",
      "unique",
      "string",
      "identify",
      "recommend",
      "format.",
      "example",
      "core",
      "cal",
      "microsoft",
      "operating",
      "system.",
      "notes",
      "alphanumeric",
      "characters",
      "underscores",
      "allowed",
      "don",
      "sys",
      "prefix",
      "value",
      "unit",
      "object",
      "based",
      "compliance",
      "calculated.",
      "optional",
      "values",
      "include"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License rules",
    "content": "SAM provides OOTB license rules for Microsoft, Oracle, and some other publishers. For more information, see Supported software titles. To manage license rules, go to the License Rules page: The System item indicates that the license rule is OOTB and can't be edited, deleted, or unpublished. The Published item indicates whether the license rule is available and visible in license rule settings. If you disable this switch, the license rule is in development status. Each license rule is a groovy script. You can view the code in the script by clicking Edit Script. Create a license rule To create a license rule, follow these steps: Click New to create license metadata and a rule script. In the New License Rule dialog box that appears, set all required fields. Parameter Description Name The name of the license rule. Code A unique string to identify the license rule. We recommend that you specify it as <Publisher>_ <License metric>_<License rule name>. Notes: Only alphanumeric characters and ",
    "url": "samlicenserules",
    "filename": "samlicenserules",
    "headings": [
      "Create a license rule",
      "Duplicate a license rule",
      "Edit a license rule",
      "Edit a license rule script",
      "SCRIPT VALIDATION",
      "CONFIGURATION",
      "TEST RESULT",
      "TEST LOG",
      "Delete a license rule",
      "Related topics"
    ],
    "keywords": [
      "license",
      "rules",
      "create",
      "rule",
      "duplicate",
      "edit",
      "script",
      "validation",
      "configuration",
      "test",
      "result",
      "log",
      "delete",
      "related",
      "topics",
      "sam",
      "provides",
      "ootb",
      "microsoft",
      "oracle",
      "publishers.",
      "information",
      "see",
      "supported",
      "software",
      "titles.",
      "manage",
      "go",
      "page",
      "system",
      "item",
      "indicates",
      "edited",
      "deleted",
      "unpublished.",
      "published",
      "whether",
      "available",
      "visible",
      "settings.",
      "disable",
      "switch",
      "development",
      "status.",
      "groovy",
      "script.",
      "view",
      "code",
      "clicking",
      "follow",
      "steps",
      "click",
      "new",
      "metadata",
      "dialog",
      "box",
      "appears",
      "set",
      "all",
      "required",
      "fields.",
      "parameter",
      "description",
      "name",
      "rule.",
      "unique",
      "string",
      "identify",
      "recommend",
      "specify",
      "notes",
      "alphanumeric",
      "characters",
      "underscores",
      "allowed.",
      "don",
      "sys",
      "prefix",
      "value",
      "field.",
      "publisher",
      "select",
      "drop-down",
      "list.",
      "metric",
      "under",
      "selected",
      "publisher.",
      "describes",
      "reference",
      "link",
      "vendor",
      "explaining",
      "calculation",
      "details.",
      "save",
      "save.",
      "both",
      "clone.",
      "clone"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license rules",
    "contentLower": "sam provides ootb license rules for microsoft, oracle, and some other publishers. for more information, see supported software titles. to manage license rules, go to the license rules page: the system item indicates that the license rule is ootb and can't be edited, deleted, or unpublished. the published item indicates whether the license rule is available and visible in license rule settings. if you disable this switch, the license rule is in development status. each license rule is a groovy script. you can view the code in the script by clicking edit script. create a license rule to create a license rule, follow these steps: click new to create license metadata and a rule script. in the new license rule dialog box that appears, set all required fields. parameter description name the name of the license rule. code a unique string to identify the license rule. we recommend that you specify it as <publisher>_ <license metric>_<license rule name>. notes: only alphanumeric characters and ",
    "keywordsLower": [
      "license",
      "rules",
      "create",
      "rule",
      "duplicate",
      "edit",
      "script",
      "validation",
      "configuration",
      "test",
      "result",
      "log",
      "delete",
      "related",
      "topics",
      "sam",
      "provides",
      "ootb",
      "microsoft",
      "oracle",
      "publishers.",
      "information",
      "see",
      "supported",
      "software",
      "titles.",
      "manage",
      "go",
      "page",
      "system",
      "item",
      "indicates",
      "edited",
      "deleted",
      "unpublished.",
      "published",
      "whether",
      "available",
      "visible",
      "settings.",
      "disable",
      "switch",
      "development",
      "status.",
      "groovy",
      "script.",
      "view",
      "code",
      "clicking",
      "follow",
      "steps",
      "click",
      "new",
      "metadata",
      "dialog",
      "box",
      "appears",
      "set",
      "all",
      "required",
      "fields.",
      "parameter",
      "description",
      "name",
      "rule.",
      "unique",
      "string",
      "identify",
      "recommend",
      "specify",
      "notes",
      "alphanumeric",
      "characters",
      "underscores",
      "allowed.",
      "don",
      "sys",
      "prefix",
      "value",
      "field.",
      "publisher",
      "select",
      "drop-down",
      "list.",
      "metric",
      "under",
      "selected",
      "publisher.",
      "describes",
      "reference",
      "link",
      "vendor",
      "explaining",
      "calculation",
      "details.",
      "save",
      "save.",
      "both",
      "clone.",
      "clone"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import SAM demo data into Service Management",
    "content": "For demonstration purposes, we can import SAM demo data into Service Management, including the Person, Company, Asset Model, License, Contract, and Device record types by utilizing the Integration Studio functionality. To complete the SAM demo data import, follow the steps below. Create an endpoint Log in to the agent interface as the tenant admin.Navigate to Administration > Utilities > Integration > Endpoints. Click Add.Select Rest Executor 2.0 as the endpoint type, enter SAM as the endpoint name, and then click Add. The system creates the endpoint. Click Configure, and then complete the following fields. Field Value Agent Select Agentless. Authentication type Select SMAX. Protocol Select https. Base URL Enter the base URL of the Service Management system: https://<FQDN> Tenant ID Enter the tenant ID of the Service Management tenant admin. Username Enter the username of the Service Management tenant admin. Password Enter the password of the Service Management tenant admin. Confirm pa",
    "url": "importsamdemodataintosmax",
    "filename": "importsamdemodataintosmax",
    "headings": [
      "Create an endpoint",
      "Import the integration",
      "Configure and run the scenario"
    ],
    "keywords": [
      "X.509",
      "https://<FQDN",
      "scenario.In",
      "SMAX.pkb",
      "Import.In",
      "base-64",
      "integration.Set",
      "2.0",
      "import",
      "sam",
      "demo",
      "data",
      "service",
      "management",
      "create",
      "endpoint",
      "integration",
      "configure",
      "run",
      "scenario",
      "demonstration",
      "purposes",
      "including",
      "person",
      "company",
      "asset",
      "model",
      "license",
      "contract",
      "device",
      "record",
      "types",
      "utilizing",
      "studio",
      "functionality.",
      "complete",
      "follow",
      "steps",
      "below.",
      "log",
      "agent",
      "interface",
      "tenant",
      "admin.navigate",
      "administration",
      "utilities",
      "endpoints.",
      "click",
      "add.select",
      "rest",
      "executor",
      "type",
      "enter",
      "name",
      "add.",
      "system",
      "creates",
      "endpoint.",
      "following",
      "fields.",
      "field",
      "value",
      "select",
      "agentless.",
      "authentication",
      "smax.",
      "protocol",
      "https.",
      "base",
      "url",
      "https",
      "id",
      "admin.",
      "username",
      "password",
      "confirm",
      "re-enter",
      "password.",
      "certificate",
      "x.509.",
      "format",
      "pem.",
      "server",
      "open",
      "web",
      "browser",
      "export",
      "root",
      "ca",
      "file",
      "encoded",
      "format.",
      "text",
      "editor",
      "copy",
      "paste",
      "content",
      "field.",
      "invalid",
      "error"
    ],
    "language": "en",
    "word_count": 114,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import sam demo data into service management",
    "contentLower": "for demonstration purposes, we can import sam demo data into service management, including the person, company, asset model, license, contract, and device record types by utilizing the integration studio functionality. to complete the sam demo data import, follow the steps below. create an endpoint log in to the agent interface as the tenant admin.navigate to administration > utilities > integration > endpoints. click add.select rest executor 2.0 as the endpoint type, enter sam as the endpoint name, and then click add. the system creates the endpoint. click configure, and then complete the following fields. field value agent select agentless. authentication type select smax. protocol select https. base url enter the base url of the service management system: https://<fqdn> tenant id enter the tenant id of the service management tenant admin. username enter the username of the service management tenant admin. password enter the password of the service management tenant admin. confirm pa",
    "keywordsLower": [
      "x.509",
      "https://<fqdn",
      "scenario.in",
      "smax.pkb",
      "import.in",
      "base-64",
      "integration.set",
      "2.0",
      "import",
      "sam",
      "demo",
      "data",
      "service",
      "management",
      "create",
      "endpoint",
      "integration",
      "configure",
      "run",
      "scenario",
      "demonstration",
      "purposes",
      "including",
      "person",
      "company",
      "asset",
      "model",
      "license",
      "contract",
      "device",
      "record",
      "types",
      "utilizing",
      "studio",
      "functionality.",
      "complete",
      "follow",
      "steps",
      "below.",
      "log",
      "agent",
      "interface",
      "tenant",
      "admin.navigate",
      "administration",
      "utilities",
      "endpoints.",
      "click",
      "add.select",
      "rest",
      "executor",
      "type",
      "enter",
      "name",
      "add.",
      "system",
      "creates",
      "endpoint.",
      "following",
      "fields.",
      "field",
      "value",
      "select",
      "agentless.",
      "authentication",
      "smax.",
      "protocol",
      "https.",
      "base",
      "url",
      "https",
      "id",
      "admin.",
      "username",
      "password",
      "confirm",
      "re-enter",
      "password.",
      "certificate",
      "x.509.",
      "format",
      "pem.",
      "server",
      "open",
      "web",
      "browser",
      "export",
      "root",
      "ca",
      "file",
      "encoded",
      "format.",
      "text",
      "editor",
      "copy",
      "paste",
      "content",
      "field.",
      "invalid",
      "error"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Microsoft 365 for SAM",
    "content": "This page describes how to configure integration with Microsoft 365 to import user entitlement data and license consumption data from Microsoft 365. This is part of the SaaS product setup in SAM. Register a Microsoft 365 application Registering a Microsoft 365 app is necessary to get the required credentials and permissions for integrating your app with Microsoft 365 services and APIs. To register a Microsoft 365 application, follow these steps. Sign in to the Azure portal Go to the Azure portal and sign in with your Azure account as a global administrator of your organization. Create a new app registration In the Azure portal, click Azure Active Directory in the left menu. Under the Manage section, click App registrations > New registration. Configure app registration Enter a name for your app in the Name field. Choose the appropriate account type in the Supported account types section, and select Accounts in any organizational directory (Any Azure AD directory - Multitenant). In the ",
    "url": "integratesamwithms365products",
    "filename": "integratesamwithms365products",
    "headings": [
      "Register a Microsoft 365 application",
      "Sign in to the Azure portal",
      "Create a new app registration",
      "Configure app registration",
      "Configure the API permissions",
      "Grant admin consent",
      "Find the essential information for configuring the endpoint",
      "Prepare an integration user",
      "Set up the integration to import license consumption and entitlement data",
      "Create an endpoint",
      "Create an integration",
      "Add the scenario",
      "Next steps"
    ],
    "keywords": [
      "Reports.Read",
      "X.509",
      "Directory.Read",
      "v2.0",
      "https://graph.microsoft.com/.default",
      "Organization.Read",
      "1.0",
      "microsoft.com",
      "https://graph.microsoft.com",
      "https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token",
      "microsoftonline.com",
      "2.0",
      "User.Read",
      "integrate",
      "microsoft",
      "365",
      "sam",
      "register",
      "application",
      "sign",
      "azure",
      "portal",
      "create",
      "new",
      "app",
      "registration",
      "configure",
      "api",
      "permissions",
      "grant",
      "admin",
      "consent",
      "find",
      "essential",
      "information",
      "configuring",
      "endpoint",
      "prepare",
      "integration",
      "user",
      "set",
      "import",
      "license",
      "consumption",
      "entitlement",
      "data",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "365.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "registering",
      "necessary",
      "get",
      "required",
      "credentials",
      "integrating",
      "services",
      "apis.",
      "follow",
      "steps.",
      "go",
      "account",
      "global",
      "administrator",
      "organization.",
      "click",
      "active",
      "directory",
      "left",
      "menu.",
      "under",
      "manage",
      "section",
      "registrations",
      "registration.",
      "enter",
      "name",
      "field.",
      "choose",
      "appropriate",
      "type",
      "supported",
      "types",
      "select",
      "accounts",
      "any",
      "organizational",
      "ad",
      "multitenant",
      "redirect",
      "uri",
      "specify"
    ],
    "language": "en",
    "word_count": 107,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with microsoft 365 for sam",
    "contentLower": "this page describes how to configure integration with microsoft 365 to import user entitlement data and license consumption data from microsoft 365. this is part of the saas product setup in sam. register a microsoft 365 application registering a microsoft 365 app is necessary to get the required credentials and permissions for integrating your app with microsoft 365 services and apis. to register a microsoft 365 application, follow these steps. sign in to the azure portal go to the azure portal and sign in with your azure account as a global administrator of your organization. create a new app registration in the azure portal, click azure active directory in the left menu. under the manage section, click app registrations > new registration. configure app registration enter a name for your app in the name field. choose the appropriate account type in the supported account types section, and select accounts in any organizational directory (any azure ad directory - multitenant). in the ",
    "keywordsLower": [
      "reports.read",
      "x.509",
      "directory.read",
      "v2.0",
      "https://graph.microsoft.com/.default",
      "organization.read",
      "1.0",
      "microsoft.com",
      "https://graph.microsoft.com",
      "https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token",
      "microsoftonline.com",
      "2.0",
      "user.read",
      "integrate",
      "microsoft",
      "365",
      "sam",
      "register",
      "application",
      "sign",
      "azure",
      "portal",
      "create",
      "new",
      "app",
      "registration",
      "configure",
      "api",
      "permissions",
      "grant",
      "admin",
      "consent",
      "find",
      "essential",
      "information",
      "configuring",
      "endpoint",
      "prepare",
      "integration",
      "user",
      "set",
      "import",
      "license",
      "consumption",
      "entitlement",
      "data",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "365.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "registering",
      "necessary",
      "get",
      "required",
      "credentials",
      "integrating",
      "services",
      "apis.",
      "follow",
      "steps.",
      "go",
      "account",
      "global",
      "administrator",
      "organization.",
      "click",
      "active",
      "directory",
      "left",
      "menu.",
      "under",
      "manage",
      "section",
      "registrations",
      "registration.",
      "enter",
      "name",
      "field.",
      "choose",
      "appropriate",
      "type",
      "supported",
      "types",
      "select",
      "accounts",
      "any",
      "organizational",
      "ad",
      "multitenant",
      "redirect",
      "uri",
      "specify"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Salesforce Sales Cloud for SAM",
    "content": "This page describes how to configure integration with Salesforce Sales Cloud to to import user entitlement data and license consumption data from Salesforce. This is part of the SaaS product setup in SAM. Register a Salesforce application Registering a Salesforce application for integration involves creating a connected app in your Salesforce organization. This enables your application to securely access Salesforce resources using OAuth authentication. The following is a step-by-step guide on how to register a Salesforce application for integration. Log in to Salesforce Log in to your Salesforce account with the necessary privileges to create and manage connected apps. Configure permission sets Permission sets in Salesforce allow you to grant specific permissions and access rights to users or applications. You need to create and assign a permission set to the API user to interact with your connected app. In the Salesforce Setup Home page, type Permission Sets in the Quick Find box on t",
    "url": "integratesamwithsalesforceproducts",
    "filename": "integratesamwithsalesforceproducts",
    "headings": [
      "Register a Salesforce application",
      "Log in to Salesforce",
      "Configure permission sets",
      "Set up an API user",
      "Access the App Manager",
      "​​​​Find the essential information for configuring the endpoint",
      "Prepare an integration user",
      "Set up the integration",
      "Create an endpoint",
      "Create an integration",
      "Add the scenario",
      "Next steps"
    ],
    "keywords": [
      "X.509",
      "https://login.salesforce.com/services/oauth2/callback",
      "https://<Base",
      "salesforce.com",
      "https://<Salesforce",
      "v58.0",
      "2.0",
      "integrate",
      "salesforce",
      "sales",
      "cloud",
      "sam",
      "register",
      "application",
      "log",
      "configure",
      "permission",
      "sets",
      "set",
      "api",
      "user",
      "access",
      "app",
      "manager",
      "find",
      "essential",
      "information",
      "configuring",
      "endpoint",
      "prepare",
      "integration",
      "create",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "import",
      "entitlement",
      "data",
      "license",
      "consumption",
      "salesforce.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "registering",
      "involves",
      "creating",
      "connected",
      "organization.",
      "enables",
      "securely",
      "resources",
      "oauth",
      "authentication.",
      "following",
      "step-by-step",
      "guide",
      "integration.",
      "account",
      "necessary",
      "privileges",
      "manage",
      "apps.",
      "allow",
      "grant",
      "specific",
      "permissions",
      "rights",
      "users",
      "applications.",
      "need",
      "assign",
      "interact",
      "app.",
      "home",
      "type",
      "quick",
      "box",
      "top-left",
      "corner",
      "open",
      "it.",
      "click",
      "new",
      "button",
      "set.",
      "enter",
      "including",
      "label",
      "name",
      "save.",
      "go",
      "system",
      "edit",
      "enable"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with salesforce sales cloud for sam",
    "contentLower": "this page describes how to configure integration with salesforce sales cloud to to import user entitlement data and license consumption data from salesforce. this is part of the saas product setup in sam. register a salesforce application registering a salesforce application for integration involves creating a connected app in your salesforce organization. this enables your application to securely access salesforce resources using oauth authentication. the following is a step-by-step guide on how to register a salesforce application for integration. log in to salesforce log in to your salesforce account with the necessary privileges to create and manage connected apps. configure permission sets permission sets in salesforce allow you to grant specific permissions and access rights to users or applications. you need to create and assign a permission set to the api user to interact with your connected app. in the salesforce setup home page, type permission sets in the quick find box on t",
    "keywordsLower": [
      "x.509",
      "https://login.salesforce.com/services/oauth2/callback",
      "https://<base",
      "salesforce.com",
      "https://<salesforce",
      "v58.0",
      "2.0",
      "integrate",
      "salesforce",
      "sales",
      "cloud",
      "sam",
      "register",
      "application",
      "log",
      "configure",
      "permission",
      "sets",
      "set",
      "api",
      "user",
      "access",
      "app",
      "manager",
      "find",
      "essential",
      "information",
      "configuring",
      "endpoint",
      "prepare",
      "integration",
      "create",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "import",
      "entitlement",
      "data",
      "license",
      "consumption",
      "salesforce.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "registering",
      "involves",
      "creating",
      "connected",
      "organization.",
      "enables",
      "securely",
      "resources",
      "oauth",
      "authentication.",
      "following",
      "step-by-step",
      "guide",
      "integration.",
      "account",
      "necessary",
      "privileges",
      "manage",
      "apps.",
      "allow",
      "grant",
      "specific",
      "permissions",
      "rights",
      "users",
      "applications.",
      "need",
      "assign",
      "interact",
      "app.",
      "home",
      "type",
      "quick",
      "box",
      "top-left",
      "corner",
      "open",
      "it.",
      "click",
      "new",
      "button",
      "set.",
      "enter",
      "including",
      "label",
      "name",
      "save.",
      "go",
      "system",
      "edit",
      "enable"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Lucid for SAM",
    "content": "This page describes how to configure the integration with Lucid to import user entitlement data (for Lucidspark and Lucidchart) from Lucid. This is part of the SaaS product setup in SAM. Enable SCIM To integrate with Lucid for SAM, a Lucid account admin needs to enable SCIM in the Lucid Admin Panel. To enable SCIM, you must: Have an Enterprise subscription for a Lucid product. Contact Lucid support to have SCIM enabled for your account. After enabling SCIM, go to SCIM Settings page (Admin > App Integrations > SCIM) and generate a bearer token. Take note of the generated token and the Lucid base URL. You will need these values during the integration configuration in Service Management . Prepare an integration user You need to prepare an integration user before setting up the integration. For details on how to prepare an integration user, see Prepare an integration user. Set up the integration To set up the integration for Lucid data import, perform the following steps. Create an endpoin",
    "url": "integratesamwithlucidproducts",
    "filename": "integratesamwithlucidproducts",
    "headings": [
      "Enable SCIM",
      "Prepare an integration user",
      "Set up the integration",
      "Create an endpoint",
      "Create an integration",
      "Add the scenario",
      "Next steps"
    ],
    "keywords": [
      "2.0",
      "integrate",
      "lucid",
      "sam",
      "enable",
      "scim",
      "prepare",
      "integration",
      "user",
      "set",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "import",
      "entitlement",
      "data",
      "lucidspark",
      "lucidchart",
      "lucid.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "account",
      "admin",
      "needs",
      "panel.",
      "enterprise",
      "subscription",
      "product.",
      "contact",
      "support",
      "enabled",
      "account.",
      "after",
      "enabling",
      "go",
      "settings",
      "app",
      "integrations",
      "generate",
      "bearer",
      "token.",
      "take",
      "note",
      "generated",
      "token",
      "base",
      "url.",
      "need",
      "values",
      "during",
      "configuration",
      "service",
      "management",
      "before",
      "setting",
      "integration.",
      "details",
      "see",
      "user.",
      "perform",
      "following",
      "steps.",
      "rest",
      "executor",
      "type",
      "log",
      "agent",
      "interface",
      "administrator.",
      "navigate",
      "administration",
      "utilities",
      "endpoints.",
      "click",
      "add.",
      "select",
      "enter",
      "name",
      "example",
      "system",
      "creates",
      "endpoint.",
      "complete",
      "fields.",
      "field",
      "value",
      "general",
      "agentless",
      "authentication",
      "protocol",
      "https"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with lucid for sam",
    "contentLower": "this page describes how to configure the integration with lucid to import user entitlement data (for lucidspark and lucidchart) from lucid. this is part of the saas product setup in sam. enable scim to integrate with lucid for sam, a lucid account admin needs to enable scim in the lucid admin panel. to enable scim, you must: have an enterprise subscription for a lucid product. contact lucid support to have scim enabled for your account. after enabling scim, go to scim settings page (admin > app integrations > scim) and generate a bearer token. take note of the generated token and the lucid base url. you will need these values during the integration configuration in service management . prepare an integration user you need to prepare an integration user before setting up the integration. for details on how to prepare an integration user, see prepare an integration user. set up the integration to set up the integration for lucid data import, perform the following steps. create an endpoin",
    "keywordsLower": [
      "2.0",
      "integrate",
      "lucid",
      "sam",
      "enable",
      "scim",
      "prepare",
      "integration",
      "user",
      "set",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "import",
      "entitlement",
      "data",
      "lucidspark",
      "lucidchart",
      "lucid.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "account",
      "admin",
      "needs",
      "panel.",
      "enterprise",
      "subscription",
      "product.",
      "contact",
      "support",
      "enabled",
      "account.",
      "after",
      "enabling",
      "go",
      "settings",
      "app",
      "integrations",
      "generate",
      "bearer",
      "token.",
      "take",
      "note",
      "generated",
      "token",
      "base",
      "url.",
      "need",
      "values",
      "during",
      "configuration",
      "service",
      "management",
      "before",
      "setting",
      "integration.",
      "details",
      "see",
      "user.",
      "perform",
      "following",
      "steps.",
      "rest",
      "executor",
      "type",
      "log",
      "agent",
      "interface",
      "administrator.",
      "navigate",
      "administration",
      "utilities",
      "endpoints.",
      "click",
      "add.",
      "select",
      "enter",
      "name",
      "example",
      "system",
      "creates",
      "endpoint.",
      "complete",
      "fields.",
      "field",
      "value",
      "general",
      "agentless",
      "authentication",
      "protocol",
      "https"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Smartsheet for SAM",
    "content": "This page describes how to configure integration with Smartsheet to import user entitlement data from Smartsheet. This is part of the SaaS product setup in SAM. Generate an API key in Smartsheet You need to enter a Smartsheet API key (token) when you set up the Smartsheet integration in Service Management. To generate the API key, perform the following steps in Smartsheet: On the left Navigation Bar, select Account. Select Apps & Integrations. In the Personal Settings form, select API Access. In the API Access tab, select Generate new access token. Copy the generated token. For security reasons, it will not be visible again in your account settings. Prepare an integration user Prepare an integration user before setting up the integration. For details of how to prepare an integration user, see Prepare an integration user. Set up the Smartsheet integration in Service Management To set up the Smartsheet integration for Smartsheet data import, perform the following steps. Create an endpoin",
    "url": "integratesamwithsmartsheet",
    "filename": "integratesamwithsmartsheet",
    "headings": [
      "Generate an API key in Smartsheet",
      "Prepare an integration user",
      "Set up the Smartsheet integration in Service Management",
      "Create an endpoint",
      "Create an integration",
      "Add the scenario",
      "Next steps"
    ],
    "keywords": [
      "2.0",
      "https://api.smartsheet.com",
      "X.509",
      "smartsheet.com",
      "integrate",
      "smartsheet",
      "sam",
      "generate",
      "api",
      "key",
      "prepare",
      "integration",
      "user",
      "set",
      "service",
      "management",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "import",
      "entitlement",
      "data",
      "smartsheet.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "need",
      "enter",
      "token",
      "management.",
      "perform",
      "following",
      "left",
      "navigation",
      "bar",
      "select",
      "account.",
      "apps",
      "integrations.",
      "personal",
      "settings",
      "form",
      "access.",
      "access",
      "tab",
      "new",
      "token.",
      "copy",
      "generated",
      "security",
      "reasons",
      "visible",
      "again",
      "account",
      "settings.",
      "before",
      "setting",
      "integration.",
      "details",
      "see",
      "user.",
      "steps.",
      "rest",
      "executor",
      "type",
      "log",
      "agent",
      "interface",
      "administrator.",
      "navigate",
      "administration",
      "utilities",
      "endpoints.",
      "click",
      "add.",
      "name",
      "example",
      "system",
      "creates",
      "endpoint.",
      "complete",
      "fields.",
      "agentless",
      "direct",
      "connection",
      "api.",
      "authentication",
      "bearer",
      "protocol",
      "https.",
      "base",
      "url"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with smartsheet for sam",
    "contentLower": "this page describes how to configure integration with smartsheet to import user entitlement data from smartsheet. this is part of the saas product setup in sam. generate an api key in smartsheet you need to enter a smartsheet api key (token) when you set up the smartsheet integration in service management. to generate the api key, perform the following steps in smartsheet: on the left navigation bar, select account. select apps & integrations. in the personal settings form, select api access. in the api access tab, select generate new access token. copy the generated token. for security reasons, it will not be visible again in your account settings. prepare an integration user prepare an integration user before setting up the integration. for details of how to prepare an integration user, see prepare an integration user. set up the smartsheet integration in service management to set up the smartsheet integration for smartsheet data import, perform the following steps. create an endpoin",
    "keywordsLower": [
      "2.0",
      "https://api.smartsheet.com",
      "x.509",
      "smartsheet.com",
      "integrate",
      "smartsheet",
      "sam",
      "generate",
      "api",
      "key",
      "prepare",
      "integration",
      "user",
      "set",
      "service",
      "management",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "import",
      "entitlement",
      "data",
      "smartsheet.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "need",
      "enter",
      "token",
      "management.",
      "perform",
      "following",
      "left",
      "navigation",
      "bar",
      "select",
      "account.",
      "apps",
      "integrations.",
      "personal",
      "settings",
      "form",
      "access.",
      "access",
      "tab",
      "new",
      "token.",
      "copy",
      "generated",
      "security",
      "reasons",
      "visible",
      "again",
      "account",
      "settings.",
      "before",
      "setting",
      "integration.",
      "details",
      "see",
      "user.",
      "steps.",
      "rest",
      "executor",
      "type",
      "log",
      "agent",
      "interface",
      "administrator.",
      "navigate",
      "administration",
      "utilities",
      "endpoints.",
      "click",
      "add.",
      "name",
      "example",
      "system",
      "creates",
      "endpoint.",
      "complete",
      "fields.",
      "agentless",
      "direct",
      "connection",
      "api.",
      "authentication",
      "bearer",
      "protocol",
      "https.",
      "base",
      "url"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Asana for SAM",
    "content": "This page describes how to configure integration with Asana to import user entitlement data from Asana. This is part of the SaaS product setup in SAM. Generate a service account token in Asana You need to enter an Asana service account token when you set up the Asana integration in Service Management. To generate the service account token, perform the following steps in Asana: Log in to the Asana admin console as a super admin. Click the Apps tab. Click Service Accounts. Click the Add Service Account button. This opens the Add service account form. Enter a name and a description for the service account, and then select Full permissions under Permission scopes. Click Save changes. Copy the generated token. For security reasons, the token is visible only once. Prepare an integration user Prepare an integration user before setting up the integration. For details of how to prepare an integration user, see Prepare an integration user. Set up the Asana integration in Service Management To se",
    "url": "integratesamwithasana",
    "filename": "integratesamwithasana",
    "headings": [
      "Generate a service account token in Asana",
      "Prepare an integration user",
      "Set up the Asana integration in Service Management",
      "Create an endpoint",
      "Create an integration",
      "Add the scenario",
      "Next steps"
    ],
    "keywords": [
      "2.0",
      "asana.com",
      "https://app.asana.com",
      "X.509",
      "integrate",
      "asana",
      "sam",
      "generate",
      "service",
      "account",
      "token",
      "prepare",
      "integration",
      "user",
      "set",
      "management",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "import",
      "entitlement",
      "data",
      "asana.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "need",
      "enter",
      "management.",
      "perform",
      "following",
      "log",
      "admin",
      "console",
      "super",
      "admin.",
      "click",
      "apps",
      "tab.",
      "accounts.",
      "button.",
      "opens",
      "form.",
      "name",
      "description",
      "select",
      "full",
      "permissions",
      "under",
      "permission",
      "scopes.",
      "save",
      "changes.",
      "copy",
      "generated",
      "token.",
      "security",
      "reasons",
      "visible",
      "once.",
      "before",
      "setting",
      "integration.",
      "details",
      "see",
      "user.",
      "steps.",
      "rest",
      "executor",
      "type",
      "agent",
      "interface",
      "administrator.",
      "navigate",
      "administration",
      "utilities",
      "endpoints.",
      "add.",
      "example",
      "system",
      "creates",
      "endpoint.",
      "complete",
      "fields.",
      "agentless",
      "direct",
      "connection",
      "api.",
      "authentication",
      "bearer",
      "protocol",
      "https."
    ],
    "language": "en",
    "word_count": 109,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with asana for sam",
    "contentLower": "this page describes how to configure integration with asana to import user entitlement data from asana. this is part of the saas product setup in sam. generate a service account token in asana you need to enter an asana service account token when you set up the asana integration in service management. to generate the service account token, perform the following steps in asana: log in to the asana admin console as a super admin. click the apps tab. click service accounts. click the add service account button. this opens the add service account form. enter a name and a description for the service account, and then select full permissions under permission scopes. click save changes. copy the generated token. for security reasons, the token is visible only once. prepare an integration user prepare an integration user before setting up the integration. for details of how to prepare an integration user, see prepare an integration user. set up the asana integration in service management to se",
    "keywordsLower": [
      "2.0",
      "asana.com",
      "https://app.asana.com",
      "x.509",
      "integrate",
      "asana",
      "sam",
      "generate",
      "service",
      "account",
      "token",
      "prepare",
      "integration",
      "user",
      "set",
      "management",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "import",
      "entitlement",
      "data",
      "asana.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "need",
      "enter",
      "management.",
      "perform",
      "following",
      "log",
      "admin",
      "console",
      "super",
      "admin.",
      "click",
      "apps",
      "tab.",
      "accounts.",
      "button.",
      "opens",
      "form.",
      "name",
      "description",
      "select",
      "full",
      "permissions",
      "under",
      "permission",
      "scopes.",
      "save",
      "changes.",
      "copy",
      "generated",
      "token.",
      "security",
      "reasons",
      "visible",
      "once.",
      "before",
      "setting",
      "integration.",
      "details",
      "see",
      "user.",
      "steps.",
      "rest",
      "executor",
      "type",
      "agent",
      "interface",
      "administrator.",
      "navigate",
      "administration",
      "utilities",
      "endpoints.",
      "add.",
      "example",
      "system",
      "creates",
      "endpoint.",
      "complete",
      "fields.",
      "agentless",
      "direct",
      "connection",
      "api.",
      "authentication",
      "bearer",
      "protocol",
      "https."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Webex for SAM",
    "content": "This page describes how to configure integration with Webex to import user entitlement data and license consumption data from Webex. This is part of the SaaS product setup in SAM. Create an integration in Webex To integrate with Webex, you need to create an integration in Webex. Perform the following steps: Log in to the Webex Developer Portal. Select My Webex Apps from the menu under your avatar at the top of the page. Click Create a New App, and then click Create an Integration to start the wizard. Provide some basic information like the integration's name, description, and logo. Enter https://<Service Management host>/rest/<Service Management tenant Id>/opb/oauth/callback in the Redirect URI field. Select these scopes: spark-admin:licenses_read, spark-admin:people_read. Click Add Integration. Take note of the generated Client ID and Client Secret. You will need to enter them when you configure the Webex endpoint in Service Management. Prepare an integration user Prepare an integrati",
    "url": "integratesamwithwebex",
    "filename": "integratesamwithwebex",
    "headings": [
      "Create an integration in Webex",
      "Prepare an integration user",
      "Set up the Webex integration in Service Management",
      "Create an endpoint",
      "Create an integration",
      "Add the scenario",
      "Next steps"
    ],
    "keywords": [
      "X.509",
      "webexapis.com",
      "https://webexapis.com/v1/access_token",
      "https://webexapis.com/v1/authorize",
      "https://<Service",
      "2.0",
      "https://webexapis.com",
      "integrate",
      "webex",
      "sam",
      "create",
      "integration",
      "prepare",
      "user",
      "set",
      "service",
      "management",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "import",
      "entitlement",
      "data",
      "license",
      "consumption",
      "webex.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "need",
      "perform",
      "following",
      "log",
      "developer",
      "portal.",
      "select",
      "apps",
      "menu",
      "under",
      "avatar",
      "top",
      "page.",
      "click",
      "new",
      "app",
      "start",
      "wizard.",
      "provide",
      "basic",
      "information",
      "like",
      "name",
      "description",
      "logo.",
      "enter",
      "https",
      "rest",
      "opb",
      "oauth",
      "callback",
      "redirect",
      "uri",
      "field.",
      "scopes",
      "spark-admin",
      "integration.",
      "take",
      "note",
      "generated",
      "client",
      "id",
      "secret.",
      "management.",
      "before",
      "setting",
      "details",
      "see",
      "user.",
      "steps.",
      "executor",
      "type",
      "agent",
      "interface",
      "administrator.",
      "navigate",
      "administration",
      "utilities",
      "endpoints.",
      "add.",
      "example",
      "system",
      "creates",
      "endpoint."
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with webex for sam",
    "contentLower": "this page describes how to configure integration with webex to import user entitlement data and license consumption data from webex. this is part of the saas product setup in sam. create an integration in webex to integrate with webex, you need to create an integration in webex. perform the following steps: log in to the webex developer portal. select my webex apps from the menu under your avatar at the top of the page. click create a new app, and then click create an integration to start the wizard. provide some basic information like the integration's name, description, and logo. enter https://<service management host>/rest/<service management tenant id>/opb/oauth/callback in the redirect uri field. select these scopes: spark-admin:licenses_read, spark-admin:people_read. click add integration. take note of the generated client id and client secret. you will need to enter them when you configure the webex endpoint in service management. prepare an integration user prepare an integrati",
    "keywordsLower": [
      "x.509",
      "webexapis.com",
      "https://webexapis.com/v1/access_token",
      "https://webexapis.com/v1/authorize",
      "https://<service",
      "2.0",
      "https://webexapis.com",
      "integrate",
      "webex",
      "sam",
      "create",
      "integration",
      "prepare",
      "user",
      "set",
      "service",
      "management",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "import",
      "entitlement",
      "data",
      "license",
      "consumption",
      "webex.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "need",
      "perform",
      "following",
      "log",
      "developer",
      "portal.",
      "select",
      "apps",
      "menu",
      "under",
      "avatar",
      "top",
      "page.",
      "click",
      "new",
      "app",
      "start",
      "wizard.",
      "provide",
      "basic",
      "information",
      "like",
      "name",
      "description",
      "logo.",
      "enter",
      "https",
      "rest",
      "opb",
      "oauth",
      "callback",
      "redirect",
      "uri",
      "field.",
      "scopes",
      "spark-admin",
      "integration.",
      "take",
      "note",
      "generated",
      "client",
      "id",
      "secret.",
      "management.",
      "before",
      "setting",
      "details",
      "see",
      "user.",
      "steps.",
      "executor",
      "type",
      "agent",
      "interface",
      "administrator.",
      "navigate",
      "administration",
      "utilities",
      "endpoints.",
      "add.",
      "example",
      "system",
      "creates",
      "endpoint."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Jira for SAM",
    "content": "This page describes how to configure an integration with Atlassian Jira Software Cloud to import license consumption and user entitlement data. This is part of the SaaS product setup in SAM. Generate an account token in Jira The Jira integration uses Basic Authentication for different APIs. To configure basic authentication, follow these steps: Log in to the Atlassian Developer console. Click your profile in the upper right corner and select Account settings. In the Contact section, select Anyone to grant access to your email address. If you are concerned about security issues related to sharing personal email addresses, you don’t need to make any changes here. When importing user entitlements, the accountId will be used as the user principal name instead of the email. Go to the Security tab page and create an API token by following the instructions in Manage API tokens for your Atlassian account. Note down this token for later use. See the Create an endpoint section below. Obtain Clou",
    "url": "integratesamwithjira",
    "filename": "integratesamwithjira",
    "headings": [
      "Generate an account token in Jira",
      "Obtain Cloud ID value",
      "Prepare an integration user",
      "Set up the Jira integration in Service Management",
      "Create an endpoint",
      "Create an integration",
      "Add the scenario",
      "Next steps"
    ],
    "keywords": [
      "X.509",
      "atlassian.com",
      "https://<FQDN",
      "https://admin.atlassian.com/o/<organization_id>/products/jira-software/<cloud_id",
      "2.0",
      "integrate",
      "jira",
      "sam",
      "generate",
      "account",
      "token",
      "obtain",
      "cloud",
      "id",
      "value",
      "prepare",
      "integration",
      "user",
      "set",
      "service",
      "management",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "atlassian",
      "software",
      "import",
      "license",
      "consumption",
      "entitlement",
      "data.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "uses",
      "basic",
      "authentication",
      "different",
      "apis.",
      "follow",
      "log",
      "developer",
      "console.",
      "click",
      "profile",
      "upper",
      "right",
      "corner",
      "select",
      "settings.",
      "contact",
      "section",
      "anyone",
      "grant",
      "access",
      "email",
      "address.",
      "concerned",
      "about",
      "security",
      "issues",
      "related",
      "sharing",
      "personal",
      "addresses",
      "don",
      "need",
      "make",
      "any",
      "changes",
      "here.",
      "importing",
      "entitlements",
      "accountid",
      "principal",
      "name",
      "instead",
      "email.",
      "go",
      "tab",
      "api",
      "following",
      "instructions",
      "manage",
      "tokens",
      "account.",
      "note",
      "later",
      "use.",
      "see",
      "below.",
      "instance."
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with jira for sam",
    "contentLower": "this page describes how to configure an integration with atlassian jira software cloud to import license consumption and user entitlement data. this is part of the saas product setup in sam. generate an account token in jira the jira integration uses basic authentication for different apis. to configure basic authentication, follow these steps: log in to the atlassian developer console. click your profile in the upper right corner and select account settings. in the contact section, select anyone to grant access to your email address. if you are concerned about security issues related to sharing personal email addresses, you don’t need to make any changes here. when importing user entitlements, the accountid will be used as the user principal name instead of the email. go to the security tab page and create an api token by following the instructions in manage api tokens for your atlassian account. note down this token for later use. see the create an endpoint section below. obtain clou",
    "keywordsLower": [
      "x.509",
      "atlassian.com",
      "https://<fqdn",
      "https://admin.atlassian.com/o/<organization_id>/products/jira-software/<cloud_id",
      "2.0",
      "integrate",
      "jira",
      "sam",
      "generate",
      "account",
      "token",
      "obtain",
      "cloud",
      "id",
      "value",
      "prepare",
      "integration",
      "user",
      "set",
      "service",
      "management",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "atlassian",
      "software",
      "import",
      "license",
      "consumption",
      "entitlement",
      "data.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "uses",
      "basic",
      "authentication",
      "different",
      "apis.",
      "follow",
      "log",
      "developer",
      "console.",
      "click",
      "profile",
      "upper",
      "right",
      "corner",
      "select",
      "settings.",
      "contact",
      "section",
      "anyone",
      "grant",
      "access",
      "email",
      "address.",
      "concerned",
      "about",
      "security",
      "issues",
      "related",
      "sharing",
      "personal",
      "addresses",
      "don",
      "need",
      "make",
      "any",
      "changes",
      "here.",
      "importing",
      "entitlements",
      "accountid",
      "principal",
      "name",
      "instead",
      "email.",
      "go",
      "tab",
      "api",
      "following",
      "instructions",
      "manage",
      "tokens",
      "account.",
      "note",
      "later",
      "use.",
      "see",
      "below.",
      "instance."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate with Zoom for SAM",
    "content": "This page describes how to configure an integration with Zoom Cloud to import user entitlement data. This is part of the SaaS product setup in SAM. Configure Zoom application and get Client ID and Client Secret Log in to Zoom App Marketplace. In the upper right corner, click Manage before your profile. Choose Develop > Build App and agree to the API license and terms of use. Select General App and click Create. In the Basic Information section, select Admin-managed to allow account admins to add manage the app, and click Save. In App Credentials, note down the values of Client ID and Client Secret for later endpoint configuration in Service Management. See the Create an endpoint section below. Specify the OAuth Redirect URL field. You can get the value during endpoint configuration in Service Management. Go to the Scopes section, click Add scopes, and enter list_users in the search box. Select View users with the user:read:list_users:admin permission and click Done. In the Add your app",
    "url": "integratesamwithzoom",
    "filename": "integratesamwithzoom",
    "headings": [
      "Configure Zoom application and get Client ID and Client Secret",
      "Prepare an integration user",
      "Set up the Zoom integration in Service Management",
      "Create an endpoint",
      "Create an integration",
      "Add the scenario",
      "Next steps"
    ],
    "keywords": [
      "https://zoom.us/oauth/authorize",
      "X.509",
      "zoom.us",
      "https://api.zoom.us",
      "https://zoom.us/oauth/token",
      "2.0",
      "api.zoom",
      "integrate",
      "zoom",
      "sam",
      "configure",
      "application",
      "get",
      "client",
      "id",
      "secret",
      "prepare",
      "integration",
      "user",
      "set",
      "service",
      "management",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "cloud",
      "import",
      "entitlement",
      "data.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "log",
      "app",
      "marketplace.",
      "upper",
      "right",
      "corner",
      "click",
      "manage",
      "before",
      "profile.",
      "choose",
      "develop",
      "build",
      "agree",
      "api",
      "license",
      "terms",
      "use.",
      "select",
      "general",
      "create.",
      "basic",
      "information",
      "section",
      "admin-managed",
      "allow",
      "account",
      "admins",
      "save.",
      "credentials",
      "note",
      "values",
      "later",
      "configuration",
      "management.",
      "see",
      "below.",
      "specify",
      "oauth",
      "redirect",
      "url",
      "field.",
      "value",
      "during",
      "go",
      "scopes",
      "enter",
      "search",
      "box.",
      "view",
      "users",
      "read",
      "admin",
      "permission",
      "done.",
      "now.",
      "setting",
      "user.",
      "data",
      "perform",
      "following"
    ],
    "language": "en",
    "word_count": 116,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate with zoom for sam",
    "contentLower": "this page describes how to configure an integration with zoom cloud to import user entitlement data. this is part of the saas product setup in sam. configure zoom application and get client id and client secret log in to zoom app marketplace. in the upper right corner, click manage before your profile. choose develop > build app and agree to the api license and terms of use. select general app and click create. in the basic information section, select admin-managed to allow account admins to add manage the app, and click save. in app credentials, note down the values of client id and client secret for later endpoint configuration in service management. see the create an endpoint section below. specify the oauth redirect url field. you can get the value during endpoint configuration in service management. go to the scopes section, click add scopes, and enter list_users in the search box. select view users with the user:read:list_users:admin permission and click done. in the add your app",
    "keywordsLower": [
      "https://zoom.us/oauth/authorize",
      "x.509",
      "zoom.us",
      "https://api.zoom.us",
      "https://zoom.us/oauth/token",
      "2.0",
      "api.zoom",
      "integrate",
      "zoom",
      "sam",
      "configure",
      "application",
      "get",
      "client",
      "id",
      "secret",
      "prepare",
      "integration",
      "user",
      "set",
      "service",
      "management",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "cloud",
      "import",
      "entitlement",
      "data.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "log",
      "app",
      "marketplace.",
      "upper",
      "right",
      "corner",
      "click",
      "manage",
      "before",
      "profile.",
      "choose",
      "develop",
      "build",
      "agree",
      "api",
      "license",
      "terms",
      "use.",
      "select",
      "general",
      "create.",
      "basic",
      "information",
      "section",
      "admin-managed",
      "allow",
      "account",
      "admins",
      "save.",
      "credentials",
      "note",
      "values",
      "later",
      "configuration",
      "management.",
      "see",
      "below.",
      "specify",
      "oauth",
      "redirect",
      "url",
      "field.",
      "value",
      "during",
      "go",
      "scopes",
      "enter",
      "search",
      "box.",
      "view",
      "users",
      "read",
      "admin",
      "permission",
      "done.",
      "now.",
      "setting",
      "user.",
      "data",
      "perform",
      "following"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integrate SAM with Wrike",
    "content": "This page describes how to configure an integration with Wrike to import license consumption and user entitlement data. This is part of the SaaS product setup in SAM. Set up the Wrike application Log in to Wrike with your username and password. In the upper-right corner, click the Wrike Assistant icon. Click Apps & Integrations, and then click the API tab. This takes you to the API Apps page. In App name, enter the application name and click Create new. On the application configuration page, note down the client ID and client secret for later use. Specify the redirect URI, which can be retrieved during endpoint configuration in Service Management. See the Create an endpoint section below. Prepare an integration user Before setting up the integration, prepare an integration user. Set up the Wrike integration in Service Management To set up the Wrike integration for data import, complete the following tasks. Create an endpoint Create an endpoint using the Rest Executor 2.0 endpoint type:",
    "url": "integratesamwithwrike",
    "filename": "integratesamwithwrike",
    "headings": [
      "Set up the Wrike application",
      "Prepare an integration user",
      "Set up the Wrike integration in Service Management",
      "Create an endpoint",
      "Create an integration",
      "Add the scenario",
      "Next steps"
    ],
    "keywords": [
      "X.509",
      "wrike.com",
      "https://login.wrike.com/oauth2/token",
      "2.0",
      "https://login.wrike.com/oauth2/authorize/v4",
      "https://www.wrike.com",
      "integrate",
      "sam",
      "wrike",
      "set",
      "application",
      "prepare",
      "integration",
      "user",
      "service",
      "management",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "import",
      "license",
      "consumption",
      "entitlement",
      "data.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "log",
      "username",
      "password.",
      "upper-right",
      "corner",
      "click",
      "assistant",
      "icon.",
      "apps",
      "integrations",
      "api",
      "tab.",
      "takes",
      "page.",
      "app",
      "name",
      "enter",
      "new.",
      "configuration",
      "note",
      "client",
      "id",
      "secret",
      "later",
      "use.",
      "specify",
      "redirect",
      "uri",
      "retrieved",
      "during",
      "management.",
      "see",
      "section",
      "below.",
      "before",
      "setting",
      "user.",
      "data",
      "complete",
      "following",
      "tasks.",
      "rest",
      "executor",
      "type",
      "agent",
      "interface",
      "administrator.",
      "navigate",
      "administration",
      "utilities",
      "endpoints.",
      "add.",
      "select",
      "example",
      "system",
      "creates",
      "endpoint.",
      "fields",
      "field",
      "value",
      "agentless",
      "direct",
      "connection",
      "wrike.",
      "authentication"
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integrate sam with wrike",
    "contentLower": "this page describes how to configure an integration with wrike to import license consumption and user entitlement data. this is part of the saas product setup in sam. set up the wrike application log in to wrike with your username and password. in the upper-right corner, click the wrike assistant icon. click apps & integrations, and then click the api tab. this takes you to the api apps page. in app name, enter the application name and click create new. on the application configuration page, note down the client id and client secret for later use. specify the redirect uri, which can be retrieved during endpoint configuration in service management. see the create an endpoint section below. prepare an integration user before setting up the integration, prepare an integration user. set up the wrike integration in service management to set up the wrike integration for data import, complete the following tasks. create an endpoint create an endpoint using the rest executor 2.0 endpoint type:",
    "keywordsLower": [
      "x.509",
      "wrike.com",
      "https://login.wrike.com/oauth2/token",
      "2.0",
      "https://login.wrike.com/oauth2/authorize/v4",
      "https://www.wrike.com",
      "integrate",
      "sam",
      "wrike",
      "set",
      "application",
      "prepare",
      "integration",
      "user",
      "service",
      "management",
      "create",
      "endpoint",
      "add",
      "scenario",
      "next",
      "steps",
      "page",
      "describes",
      "configure",
      "import",
      "license",
      "consumption",
      "entitlement",
      "data.",
      "part",
      "saas",
      "product",
      "setup",
      "sam.",
      "log",
      "username",
      "password.",
      "upper-right",
      "corner",
      "click",
      "assistant",
      "icon.",
      "apps",
      "integrations",
      "api",
      "tab.",
      "takes",
      "page.",
      "app",
      "name",
      "enter",
      "new.",
      "configuration",
      "note",
      "client",
      "id",
      "secret",
      "later",
      "use.",
      "specify",
      "redirect",
      "uri",
      "retrieved",
      "during",
      "management.",
      "see",
      "section",
      "below.",
      "before",
      "setting",
      "user.",
      "data",
      "complete",
      "following",
      "tasks.",
      "rest",
      "executor",
      "type",
      "agent",
      "interface",
      "administrator.",
      "navigate",
      "administration",
      "utilities",
      "endpoints.",
      "add.",
      "select",
      "example",
      "system",
      "creates",
      "endpoint.",
      "fields",
      "field",
      "value",
      "agentless",
      "direct",
      "connection",
      "wrike.",
      "authentication"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration Studio scenario rules for SAM",
    "content": "This page describes some important Integration Studio rules/actions used in various integration scenarios that enable the compliance calculation of SaaS products in SAM. Get context information To use the context data passed from SAM in scenario rules, use the Prepare Data rule to define an object (as an example, we name it as Context), and assign the values specified in the table below to various properties of the Context object. This rule should be added at the beginning of the scenarios that require the SAM context data. Property Description Value ProductId SaaS product Id. json_parser(payload.data, '$.ProductId') ProductName SaaS product name. json_parser(payload.data, '$.ProductName') JobInstanceId The job instance Id. json_parser(payload.data, '$.JobInstanceId') SessionIdentifier A unique identifier for the integration session. json_parser(payload.data, '$.SessionIdentifier') The following actions can be found in the Local System category on the Select action form when you add ru",
    "url": "samxieactions",
    "filename": "samxieactions",
    "headings": [
      "Get context information",
      "Create or update license (SaaS products)",
      "Create license consumption (SaaS products)",
      "Create license consumption (on-premises products)",
      "Create or update user entitlement (SaaS products)",
      "Initialize user entitlement (SaaS products)",
      "Update job status"
    ],
    "keywords": [
      "payload.data",
      "integration",
      "studio",
      "scenario",
      "rules",
      "sam",
      "get",
      "context",
      "information",
      "create",
      "update",
      "license",
      "saas",
      "products",
      "consumption",
      "on-premises",
      "user",
      "entitlement",
      "initialize",
      "job",
      "status",
      "page",
      "describes",
      "important",
      "actions",
      "various",
      "scenarios",
      "enable",
      "compliance",
      "calculation",
      "sam.",
      "data",
      "passed",
      "prepare",
      "rule",
      "define",
      "object",
      "example",
      "name",
      "assign",
      "values",
      "specified",
      "table",
      "below",
      "properties",
      "object.",
      "added",
      "beginning",
      "require",
      "data.",
      "property",
      "description",
      "value",
      "productid",
      "product",
      "id.",
      ".productid",
      "productname",
      "name.",
      ".productname",
      "jobinstanceid",
      "instance",
      ".jobinstanceid",
      "sessionidentifier",
      "unique",
      "identifier",
      "session.",
      ".sessionidentifier",
      "following",
      "found",
      "local",
      "system",
      "category",
      "select",
      "action",
      "form",
      "add",
      "scenario.",
      "after",
      "calls",
      "api",
      "retrieve",
      "note",
      "created",
      "new",
      "phase",
      "need",
      "move",
      "manually",
      "through",
      "workflow",
      "proper",
      "required.",
      "version",
      "records",
      "record",
      "already",
      "service",
      "retrieved",
      "model"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration studio scenario rules for sam",
    "contentLower": "this page describes some important integration studio rules/actions used in various integration scenarios that enable the compliance calculation of saas products in sam. get context information to use the context data passed from sam in scenario rules, use the prepare data rule to define an object (as an example, we name it as context), and assign the values specified in the table below to various properties of the context object. this rule should be added at the beginning of the scenarios that require the sam context data. property description value productid saas product id. json_parser(payload.data, '$.productid') productname saas product name. json_parser(payload.data, '$.productname') jobinstanceid the job instance id. json_parser(payload.data, '$.jobinstanceid') sessionidentifier a unique identifier for the integration session. json_parser(payload.data, '$.sessionidentifier') the following actions can be found in the local system category on the select action form when you add ru",
    "keywordsLower": [
      "payload.data",
      "integration",
      "studio",
      "scenario",
      "rules",
      "sam",
      "get",
      "context",
      "information",
      "create",
      "update",
      "license",
      "saas",
      "products",
      "consumption",
      "on-premises",
      "user",
      "entitlement",
      "initialize",
      "job",
      "status",
      "page",
      "describes",
      "important",
      "actions",
      "various",
      "scenarios",
      "enable",
      "compliance",
      "calculation",
      "sam.",
      "data",
      "passed",
      "prepare",
      "rule",
      "define",
      "object",
      "example",
      "name",
      "assign",
      "values",
      "specified",
      "table",
      "below",
      "properties",
      "object.",
      "added",
      "beginning",
      "require",
      "data.",
      "property",
      "description",
      "value",
      "productid",
      "product",
      "id.",
      ".productid",
      "productname",
      "name.",
      ".productname",
      "jobinstanceid",
      "instance",
      ".jobinstanceid",
      "sessionidentifier",
      "unique",
      "identifier",
      "session.",
      ".sessionidentifier",
      "following",
      "found",
      "local",
      "system",
      "category",
      "select",
      "action",
      "form",
      "add",
      "scenario.",
      "after",
      "calls",
      "api",
      "retrieve",
      "note",
      "created",
      "new",
      "phase",
      "need",
      "move",
      "manually",
      "through",
      "workflow",
      "proper",
      "required.",
      "version",
      "records",
      "record",
      "already",
      "service",
      "retrieved",
      "model"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import fixed asset records",
    "content": "You need to import fixed asset records to the Financial Management module when the records are updated. From the main menu, select Run > Financials > Fixed Assets. Click More > Import CSV. In the Import fixed assets dialog box, Click Select CSV. Your file browser opens. Select a CSV file from your local drive and click Open. Service Management will import the file and refresh the fixed asset records. (Optional) You can click More > Import Status to view the import logs. Click here to view a spreadsheet with sample CSV files for a series of records. Import the files sequentially according to their number. Related topics Fixed assets",
    "url": "importfixedasset",
    "filename": "importfixedasset",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "import",
      "fixed",
      "asset",
      "records",
      "related",
      "topics",
      "need",
      "financial",
      "management",
      "module",
      "updated.",
      "main",
      "menu",
      "select",
      "run",
      "financials",
      "assets.",
      "click",
      "csv.",
      "assets",
      "dialog",
      "box",
      "file",
      "browser",
      "opens.",
      "csv",
      "local",
      "drive",
      "open.",
      "service",
      "refresh",
      "records.",
      "optional",
      "status",
      "view",
      "logs.",
      "here",
      "spreadsheet",
      "sample",
      "files",
      "series",
      "sequentially",
      "according",
      "number."
    ],
    "language": "en",
    "word_count": 75,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import fixed asset records",
    "contentLower": "you need to import fixed asset records to the financial management module when the records are updated. from the main menu, select run > financials > fixed assets. click more > import csv. in the import fixed assets dialog box, click select csv. your file browser opens. select a csv file from your local drive and click open. service management will import the file and refresh the fixed asset records. (optional) you can click more > import status to view the import logs. click here to view a spreadsheet with sample csv files for a series of records. import the files sequentially according to their number. related topics fixed assets",
    "keywordsLower": [
      "import",
      "fixed",
      "asset",
      "records",
      "related",
      "topics",
      "need",
      "financial",
      "management",
      "module",
      "updated.",
      "main",
      "menu",
      "select",
      "run",
      "financials",
      "assets.",
      "click",
      "csv.",
      "assets",
      "dialog",
      "box",
      "file",
      "browser",
      "opens.",
      "csv",
      "local",
      "drive",
      "open.",
      "service",
      "refresh",
      "records.",
      "optional",
      "status",
      "view",
      "logs.",
      "here",
      "spreadsheet",
      "sample",
      "files",
      "series",
      "sequentially",
      "according",
      "number."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import and export a service design",
    "content": "In this topic, artifacts refer to service design and its associated resource offering(s). You can import and export many of the artifacts that provide the basis for cloud automation. The export operation provides the ability to preserve the selected artifacts so they can be used to replicate the services on another system or to restore the artifacts. These exported archive files are preserved in an industry-standard zip archive file format. Installing or replacing artifacts on the target system is supported by import and update operations. The import operation only adds artifacts, whereas the update operation replaces matching artifacts. See the Tasks section below for more information. Note that an import does not automatically associate the resource offering to existing resource providers on the system. After the import, you should manually associate the imported resource offering with the appropriate resource provider. For more information, refer to Select or remove providers for a ",
    "url": "mgmtconssdimportexport",
    "filename": "mgmtconssdimportexport",
    "headings": [
      "Process information",
      "Tasks",
      "Related topics"
    ],
    "keywords": [
      "csa.war",
      "import",
      "export",
      "service",
      "design",
      "process",
      "information",
      "tasks",
      "related",
      "topics",
      "topic",
      "artifacts",
      "refer",
      "associated",
      "resource",
      "offering",
      "many",
      "provide",
      "basis",
      "cloud",
      "automation.",
      "operation",
      "provides",
      "ability",
      "preserve",
      "selected",
      "replicate",
      "services",
      "another",
      "system",
      "restore",
      "artifacts.",
      "exported",
      "archive",
      "files",
      "preserved",
      "industry-standard",
      "zip",
      "file",
      "format.",
      "installing",
      "replacing",
      "target",
      "supported",
      "update",
      "operations.",
      "adds",
      "whereas",
      "replaces",
      "matching",
      "see",
      "section",
      "below",
      "information.",
      "note",
      "automatically",
      "associate",
      "existing",
      "providers",
      "system.",
      "after",
      "manually",
      "imported",
      "appropriate",
      "provider.",
      "select",
      "remove",
      "offering.",
      "custom",
      "component",
      "types",
      "want",
      "dependency",
      "any",
      "before",
      "imported.",
      "configuration",
      "options",
      "active",
      "match",
      "otherwise",
      "doesn",
      "succeed.",
      "destructive",
      "data.",
      "understand",
      "differences",
      "between",
      "ensure",
      "choose",
      "operations",
      "expectations.",
      "flows",
      "during",
      "preview",
      "required",
      "dependencies",
      "don",
      "exist",
      "orchestration"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import and export a service design",
    "contentLower": "in this topic, artifacts refer to service design and its associated resource offering(s). you can import and export many of the artifacts that provide the basis for cloud automation. the export operation provides the ability to preserve the selected artifacts so they can be used to replicate the services on another system or to restore the artifacts. these exported archive files are preserved in an industry-standard zip archive file format. installing or replacing artifacts on the target system is supported by import and update operations. the import operation only adds artifacts, whereas the update operation replaces matching artifacts. see the tasks section below for more information. note that an import does not automatically associate the resource offering to existing resource providers on the system. after the import, you should manually associate the imported resource offering with the appropriate resource provider. for more information, refer to select or remove providers for a ",
    "keywordsLower": [
      "csa.war",
      "import",
      "export",
      "service",
      "design",
      "process",
      "information",
      "tasks",
      "related",
      "topics",
      "topic",
      "artifacts",
      "refer",
      "associated",
      "resource",
      "offering",
      "many",
      "provide",
      "basis",
      "cloud",
      "automation.",
      "operation",
      "provides",
      "ability",
      "preserve",
      "selected",
      "replicate",
      "services",
      "another",
      "system",
      "restore",
      "artifacts.",
      "exported",
      "archive",
      "files",
      "preserved",
      "industry-standard",
      "zip",
      "file",
      "format.",
      "installing",
      "replacing",
      "target",
      "supported",
      "update",
      "operations.",
      "adds",
      "whereas",
      "replaces",
      "matching",
      "see",
      "section",
      "below",
      "information.",
      "note",
      "automatically",
      "associate",
      "existing",
      "providers",
      "system.",
      "after",
      "manually",
      "imported",
      "appropriate",
      "provider.",
      "select",
      "remove",
      "offering.",
      "custom",
      "component",
      "types",
      "want",
      "dependency",
      "any",
      "before",
      "imported.",
      "configuration",
      "options",
      "active",
      "match",
      "otherwise",
      "doesn",
      "succeed.",
      "destructive",
      "data.",
      "understand",
      "differences",
      "between",
      "ensure",
      "choose",
      "operations",
      "expectations.",
      "flows",
      "during",
      "preview",
      "required",
      "dependencies",
      "don",
      "exist",
      "orchestration"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Lifecycle actions for service design",
    "content": "This page describes the service component lifecycle and the lifecycle actions involved in the component lifecycle. Service component lifecycle A service component lifecycle is a collection of actions defined for a service component. The Lifecycle area allows you to specify the actions that are needed to provision, update, and deprovision service components. Use the action selection wizard to add a lifecycle action to a service component from one of the available process engines. Lifecycle actions A lifecycle action is a function that's either run automatically at a specified lifecycle stage or phase, or that's exposed to the subscriber. Most lifecycle actions correspond to OO flows, which contain the logic for running the function. The product also includes actions that run internally. Many internal actions relate to provider and pool selection and resource accounting. Actions include input parameters that offer configuration information to the function. Types of lifecycle actions You ",
    "url": "mgmtconssdeditcomplifecycleactions",
    "filename": "mgmtconssdeditcomplifecycleactions",
    "headings": [
      "Service component lifecycle",
      "Lifecycle actions",
      "Types of lifecycle actions",
      "Lifecycle stages",
      "Provisioning",
      "Operational",
      "Deprovisioning",
      "Lifecycle phases",
      "Lifecycle action sequencing",
      "Tasks"
    ],
    "keywords": [
      "lifecycle",
      "actions",
      "service",
      "design",
      "component",
      "types",
      "stages",
      "provisioning",
      "operational",
      "deprovisioning",
      "phases",
      "action",
      "sequencing",
      "tasks",
      "page",
      "describes",
      "involved",
      "lifecycle.",
      "collection",
      "defined",
      "component.",
      "area",
      "allows",
      "specify",
      "needed",
      "provision",
      "update",
      "deprovision",
      "components.",
      "selection",
      "wizard",
      "add",
      "one",
      "available",
      "process",
      "engines.",
      "function",
      "either",
      "run",
      "automatically",
      "specified",
      "stage",
      "phase",
      "exposed",
      "subscriber.",
      "most",
      "correspond",
      "oo",
      "flows",
      "contain",
      "logic",
      "running",
      "function.",
      "product",
      "includes",
      "internally.",
      "many",
      "internal",
      "relate",
      "provider",
      "pool",
      "resource",
      "accounting.",
      "include",
      "input",
      "parameters",
      "offer",
      "configuration",
      "information",
      "find",
      "two",
      "depending",
      "engines",
      "default",
      "engine",
      "rich",
      "library",
      "automating",
      "software",
      "processes",
      "interacting",
      "applications.",
      "several",
      "special",
      "functions",
      "don",
      "require",
      "interaction",
      "external",
      "systems.",
      "rather",
      "interact",
      "state.",
      "represents",
      "step",
      "three",
      "categories",
      "deprovisioning.",
      "initializing",
      "deploying."
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "lifecycle actions for service design",
    "contentLower": "this page describes the service component lifecycle and the lifecycle actions involved in the component lifecycle. service component lifecycle a service component lifecycle is a collection of actions defined for a service component. the lifecycle area allows you to specify the actions that are needed to provision, update, and deprovision service components. use the action selection wizard to add a lifecycle action to a service component from one of the available process engines. lifecycle actions a lifecycle action is a function that's either run automatically at a specified lifecycle stage or phase, or that's exposed to the subscriber. most lifecycle actions correspond to oo flows, which contain the logic for running the function. the product also includes actions that run internally. many internal actions relate to provider and pool selection and resource accounting. actions include input parameters that offer configuration information to the function. types of lifecycle actions you ",
    "keywordsLower": [
      "lifecycle",
      "actions",
      "service",
      "design",
      "component",
      "types",
      "stages",
      "provisioning",
      "operational",
      "deprovisioning",
      "phases",
      "action",
      "sequencing",
      "tasks",
      "page",
      "describes",
      "involved",
      "lifecycle.",
      "collection",
      "defined",
      "component.",
      "area",
      "allows",
      "specify",
      "needed",
      "provision",
      "update",
      "deprovision",
      "components.",
      "selection",
      "wizard",
      "add",
      "one",
      "available",
      "process",
      "engines.",
      "function",
      "either",
      "run",
      "automatically",
      "specified",
      "stage",
      "phase",
      "exposed",
      "subscriber.",
      "most",
      "correspond",
      "oo",
      "flows",
      "contain",
      "logic",
      "running",
      "function.",
      "product",
      "includes",
      "internally.",
      "many",
      "internal",
      "relate",
      "provider",
      "pool",
      "resource",
      "accounting.",
      "include",
      "input",
      "parameters",
      "offer",
      "configuration",
      "information",
      "find",
      "two",
      "depending",
      "engines",
      "default",
      "engine",
      "rich",
      "library",
      "automating",
      "software",
      "processes",
      "interacting",
      "applications.",
      "several",
      "special",
      "functions",
      "don",
      "require",
      "interaction",
      "external",
      "systems.",
      "rather",
      "interact",
      "state.",
      "represents",
      "step",
      "three",
      "categories",
      "deprovisioning.",
      "initializing",
      "deploying."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Lifecycle action selection wizard for service designs",
    "content": "Use the action selection wizard to create or edit action for the selected lifecycle phase. Required parameters are indicated with an asterisk. If you don't define a value, then you will see a warning message that explains possible issues but you can still apply the change. Tasks Select an action — Search for flows or actions by name or select the process engine from which flow or action is selected. Best practice for creating the OO Flows that will be used in Cloud Management lifecycle actions It's recommended that the OO flows are created in the below packages so that they automatically appear in the default CSA lifecycle action field. /Library/io/cloudslang/amazon /Library/io/cloudslang/microfocus /Library/Integrations /Library/CSA Content Pack If you have OO flows that aren't created within the above folders, you can update the property application.oo_content_root.lifecycle_action by appending the OO folder path that needs to be listed in the lifecycle action wizard. To append the O",
    "url": "mgmtconssdlcactionwiz",
    "filename": "mgmtconssdlcactionwiz",
    "headings": [
      "Tasks",
      "Resume a paused service instance using an API",
      "Related topics"
    ],
    "keywords": [
      "https://<hostname>/dnd-operations-gateway/?TENANTID=<Tenant_id>#/instance/<instanceId>/components",
      "myLab.net",
      "https://<hostname>/dnd/api/service/instance/<instanceId>/resume",
      "https://myHost.myLab.net/dnd-operations-gateway/?TENANTID=174106658#/instance/2c9082c37e828b97017eb4bca2a5003a/components",
      "lifecycle",
      "action",
      "selection",
      "wizard",
      "service",
      "designs",
      "tasks",
      "resume",
      "paused",
      "instance",
      "api",
      "related",
      "topics",
      "create",
      "edit",
      "selected",
      "phase.",
      "required",
      "parameters",
      "indicated",
      "asterisk.",
      "don",
      "define",
      "value",
      "see",
      "warning",
      "message",
      "explains",
      "possible",
      "issues",
      "still",
      "apply",
      "change.",
      "select",
      "search",
      "flows",
      "actions",
      "name",
      "process",
      "engine",
      "flow",
      "selected.",
      "best",
      "practice",
      "creating",
      "oo",
      "cloud",
      "management",
      "recommended",
      "created",
      "below",
      "packages",
      "automatically",
      "appear",
      "default",
      "csa",
      "field.",
      "library",
      "io",
      "cloudslang",
      "amazon",
      "microfocus",
      "integrations",
      "content",
      "pack",
      "aren",
      "above",
      "folders",
      "update",
      "property",
      "appending",
      "folder",
      "path",
      "needs",
      "listed",
      "wizard.",
      "append",
      "perform",
      "following",
      "steps",
      "dnd",
      "application",
      "apis",
      "generate",
      "authorization",
      "token.",
      "get",
      "list",
      "all",
      "properties",
      "getallapplicationproperties",
      "details",
      "getapplicationproperty.",
      "custom",
      "paths.",
      "updateapplicationproperty."
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "lifecycle action selection wizard for service designs",
    "contentLower": "use the action selection wizard to create or edit action for the selected lifecycle phase. required parameters are indicated with an asterisk. if you don't define a value, then you will see a warning message that explains possible issues but you can still apply the change. tasks select an action — search for flows or actions by name or select the process engine from which flow or action is selected. best practice for creating the oo flows that will be used in cloud management lifecycle actions it's recommended that the oo flows are created in the below packages so that they automatically appear in the default csa lifecycle action field. /library/io/cloudslang/amazon /library/io/cloudslang/microfocus /library/integrations /library/csa content pack if you have oo flows that aren't created within the above folders, you can update the property application.oo_content_root.lifecycle_action by appending the oo folder path that needs to be listed in the lifecycle action wizard. to append the o",
    "keywordsLower": [
      "https://<hostname>/dnd-operations-gateway/?tenantid=<tenant_id>#/instance/<instanceid>/components",
      "mylab.net",
      "https://<hostname>/dnd/api/service/instance/<instanceid>/resume",
      "https://myhost.mylab.net/dnd-operations-gateway/?tenantid=174106658#/instance/2c9082c37e828b97017eb4bca2a5003a/components",
      "lifecycle",
      "action",
      "selection",
      "wizard",
      "service",
      "designs",
      "tasks",
      "resume",
      "paused",
      "instance",
      "api",
      "related",
      "topics",
      "create",
      "edit",
      "selected",
      "phase.",
      "required",
      "parameters",
      "indicated",
      "asterisk.",
      "don",
      "define",
      "value",
      "see",
      "warning",
      "message",
      "explains",
      "possible",
      "issues",
      "still",
      "apply",
      "change.",
      "select",
      "search",
      "flows",
      "actions",
      "name",
      "process",
      "engine",
      "flow",
      "selected.",
      "best",
      "practice",
      "creating",
      "oo",
      "cloud",
      "management",
      "recommended",
      "created",
      "below",
      "packages",
      "automatically",
      "appear",
      "default",
      "csa",
      "field.",
      "library",
      "io",
      "cloudslang",
      "amazon",
      "microfocus",
      "integrations",
      "content",
      "pack",
      "aren",
      "above",
      "folders",
      "update",
      "property",
      "appending",
      "folder",
      "path",
      "needs",
      "listed",
      "wizard.",
      "append",
      "perform",
      "following",
      "steps",
      "dnd",
      "application",
      "apis",
      "generate",
      "authorization",
      "token.",
      "get",
      "list",
      "all",
      "properties",
      "getallapplicationproperties",
      "details",
      "getapplicationproperty.",
      "custom",
      "paths.",
      "updateapplicationproperty."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Lifecycle action internal actions",
    "content": "The following internal actions are available: Internal action Description Applies to Build Resource Provider and Pool List Builds a candidate list of resource providers and associated resource pools that meet the following requirements: Support the resource offering. Have an Availability of Enabled. If the service offering that references the service design with this action is in a service catalog with resource environments selected, the candidate list is further restricted to only include resource providers in one or more of the selected resource environments. The provider's resource pool has sufficient resource capacity. To determine this, you must consider all measurable properties as configured in the Measurable Properties tab for the resource offerings, as well as the optional Multiplier Property Name field. The resource pool must have enough resource capacity to support all the properties, which requires that each necessary resource type (for example: CPU, Memory, and Storage), b",
    "url": "mgmtconssdlcinternalactions",
    "filename": "mgmtconssdlcinternalactions",
    "headings": [],
    "keywords": [
      "csa.log",
      "lifecycle",
      "action",
      "internal",
      "actions",
      "following",
      "available",
      "description",
      "applies",
      "build",
      "resource",
      "provider",
      "pool",
      "list",
      "builds",
      "candidate",
      "providers",
      "associated",
      "pools",
      "meet",
      "requirements",
      "support",
      "offering.",
      "availability",
      "enabled.",
      "service",
      "offering",
      "references",
      "design",
      "catalog",
      "environments",
      "selected",
      "further",
      "restricted",
      "include",
      "one",
      "environments.",
      "sufficient",
      "capacity.",
      "determine",
      "consider",
      "all",
      "measurable",
      "properties",
      "configured",
      "tab",
      "offerings",
      "well",
      "optional",
      "multiplier",
      "property",
      "name",
      "field.",
      "enough",
      "capacity",
      "requires",
      "necessary",
      "type",
      "example",
      "cpu",
      "memory",
      "storage",
      "based",
      "either",
      "unlimited",
      "available.",
      "difference",
      "between",
      "total",
      "current",
      "utilization",
      "requirements.",
      "component",
      "clone",
      "pattern",
      "clones",
      "marked",
      "non-pattern",
      "components.",
      "number",
      "components",
      "created",
      "determined",
      "value",
      "specified",
      "count.",
      "decrease",
      "decreases",
      "resources",
      "values",
      "component.",
      "note",
      "added",
      "template",
      "runs",
      "automatically",
      "during",
      "un-reserving",
      "accounting",
      "enabled"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "lifecycle action internal actions",
    "contentLower": "the following internal actions are available: internal action description applies to build resource provider and pool list builds a candidate list of resource providers and associated resource pools that meet the following requirements: support the resource offering. have an availability of enabled. if the service offering that references the service design with this action is in a service catalog with resource environments selected, the candidate list is further restricted to only include resource providers in one or more of the selected resource environments. the provider's resource pool has sufficient resource capacity. to determine this, you must consider all measurable properties as configured in the measurable properties tab for the resource offerings, as well as the optional multiplier property name field. the resource pool must have enough resource capacity to support all the properties, which requires that each necessary resource type (for example: cpu, memory, and storage), b",
    "keywordsLower": [
      "csa.log",
      "lifecycle",
      "action",
      "internal",
      "actions",
      "following",
      "available",
      "description",
      "applies",
      "build",
      "resource",
      "provider",
      "pool",
      "list",
      "builds",
      "candidate",
      "providers",
      "associated",
      "pools",
      "meet",
      "requirements",
      "support",
      "offering.",
      "availability",
      "enabled.",
      "service",
      "offering",
      "references",
      "design",
      "catalog",
      "environments",
      "selected",
      "further",
      "restricted",
      "include",
      "one",
      "environments.",
      "sufficient",
      "capacity.",
      "determine",
      "consider",
      "all",
      "measurable",
      "properties",
      "configured",
      "tab",
      "offerings",
      "well",
      "optional",
      "multiplier",
      "property",
      "name",
      "field.",
      "enough",
      "capacity",
      "requires",
      "necessary",
      "type",
      "example",
      "cpu",
      "memory",
      "storage",
      "based",
      "either",
      "unlimited",
      "available.",
      "difference",
      "between",
      "total",
      "current",
      "utilization",
      "requirements.",
      "component",
      "clone",
      "pattern",
      "clones",
      "marked",
      "non-pattern",
      "components.",
      "number",
      "components",
      "created",
      "determined",
      "value",
      "specified",
      "count.",
      "decrease",
      "decreases",
      "resources",
      "values",
      "component.",
      "note",
      "added",
      "template",
      "runs",
      "automatically",
      "during",
      "un-reserving",
      "accounting",
      "enabled"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import and export a component palette",
    "content": "In this topic, artifacts refer to a component palette and its associated component types, templates, component type constraints, and resource offerings. You can import and export many of the artifacts that provide the basis for cloud automation. The export operation provides the ability to preserve the selected artifacts so they can be used to replicate the services on another system or to restore the artifacts. These exported archive files are preserved in an industry-standard zip archive file format. The import operation provides the ability to install and replace artifacts. See the Tasks section below for more information. Process information Resource offerings - If a component template in a component palette is associated with a resource offering, when the component palette is exported, resource offering XML files are included in the component palette archive. Therefore, when importing this archive, the resource offerings are also imported. Import operation - The import operation i",
    "url": "mgmtconscomppaletteimpexp",
    "filename": "mgmtconscomppaletteimpexp",
    "headings": [
      "Process information",
      "Tasks"
    ],
    "keywords": [
      "csa.war",
      "import",
      "export",
      "component",
      "palette",
      "process",
      "information",
      "tasks",
      "topic",
      "artifacts",
      "refer",
      "associated",
      "types",
      "templates",
      "type",
      "constraints",
      "resource",
      "offerings.",
      "many",
      "provide",
      "basis",
      "cloud",
      "automation.",
      "operation",
      "provides",
      "ability",
      "preserve",
      "selected",
      "replicate",
      "services",
      "another",
      "system",
      "restore",
      "artifacts.",
      "exported",
      "archive",
      "files",
      "preserved",
      "industry-standard",
      "zip",
      "file",
      "format.",
      "install",
      "replace",
      "see",
      "section",
      "below",
      "information.",
      "offerings",
      "template",
      "offering",
      "xml",
      "included",
      "archive.",
      "therefore",
      "importing",
      "imported.",
      "destructive",
      "existing",
      "data",
      "unlike",
      "operations",
      "service",
      "designs",
      "there",
      "options",
      "new",
      "overwriting",
      "ones",
      "imported",
      "artifact",
      "exists",
      "target",
      "overwritten.",
      "example",
      "contains",
      "overwritten",
      "select",
      "preview",
      "button",
      "view",
      "prospective",
      "results",
      "process.",
      "integrity",
      "palettes",
      "always",
      "ensures",
      "respect",
      "derivation",
      "property",
      "propagation.",
      "say",
      "modified",
      "properties",
      "called",
      "derived",
      "type.",
      "update",
      "occurs"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import and export a component palette",
    "contentLower": "in this topic, artifacts refer to a component palette and its associated component types, templates, component type constraints, and resource offerings. you can import and export many of the artifacts that provide the basis for cloud automation. the export operation provides the ability to preserve the selected artifacts so they can be used to replicate the services on another system or to restore the artifacts. these exported archive files are preserved in an industry-standard zip archive file format. the import operation provides the ability to install and replace artifacts. see the tasks section below for more information. process information resource offerings - if a component template in a component palette is associated with a resource offering, when the component palette is exported, resource offering xml files are included in the component palette archive. therefore, when importing this archive, the resource offerings are also imported. import operation - the import operation i",
    "keywordsLower": [
      "csa.war",
      "import",
      "export",
      "component",
      "palette",
      "process",
      "information",
      "tasks",
      "topic",
      "artifacts",
      "refer",
      "associated",
      "types",
      "templates",
      "type",
      "constraints",
      "resource",
      "offerings.",
      "many",
      "provide",
      "basis",
      "cloud",
      "automation.",
      "operation",
      "provides",
      "ability",
      "preserve",
      "selected",
      "replicate",
      "services",
      "another",
      "system",
      "restore",
      "artifacts.",
      "exported",
      "archive",
      "files",
      "preserved",
      "industry-standard",
      "zip",
      "file",
      "format.",
      "install",
      "replace",
      "see",
      "section",
      "below",
      "information.",
      "offerings",
      "template",
      "offering",
      "xml",
      "included",
      "archive.",
      "therefore",
      "importing",
      "imported.",
      "destructive",
      "existing",
      "data",
      "unlike",
      "operations",
      "service",
      "designs",
      "there",
      "options",
      "new",
      "overwriting",
      "ones",
      "imported",
      "artifact",
      "exists",
      "target",
      "overwritten.",
      "example",
      "contains",
      "overwritten",
      "select",
      "preview",
      "button",
      "view",
      "prospective",
      "results",
      "process.",
      "integrity",
      "palettes",
      "always",
      "ensures",
      "respect",
      "derivation",
      "property",
      "propagation.",
      "say",
      "modified",
      "properties",
      "called",
      "derived",
      "type.",
      "update",
      "occurs"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Lifecycle actions for component templates",
    "content": "What's a component template lifecycle? A component template lifecycle is a collection of actions defined for a component template. The Lifecycle area allows you to specify the actions that are needed to provision, update, and deprovision component templates. Use the action selection wizard to add a lifecycle action to a component template from one of the available process engines. Lifecycle actions A lifecycle action is a function that's either run automatically at a specified lifecycle stage or phase, or that's exposed to the subscriber. Most lifecycle actions correspond to OO flows, which contain the logic for running the function. Lifecycle actions also include actions that run internally. Many internal actions relate to provider and pool selection and resource accounting. Actions include input parameters that provide configuration information to the function. Lifecycle stages A lifecycle stage represents a step within the service lifecycle. There are three categories of lifecycle s",
    "url": "comptmpltlifecycleactionabout",
    "filename": "comptmpltlifecycleactionabout",
    "headings": [
      "Lifecycle actions",
      "Lifecycle stages",
      "Lifecycle phases",
      "Tasks",
      "Related topics"
    ],
    "keywords": [
      "lifecycle",
      "actions",
      "component",
      "templates",
      "stages",
      "phases",
      "tasks",
      "related",
      "topics",
      "what",
      "template",
      "collection",
      "defined",
      "template.",
      "area",
      "allows",
      "specify",
      "needed",
      "provision",
      "update",
      "deprovision",
      "templates.",
      "action",
      "selection",
      "wizard",
      "add",
      "one",
      "available",
      "process",
      "engines.",
      "function",
      "either",
      "run",
      "automatically",
      "specified",
      "stage",
      "phase",
      "exposed",
      "subscriber.",
      "most",
      "correspond",
      "oo",
      "flows",
      "contain",
      "logic",
      "running",
      "function.",
      "include",
      "internally.",
      "many",
      "internal",
      "relate",
      "provider",
      "pool",
      "resource",
      "accounting.",
      "input",
      "parameters",
      "provide",
      "configuration",
      "information",
      "represents",
      "step",
      "service",
      "lifecycle.",
      "there",
      "three",
      "categories",
      "provisioning",
      "operational",
      "de-provisioning.",
      "initializing",
      "first",
      "processed",
      "during",
      "provisioning.",
      "perform",
      "any",
      "type",
      "initialization",
      "required",
      "before",
      "proceeding",
      "reserving",
      "such",
      "validation",
      "create",
      "change",
      "request",
      "records.",
      "second",
      "after",
      "stage.",
      "offering",
      "associated",
      "configured",
      "accounting",
      "enabled",
      "reserve",
      "resources"
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "lifecycle actions for component templates",
    "contentLower": "what's a component template lifecycle? a component template lifecycle is a collection of actions defined for a component template. the lifecycle area allows you to specify the actions that are needed to provision, update, and deprovision component templates. use the action selection wizard to add a lifecycle action to a component template from one of the available process engines. lifecycle actions a lifecycle action is a function that's either run automatically at a specified lifecycle stage or phase, or that's exposed to the subscriber. most lifecycle actions correspond to oo flows, which contain the logic for running the function. lifecycle actions also include actions that run internally. many internal actions relate to provider and pool selection and resource accounting. actions include input parameters that provide configuration information to the function. lifecycle stages a lifecycle stage represents a step within the service lifecycle. there are three categories of lifecycle s",
    "keywordsLower": [
      "lifecycle",
      "actions",
      "component",
      "templates",
      "stages",
      "phases",
      "tasks",
      "related",
      "topics",
      "what",
      "template",
      "collection",
      "defined",
      "template.",
      "area",
      "allows",
      "specify",
      "needed",
      "provision",
      "update",
      "deprovision",
      "templates.",
      "action",
      "selection",
      "wizard",
      "add",
      "one",
      "available",
      "process",
      "engines.",
      "function",
      "either",
      "run",
      "automatically",
      "specified",
      "stage",
      "phase",
      "exposed",
      "subscriber.",
      "most",
      "correspond",
      "oo",
      "flows",
      "contain",
      "logic",
      "running",
      "function.",
      "include",
      "internally.",
      "many",
      "internal",
      "relate",
      "provider",
      "pool",
      "resource",
      "accounting.",
      "input",
      "parameters",
      "provide",
      "configuration",
      "information",
      "represents",
      "step",
      "service",
      "lifecycle.",
      "there",
      "three",
      "categories",
      "provisioning",
      "operational",
      "de-provisioning.",
      "initializing",
      "first",
      "processed",
      "during",
      "provisioning.",
      "perform",
      "any",
      "type",
      "initialization",
      "required",
      "before",
      "proceeding",
      "reserving",
      "such",
      "validation",
      "create",
      "change",
      "request",
      "records.",
      "second",
      "after",
      "stage.",
      "offering",
      "associated",
      "configured",
      "accounting",
      "enabled",
      "reserve",
      "resources"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Lifecycle action selection wizard for component template",
    "content": "Use the action selection wizard to create or edit action for the selected lifecycle phase. Required parameters are indicated with an asterisk. If you don't define a value, then you will see a warning message that explains possible issues but you can still apply the change. Tasks Select an action — Search for flows or actions by name or select the process engine from which flow or action is selected. Best practice for creating the OO Flows that will be used in Cloud Management lifecycle actions It's recommended that the OO flows are created in the below packages so that they automatically appear in the default CSA lifecycle action field. /Library/io/cloudslang/amazon /Library/io/cloudslang/microfocus /Library/Integrations /Library/CSA Content Pack If you have OO flows that aren't created within the above folders, you can update the property application.oo_content_root.lifecycle_action by appending the OO folder path that needs to be listed in the lifecycle action wizard. To append the O",
    "url": "comptmpltlifecycleactionwiz",
    "filename": "comptmpltlifecycleactionwiz",
    "headings": [
      "Tasks",
      "Resume a paused service instance using an API",
      "Related topics"
    ],
    "keywords": [
      "https://<hostname>/dnd-operations-gateway/?TENANTID=<Tenant_id>#/instance/<instanceId>/components",
      "myLab.net",
      "https://<hostname>/dnd/api/service/instance/<instanceId>/resume",
      "https://myHost.myLab.net/dnd-operations-gateway/?TENANTID=174106658#/instance/2c9082c37e828b97017eb4bca2a5003a/components",
      "lifecycle",
      "action",
      "selection",
      "wizard",
      "component",
      "template",
      "tasks",
      "resume",
      "paused",
      "service",
      "instance",
      "api",
      "related",
      "topics",
      "create",
      "edit",
      "selected",
      "phase.",
      "required",
      "parameters",
      "indicated",
      "asterisk.",
      "don",
      "define",
      "value",
      "see",
      "warning",
      "message",
      "explains",
      "possible",
      "issues",
      "still",
      "apply",
      "change.",
      "select",
      "search",
      "flows",
      "actions",
      "name",
      "process",
      "engine",
      "flow",
      "selected.",
      "best",
      "practice",
      "creating",
      "oo",
      "cloud",
      "management",
      "recommended",
      "created",
      "below",
      "packages",
      "automatically",
      "appear",
      "default",
      "csa",
      "field.",
      "library",
      "io",
      "cloudslang",
      "amazon",
      "microfocus",
      "integrations",
      "content",
      "pack",
      "aren",
      "above",
      "folders",
      "update",
      "property",
      "appending",
      "folder",
      "path",
      "needs",
      "listed",
      "wizard.",
      "append",
      "perform",
      "following",
      "steps",
      "dnd",
      "application",
      "apis",
      "generate",
      "authorization",
      "token.",
      "get",
      "list",
      "all",
      "properties",
      "getallapplicationproperties",
      "details",
      "getapplicationproperty.",
      "custom",
      "paths."
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "lifecycle action selection wizard for component template",
    "contentLower": "use the action selection wizard to create or edit action for the selected lifecycle phase. required parameters are indicated with an asterisk. if you don't define a value, then you will see a warning message that explains possible issues but you can still apply the change. tasks select an action — search for flows or actions by name or select the process engine from which flow or action is selected. best practice for creating the oo flows that will be used in cloud management lifecycle actions it's recommended that the oo flows are created in the below packages so that they automatically appear in the default csa lifecycle action field. /library/io/cloudslang/amazon /library/io/cloudslang/microfocus /library/integrations /library/csa content pack if you have oo flows that aren't created within the above folders, you can update the property application.oo_content_root.lifecycle_action by appending the oo folder path that needs to be listed in the lifecycle action wizard. to append the o",
    "keywordsLower": [
      "https://<hostname>/dnd-operations-gateway/?tenantid=<tenant_id>#/instance/<instanceid>/components",
      "mylab.net",
      "https://<hostname>/dnd/api/service/instance/<instanceid>/resume",
      "https://myhost.mylab.net/dnd-operations-gateway/?tenantid=174106658#/instance/2c9082c37e828b97017eb4bca2a5003a/components",
      "lifecycle",
      "action",
      "selection",
      "wizard",
      "component",
      "template",
      "tasks",
      "resume",
      "paused",
      "service",
      "instance",
      "api",
      "related",
      "topics",
      "create",
      "edit",
      "selected",
      "phase.",
      "required",
      "parameters",
      "indicated",
      "asterisk.",
      "don",
      "define",
      "value",
      "see",
      "warning",
      "message",
      "explains",
      "possible",
      "issues",
      "still",
      "apply",
      "change.",
      "select",
      "search",
      "flows",
      "actions",
      "name",
      "process",
      "engine",
      "flow",
      "selected.",
      "best",
      "practice",
      "creating",
      "oo",
      "cloud",
      "management",
      "recommended",
      "created",
      "below",
      "packages",
      "automatically",
      "appear",
      "default",
      "csa",
      "field.",
      "library",
      "io",
      "cloudslang",
      "amazon",
      "microfocus",
      "integrations",
      "content",
      "pack",
      "aren",
      "above",
      "folders",
      "update",
      "property",
      "appending",
      "folder",
      "path",
      "needs",
      "listed",
      "wizard.",
      "append",
      "perform",
      "following",
      "steps",
      "dnd",
      "application",
      "apis",
      "generate",
      "authorization",
      "token.",
      "get",
      "list",
      "all",
      "properties",
      "getallapplicationproperties",
      "details",
      "getapplicationproperty.",
      "custom",
      "paths."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Lifecycle action internal actions",
    "content": "The following internal actions are shipped with the product: Internal action Description Applies to Build Resource Provider and Pool List Builds a candidate list of resource providers and associated resource pools that meet the following requirements: Support the resource offering. Have an Availability of Enabled. If the service offering that references the service design with this action is in a service catalog with resource environments selected, the candidate list is further restricted to only include resource providers in one or more of the selected resource environments. The provider's resource pool has sufficient resource capacity. To determine this, you must consider all measurable properties as configured in the Measurable Properties tab for the resource offerings, as well as the optional Multiplier Property Name field. The resource pool must have enough resource capacity to support all the properties, which requires that each necessary resource type (for example: CPU, Memory, ",
    "url": "comptmpltlifecycleactionintactions",
    "filename": "comptmpltlifecycleactionintactions",
    "headings": [],
    "keywords": [
      "csa.log",
      "lifecycle",
      "action",
      "internal",
      "actions",
      "following",
      "shipped",
      "product",
      "description",
      "applies",
      "build",
      "resource",
      "provider",
      "pool",
      "list",
      "builds",
      "candidate",
      "providers",
      "associated",
      "pools",
      "meet",
      "requirements",
      "support",
      "offering.",
      "availability",
      "enabled.",
      "service",
      "offering",
      "references",
      "design",
      "catalog",
      "environments",
      "selected",
      "further",
      "restricted",
      "include",
      "one",
      "environments.",
      "sufficient",
      "capacity.",
      "determine",
      "consider",
      "all",
      "measurable",
      "properties",
      "configured",
      "tab",
      "offerings",
      "well",
      "optional",
      "multiplier",
      "property",
      "name",
      "field.",
      "enough",
      "capacity",
      "requires",
      "necessary",
      "type",
      "example",
      "cpu",
      "memory",
      "storage",
      "based",
      "either",
      "unlimited",
      "available.",
      "available",
      "difference",
      "between",
      "total",
      "csa",
      "current",
      "utilization",
      "requirements.",
      "component",
      "clone",
      "pattern",
      "clones",
      "marked",
      "non-pattern",
      "components.",
      "number",
      "components",
      "created",
      "determined",
      "value",
      "specified",
      "count.",
      "decrease",
      "decreases",
      "resources",
      "values",
      "component.",
      "note",
      "added",
      "template",
      "runs",
      "automatically",
      "during"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "lifecycle action internal actions",
    "contentLower": "the following internal actions are shipped with the product: internal action description applies to build resource provider and pool list builds a candidate list of resource providers and associated resource pools that meet the following requirements: support the resource offering. have an availability of enabled. if the service offering that references the service design with this action is in a service catalog with resource environments selected, the candidate list is further restricted to only include resource providers in one or more of the selected resource environments. the provider's resource pool has sufficient resource capacity. to determine this, you must consider all measurable properties as configured in the measurable properties tab for the resource offerings, as well as the optional multiplier property name field. the resource pool must have enough resource capacity to support all the properties, which requires that each necessary resource type (for example: cpu, memory, ",
    "keywordsLower": [
      "csa.log",
      "lifecycle",
      "action",
      "internal",
      "actions",
      "following",
      "shipped",
      "product",
      "description",
      "applies",
      "build",
      "resource",
      "provider",
      "pool",
      "list",
      "builds",
      "candidate",
      "providers",
      "associated",
      "pools",
      "meet",
      "requirements",
      "support",
      "offering.",
      "availability",
      "enabled.",
      "service",
      "offering",
      "references",
      "design",
      "catalog",
      "environments",
      "selected",
      "further",
      "restricted",
      "include",
      "one",
      "environments.",
      "sufficient",
      "capacity.",
      "determine",
      "consider",
      "all",
      "measurable",
      "properties",
      "configured",
      "tab",
      "offerings",
      "well",
      "optional",
      "multiplier",
      "property",
      "name",
      "field.",
      "enough",
      "capacity",
      "requires",
      "necessary",
      "type",
      "example",
      "cpu",
      "memory",
      "storage",
      "based",
      "either",
      "unlimited",
      "available.",
      "available",
      "difference",
      "between",
      "total",
      "csa",
      "current",
      "utilization",
      "requirements.",
      "component",
      "clone",
      "pattern",
      "clones",
      "marked",
      "non-pattern",
      "components.",
      "number",
      "components",
      "created",
      "determined",
      "value",
      "specified",
      "count.",
      "decrease",
      "decreases",
      "resources",
      "values",
      "component.",
      "note",
      "added",
      "template",
      "runs",
      "automatically",
      "during"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import and export a resource offering",
    "content": "In this topic, an artifact refers to a resource offering. You can import and export many of the artifacts that provide the basis for cloud automation. The export operation provides the ability to preserve the selected artifacts so they can be used to replicate the services on another system or to restore the artifacts. These exported archive files are preserved in an industry-standard zip archive file format. Installing or replacing artifacts on the target system is supported by import and update operations. The import operation only adds artifacts, whereas the update operation replaces matching artifacts. See the Tasks section below for more information. Note that an import does not automatically associate the resource offering to existing resource providers on the system. After the import, you should manually associate the imported resource offering with the appropriate resource provider. For more information, refer to Select or remove providers for a resource offering. Process infor",
    "url": "mgmtconsroimportexport",
    "filename": "mgmtconsroimportexport",
    "headings": [
      "Process information",
      "Tasks",
      "Identify matching artifacts",
      "Related topics"
    ],
    "keywords": [
      "csa.war",
      "import",
      "export",
      "resource",
      "offering",
      "process",
      "information",
      "tasks",
      "identify",
      "matching",
      "artifacts",
      "related",
      "topics",
      "topic",
      "artifact",
      "refers",
      "offering.",
      "many",
      "provide",
      "basis",
      "cloud",
      "automation.",
      "operation",
      "provides",
      "ability",
      "preserve",
      "selected",
      "replicate",
      "services",
      "another",
      "system",
      "restore",
      "artifacts.",
      "exported",
      "archive",
      "files",
      "preserved",
      "industry-standard",
      "zip",
      "file",
      "format.",
      "installing",
      "replacing",
      "target",
      "supported",
      "update",
      "operations.",
      "adds",
      "whereas",
      "replaces",
      "see",
      "section",
      "below",
      "information.",
      "note",
      "automatically",
      "associate",
      "existing",
      "providers",
      "system.",
      "after",
      "manually",
      "imported",
      "appropriate",
      "provider.",
      "refer",
      "select",
      "remove",
      "categories",
      "provider",
      "types",
      "such",
      "compute",
      "vmware",
      "vcenter",
      "resolved",
      "first",
      "name",
      "second",
      "display",
      "during",
      "out-of-the",
      "box",
      "identical",
      "values",
      "all",
      "installations",
      "product",
      "resolve",
      "correctly",
      "import.",
      "user-created",
      "don",
      "match",
      "different",
      "instead",
      "name.",
      "example",
      "user",
      "created"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import and export a resource offering",
    "contentLower": "in this topic, an artifact refers to a resource offering. you can import and export many of the artifacts that provide the basis for cloud automation. the export operation provides the ability to preserve the selected artifacts so they can be used to replicate the services on another system or to restore the artifacts. these exported archive files are preserved in an industry-standard zip archive file format. installing or replacing artifacts on the target system is supported by import and update operations. the import operation only adds artifacts, whereas the update operation replaces matching artifacts. see the tasks section below for more information. note that an import does not automatically associate the resource offering to existing resource providers on the system. after the import, you should manually associate the imported resource offering with the appropriate resource provider. for more information, refer to select or remove providers for a resource offering. process infor",
    "keywordsLower": [
      "csa.war",
      "import",
      "export",
      "resource",
      "offering",
      "process",
      "information",
      "tasks",
      "identify",
      "matching",
      "artifacts",
      "related",
      "topics",
      "topic",
      "artifact",
      "refers",
      "offering.",
      "many",
      "provide",
      "basis",
      "cloud",
      "automation.",
      "operation",
      "provides",
      "ability",
      "preserve",
      "selected",
      "replicate",
      "services",
      "another",
      "system",
      "restore",
      "artifacts.",
      "exported",
      "archive",
      "files",
      "preserved",
      "industry-standard",
      "zip",
      "file",
      "format.",
      "installing",
      "replacing",
      "target",
      "supported",
      "update",
      "operations.",
      "adds",
      "whereas",
      "replaces",
      "see",
      "section",
      "below",
      "information.",
      "note",
      "automatically",
      "associate",
      "existing",
      "providers",
      "system.",
      "after",
      "manually",
      "imported",
      "appropriate",
      "provider.",
      "refer",
      "select",
      "remove",
      "categories",
      "provider",
      "types",
      "such",
      "compute",
      "vmware",
      "vcenter",
      "resolved",
      "first",
      "name",
      "second",
      "display",
      "during",
      "out-of-the",
      "box",
      "identical",
      "values",
      "all",
      "installations",
      "product",
      "resolve",
      "correctly",
      "import.",
      "user-created",
      "don",
      "match",
      "different",
      "instead",
      "name.",
      "example",
      "user",
      "created"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Lifecycle actions for resource offerings",
    "content": "A resource offering lifecycle is a collection of actions defined for a resource offering. The Lifecycle area allows you to specify the actions that are needed to provision, update, and deprovision resource offerings. Use the action selection wizard to add a lifecycle action to a resource offering from one of the available process engines. Lifecycle actions A lifecycle action is a function that's either run automatically at a specified lifecycle stage or phase, or that's exposed to the subscriber. Most lifecycle actions correspond to OO flows, which contain the logic for running the function. There are actions that run internally. Many internal actions relate to provider and pool selection and resource accounting. Actions include input parameters that provide configuration information to the function. Lifecycle stages A lifecycle stage represents a step within the service lifecycle. There are three categories of lifecycle stages: Provisioning, Operational, and De-provisioning. The Provi",
    "url": "mgmtconsrolifecycleactionabout",
    "filename": "mgmtconsrolifecycleactionabout",
    "headings": [
      "Lifecycle actions",
      "Lifecycle stages",
      "Lifecycle phases",
      "Tasks",
      "Related topics"
    ],
    "keywords": [
      "lifecycle",
      "actions",
      "resource",
      "offerings",
      "stages",
      "phases",
      "tasks",
      "related",
      "topics",
      "offering",
      "collection",
      "defined",
      "offering.",
      "area",
      "allows",
      "specify",
      "needed",
      "provision",
      "update",
      "deprovision",
      "offerings.",
      "action",
      "selection",
      "wizard",
      "add",
      "one",
      "available",
      "process",
      "engines.",
      "function",
      "either",
      "run",
      "automatically",
      "specified",
      "stage",
      "phase",
      "exposed",
      "subscriber.",
      "most",
      "correspond",
      "oo",
      "flows",
      "contain",
      "logic",
      "running",
      "function.",
      "there",
      "internally.",
      "many",
      "internal",
      "relate",
      "provider",
      "pool",
      "accounting.",
      "include",
      "input",
      "parameters",
      "provide",
      "configuration",
      "information",
      "represents",
      "step",
      "service",
      "lifecycle.",
      "three",
      "categories",
      "provisioning",
      "operational",
      "de-provisioning.",
      "initializing",
      "first",
      "processed",
      "during",
      "provisioning.",
      "perform",
      "any",
      "type",
      "initialization",
      "required",
      "before",
      "proceeding",
      "reserving",
      "such",
      "validation",
      "create",
      "change",
      "request",
      "records.",
      "second",
      "after",
      "stage.",
      "associated",
      "component",
      "configured",
      "accounting",
      "enabled",
      "reserve",
      "resources",
      "general",
      "storage"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "lifecycle actions for resource offerings",
    "contentLower": "a resource offering lifecycle is a collection of actions defined for a resource offering. the lifecycle area allows you to specify the actions that are needed to provision, update, and deprovision resource offerings. use the action selection wizard to add a lifecycle action to a resource offering from one of the available process engines. lifecycle actions a lifecycle action is a function that's either run automatically at a specified lifecycle stage or phase, or that's exposed to the subscriber. most lifecycle actions correspond to oo flows, which contain the logic for running the function. there are actions that run internally. many internal actions relate to provider and pool selection and resource accounting. actions include input parameters that provide configuration information to the function. lifecycle stages a lifecycle stage represents a step within the service lifecycle. there are three categories of lifecycle stages: provisioning, operational, and de-provisioning. the provi",
    "keywordsLower": [
      "lifecycle",
      "actions",
      "resource",
      "offerings",
      "stages",
      "phases",
      "tasks",
      "related",
      "topics",
      "offering",
      "collection",
      "defined",
      "offering.",
      "area",
      "allows",
      "specify",
      "needed",
      "provision",
      "update",
      "deprovision",
      "offerings.",
      "action",
      "selection",
      "wizard",
      "add",
      "one",
      "available",
      "process",
      "engines.",
      "function",
      "either",
      "run",
      "automatically",
      "specified",
      "stage",
      "phase",
      "exposed",
      "subscriber.",
      "most",
      "correspond",
      "oo",
      "flows",
      "contain",
      "logic",
      "running",
      "function.",
      "there",
      "internally.",
      "many",
      "internal",
      "relate",
      "provider",
      "pool",
      "accounting.",
      "include",
      "input",
      "parameters",
      "provide",
      "configuration",
      "information",
      "represents",
      "step",
      "service",
      "lifecycle.",
      "three",
      "categories",
      "provisioning",
      "operational",
      "de-provisioning.",
      "initializing",
      "first",
      "processed",
      "during",
      "provisioning.",
      "perform",
      "any",
      "type",
      "initialization",
      "required",
      "before",
      "proceeding",
      "reserving",
      "such",
      "validation",
      "create",
      "change",
      "request",
      "records.",
      "second",
      "after",
      "stage.",
      "associated",
      "component",
      "configured",
      "accounting",
      "enabled",
      "reserve",
      "resources",
      "general",
      "storage"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Lifecycle action selection wizard for resource offering",
    "content": "Use the action selection wizard to create or edit action for the selected lifecycle phase. Required parameters are indicated with an asterisk. If you don't define a value, then you will see a warning message that explains possible issues but you can still apply the change.TasksSelect an action — Search for flows or actions by name. Best practice for creating the OO Flows that will be used in Cloud Management lifecycle actions It's recommended that the OO flows are created in the below packages so that they automatically appear in the default CSA lifecycle action field. /Library/io/cloudslang/amazon/Library/io/cloudslang/microfocus/Library/Integrations/Library/CSA Content PackIf you have OO flows that aren't created within the above folders, you can configure the property DND_PROP_CSA_OO_CONTENT_ROOT_LIFECYCLE_ACTION to list the OO flow in the lifecycle action wizard. For more information on how to configure the OO properties, see the section Operation Orchestration root content configu",
    "url": "mgmtconsrolifecycleactionwiz",
    "filename": "mgmtconsrolifecycleactionwiz",
    "headings": [
      "Tasks",
      "Resume a paused service instance using an API",
      "Relates topics"
    ],
    "keywords": [
      "mapping.Auto",
      "myLab.net",
      "2c9082c37e828b97017eb4bca2a5003a.If",
      "parameter.Not",
      "resume.Set",
      "created.Edit",
      "https://<hostname>/dnd/api/service/instance/<instanceId>/resume.Set",
      "https://<hostname>/dnd-operations-gateway/?TENANTID=<Tenant_id>#/instance/<instanceId>/components",
      "https://myHost.myLab.net/dnd-operations-gateway/?TENANTID=174106658#/instance/2c9082c37e828b97017eb4bca2a5003a/components",
      "action.The",
      "lifecycle",
      "action",
      "selection",
      "wizard",
      "resource",
      "offering",
      "tasks",
      "resume",
      "paused",
      "service",
      "instance",
      "api",
      "relates",
      "topics",
      "create",
      "edit",
      "selected",
      "phase.",
      "required",
      "parameters",
      "indicated",
      "asterisk.",
      "don",
      "define",
      "value",
      "see",
      "warning",
      "message",
      "explains",
      "possible",
      "issues",
      "still",
      "apply",
      "change.tasksselect",
      "search",
      "flows",
      "actions",
      "name.",
      "best",
      "practice",
      "creating",
      "oo",
      "cloud",
      "management",
      "recommended",
      "created",
      "below",
      "packages",
      "automatically",
      "appear",
      "default",
      "csa",
      "field.",
      "library",
      "io",
      "cloudslang",
      "amazon",
      "microfocus",
      "integrations",
      "content",
      "packif",
      "aren",
      "above",
      "folders",
      "configure",
      "property",
      "list",
      "flow",
      "wizard.",
      "information",
      "properties",
      "section",
      "operation",
      "orchestration",
      "root",
      "configuration",
      "below.configure",
      "following",
      "itemdescriptionprocess",
      "engineread-only.",
      "container",
      "internal",
      "action.locatorread-only.",
      "location",
      "action.display",
      "namethe",
      "name",
      "provide",
      "action.descriptionthe",
      "description"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "lifecycle action selection wizard for resource offering",
    "contentLower": "use the action selection wizard to create or edit action for the selected lifecycle phase. required parameters are indicated with an asterisk. if you don't define a value, then you will see a warning message that explains possible issues but you can still apply the change.tasksselect an action — search for flows or actions by name. best practice for creating the oo flows that will be used in cloud management lifecycle actions it's recommended that the oo flows are created in the below packages so that they automatically appear in the default csa lifecycle action field. /library/io/cloudslang/amazon/library/io/cloudslang/microfocus/library/integrations/library/csa content packif you have oo flows that aren't created within the above folders, you can configure the property dnd_prop_csa_oo_content_root_lifecycle_action to list the oo flow in the lifecycle action wizard. for more information on how to configure the oo properties, see the section operation orchestration root content configu",
    "keywordsLower": [
      "mapping.auto",
      "mylab.net",
      "2c9082c37e828b97017eb4bca2a5003a.if",
      "parameter.not",
      "resume.set",
      "created.edit",
      "https://<hostname>/dnd/api/service/instance/<instanceid>/resume.set",
      "https://<hostname>/dnd-operations-gateway/?tenantid=<tenant_id>#/instance/<instanceid>/components",
      "https://myhost.mylab.net/dnd-operations-gateway/?tenantid=174106658#/instance/2c9082c37e828b97017eb4bca2a5003a/components",
      "action.the",
      "lifecycle",
      "action",
      "selection",
      "wizard",
      "resource",
      "offering",
      "tasks",
      "resume",
      "paused",
      "service",
      "instance",
      "api",
      "relates",
      "topics",
      "create",
      "edit",
      "selected",
      "phase.",
      "required",
      "parameters",
      "indicated",
      "asterisk.",
      "don",
      "define",
      "value",
      "see",
      "warning",
      "message",
      "explains",
      "possible",
      "issues",
      "still",
      "apply",
      "change.tasksselect",
      "search",
      "flows",
      "actions",
      "name.",
      "best",
      "practice",
      "creating",
      "oo",
      "cloud",
      "management",
      "recommended",
      "created",
      "below",
      "packages",
      "automatically",
      "appear",
      "default",
      "csa",
      "field.",
      "library",
      "io",
      "cloudslang",
      "amazon",
      "microfocus",
      "integrations",
      "content",
      "packif",
      "aren",
      "above",
      "folders",
      "configure",
      "property",
      "list",
      "flow",
      "wizard.",
      "information",
      "properties",
      "section",
      "operation",
      "orchestration",
      "root",
      "configuration",
      "below.configure",
      "following",
      "itemdescriptionprocess",
      "engineread-only.",
      "container",
      "internal",
      "action.locatorread-only.",
      "location",
      "action.display",
      "namethe",
      "name",
      "provide",
      "action.descriptionthe",
      "description"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Lifecycle action internal actions",
    "content": "The following internal actions are shipped with the product: Internal action Description Applies to Build Resource Provider and Pool List Builds a candidate list of resource providers and associated resource pools that meet the following requirements: Support the resource offering. Have an Availability of Enabled. If the service offering that references the service design with this action is in a service catalog with resource environments selected, the candidate list is further restricted to only include resource providers in one or more of the selected resource environments. The provider's resource pool has sufficient resource capacity. To determine this, you must consider all measurable properties as configured in the Measurable Properties tab for the resource offerings, as well as the optional Multiplier Property Name field. The resource pool must have enough resource capacity to support all the properties, which requires that each necessary resource type (for example: CPU, Memory, ",
    "url": "mgmtconsrolifecycleactionintactions",
    "filename": "mgmtconsrolifecycleactionintactions",
    "headings": [],
    "keywords": [
      "csa.log",
      "lifecycle",
      "action",
      "internal",
      "actions",
      "following",
      "shipped",
      "product",
      "description",
      "applies",
      "build",
      "resource",
      "provider",
      "pool",
      "list",
      "builds",
      "candidate",
      "providers",
      "associated",
      "pools",
      "meet",
      "requirements",
      "support",
      "offering.",
      "availability",
      "enabled.",
      "service",
      "offering",
      "references",
      "design",
      "catalog",
      "environments",
      "selected",
      "further",
      "restricted",
      "include",
      "one",
      "environments.",
      "sufficient",
      "capacity.",
      "determine",
      "consider",
      "all",
      "measurable",
      "properties",
      "configured",
      "tab",
      "offerings",
      "well",
      "optional",
      "multiplier",
      "property",
      "name",
      "field.",
      "enough",
      "capacity",
      "requires",
      "necessary",
      "type",
      "example",
      "cpu",
      "memory",
      "storage",
      "based",
      "either",
      "unlimited",
      "available.",
      "available",
      "difference",
      "between",
      "total",
      "csa",
      "current",
      "utilization",
      "requirements.",
      "component",
      "clone",
      "pattern",
      "clones",
      "marked",
      "non-pattern",
      "components.",
      "number",
      "components",
      "created",
      "determined",
      "value",
      "specified",
      "count.",
      "decrease",
      "decreases",
      "resources",
      "values",
      "component.",
      "added",
      "template",
      "runs",
      "automatically",
      "during",
      "un-reserving"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "lifecycle action internal actions",
    "contentLower": "the following internal actions are shipped with the product: internal action description applies to build resource provider and pool list builds a candidate list of resource providers and associated resource pools that meet the following requirements: support the resource offering. have an availability of enabled. if the service offering that references the service design with this action is in a service catalog with resource environments selected, the candidate list is further restricted to only include resource providers in one or more of the selected resource environments. the provider's resource pool has sufficient resource capacity. to determine this, you must consider all measurable properties as configured in the measurable properties tab for the resource offerings, as well as the optional multiplier property name field. the resource pool must have enough resource capacity to support all the properties, which requires that each necessary resource type (for example: cpu, memory, ",
    "keywordsLower": [
      "csa.log",
      "lifecycle",
      "action",
      "internal",
      "actions",
      "following",
      "shipped",
      "product",
      "description",
      "applies",
      "build",
      "resource",
      "provider",
      "pool",
      "list",
      "builds",
      "candidate",
      "providers",
      "associated",
      "pools",
      "meet",
      "requirements",
      "support",
      "offering.",
      "availability",
      "enabled.",
      "service",
      "offering",
      "references",
      "design",
      "catalog",
      "environments",
      "selected",
      "further",
      "restricted",
      "include",
      "one",
      "environments.",
      "sufficient",
      "capacity.",
      "determine",
      "consider",
      "all",
      "measurable",
      "properties",
      "configured",
      "tab",
      "offerings",
      "well",
      "optional",
      "multiplier",
      "property",
      "name",
      "field.",
      "enough",
      "capacity",
      "requires",
      "necessary",
      "type",
      "example",
      "cpu",
      "memory",
      "storage",
      "based",
      "either",
      "unlimited",
      "available.",
      "available",
      "difference",
      "between",
      "total",
      "csa",
      "current",
      "utilization",
      "requirements.",
      "component",
      "clone",
      "pattern",
      "clones",
      "marked",
      "non-pattern",
      "components.",
      "number",
      "components",
      "created",
      "determined",
      "value",
      "specified",
      "count.",
      "decrease",
      "decreases",
      "resources",
      "values",
      "component.",
      "added",
      "template",
      "runs",
      "automatically",
      "during",
      "un-reserving"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Image Aggregation",
    "content": "With Image Aggregation (IA), you can create offerings in a short period of time, thereby reducing the time required to create resource offerings for production/testing environments. The images are aggregated from multiple cloud providers on a single page, allowing you to easily search for and locate the required image, and create offerings. IA helps you overcome the manual tasks and enables you: In aggregating and normalizing images from AWS, Microsoft Azure, VMware vCenter, and GCP. Provide a UI to enter the filter criteria Display the much smaller and more manageable filter results Choose what to make available in Cloud Management, with the click of a button The 'Image Aggregation' page presents a consolidated or aggregated view of services from multiple cloud providers. This unified view provides a centralized location where you can identify relevant cloud services and quickly make them available to consumers. Related topics To configure authorization for users in DND, see Configure",
    "url": "imageaggregation",
    "filename": "imageaggregation",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "image",
      "aggregation",
      "related",
      "topics",
      "ia",
      "create",
      "offerings",
      "short",
      "period",
      "time",
      "thereby",
      "reducing",
      "required",
      "resource",
      "production",
      "testing",
      "environments.",
      "images",
      "aggregated",
      "multiple",
      "cloud",
      "providers",
      "single",
      "page",
      "allowing",
      "easily",
      "search",
      "locate",
      "offerings.",
      "helps",
      "overcome",
      "manual",
      "tasks",
      "enables",
      "aggregating",
      "normalizing",
      "aws",
      "microsoft",
      "azure",
      "vmware",
      "vcenter",
      "gcp.",
      "provide",
      "ui",
      "enter",
      "filter",
      "criteria",
      "display",
      "much",
      "smaller",
      "manageable",
      "results",
      "choose",
      "what",
      "make",
      "available",
      "management",
      "click",
      "button",
      "presents",
      "consolidated",
      "view",
      "services",
      "providers.",
      "unified",
      "provides",
      "centralized",
      "location",
      "identify",
      "relevant",
      "quickly",
      "consumers.",
      "configure",
      "authorization",
      "users",
      "dnd",
      "see",
      "dnd.",
      "assign",
      "role",
      "user",
      "roles.",
      "service",
      "designs",
      "images.",
      "troubleshoot",
      "issues",
      "refer",
      "logs",
      "following",
      "microservices",
      "accounts",
      "scheduler",
      "integration-gateway",
      "image-catalog",
      "image-catalog-gateway"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "image aggregation",
    "contentLower": "with image aggregation (ia), you can create offerings in a short period of time, thereby reducing the time required to create resource offerings for production/testing environments. the images are aggregated from multiple cloud providers on a single page, allowing you to easily search for and locate the required image, and create offerings. ia helps you overcome the manual tasks and enables you: in aggregating and normalizing images from aws, microsoft azure, vmware vcenter, and gcp. provide a ui to enter the filter criteria display the much smaller and more manageable filter results choose what to make available in cloud management, with the click of a button the 'image aggregation' page presents a consolidated or aggregated view of services from multiple cloud providers. this unified view provides a centralized location where you can identify relevant cloud services and quickly make them available to consumers. related topics to configure authorization for users in dnd, see configure",
    "keywordsLower": [
      "image",
      "aggregation",
      "related",
      "topics",
      "ia",
      "create",
      "offerings",
      "short",
      "period",
      "time",
      "thereby",
      "reducing",
      "required",
      "resource",
      "production",
      "testing",
      "environments.",
      "images",
      "aggregated",
      "multiple",
      "cloud",
      "providers",
      "single",
      "page",
      "allowing",
      "easily",
      "search",
      "locate",
      "offerings.",
      "helps",
      "overcome",
      "manual",
      "tasks",
      "enables",
      "aggregating",
      "normalizing",
      "aws",
      "microsoft",
      "azure",
      "vmware",
      "vcenter",
      "gcp.",
      "provide",
      "ui",
      "enter",
      "filter",
      "criteria",
      "display",
      "much",
      "smaller",
      "manageable",
      "results",
      "choose",
      "what",
      "make",
      "available",
      "management",
      "click",
      "button",
      "presents",
      "consolidated",
      "view",
      "services",
      "providers.",
      "unified",
      "provides",
      "centralized",
      "location",
      "identify",
      "relevant",
      "quickly",
      "consumers.",
      "configure",
      "authorization",
      "users",
      "dnd",
      "see",
      "dnd.",
      "assign",
      "role",
      "user",
      "roles.",
      "service",
      "designs",
      "images.",
      "troubleshoot",
      "issues",
      "refer",
      "logs",
      "following",
      "microservices",
      "accounts",
      "scheduler",
      "integration-gateway",
      "image-catalog",
      "image-catalog-gateway"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import Content Packs to the Dependencies Pane",
    "content": "As an author you can import the content packs created using OO Workflow Designer (CloudSlang content packs) and OO Desktop Studio (OO native content packs). Import content packs to the workspace Before importing the content packs you must prepare the content packs to be imported: Certificate Status Icon Description Deployment State Digital signature or Content pack is altered This content pack can't be deployed. Digital signature is expired Content Pack isn't signed Content Pack is signed but the signed CA isn't in the client.truststore This content pack can be deployed. Digital signature has passed and the signature's CA is known to OO Workflow Designer. This content pack can be deployed. Locate your required Content Packs, consider to: Use the content packs that were included when you installed OO Workflow Designer. Make sure to import a base content pack first as it's a dependency for all the other content packs before importing a content pack. Download additional content packs from",
    "url": "designerdependenciespane",
    "filename": "designerdependenciespane",
    "headings": [
      "Import content packs to the workspace",
      "Assign an imported content pack",
      "Assign content pack",
      "Enable assign content pack feature",
      "Filter the dependencies",
      "View the details of a read-only item from the dependencies pane",
      "Identifying content packs created without using OO Workflow Designer",
      "Related topics"
    ],
    "keywords": [
      "https://<FQDN>:<PORT>/oo-designer/rest/v0/xfeatures/312",
      "import",
      "content",
      "packs",
      "dependencies",
      "pane",
      "workspace",
      "assign",
      "imported",
      "pack",
      "enable",
      "feature",
      "filter",
      "view",
      "details",
      "read-only",
      "item",
      "identifying",
      "created",
      "oo",
      "workflow",
      "designer",
      "related",
      "topics",
      "author",
      "cloudslang",
      "desktop",
      "studio",
      "native",
      "before",
      "importing",
      "prepare",
      "certificate",
      "status",
      "icon",
      "description",
      "deployment",
      "state",
      "digital",
      "signature",
      "altered",
      "deployed.",
      "expired",
      "isn",
      "signed",
      "ca",
      "client.truststore",
      "passed",
      "known",
      "designer.",
      "locate",
      "required",
      "consider",
      "included",
      "installed",
      "make",
      "sure",
      "base",
      "first",
      "dependency",
      "all",
      "pack.",
      "download",
      "additional",
      "itom",
      "marketplace.",
      "create",
      "existing",
      "content.",
      "click",
      "button",
      "pane.",
      "dialog",
      "box",
      "new",
      "browse",
      "select",
      "one",
      "open",
      "upload.",
      "drag",
      "drop",
      "dialog.",
      "upload",
      "checks",
      "signature.",
      "following",
      "possible",
      "statuses",
      "see",
      "issuing",
      "authority.",
      "selected",
      "digitally",
      "successfully",
      "uploaded",
      "enabled.",
      "any",
      "fails",
      "already"
    ],
    "language": "en",
    "word_count": 114,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import content packs to the dependencies pane",
    "contentLower": "as an author you can import the content packs created using oo workflow designer (cloudslang content packs) and oo desktop studio (oo native content packs). import content packs to the workspace before importing the content packs you must prepare the content packs to be imported: certificate status icon description deployment state digital signature or content pack is altered this content pack can't be deployed. digital signature is expired content pack isn't signed content pack is signed but the signed ca isn't in the client.truststore this content pack can be deployed. digital signature has passed and the signature's ca is known to oo workflow designer. this content pack can be deployed. locate your required content packs, consider to: use the content packs that were included when you installed oo workflow designer. make sure to import a base content pack first as it's a dependency for all the other content packs before importing a content pack. download additional content packs from",
    "keywordsLower": [
      "https://<fqdn>:<port>/oo-designer/rest/v0/xfeatures/312",
      "import",
      "content",
      "packs",
      "dependencies",
      "pane",
      "workspace",
      "assign",
      "imported",
      "pack",
      "enable",
      "feature",
      "filter",
      "view",
      "details",
      "read-only",
      "item",
      "identifying",
      "created",
      "oo",
      "workflow",
      "designer",
      "related",
      "topics",
      "author",
      "cloudslang",
      "desktop",
      "studio",
      "native",
      "before",
      "importing",
      "prepare",
      "certificate",
      "status",
      "icon",
      "description",
      "deployment",
      "state",
      "digital",
      "signature",
      "altered",
      "deployed.",
      "expired",
      "isn",
      "signed",
      "ca",
      "client.truststore",
      "passed",
      "known",
      "designer.",
      "locate",
      "required",
      "consider",
      "included",
      "installed",
      "make",
      "sure",
      "base",
      "first",
      "dependency",
      "all",
      "pack.",
      "download",
      "additional",
      "itom",
      "marketplace.",
      "create",
      "existing",
      "content.",
      "click",
      "button",
      "pane.",
      "dialog",
      "box",
      "new",
      "browse",
      "select",
      "one",
      "open",
      "upload.",
      "drag",
      "drop",
      "dialog.",
      "upload",
      "checks",
      "signature.",
      "following",
      "possible",
      "statuses",
      "see",
      "issuing",
      "authority.",
      "selected",
      "digitally",
      "successfully",
      "uploaded",
      "enabled.",
      "any",
      "fails",
      "already"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Keyboard shortcuts",
    "content": "To enhance productivity and speed up navigation and common repetitive actions, Operations Orchestration (OO) Workflow Designer offers keyboard shortcuts. You can use these as options to clicking buttons and selecting options in the user interface. The main operation key for shortcuts is Alt, together with one letter. Global Press this key Click OK Enter Click Cancel ESC PROJECTS Pane Open the selected tree node in the Authoring pane Enter Copy the selected tree node Ctrl + C Paste the selected tree node Ctrl + V Delete the selected tree node Delete Create a new project Alt + P Create a new folder Alt + L Create a new flow Alt + W Create a new Python Operation Alt + O Create a new system property Alt + S Create a content pack Alt + C DEPENDENCIES Pane Import a content pack Alt + I GRAPH Pane Copy a selected step Ctrl + C Cut a selected step Ctrl + X Paste a selected step Ctrl + V Redo an action Ctrl + Y Undo an action Ctrl + Z DEBUG Pane Start to debug Alt + F11 Cancel debug Alt + F12 E",
    "url": "designerkeyboardshortcuts",
    "filename": "designerkeyboardshortcuts",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "keyboard",
      "shortcuts",
      "related",
      "topics",
      "enhance",
      "productivity",
      "speed",
      "navigation",
      "common",
      "repetitive",
      "actions",
      "operations",
      "orchestration",
      "oo",
      "workflow",
      "designer",
      "offers",
      "shortcuts.",
      "options",
      "clicking",
      "buttons",
      "selecting",
      "user",
      "interface.",
      "main",
      "operation",
      "key",
      "alt",
      "together",
      "one",
      "letter.",
      "global",
      "press",
      "click",
      "ok",
      "enter",
      "cancel",
      "esc",
      "projects",
      "pane",
      "open",
      "selected",
      "tree",
      "node",
      "authoring",
      "copy",
      "ctrl",
      "paste",
      "delete",
      "create",
      "new",
      "project",
      "folder",
      "flow",
      "python",
      "system",
      "property",
      "content",
      "pack",
      "dependencies",
      "import",
      "graph",
      "step",
      "cut",
      "redo",
      "action",
      "undo",
      "debug",
      "start",
      "f11",
      "f12",
      "expand",
      "shift",
      "spacebar",
      "know",
      "about",
      "interface",
      "see",
      "instructions",
      "author",
      "flows",
      "designer."
    ],
    "language": "en",
    "word_count": 141,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "keyboard shortcuts",
    "contentLower": "to enhance productivity and speed up navigation and common repetitive actions, operations orchestration (oo) workflow designer offers keyboard shortcuts. you can use these as options to clicking buttons and selecting options in the user interface. the main operation key for shortcuts is alt, together with one letter. global press this key click ok enter click cancel esc projects pane open the selected tree node in the authoring pane enter copy the selected tree node ctrl + c paste the selected tree node ctrl + v delete the selected tree node delete create a new project alt + p create a new folder alt + l create a new flow alt + w create a new python operation alt + o create a new system property alt + s create a content pack alt + c dependencies pane import a content pack alt + i graph pane copy a selected step ctrl + c cut a selected step ctrl + x paste a selected step ctrl + v redo an action ctrl + y undo an action ctrl + z debug pane start to debug alt + f11 cancel debug alt + f12 e",
    "keywordsLower": [
      "keyboard",
      "shortcuts",
      "related",
      "topics",
      "enhance",
      "productivity",
      "speed",
      "navigation",
      "common",
      "repetitive",
      "actions",
      "operations",
      "orchestration",
      "oo",
      "workflow",
      "designer",
      "offers",
      "shortcuts.",
      "options",
      "clicking",
      "buttons",
      "selecting",
      "user",
      "interface.",
      "main",
      "operation",
      "key",
      "alt",
      "together",
      "one",
      "letter.",
      "global",
      "press",
      "click",
      "ok",
      "enter",
      "cancel",
      "esc",
      "projects",
      "pane",
      "open",
      "selected",
      "tree",
      "node",
      "authoring",
      "copy",
      "ctrl",
      "paste",
      "delete",
      "create",
      "new",
      "project",
      "folder",
      "flow",
      "python",
      "system",
      "property",
      "content",
      "pack",
      "dependencies",
      "import",
      "graph",
      "step",
      "cut",
      "redo",
      "action",
      "undo",
      "debug",
      "start",
      "f11",
      "f12",
      "expand",
      "shift",
      "spacebar",
      "know",
      "about",
      "interface",
      "see",
      "instructions",
      "author",
      "flows",
      "designer."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "List of allowed HTML tags in Service Management",
    "content": "This page lists the allowed HTML tags in Service Management. The following tags are allowed with attributes: style, id and all styles: Document metadata: <style>* Content sectioning: <body>, <h1>, <h2>, <h3>, <h4>, <h5>, <h6> Text content: <blockquote>, <dd>, <div>*, <dl>, <dt>, <ol>, <ul>, <li>, <p>, <pre> Inline text semantics: <a>*, <b>, <br>, <cite>, <code>, <em>, <i>, <q>, <small>, <span>*, <strong>, <sub>, <sup>, <u> Table content: <caption>, <col>, <colgroup>, <table>*, <tbody>, <td>*, <tfoot>, <th>*, <thead>, <tr> Image: <img>* Deprecated: <strike>, <nobr> Defined by Service Management: <highlight>*, <bookmark>*   Note: Please don't use these two tags when you define HTML formats. *  In addition, there are specific rules for the tags marked with * : Tag <highlight> is only allowed with classes: highlight, detected-ci, icon-involved-CIs, new-detected-ci and all attributes. Tag <a> is only allowed with attributes: name, href, target, title, hreflang, media, rel, type, conclick an",
    "url": "allowedhtmltags",
    "filename": "allowedhtmltags",
    "headings": [],
    "keywords": [
      "4.1",
      "list",
      "allowed",
      "html",
      "tags",
      "service",
      "management",
      "page",
      "lists",
      "management.",
      "following",
      "attributes",
      "style",
      "id",
      "all",
      "styles",
      "document",
      "metadata",
      "content",
      "sectioning",
      "text",
      "inline",
      "semantics",
      "table",
      "image",
      "deprecated",
      "defined",
      "note",
      "please",
      "don",
      "two",
      "define",
      "formats.",
      "addition",
      "there",
      "specific",
      "rules",
      "marked",
      "tag",
      "classes",
      "highlight",
      "detected-ci",
      "icon-involved-cis",
      "new-detected-ci",
      "attributes.",
      "name",
      "href",
      "target",
      "title",
      "hreflang",
      "media",
      "rel",
      "type",
      "conclick",
      "required.",
      "doesn",
      "include",
      "whole",
      "removed",
      "system.",
      "src",
      "alt",
      "width",
      "height",
      "attribute",
      "saw-collapsible.",
      "simpleuploadstmpwrapper",
      "icon-priority-critical",
      "icon-priority-high",
      "icon-priority-low",
      "icon-priority-medium",
      "drop-down-editor-icon",
      "icon-collision.",
      "class",
      "cursor-invisible.",
      "accesskey",
      "contenteditable",
      "contextmenu",
      "dir",
      "hidden",
      "lang",
      "tabindex",
      "translate",
      "scoped",
      "disabled.",
      "cellspan",
      "cellpadding",
      "border",
      "cellspacing.",
      "4.1.",
      "keep",
      "backward",
      "compatibility.",
      "recommended",
      "css",
      "tables.",
      "colspan",
      "align",
      "valign",
      "bgcolor."
    ],
    "language": "en",
    "word_count": 73,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "list of allowed html tags in service management",
    "contentLower": "this page lists the allowed html tags in service management. the following tags are allowed with attributes: style, id and all styles: document metadata: <style>* content sectioning: <body>, <h1>, <h2>, <h3>, <h4>, <h5>, <h6> text content: <blockquote>, <dd>, <div>*, <dl>, <dt>, <ol>, <ul>, <li>, <p>, <pre> inline text semantics: <a>*, <b>, <br>, <cite>, <code>, <em>, <i>, <q>, <small>, <span>*, <strong>, <sub>, <sup>, <u> table content: <caption>, <col>, <colgroup>, <table>*, <tbody>, <td>*, <tfoot>, <th>*, <thead>, <tr> image: <img>* deprecated: <strike>, <nobr> defined by service management: <highlight>*, <bookmark>*   note: please don't use these two tags when you define html formats. *  in addition, there are specific rules for the tags marked with * : tag <highlight> is only allowed with classes: highlight, detected-ci, icon-involved-cis, new-detected-ci and all attributes. tag <a> is only allowed with attributes: name, href, target, title, hreflang, media, rel, type, conclick an",
    "keywordsLower": [
      "4.1",
      "list",
      "allowed",
      "html",
      "tags",
      "service",
      "management",
      "page",
      "lists",
      "management.",
      "following",
      "attributes",
      "style",
      "id",
      "all",
      "styles",
      "document",
      "metadata",
      "content",
      "sectioning",
      "text",
      "inline",
      "semantics",
      "table",
      "image",
      "deprecated",
      "defined",
      "note",
      "please",
      "don",
      "two",
      "define",
      "formats.",
      "addition",
      "there",
      "specific",
      "rules",
      "marked",
      "tag",
      "classes",
      "highlight",
      "detected-ci",
      "icon-involved-cis",
      "new-detected-ci",
      "attributes.",
      "name",
      "href",
      "target",
      "title",
      "hreflang",
      "media",
      "rel",
      "type",
      "conclick",
      "required.",
      "doesn",
      "include",
      "whole",
      "removed",
      "system.",
      "src",
      "alt",
      "width",
      "height",
      "attribute",
      "saw-collapsible.",
      "simpleuploadstmpwrapper",
      "icon-priority-critical",
      "icon-priority-high",
      "icon-priority-low",
      "icon-priority-medium",
      "drop-down-editor-icon",
      "icon-collision.",
      "class",
      "cursor-invisible.",
      "accesskey",
      "contenteditable",
      "contextmenu",
      "dir",
      "hidden",
      "lang",
      "tabindex",
      "translate",
      "scoped",
      "disabled.",
      "cellspan",
      "cellpadding",
      "border",
      "cellspacing.",
      "4.1.",
      "keep",
      "backward",
      "compatibility.",
      "recommended",
      "css",
      "tables.",
      "colspan",
      "align",
      "valign",
      "bgcolor."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Inbound rule port change",
    "content": "After adopting the native ingress controller, a new NodePort is assigned. Ideally, updating the ingress should automatically update the port. However, in some cases, this update may not occur. If that happens, verify the inbound rule and either modify it to include the new port or add a new inbound rule for the assigned port. BO not accessible - 503 Service Unavailable after upgrading to 25.4 Cause In release 25.4, we are adopting the native ingress controller instead of the ESM-specific ingress controller. Due to this change, the NodePort value changes. In some cases, because of errors during AWS reconciliation, the new NodePort may not be updated in the inbound rules. Solution Get the node port of itom-nginx-ingress-svc kubectl -n <suite namespace> get svc | grep itom-nginx-ingress-svc Find the Node Group Security Group Go to AWS Console → EC2 → Auto Scaling Groups.Locate the ASG for your EKS node group.Check the Security Group(s) attached to the EC2 instances. Edit Inbound Rules Nav",
    "url": "inboundruleportchange",
    "filename": "inboundruleportchange",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "25.4",
      "0.0.0.0",
      "Rules.Add",
      "0.0.0",
      "inbound",
      "rule",
      "port",
      "change",
      "cause",
      "solution",
      "after",
      "adopting",
      "native",
      "ingress",
      "controller",
      "new",
      "nodeport",
      "assigned.",
      "ideally",
      "updating",
      "automatically",
      "update",
      "port.",
      "however",
      "cases",
      "occur.",
      "happens",
      "verify",
      "either",
      "modify",
      "include",
      "add",
      "assigned",
      "bo",
      "accessible",
      "503",
      "service",
      "unavailable",
      "upgrading",
      "release",
      "instead",
      "esm-specific",
      "controller.",
      "due",
      "value",
      "changes.",
      "because",
      "errors",
      "during",
      "aws",
      "reconciliation",
      "updated",
      "rules.",
      "get",
      "node",
      "itom-nginx-ingress-svc",
      "kubectl",
      "-n",
      "svc",
      "grep",
      "find",
      "group",
      "security",
      "go",
      "console",
      "ec2",
      "auto",
      "scaling",
      "groups.locate",
      "asg",
      "eks",
      "group.check",
      "attached",
      "instances.",
      "edit",
      "rules",
      "navigate",
      "groups",
      "sg",
      "type",
      "custom",
      "tcp",
      "ruleport",
      "range",
      "source",
      "load",
      "balancer",
      "testing",
      "recommended",
      "production",
      "save"
    ],
    "language": "en",
    "word_count": 109,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "inbound rule port change",
    "contentLower": "after adopting the native ingress controller, a new nodeport is assigned. ideally, updating the ingress should automatically update the port. however, in some cases, this update may not occur. if that happens, verify the inbound rule and either modify it to include the new port or add a new inbound rule for the assigned port. bo not accessible - 503 service unavailable after upgrading to 25.4 cause in release 25.4, we are adopting the native ingress controller instead of the esm-specific ingress controller. due to this change, the nodeport value changes. in some cases, because of errors during aws reconciliation, the new nodeport may not be updated in the inbound rules. solution get the node port of itom-nginx-ingress-svc kubectl -n <suite namespace> get svc | grep itom-nginx-ingress-svc find the node group security group go to aws console → ec2 → auto scaling groups.locate the asg for your eks node group.check the security group(s) attached to the ec2 instances. edit inbound rules nav",
    "keywordsLower": [
      "25.4",
      "0.0.0.0",
      "rules.add",
      "0.0.0",
      "inbound",
      "rule",
      "port",
      "change",
      "cause",
      "solution",
      "after",
      "adopting",
      "native",
      "ingress",
      "controller",
      "new",
      "nodeport",
      "assigned.",
      "ideally",
      "updating",
      "automatically",
      "update",
      "port.",
      "however",
      "cases",
      "occur.",
      "happens",
      "verify",
      "either",
      "modify",
      "include",
      "add",
      "assigned",
      "bo",
      "accessible",
      "503",
      "service",
      "unavailable",
      "upgrading",
      "release",
      "instead",
      "esm-specific",
      "controller.",
      "due",
      "value",
      "changes.",
      "because",
      "errors",
      "during",
      "aws",
      "reconciliation",
      "updated",
      "rules.",
      "get",
      "node",
      "itom-nginx-ingress-svc",
      "kubectl",
      "-n",
      "svc",
      "grep",
      "find",
      "group",
      "security",
      "go",
      "console",
      "ec2",
      "auto",
      "scaling",
      "groups.locate",
      "asg",
      "eks",
      "group.check",
      "attached",
      "instances.",
      "edit",
      "rules",
      "navigate",
      "groups",
      "sg",
      "type",
      "custom",
      "tcp",
      "ruleport",
      "range",
      "source",
      "load",
      "balancer",
      "testing",
      "recommended",
      "production",
      "save"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incorrect suite version and status in OMT management portal after multi-hop upgrade",
    "content": "After completing a multi-hop upgrade, the OMT management portal displays a wrong suite version or an incorrect status. Cause This issue can occur when the suite metadata is yet to be uploaded, causing a mismatch in the OMT management portal. Solution Use these steps below to rectify this issue. If you haven't already done so, upload the metadata file of the target SMAX version you wish to upgrade to. Go to the SMA Support Assistant container Navigate to ../multiple_hop_upgrade/src Run the command below to fix the version and status issue: ./setSMAXVersion.sh ----cdf-version 2x.x --smax-version 2x.x Replace the SMAX and CDF version with your desired target SMAX and OMT version. Go to the OMT management portal and confirm the change has taken effect. The version should be updated to \"2x.x\" and status should be \"Installed\" on the portal UI.",
    "url": "incorrectversionstatus",
    "filename": "incorrectversionstatus",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "setSMAXVersion.sh",
      "incorrect",
      "suite",
      "version",
      "status",
      "omt",
      "management",
      "portal",
      "after",
      "multi-hop",
      "upgrade",
      "cause",
      "solution",
      "completing",
      "displays",
      "wrong",
      "status.",
      "issue",
      "occur",
      "metadata",
      "yet",
      "uploaded",
      "causing",
      "mismatch",
      "portal.",
      "steps",
      "below",
      "rectify",
      "issue.",
      "haven",
      "already",
      "done",
      "upload",
      "file",
      "target",
      "smax",
      "wish",
      "to.",
      "go",
      "sma",
      "support",
      "assistant",
      "container",
      "navigate",
      "src",
      "run",
      "command",
      "fix",
      "----cdf-version",
      "2x.x",
      "--smax-version",
      "replace",
      "cdf",
      "desired",
      "version.",
      "confirm",
      "change",
      "taken",
      "effect.",
      "updated",
      "installed",
      "ui."
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incorrect suite version and status in omt management portal after multi-hop upgrade",
    "contentLower": "after completing a multi-hop upgrade, the omt management portal displays a wrong suite version or an incorrect status. cause this issue can occur when the suite metadata is yet to be uploaded, causing a mismatch in the omt management portal. solution use these steps below to rectify this issue. if you haven't already done so, upload the metadata file of the target smax version you wish to upgrade to. go to the sma support assistant container navigate to ../multiple_hop_upgrade/src run the command below to fix the version and status issue: ./setsmaxversion.sh ----cdf-version 2x.x --smax-version 2x.x replace the smax and cdf version with your desired target smax and omt version. go to the omt management portal and confirm the change has taken effect. the version should be updated to \"2x.x\" and status should be \"installed\" on the portal ui.",
    "keywordsLower": [
      "setsmaxversion.sh",
      "incorrect",
      "suite",
      "version",
      "status",
      "omt",
      "management",
      "portal",
      "after",
      "multi-hop",
      "upgrade",
      "cause",
      "solution",
      "completing",
      "displays",
      "wrong",
      "status.",
      "issue",
      "occur",
      "metadata",
      "yet",
      "uploaded",
      "causing",
      "mismatch",
      "portal.",
      "steps",
      "below",
      "rectify",
      "issue.",
      "haven",
      "already",
      "done",
      "upload",
      "file",
      "target",
      "smax",
      "wish",
      "to.",
      "go",
      "sma",
      "support",
      "assistant",
      "container",
      "navigate",
      "src",
      "run",
      "command",
      "fix",
      "----cdf-version",
      "2x.x",
      "--smax-version",
      "replace",
      "cdf",
      "desired",
      "version.",
      "confirm",
      "change",
      "taken",
      "effect.",
      "updated",
      "installed",
      "ui."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "IdM pod crashes during OMT upgrade",
    "content": "When you try to upgrade OMT, the IdM pod crashes. When this issue occurs, error messages that resemble the following are written to the IdM log file: Updating key-configuration.properties... update_security_files normal update applicationContext.properties and hpssoConfig.xml from environments Update database settings for IDM... using default postgresql driver ERROR get password from vault, please contact an administrator. Cause The cause of this issue is unknown. Resolution To fix this issue, delete the IdM pod.",
    "url": "idmcrashduringupgrade",
    "filename": "idmcrashduringupgrade",
    "headings": [
      "Cause",
      "Resolution"
    ],
    "keywords": [
      "hpssoConfig.xml",
      "idm",
      "pod",
      "crashes",
      "during",
      "omt",
      "upgrade",
      "cause",
      "resolution",
      "try",
      "crashes.",
      "issue",
      "occurs",
      "error",
      "messages",
      "resemble",
      "following",
      "written",
      "log",
      "file",
      "updating",
      "key-configuration.properties...",
      "normal",
      "update",
      "applicationcontext.properties",
      "environments",
      "database",
      "settings",
      "idm...",
      "default",
      "postgresql",
      "driver",
      "get",
      "password",
      "vault",
      "please",
      "contact",
      "administrator.",
      "unknown.",
      "fix",
      "delete",
      "pod."
    ],
    "language": "en",
    "word_count": 54,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "idm pod crashes during omt upgrade",
    "contentLower": "when you try to upgrade omt, the idm pod crashes. when this issue occurs, error messages that resemble the following are written to the idm log file: updating key-configuration.properties... update_security_files normal update applicationcontext.properties and hpssoconfig.xml from environments update database settings for idm... using default postgresql driver error get password from vault, please contact an administrator. cause the cause of this issue is unknown. resolution to fix this issue, delete the idm pod.",
    "keywordsLower": [
      "hpssoconfig.xml",
      "idm",
      "pod",
      "crashes",
      "during",
      "omt",
      "upgrade",
      "cause",
      "resolution",
      "try",
      "crashes.",
      "issue",
      "occurs",
      "error",
      "messages",
      "resemble",
      "following",
      "written",
      "log",
      "file",
      "updating",
      "key-configuration.properties...",
      "normal",
      "update",
      "applicationcontext.properties",
      "environments",
      "database",
      "settings",
      "idm...",
      "default",
      "postgresql",
      "driver",
      "get",
      "password",
      "vault",
      "please",
      "contact",
      "administrator.",
      "unknown.",
      "fix",
      "delete",
      "pod."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Jobs don't run after an OMT upgrade",
    "content": "Jobs no longer run after you successfully upgrade OMT. Cause This issue occurs because images used as Init Containers or containers aren't updated to the new version in the jobs, even though the images are updated in the deployment. To confirm that you are experiencing this issue, follow these steps: Run the following command to identify the suite namespace. Replace <suite_name> with the name of the ITOM product that's deployed on top of OMT. kubectl get ns|grep <suite_name> Run the following command to confirm whether an image was updated in the jobs. Replace <suite_namespace> with the value returned by the previous command. Replace <old_image_version> with the path to the image that you want to check. This value must be the full path to the image, including the registry URL and the organization name. For example, localhost:5000/hpeswitom/itom-suite-content:1.12.0.46 kubectl get job -n <suite_namespace> -o json| jq '.items[]| select(.spec.template.spec.containers[].image==\"<old_image_",
    "url": "jobsnotupdatedupgrade",
    "filename": "jobsnotupdatedupgrade",
    "headings": [
      "Cause",
      "Solution",
      "Step 1: Identify the old image version and new image version",
      "Step 2: Update the job's JSON file"
    ],
    "keywords": [
      "dont",
      "1.12.0",
      "su.id",
      "1.12.0.46",
      "0.46",
      "0.0.1",
      "1.12",
      "template.spec",
      "metadata.name",
      "d.id",
      "jobs",
      "don",
      "run",
      "after",
      "omt",
      "upgrade",
      "cause",
      "solution",
      "step",
      "identify",
      "old",
      "image",
      "version",
      "new",
      "update",
      "job",
      "json",
      "file",
      "longer",
      "successfully",
      "omt.",
      "issue",
      "occurs",
      "because",
      "images",
      "init",
      "containers",
      "aren",
      "updated",
      "even",
      "though",
      "deployment.",
      "confirm",
      "experiencing",
      "follow",
      "steps",
      "following",
      "command",
      "suite",
      "namespace.",
      "replace",
      "name",
      "itom",
      "product",
      "deployed",
      "top",
      "kubectl",
      "get",
      "ns",
      "grep",
      "whether",
      "jobs.",
      "value",
      "returned",
      "previous",
      "command.",
      "path",
      "want",
      "check.",
      "full",
      "including",
      "registry",
      "url",
      "organization",
      "name.",
      "example",
      "localhost",
      "5000",
      "hpeswitom",
      "itom-suite-content",
      "-n",
      "-o",
      "jq",
      ".items",
      "select",
      ".spec.template.spec.containers",
      ".image",
      ".spec.template.spec.initcontainers",
      ".metadata.name",
      "-r",
      "returns",
      "wasn",
      "commands",
      "enter",
      "cdfapiserver-db",
      "pod",
      "namespace",
      "pods",
      "cdfapiserver-postgresql",
      "awk"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "jobs don't run after an omt upgrade",
    "contentLower": "jobs no longer run after you successfully upgrade omt. cause this issue occurs because images used as init containers or containers aren't updated to the new version in the jobs, even though the images are updated in the deployment. to confirm that you are experiencing this issue, follow these steps: run the following command to identify the suite namespace. replace <suite_name> with the name of the itom product that's deployed on top of omt. kubectl get ns|grep <suite_name> run the following command to confirm whether an image was updated in the jobs. replace <suite_namespace> with the value returned by the previous command. replace <old_image_version> with the path to the image that you want to check. this value must be the full path to the image, including the registry url and the organization name. for example, localhost:5000/hpeswitom/itom-suite-content:1.12.0.46 kubectl get job -n <suite_namespace> -o json| jq '.items[]| select(.spec.template.spec.containers[].image==\"<old_image_",
    "keywordsLower": [
      "dont",
      "1.12.0",
      "su.id",
      "1.12.0.46",
      "0.46",
      "0.0.1",
      "1.12",
      "template.spec",
      "metadata.name",
      "d.id",
      "jobs",
      "don",
      "run",
      "after",
      "omt",
      "upgrade",
      "cause",
      "solution",
      "step",
      "identify",
      "old",
      "image",
      "version",
      "new",
      "update",
      "job",
      "json",
      "file",
      "longer",
      "successfully",
      "omt.",
      "issue",
      "occurs",
      "because",
      "images",
      "init",
      "containers",
      "aren",
      "updated",
      "even",
      "though",
      "deployment.",
      "confirm",
      "experiencing",
      "follow",
      "steps",
      "following",
      "command",
      "suite",
      "namespace.",
      "replace",
      "name",
      "itom",
      "product",
      "deployed",
      "top",
      "kubectl",
      "get",
      "ns",
      "grep",
      "whether",
      "jobs.",
      "value",
      "returned",
      "previous",
      "command.",
      "path",
      "want",
      "check.",
      "full",
      "including",
      "registry",
      "url",
      "organization",
      "name.",
      "example",
      "localhost",
      "5000",
      "hpeswitom",
      "itom-suite-content",
      "-n",
      "-o",
      "jq",
      ".items",
      "select",
      ".spec.template.spec.containers",
      ".image",
      ".spec.template.spec.initcontainers",
      ".metadata.name",
      "-r",
      "returns",
      "wasn",
      "commands",
      "enter",
      "cdfapiserver-db",
      "pod",
      "namespace",
      "pods",
      "cdfapiserver-postgresql",
      "awk"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Ingress service is pending on GCP after upgrade",
    "content": "If you've set up a Layer 7 load balancer to work with the suite deployed on GCP, the itom-ingress-controller-svc service is pending after the suite upgrade. You can check the status of the itom-ingress-controller-svc service with this command: kubectl get svc --all-namespaces|grep itom-ingress-controller-svc Cause The service type of Ingress is automatically changed from NodePort to LoadBalancer after the upgrade. Solution To reconfigure the service type of itom-ingress-controller-svc back to NodePort, follow these steps: Edit the itom-ingress-controller-svc service YAML file: Locate the YAML file in directory <Filestore_server>:/var/vols/itom/itsma/global-volume/yamls_output/2021.05/itom-ingress-x.x.x/yamls/itom-ingress-controller-svc.yaml. Make sure you set the parameter cloud.google.com/app-protocols to '{\"https-port\":\"HTTPS\"}' so that the service backend uses HTTPS protocol. This YAML file should resemble the following example: kind: Service apiVersion: v1 metadata: name: itom-ingr",
    "url": "gcpingresspending",
    "filename": "gcpingresspending",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "https://console.cloud.google.com",
      "2021.05",
      "svc.yaml",
      "google.com",
      "ingress",
      "service",
      "pending",
      "gcp",
      "after",
      "upgrade",
      "cause",
      "solution",
      "ve",
      "set",
      "layer",
      "load",
      "balancer",
      "work",
      "suite",
      "deployed",
      "itom-ingress-controller-svc",
      "upgrade.",
      "check",
      "status",
      "command",
      "kubectl",
      "get",
      "svc",
      "--all-namespaces",
      "grep",
      "type",
      "automatically",
      "changed",
      "nodeport",
      "loadbalancer",
      "reconfigure",
      "back",
      "follow",
      "steps",
      "edit",
      "yaml",
      "file",
      "locate",
      "directory",
      "var",
      "vols",
      "itom",
      "itsma",
      "global-volume",
      "itom-ingress-x.x.x",
      "yamls",
      "itom-ingress-controller-svc.yaml.",
      "make",
      "sure",
      "parameter",
      "cloud.google.com",
      "app-protocols",
      "https-port",
      "https",
      "backend",
      "uses",
      "protocol.",
      "resemble",
      "following",
      "example",
      "kind",
      "apiversion",
      "v1",
      "metadata",
      "name",
      "namespace",
      "labels",
      "itsmaservice",
      "itom-ingress",
      "itsmarelease",
      "annotations",
      "spec",
      "selector",
      "app",
      "itom-nginx-ingress",
      "ports",
      "protocol",
      "tcp",
      "port",
      "443",
      "targetport",
      "https-svc",
      "run",
      "commands",
      "recreate",
      "modified",
      "delete",
      "-f",
      "itom-nginx-ingress-byok-gcp-svc.yaml",
      "create",
      "configure",
      "health",
      "path",
      "log",
      "google"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "ingress service is pending on gcp after upgrade",
    "contentLower": "if you've set up a layer 7 load balancer to work with the suite deployed on gcp, the itom-ingress-controller-svc service is pending after the suite upgrade. you can check the status of the itom-ingress-controller-svc service with this command: kubectl get svc --all-namespaces|grep itom-ingress-controller-svc cause the service type of ingress is automatically changed from nodeport to loadbalancer after the upgrade. solution to reconfigure the service type of itom-ingress-controller-svc back to nodeport, follow these steps: edit the itom-ingress-controller-svc service yaml file: locate the yaml file in directory <filestore_server>:/var/vols/itom/itsma/global-volume/yamls_output/2021.05/itom-ingress-x.x.x/yamls/itom-ingress-controller-svc.yaml. make sure you set the parameter cloud.google.com/app-protocols to '{\"https-port\":\"https\"}' so that the service backend uses https protocol. this yaml file should resemble the following example: kind: service apiversion: v1 metadata: name: itom-ingr",
    "keywordsLower": [
      "https://console.cloud.google.com",
      "2021.05",
      "svc.yaml",
      "google.com",
      "ingress",
      "service",
      "pending",
      "gcp",
      "after",
      "upgrade",
      "cause",
      "solution",
      "ve",
      "set",
      "layer",
      "load",
      "balancer",
      "work",
      "suite",
      "deployed",
      "itom-ingress-controller-svc",
      "upgrade.",
      "check",
      "status",
      "command",
      "kubectl",
      "get",
      "svc",
      "--all-namespaces",
      "grep",
      "type",
      "automatically",
      "changed",
      "nodeport",
      "loadbalancer",
      "reconfigure",
      "back",
      "follow",
      "steps",
      "edit",
      "yaml",
      "file",
      "locate",
      "directory",
      "var",
      "vols",
      "itom",
      "itsma",
      "global-volume",
      "itom-ingress-x.x.x",
      "yamls",
      "itom-ingress-controller-svc.yaml.",
      "make",
      "sure",
      "parameter",
      "cloud.google.com",
      "app-protocols",
      "https-port",
      "https",
      "backend",
      "uses",
      "protocol.",
      "resemble",
      "following",
      "example",
      "kind",
      "apiversion",
      "v1",
      "metadata",
      "name",
      "namespace",
      "labels",
      "itsmaservice",
      "itom-ingress",
      "itsmarelease",
      "annotations",
      "spec",
      "selector",
      "app",
      "itom-nginx-ingress",
      "ports",
      "protocol",
      "tcp",
      "port",
      "443",
      "targetport",
      "https-svc",
      "run",
      "commands",
      "recreate",
      "modified",
      "delete",
      "-f",
      "itom-nginx-ingress-byok-gcp-svc.yaml",
      "create",
      "configure",
      "health",
      "path",
      "log",
      "google"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration user failed to establish a login session",
    "content": "You can't establish a login session when using an integration user to connect to the REST API. Cause The integration user's login name isn't unique in the suite instance. Solution Create another integration user with a unique login name.",
    "url": "integrationuserloginerror",
    "filename": "integrationuserloginerror",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "integration",
      "user",
      "failed",
      "establish",
      "login",
      "session",
      "cause",
      "solution",
      "connect",
      "rest",
      "api.",
      "name",
      "isn",
      "unique",
      "suite",
      "instance.",
      "create",
      "another",
      "name."
    ],
    "language": "en",
    "word_count": 34,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration user failed to establish a login session",
    "contentLower": "you can't establish a login session when using an integration user to connect to the rest api. cause the integration user's login name isn't unique in the suite instance. solution create another integration user with a unique login name.",
    "keywordsLower": [
      "integration",
      "user",
      "failed",
      "establish",
      "login",
      "session",
      "cause",
      "solution",
      "connect",
      "rest",
      "api.",
      "name",
      "isn",
      "unique",
      "suite",
      "instance.",
      "create",
      "another",
      "name."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Invalid token when SAML users log in to the system",
    "content": "When a SAML user tries to log in to the system, the following error message is displayed: INVALID TOKEN The request token is invalid. It may have already been used or has expired. Cause This issue can be caused by the time difference between the SAML IdP server and the suite environment. Solution To resolve the issue, make sure that the time on all hosts in the SMA suite cluster and your SAML IdP server is synchronized. Otherwise, users might fail to log in to the system.",
    "url": "invalidtokensamllogin",
    "filename": "invalidtokensamllogin",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "invalid",
      "token",
      "saml",
      "users",
      "log",
      "system",
      "cause",
      "solution",
      "user",
      "tries",
      "following",
      "error",
      "message",
      "displayed",
      "request",
      "invalid.",
      "already",
      "expired.",
      "issue",
      "caused",
      "time",
      "difference",
      "between",
      "idp",
      "server",
      "suite",
      "environment.",
      "resolve",
      "make",
      "sure",
      "all",
      "hosts",
      "sma",
      "cluster",
      "synchronized.",
      "otherwise",
      "fail",
      "system."
    ],
    "language": "en",
    "word_count": 53,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "invalid token when saml users log in to the system",
    "contentLower": "when a saml user tries to log in to the system, the following error message is displayed: invalid token the request token is invalid. it may have already been used or has expired. cause this issue can be caused by the time difference between the saml idp server and the suite environment. solution to resolve the issue, make sure that the time on all hosts in the sma suite cluster and your saml idp server is synchronized. otherwise, users might fail to log in to the system.",
    "keywordsLower": [
      "invalid",
      "token",
      "saml",
      "users",
      "log",
      "system",
      "cause",
      "solution",
      "user",
      "tries",
      "following",
      "error",
      "message",
      "displayed",
      "request",
      "invalid.",
      "already",
      "expired.",
      "issue",
      "caused",
      "time",
      "difference",
      "between",
      "idp",
      "server",
      "suite",
      "environment.",
      "resolve",
      "make",
      "sure",
      "all",
      "hosts",
      "sma",
      "cluster",
      "synchronized.",
      "otherwise",
      "fail",
      "system."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "IA: Editing a design fails with 'auto_scaling_config error'",
    "content": "After configuring a design in image aggregation using Azure, GCP, or AWS images, if you click Edit Design >  Configuration Options tab, and click Save without any modifications, the following error appears: Property 'auto_scaling_configurations/auto_scaling/scaling_scheduler_state' does not exist or is not accessible (location: Auto Scaling Configuration / Scaling Configuration / Auto Scaling Scheduler / Scale Up) Cause This issue occurs because if a client token has a mapping, image aggregation can't resolve it. Solution Go to Auto Scaling Configuration > Auto Scaling > Scaling Configuration. Click Scale Up > Properties > Scale Up Server Count > Configure Parameters. In the Configure Parameters dialog, for the \"scaling_state\" parameter, click the gear icon in the Parameter Value field and select Create Mapping. The Select Property to Map to Scaling Parameter scaling_state dialog appears. Select 'Scaling Scheduler State'. In the Configure Parameters dialog, click Validate. Save the cha",
    "url": "iaeditdesignfails",
    "filename": "iaeditdesignfails",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "auto_scaling_config",
      "ia",
      "editing",
      "design",
      "fails",
      "error",
      "cause",
      "solution",
      "after",
      "configuring",
      "image",
      "aggregation",
      "azure",
      "gcp",
      "aws",
      "images",
      "click",
      "edit",
      "configuration",
      "options",
      "tab",
      "save",
      "any",
      "modifications",
      "following",
      "appears",
      "property",
      "exist",
      "accessible",
      "location",
      "auto",
      "scaling",
      "scheduler",
      "scale",
      "issue",
      "occurs",
      "because",
      "client",
      "token",
      "mapping",
      "resolve",
      "it.",
      "go",
      "configuration.",
      "properties",
      "server",
      "count",
      "configure",
      "parameters.",
      "parameters",
      "dialog",
      "parameter",
      "gear",
      "icon",
      "value",
      "field",
      "select",
      "create",
      "mapping.",
      "map",
      "appears.",
      "state",
      "validate.",
      "changes."
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "ia: editing a design fails with 'auto_scaling_config error'",
    "contentLower": "after configuring a design in image aggregation using azure, gcp, or aws images, if you click edit design >  configuration options tab, and click save without any modifications, the following error appears: property 'auto_scaling_configurations/auto_scaling/scaling_scheduler_state' does not exist or is not accessible (location: auto scaling configuration / scaling configuration / auto scaling scheduler / scale up) cause this issue occurs because if a client token has a mapping, image aggregation can't resolve it. solution go to auto scaling configuration > auto scaling > scaling configuration. click scale up > properties > scale up server count > configure parameters. in the configure parameters dialog, for the \"scaling_state\" parameter, click the gear icon in the parameter value field and select create mapping. the select property to map to scaling parameter scaling_state dialog appears. select 'scaling scheduler state'. in the configure parameters dialog, click validate. save the cha",
    "keywordsLower": [
      "auto_scaling_config",
      "ia",
      "editing",
      "design",
      "fails",
      "error",
      "cause",
      "solution",
      "after",
      "configuring",
      "image",
      "aggregation",
      "azure",
      "gcp",
      "aws",
      "images",
      "click",
      "edit",
      "configuration",
      "options",
      "tab",
      "save",
      "any",
      "modifications",
      "following",
      "appears",
      "property",
      "exist",
      "accessible",
      "location",
      "auto",
      "scaling",
      "scheduler",
      "scale",
      "issue",
      "occurs",
      "because",
      "client",
      "token",
      "mapping",
      "resolve",
      "it.",
      "go",
      "configuration.",
      "properties",
      "server",
      "count",
      "configure",
      "parameters.",
      "parameters",
      "dialog",
      "parameter",
      "gear",
      "icon",
      "value",
      "field",
      "select",
      "create",
      "mapping.",
      "map",
      "appears.",
      "state",
      "validate.",
      "changes."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Image aggregation page fails to load",
    "content": "Image Aggregation page isn't loaded when accessed from Main Menu > Build > Design > Image Aggregation. Cause The cause is unknown. Solution Refresh the page to load the Image Aggregation page.",
    "url": "iapagefailtoload",
    "filename": "iapagefailtoload",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "image",
      "aggregation",
      "page",
      "fails",
      "load",
      "cause",
      "solution",
      "isn",
      "loaded",
      "accessed",
      "main",
      "menu",
      "build",
      "design",
      "aggregation.",
      "unknown.",
      "refresh",
      "page."
    ],
    "language": "en",
    "word_count": 28,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "image aggregation page fails to load",
    "contentLower": "image aggregation page isn't loaded when accessed from main menu > build > design > image aggregation. cause the cause is unknown. solution refresh the page to load the image aggregation page.",
    "keywordsLower": [
      "image",
      "aggregation",
      "page",
      "fails",
      "load",
      "cause",
      "solution",
      "isn",
      "loaded",
      "accessed",
      "main",
      "menu",
      "build",
      "design",
      "aggregation.",
      "unknown.",
      "refresh",
      "page."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Image Aggregation - Null value in 'tenant_id' column",
    "content": "After an upgrade, the system occasionally creates an Image Aggregation provider with a null value in the 'tenant_ID' column. As a result, the collection of images fails. The following error occurs: 2020-12-15 07:56:53.719 ERROR 1 --- [pool-3-thread-3] o.h.engine.jdbc.spi.SqlExceptionHelper   : ERROR: null value in column \"tenant_id\" violates not-null constraint Detail: Failing row contains (null, 044154d5cb5f4a10bf0888b21af4d9de). 2020-12-15 07:56:53.723  WARN 1 --- [pool-3-thread-3] c.m.h.b.catalog.index.IndexingManager    : index job 4592fcdd255e467398e9fb15504b1267 - failed: UNKNOWN_ERROR Cause System limitation. Solution Create the Image Aggregation provider again or change the provider name.",
    "url": "nulltenantid",
    "filename": "nulltenantid",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "tenant_id",
      "53.719",
      "53.723",
      "engine.jdbc",
      "image",
      "aggregation",
      "null",
      "value",
      "column",
      "cause",
      "solution",
      "after",
      "upgrade",
      "system",
      "occasionally",
      "creates",
      "provider",
      "column.",
      "result",
      "collection",
      "images",
      "fails.",
      "following",
      "error",
      "occurs",
      "2020-12-15",
      "07",
      "56",
      "pool-3-thread-3",
      "o.h.engine.jdbc.spi.sqlexceptionhelper",
      "violates",
      "not-null",
      "constraint",
      "detail",
      "failing",
      "row",
      "contains",
      "044154d5cb5f4a10bf0888b21af4d9de",
      "warn",
      "c.m.h.b.catalog.index.indexingmanager",
      "index",
      "job",
      "4592fcdd255e467398e9fb15504b1267",
      "failed",
      "limitation.",
      "create",
      "again",
      "change",
      "name."
    ],
    "language": "en",
    "word_count": 74,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "image aggregation - null value in 'tenant_id' column",
    "contentLower": "after an upgrade, the system occasionally creates an image aggregation provider with a null value in the 'tenant_id' column. as a result, the collection of images fails. the following error occurs: 2020-12-15 07:56:53.719 error 1 --- [pool-3-thread-3] o.h.engine.jdbc.spi.sqlexceptionhelper   : error: null value in column \"tenant_id\" violates not-null constraint detail: failing row contains (null, 044154d5cb5f4a10bf0888b21af4d9de). 2020-12-15 07:56:53.723  warn 1 --- [pool-3-thread-3] c.m.h.b.catalog.index.indexingmanager    : index job 4592fcdd255e467398e9fb15504b1267 - failed: unknown_error cause system limitation. solution create the image aggregation provider again or change the provider name.",
    "keywordsLower": [
      "tenant_id",
      "53.719",
      "53.723",
      "engine.jdbc",
      "image",
      "aggregation",
      "null",
      "value",
      "column",
      "cause",
      "solution",
      "after",
      "upgrade",
      "system",
      "occasionally",
      "creates",
      "provider",
      "column.",
      "result",
      "collection",
      "images",
      "fails.",
      "following",
      "error",
      "occurs",
      "2020-12-15",
      "07",
      "56",
      "pool-3-thread-3",
      "o.h.engine.jdbc.spi.sqlexceptionhelper",
      "violates",
      "not-null",
      "constraint",
      "detail",
      "failing",
      "row",
      "contains",
      "044154d5cb5f4a10bf0888b21af4d9de",
      "warn",
      "c.m.h.b.catalog.index.indexingmanager",
      "index",
      "job",
      "4592fcdd255e467398e9fb15504b1267",
      "failed",
      "limitation.",
      "create",
      "again",
      "change",
      "name."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Incorrect message displayed while accessing the menu \"Design\"",
    "content": "While accessing Main Menu > Plan > Design, an incorrect message of Login name or password is incorrect is displayed instead of Unauthorized message. Cause The user does not have permission to access Main Menu > Plan > Design. Solution To give access to Main Menu > Plan > Design, add the user to DND_SERVICE_DESIGNERS group in IDM. For more information on how to enable permissions, see Configure authorization for DND.",
    "url": "invalidloginscreendesignmenu",
    "filename": "invalidloginscreendesignmenu",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "incorrect",
      "message",
      "displayed",
      "while",
      "accessing",
      "menu",
      "design",
      "cause",
      "solution",
      "main",
      "plan",
      "login",
      "name",
      "password",
      "instead",
      "unauthorized",
      "message.",
      "user",
      "permission",
      "access",
      "design.",
      "give",
      "add",
      "group",
      "idm.",
      "information",
      "enable",
      "permissions",
      "see",
      "configure",
      "authorization",
      "dnd."
    ],
    "language": "en",
    "word_count": 50,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "incorrect message displayed while accessing the menu \"design\"",
    "contentLower": "while accessing main menu > plan > design, an incorrect message of login name or password is incorrect is displayed instead of unauthorized message. cause the user does not have permission to access main menu > plan > design. solution to give access to main menu > plan > design, add the user to dnd_service_designers group in idm. for more information on how to enable permissions, see configure authorization for dnd.",
    "keywordsLower": [
      "incorrect",
      "message",
      "displayed",
      "while",
      "accessing",
      "menu",
      "design",
      "cause",
      "solution",
      "main",
      "plan",
      "login",
      "name",
      "password",
      "instead",
      "unauthorized",
      "message.",
      "user",
      "permission",
      "access",
      "design.",
      "give",
      "add",
      "group",
      "idm.",
      "information",
      "enable",
      "permissions",
      "see",
      "configure",
      "authorization",
      "dnd."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Instance type doesn't load for IA running on public cloud",
    "content": "When creating a service design in Image Aggregation (IA), the instance type doesn't load for IA running on the public cloud. Cause If your external OO RAS is configured on an on-premises environment, the Deployment Resource Providers must be created with proxy settings to connect with Azure APIs through OO flows. Since Image Aggregation, which runs on public clouds, does not require proxy settings, during design creation, the instance types won’t load, because the OO flow (running on-premises) and IA (running on the public cloud) both are using the same Deployment Resource Provider. Workaround Create one more Deployment Resource Provider with the same credentials which were used for IA integration and configure proxy details on that Deployment Resource Provider. This way, when creating a design, the IA will use the default provider which was created during integration creation that doesn't have a proxy, and during subscription creation, the service portal user can select the alternate ",
    "url": "instancetypedoesntload",
    "filename": "instancetypedoesntload",
    "headings": [
      "Cause",
      "Workaround"
    ],
    "keywords": [
      "doesnt",
      "instance",
      "type",
      "doesn",
      "load",
      "ia",
      "running",
      "public",
      "cloud",
      "cause",
      "workaround",
      "creating",
      "service",
      "design",
      "image",
      "aggregation",
      "cloud.",
      "external",
      "oo",
      "ras",
      "configured",
      "on-premises",
      "environment",
      "deployment",
      "resource",
      "providers",
      "created",
      "proxy",
      "settings",
      "connect",
      "azure",
      "apis",
      "through",
      "flows.",
      "since",
      "runs",
      "clouds",
      "require",
      "during",
      "creation",
      "types",
      "won",
      "because",
      "flow",
      "both",
      "same",
      "provider.",
      "create",
      "one",
      "provider",
      "credentials",
      "integration",
      "configure",
      "details",
      "way",
      "default",
      "subscription",
      "portal",
      "user",
      "select",
      "alternate",
      "fulfillment",
      "subscription."
    ],
    "language": "en",
    "word_count": 111,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "instance type doesn't load for ia running on public cloud",
    "contentLower": "when creating a service design in image aggregation (ia), the instance type doesn't load for ia running on the public cloud. cause if your external oo ras is configured on an on-premises environment, the deployment resource providers must be created with proxy settings to connect with azure apis through oo flows. since image aggregation, which runs on public clouds, does not require proxy settings, during design creation, the instance types won’t load, because the oo flow (running on-premises) and ia (running on the public cloud) both are using the same deployment resource provider. workaround create one more deployment resource provider with the same credentials which were used for ia integration and configure proxy details on that deployment resource provider. this way, when creating a design, the ia will use the default provider which was created during integration creation that doesn't have a proxy, and during subscription creation, the service portal user can select the alternate ",
    "keywordsLower": [
      "doesnt",
      "instance",
      "type",
      "doesn",
      "load",
      "ia",
      "running",
      "public",
      "cloud",
      "cause",
      "workaround",
      "creating",
      "service",
      "design",
      "image",
      "aggregation",
      "cloud.",
      "external",
      "oo",
      "ras",
      "configured",
      "on-premises",
      "environment",
      "deployment",
      "resource",
      "providers",
      "created",
      "proxy",
      "settings",
      "connect",
      "azure",
      "apis",
      "through",
      "flows.",
      "since",
      "runs",
      "clouds",
      "require",
      "during",
      "creation",
      "types",
      "won",
      "because",
      "flow",
      "both",
      "same",
      "provider.",
      "create",
      "one",
      "provider",
      "credentials",
      "integration",
      "configure",
      "details",
      "way",
      "default",
      "subscription",
      "portal",
      "user",
      "select",
      "alternate",
      "fulfillment",
      "subscription."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "java.lang.RuntimeException error while viewing status in Deployment Operations",
    "content": "The event details page of Deployment Operations > Instance Management does not display the current state of an event with the following error. java.lang.RuntimeException: Server returned HTTP response code :500 Cause OO RAS is offline. Solution Check if RAS is available on OO server.",
    "url": "javaexceptiondeploymentoperations",
    "filename": "javaexceptiondeploymentoperations",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "java.lang",
      "java.lang.runtimeexception",
      "error",
      "while",
      "viewing",
      "status",
      "deployment",
      "operations",
      "cause",
      "solution",
      "event",
      "details",
      "page",
      "instance",
      "management",
      "display",
      "current",
      "state",
      "following",
      "error.",
      "server",
      "returned",
      "http",
      "response",
      "code",
      "500",
      "oo",
      "ras",
      "offline.",
      "check",
      "available",
      "server."
    ],
    "language": "en",
    "word_count": 37,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "java.lang.runtimeexception error while viewing status in deployment operations",
    "contentLower": "the event details page of deployment operations > instance management does not display the current state of an event with the following error. java.lang.runtimeexception: server returned http response code :500 cause oo ras is offline. solution check if ras is available on oo server.",
    "keywordsLower": [
      "java.lang",
      "java.lang.runtimeexception",
      "error",
      "while",
      "viewing",
      "status",
      "deployment",
      "operations",
      "cause",
      "solution",
      "event",
      "details",
      "page",
      "instance",
      "management",
      "display",
      "current",
      "state",
      "following",
      "error.",
      "server",
      "returned",
      "http",
      "response",
      "code",
      "500",
      "oo",
      "ras",
      "offline.",
      "check",
      "available",
      "server."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Insights aren't collected for Azure",
    "content": "CMP FinOps Insights doesn't collect or display recommendations for Azure, even after a successful billing data collection. When this issue occurs, errors that resemble the following are written to the log: 2023-12-20 08:51:09.474 INFO [System ] [pool-5-thread-15] c.m.h.i.e.i.InsightJobEventProducer : successfully sent insight event to [539826139.insights.insight-job.lifecycle.failed] 2023-12-20 08:51:09.475 WARN [System ] [pool-5-thread-15] c.m.h.i.engine.engine.InsightEngine : insight job failed org.springframework.web.client.HttpClientErrorException$TooManyRequests: 429 Too Many Requests: \"{\"error\":{\"code\":\"429\",\"message\":\"Too many requests. Please Retry after 30 seconds.\"}}\" at org.springframework.web.client.HttpClientErrorException.create(HttpClientErrorException.java:130) at org.springframework.web.client.DefaultResponseErrorHandler.handleError(DefaultResponseErrorHandler.java:183) at org.springframework.web.client.DefaultResponseErrorHandler.handleError(DefaultResponseErrorHandle",
    "url": "azureinsightsnotcollected",
    "filename": "azureinsightsnotcollected",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "arent",
      "ScheduledDirectTask.java",
      "SubscribeTask.run",
      "09.474",
      "InsightEngine.java",
      "AzureSavingsPlanRecommendationInsightRunner.java",
      "RestTemplate.java",
      "rxjava3.core",
      "AzureSavingsPlanRecommendationClient.java",
      "java.lang",
      "Thread.run",
      "springframework.web",
      "FutureTask.java",
      "ObservableFromCallable.java",
      "microfocus.hcm",
      "09.475",
      "ThreadPoolExecutor.java",
      "java.base",
      "java.util",
      "Thread.java",
      "DefaultResponseErrorHandler.java",
      "AzureBenefitRecommendationsApiClient.java",
      "ResponseErrorHandler.java",
      "ObservableSubscribeOn.java",
      "FutureTask.run",
      "Observable.java",
      "Worker.run",
      "ScheduledDirectTask.call",
      "HttpClientErrorException.java",
      "AzureSavingsPlanProviderInsightManager.java",
      "insights",
      "aren",
      "collected",
      "azure",
      "cause",
      "solution",
      "cmp",
      "finops",
      "doesn",
      "collect",
      "display",
      "recommendations",
      "even",
      "after",
      "successful",
      "billing",
      "data",
      "collection.",
      "issue",
      "occurs",
      "errors",
      "resemble",
      "following",
      "written",
      "log",
      "2023-12-20",
      "08",
      "51",
      "info",
      "system",
      "pool-5-thread-15",
      "c.m.h.i.e.i.insightjobeventproducer",
      "successfully",
      "sent",
      "insight",
      "event",
      "539826139.insights.insight-job.lifecycle.failed",
      "warn",
      "c.m.h.i.engine.engine.insightengine",
      "job",
      "failed",
      "org.springframework.web.client.httpclienterrorexception",
      "toomanyrequests",
      "429",
      "too",
      "many",
      "requests",
      "error",
      "code",
      "message",
      "requests.",
      "please",
      "retry",
      "30",
      "seconds.",
      "org.springframework.web.client.httpclienterrorexception.create",
      "130",
      "org.springframework.web.client.defaultresponseerrorhandler.handleerror",
      "183",
      "137",
      "org.springframework.web.client.responseerrorhandler.handleerror",
      "63",
      "org.springframework.web.client.resttemplate.handleresponse",
      "915",
      "org.springframework.web.client.resttemplate.doexecute",
      "864",
      "org.springframework.web.client.resttemplate.execute",
      "764",
      "org.springframework.web.client.resttemplate.exchange",
      "646"
    ],
    "language": "en",
    "word_count": 77,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "insights aren't collected for azure",
    "contentLower": "cmp finops insights doesn't collect or display recommendations for azure, even after a successful billing data collection. when this issue occurs, errors that resemble the following are written to the log: 2023-12-20 08:51:09.474 info [system ] [pool-5-thread-15] c.m.h.i.e.i.insightjobeventproducer : successfully sent insight event to [539826139.insights.insight-job.lifecycle.failed] 2023-12-20 08:51:09.475 warn [system ] [pool-5-thread-15] c.m.h.i.engine.engine.insightengine : insight job failed org.springframework.web.client.httpclienterrorexception$toomanyrequests: 429 too many requests: \"{\"error\":{\"code\":\"429\",\"message\":\"too many requests. please retry after 30 seconds.\"}}\" at org.springframework.web.client.httpclienterrorexception.create(httpclienterrorexception.java:130) at org.springframework.web.client.defaultresponseerrorhandler.handleerror(defaultresponseerrorhandler.java:183) at org.springframework.web.client.defaultresponseerrorhandler.handleerror(defaultresponseerrorhandle",
    "keywordsLower": [
      "arent",
      "scheduleddirecttask.java",
      "subscribetask.run",
      "09.474",
      "insightengine.java",
      "azuresavingsplanrecommendationinsightrunner.java",
      "resttemplate.java",
      "rxjava3.core",
      "azuresavingsplanrecommendationclient.java",
      "java.lang",
      "thread.run",
      "springframework.web",
      "futuretask.java",
      "observablefromcallable.java",
      "microfocus.hcm",
      "09.475",
      "threadpoolexecutor.java",
      "java.base",
      "java.util",
      "thread.java",
      "defaultresponseerrorhandler.java",
      "azurebenefitrecommendationsapiclient.java",
      "responseerrorhandler.java",
      "observablesubscribeon.java",
      "futuretask.run",
      "observable.java",
      "worker.run",
      "scheduleddirecttask.call",
      "httpclienterrorexception.java",
      "azuresavingsplanproviderinsightmanager.java",
      "insights",
      "aren",
      "collected",
      "azure",
      "cause",
      "solution",
      "cmp",
      "finops",
      "doesn",
      "collect",
      "display",
      "recommendations",
      "even",
      "after",
      "successful",
      "billing",
      "data",
      "collection.",
      "issue",
      "occurs",
      "errors",
      "resemble",
      "following",
      "written",
      "log",
      "2023-12-20",
      "08",
      "51",
      "info",
      "system",
      "pool-5-thread-15",
      "c.m.h.i.e.i.insightjobeventproducer",
      "successfully",
      "sent",
      "insight",
      "event",
      "539826139.insights.insight-job.lifecycle.failed",
      "warn",
      "c.m.h.i.engine.engine.insightengine",
      "job",
      "failed",
      "org.springframework.web.client.httpclienterrorexception",
      "toomanyrequests",
      "429",
      "too",
      "many",
      "requests",
      "error",
      "code",
      "message",
      "requests.",
      "please",
      "retry",
      "30",
      "seconds.",
      "org.springframework.web.client.httpclienterrorexception.create",
      "130",
      "org.springframework.web.client.defaultresponseerrorhandler.handleerror",
      "183",
      "137",
      "org.springframework.web.client.responseerrorhandler.handleerror",
      "63",
      "org.springframework.web.client.resttemplate.handleresponse",
      "915",
      "org.springframework.web.client.resttemplate.doexecute",
      "864",
      "org.springframework.web.client.resttemplate.execute",
      "764",
      "org.springframework.web.client.resttemplate.exchange",
      "646"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Internal RAS restarts with an access denied \"shutdownHooks\" exception",
    "content": "You have configured an AWS or Azure cloud cost provider in your Cloud Management environment. You haven't configured an external Remote Action Server (RAS) in this environment, and you're using the internal RAS. In this situation, the internal RAS pod restarts and flow execution may be interrupted when scheduled AWS EC2 resource sync flows (for example, CSA Content Pack/Providers/AWS/Resource Pool Sync/Actions/EC2 Resource Sync v2.0) are running. Additionally, an exception that resembles the following is written to the wrapper.log file of the internal RAS. Note The path to the wrapper.log file resembles the following: <OO_PV_PATH>/oo_ras_logs_vol/ooras/273267286/oo-helm/itom-ooras-273267286-0__itom-ooras-app-273267286___wrapper/wrapper.log INFO   | jvm 1    | 2024/03/17 22:48:27 | SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\". INFO   | jvm 1    | 2024/03/17 22:48:27 | SLF4J: Defaulting to no-operation (NOP) logger implementation INFO   | jvm 1    | 2024/03/17 22:48:27",
    "url": "internalrasrestarts",
    "filename": "internalrasrestarts",
    "headings": [
      "Solution"
    ],
    "keywords": [
      "SecurityManager.java",
      "slf4j.org",
      "http://www.slf4j.org/codes.html#StaticLoggerBinder",
      "v2.0",
      "Log4jContextFactory.java",
      "java.lang",
      "AccessControlContext.java",
      "AccessController.java",
      "log4j.core",
      "NativeConstructorAccessorImpl.java",
      "DelegatingConstructorAccessorImpl.java",
      "Runtime.java",
      "DefaultShutdownCallbackRegistry.java",
      "wrapper.log",
      "codes.html",
      "slf4j.impl",
      "internal",
      "ras",
      "restarts",
      "access",
      "denied",
      "shutdownhooks",
      "exception",
      "solution",
      "configured",
      "aws",
      "azure",
      "cloud",
      "cost",
      "provider",
      "management",
      "environment.",
      "haven",
      "external",
      "remote",
      "action",
      "server",
      "environment",
      "re",
      "ras.",
      "situation",
      "pod",
      "flow",
      "execution",
      "interrupted",
      "scheduled",
      "ec2",
      "resource",
      "sync",
      "flows",
      "example",
      "csa",
      "content",
      "pack",
      "providers",
      "pool",
      "actions",
      "running.",
      "additionally",
      "resembles",
      "following",
      "written",
      "file",
      "note",
      "path",
      "ooras",
      "273267286",
      "oo-helm",
      "info",
      "jvm",
      "2024",
      "03",
      "17",
      "22",
      "48",
      "27",
      "slf4j",
      "failed",
      "load",
      "class",
      "org.slf4j.impl.staticloggerbinder",
      "defaulting",
      "no-operation",
      "nop",
      "logger",
      "implementation",
      "see",
      "http",
      "www.slf4j.org",
      "staticloggerbinder",
      "further",
      "details.",
      "23",
      "26",
      "36",
      "error",
      "statuslogger",
      "catching",
      "java.security.accesscontrolexception",
      "java.lang.runtimepermission"
    ],
    "language": "en",
    "word_count": 116,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "internal ras restarts with an access denied \"shutdownhooks\" exception",
    "contentLower": "you have configured an aws or azure cloud cost provider in your cloud management environment. you haven't configured an external remote action server (ras) in this environment, and you're using the internal ras. in this situation, the internal ras pod restarts and flow execution may be interrupted when scheduled aws ec2 resource sync flows (for example, csa content pack/providers/aws/resource pool sync/actions/ec2 resource sync v2.0) are running. additionally, an exception that resembles the following is written to the wrapper.log file of the internal ras. note the path to the wrapper.log file resembles the following: <oo_pv_path>/oo_ras_logs_vol/ooras/273267286/oo-helm/itom-ooras-273267286-0__itom-ooras-app-273267286___wrapper/wrapper.log info   | jvm 1    | 2024/03/17 22:48:27 | slf4j: failed to load class \"org.slf4j.impl.staticloggerbinder\". info   | jvm 1    | 2024/03/17 22:48:27 | slf4j: defaulting to no-operation (nop) logger implementation info   | jvm 1    | 2024/03/17 22:48:27",
    "keywordsLower": [
      "securitymanager.java",
      "slf4j.org",
      "http://www.slf4j.org/codes.html#staticloggerbinder",
      "v2.0",
      "log4jcontextfactory.java",
      "java.lang",
      "accesscontrolcontext.java",
      "accesscontroller.java",
      "log4j.core",
      "nativeconstructoraccessorimpl.java",
      "delegatingconstructoraccessorimpl.java",
      "runtime.java",
      "defaultshutdowncallbackregistry.java",
      "wrapper.log",
      "codes.html",
      "slf4j.impl",
      "internal",
      "ras",
      "restarts",
      "access",
      "denied",
      "shutdownhooks",
      "exception",
      "solution",
      "configured",
      "aws",
      "azure",
      "cloud",
      "cost",
      "provider",
      "management",
      "environment.",
      "haven",
      "external",
      "remote",
      "action",
      "server",
      "environment",
      "re",
      "ras.",
      "situation",
      "pod",
      "flow",
      "execution",
      "interrupted",
      "scheduled",
      "ec2",
      "resource",
      "sync",
      "flows",
      "example",
      "csa",
      "content",
      "pack",
      "providers",
      "pool",
      "actions",
      "running.",
      "additionally",
      "resembles",
      "following",
      "written",
      "file",
      "note",
      "path",
      "ooras",
      "273267286",
      "oo-helm",
      "info",
      "jvm",
      "2024",
      "03",
      "17",
      "22",
      "48",
      "27",
      "slf4j",
      "failed",
      "load",
      "class",
      "org.slf4j.impl.staticloggerbinder",
      "defaulting",
      "no-operation",
      "nop",
      "logger",
      "implementation",
      "see",
      "http",
      "www.slf4j.org",
      "staticloggerbinder",
      "further",
      "details.",
      "23",
      "26",
      "36",
      "error",
      "statuslogger",
      "catching",
      "java.security.accesscontrolexception",
      "java.lang.runtimepermission"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "IDOL search may have performance issues because of the unstable content server",
    "content": "IDOL search may have the following performance issues when enabling the access filter in DashboardDefinition. In the result returned by the kubectl get po -n itsma-xxx command, the status of the saw-dah is 1/2. The saw-dah pod auto-restarts occasionally. The search function has poor performance.  For example in the action.log file, you could find many queries that took more than 10 seconds to finish. In the action.log file of saw-dah, more than 10% of the queries consist of RLAC-OWNED-BY-GROUP,RLAC-OWNED-BY-PERSON,LAST-UPDATED-BY-PERSON,RLAC-CREATED-BY-PERSON. But since the query line in the action.log file is encoded in URL format, you can use RLAC%2DOWNED%2DBY%2DPERSON or similar pattern to search the file for verification. In the application log of DAH, you will find that the status of the content server is very unstable and it affects the DAH server. Many logs can be found as shown below, indicating poor communication between DAH and the content server: 06/09/2023 11:56:50 [10] 70-",
    "url": "idolsearchissues",
    "filename": "idolsearchissues",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "1.93",
      "action.log",
      "content.cfg",
      "https://localhost:1444/DREREGENERATE?FieldProcessingSection=SetMatchFields",
      "idol",
      "search",
      "performance",
      "issues",
      "because",
      "unstable",
      "content",
      "server",
      "cause",
      "solution",
      "following",
      "enabling",
      "access",
      "filter",
      "dashboarddefinition.",
      "result",
      "returned",
      "kubectl",
      "get",
      "po",
      "-n",
      "itsma-xxx",
      "command",
      "status",
      "saw-dah",
      "2.",
      "pod",
      "auto-restarts",
      "occasionally.",
      "function",
      "poor",
      "performance.",
      "example",
      "file",
      "find",
      "many",
      "queries",
      "took",
      "10",
      "seconds",
      "finish.",
      "consist",
      "rlac-owned-by-group",
      "rlac-owned-by-person",
      "last-updated-by-person",
      "rlac-created-by-person.",
      "since",
      "query",
      "line",
      "encoded",
      "url",
      "format",
      "rlac",
      "2downed",
      "2dby",
      "2dperson",
      "similar",
      "pattern",
      "verification.",
      "application",
      "log",
      "dah",
      "very",
      "affects",
      "server.",
      "logs",
      "found",
      "shown",
      "below",
      "indicating",
      "communication",
      "between",
      "06",
      "09",
      "2023",
      "11",
      "56",
      "50",
      "70-error",
      "error",
      "connect",
      "engine",
      "1.",
      "58",
      "14",
      "ssl",
      "handshake",
      "failed",
      "sslaccept",
      "-1",
      "nil",
      "20",
      "12",
      "08",
      "00",
      "50-warning"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "idol search may have performance issues because of the unstable content server",
    "contentLower": "idol search may have the following performance issues when enabling the access filter in dashboarddefinition. in the result returned by the kubectl get po -n itsma-xxx command, the status of the saw-dah is 1/2. the saw-dah pod auto-restarts occasionally. the search function has poor performance.  for example in the action.log file, you could find many queries that took more than 10 seconds to finish. in the action.log file of saw-dah, more than 10% of the queries consist of rlac-owned-by-group,rlac-owned-by-person,last-updated-by-person,rlac-created-by-person. but since the query line in the action.log file is encoded in url format, you can use rlac%2downed%2dby%2dperson or similar pattern to search the file for verification. in the application log of dah, you will find that the status of the content server is very unstable and it affects the dah server. many logs can be found as shown below, indicating poor communication between dah and the content server: 06/09/2023 11:56:50 [10] 70-",
    "keywordsLower": [
      "1.93",
      "action.log",
      "content.cfg",
      "https://localhost:1444/dreregenerate?fieldprocessingsection=setmatchfields",
      "idol",
      "search",
      "performance",
      "issues",
      "because",
      "unstable",
      "content",
      "server",
      "cause",
      "solution",
      "following",
      "enabling",
      "access",
      "filter",
      "dashboarddefinition.",
      "result",
      "returned",
      "kubectl",
      "get",
      "po",
      "-n",
      "itsma-xxx",
      "command",
      "status",
      "saw-dah",
      "2.",
      "pod",
      "auto-restarts",
      "occasionally.",
      "function",
      "poor",
      "performance.",
      "example",
      "file",
      "find",
      "many",
      "queries",
      "took",
      "10",
      "seconds",
      "finish.",
      "consist",
      "rlac-owned-by-group",
      "rlac-owned-by-person",
      "last-updated-by-person",
      "rlac-created-by-person.",
      "since",
      "query",
      "line",
      "encoded",
      "url",
      "format",
      "rlac",
      "2downed",
      "2dby",
      "2dperson",
      "similar",
      "pattern",
      "verification.",
      "application",
      "log",
      "dah",
      "very",
      "affects",
      "server.",
      "logs",
      "found",
      "shown",
      "below",
      "indicating",
      "communication",
      "between",
      "06",
      "09",
      "2023",
      "11",
      "56",
      "50",
      "70-error",
      "error",
      "connect",
      "engine",
      "1.",
      "58",
      "14",
      "ssl",
      "handshake",
      "failed",
      "sslaccept",
      "-1",
      "nil",
      "20",
      "12",
      "08",
      "00",
      "50-warning"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "IDOL search may have performance issues when enabling the access filter in DashboardDefinition",
    "content": "IDOL search may have the following performance issues when enabling the access filter in DashboardDefinition. In the result returned by the kubectl get po -n itsma-xxx command, the status of the saw-dah is 1/2. The saw-dah pod auto-restarts occasionally. The search function has poor performance. In the action.log file of saw-dah, more than 10% of the queries consist of RLAC-OWNED-BY-GROUP,RLAC-OWNED-BY-PERSON,LAST-UPDATED-BY-PERSON,RLAC-CREATED-BY-PERSON. Cause This issue occurs because fields with incorrectly defined indexes are used in specific search conditions. Solution Perform the following steps as a workaround. Update the content.cfg file as follows on all saw-con and sawarc-con pods (including saw-con-a and sawarc-con-a). Search for [SetMatchFields]. Search for the PropertyFieldCSVs definition that is closest to the search result in step a. Note that this definition must be within the [SetMatchFields] section. Append the following code to the end of the line that begins with Pr",
    "url": "idolperformissues",
    "filename": "idolperformissues",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "action.log",
      "content.cfg",
      "https://localhost:1444/DREREGENERATE?FieldProcessingSection=SetMatchFields",
      "idol",
      "search",
      "performance",
      "issues",
      "enabling",
      "access",
      "filter",
      "dashboarddefinition",
      "cause",
      "solution",
      "following",
      "dashboarddefinition.",
      "result",
      "returned",
      "kubectl",
      "get",
      "po",
      "-n",
      "itsma-xxx",
      "command",
      "status",
      "saw-dah",
      "2.",
      "pod",
      "auto-restarts",
      "occasionally.",
      "function",
      "poor",
      "performance.",
      "file",
      "10",
      "queries",
      "consist",
      "rlac-owned-by-group",
      "rlac-owned-by-person",
      "last-updated-by-person",
      "rlac-created-by-person.",
      "issue",
      "occurs",
      "because",
      "fields",
      "incorrectly",
      "defined",
      "indexes",
      "specific",
      "conditions.",
      "perform",
      "steps",
      "workaround.",
      "update",
      "follows",
      "all",
      "saw-con",
      "sawarc-con",
      "pods",
      "including",
      "saw-con-a",
      "sawarc-con-a",
      "setmatchfields",
      "propertyfieldcsvs",
      "definition",
      "closest",
      "step",
      "a.",
      "note",
      "section.",
      "append",
      "code",
      "end",
      "line",
      "begins",
      "propertyfieldcsvs.",
      "rlac-created-by-person",
      "see",
      "screenshot",
      "example",
      "restart",
      "run",
      "exec",
      "curl",
      "-k",
      "https",
      "localhost",
      "1444",
      "dreregenerate",
      "fieldprocessingsection",
      "depending",
      "number",
      "documents",
      "process",
      "regenerates",
      "index",
      "take",
      "few",
      "seconds",
      "minutes."
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "idol search may have performance issues when enabling the access filter in dashboarddefinition",
    "contentLower": "idol search may have the following performance issues when enabling the access filter in dashboarddefinition. in the result returned by the kubectl get po -n itsma-xxx command, the status of the saw-dah is 1/2. the saw-dah pod auto-restarts occasionally. the search function has poor performance. in the action.log file of saw-dah, more than 10% of the queries consist of rlac-owned-by-group,rlac-owned-by-person,last-updated-by-person,rlac-created-by-person. cause this issue occurs because fields with incorrectly defined indexes are used in specific search conditions. solution perform the following steps as a workaround. update the content.cfg file as follows on all saw-con and sawarc-con pods (including saw-con-a and sawarc-con-a). search for [setmatchfields]. search for the propertyfieldcsvs definition that is closest to the search result in step a. note that this definition must be within the [setmatchfields] section. append the following code to the end of the line that begins with pr",
    "keywordsLower": [
      "action.log",
      "content.cfg",
      "https://localhost:1444/dreregenerate?fieldprocessingsection=setmatchfields",
      "idol",
      "search",
      "performance",
      "issues",
      "enabling",
      "access",
      "filter",
      "dashboarddefinition",
      "cause",
      "solution",
      "following",
      "dashboarddefinition.",
      "result",
      "returned",
      "kubectl",
      "get",
      "po",
      "-n",
      "itsma-xxx",
      "command",
      "status",
      "saw-dah",
      "2.",
      "pod",
      "auto-restarts",
      "occasionally.",
      "function",
      "poor",
      "performance.",
      "file",
      "10",
      "queries",
      "consist",
      "rlac-owned-by-group",
      "rlac-owned-by-person",
      "last-updated-by-person",
      "rlac-created-by-person.",
      "issue",
      "occurs",
      "because",
      "fields",
      "incorrectly",
      "defined",
      "indexes",
      "specific",
      "conditions.",
      "perform",
      "steps",
      "workaround.",
      "update",
      "follows",
      "all",
      "saw-con",
      "sawarc-con",
      "pods",
      "including",
      "saw-con-a",
      "sawarc-con-a",
      "setmatchfields",
      "propertyfieldcsvs",
      "definition",
      "closest",
      "step",
      "a.",
      "note",
      "section.",
      "append",
      "code",
      "end",
      "line",
      "begins",
      "propertyfieldcsvs.",
      "rlac-created-by-person",
      "see",
      "screenshot",
      "example",
      "restart",
      "run",
      "exec",
      "curl",
      "-k",
      "https",
      "localhost",
      "1444",
      "dreregenerate",
      "fieldprocessingsection",
      "depending",
      "number",
      "documents",
      "process",
      "regenerates",
      "index",
      "take",
      "few",
      "seconds",
      "minutes."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "IDOL data corrupts",
    "content": "If Smart Analytics fails or doesn't work as expected, we suggest that you check the statuses of content pods to verify whether IDOL data is corrupted. Cause A possible cause of this issue is that IDOL is shut down forcibly or unexpectedly. Solution Log in to a control plane node of the suite and then run the kubectl get pods --all-namespaces | grep smarta | grep con command to display a list of the IDOL content pods. For example, NAMESPACE NAME READY STATUS RESTARTS AGE itsma1 smarta-saw-con-0 1/1 Running 0 1d itsma1 smarta-saw-con-1 1/1 Running 0 1d itsma1 smarta-sawarc-con-0 0/1 Running 12 2h itsma1 smarta-sawarc-con-1 1/1 Running 0 1d itsma1 smarta-sawmeta-con-0 1/1 Running 0 1d itsma1 smarta-sawmeta-con-1 1/1 Running 0 1d ... ... ... ... ... ... Take smarta-sawarc-con-0 in the above table as an example. The READY column displays 0/1. The RESTARTS column displays 12, which indicates that the system has tried to restart this pod for many times. In this case, the pod is most likely in",
    "url": "smartatsdatacorrupt",
    "filename": "smartatsdatacorrupt",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "idol",
      "data",
      "corrupts",
      "cause",
      "solution",
      "smart",
      "analytics",
      "fails",
      "doesn",
      "work",
      "expected",
      "suggest",
      "check",
      "statuses",
      "content",
      "pods",
      "verify",
      "whether",
      "corrupted.",
      "possible",
      "issue",
      "shut",
      "forcibly",
      "unexpectedly.",
      "log",
      "control",
      "plane",
      "node",
      "suite",
      "run",
      "kubectl",
      "get",
      "--all-namespaces",
      "grep",
      "smarta",
      "con",
      "command",
      "display",
      "list",
      "pods.",
      "example",
      "namespace",
      "name",
      "ready",
      "status",
      "restarts",
      "age",
      "itsma1",
      "smarta-saw-con-0",
      "running",
      "1d",
      "smarta-saw-con-1",
      "smarta-sawarc-con-0",
      "12",
      "2h",
      "smarta-sawarc-con-1",
      "smarta-sawmeta-con-0",
      "smarta-sawmeta-con-1",
      "take",
      "above",
      "table",
      "example.",
      "column",
      "displays",
      "1.",
      "indicates",
      "system",
      "tried",
      "restart",
      "pod",
      "many",
      "times.",
      "case",
      "most",
      "likely",
      "corruption.",
      "follow",
      "following",
      "steps",
      "recover",
      "encounter",
      "same",
      "mentioned",
      "above.",
      "otherwise",
      "need",
      "find",
      "another",
      "solution.",
      "warning",
      "procedure",
      "delete",
      "all",
      "files",
      "crushed",
      "reindexing.",
      "statefulset",
      "number",
      "under",
      "it."
    ],
    "language": "en",
    "word_count": 129,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "idol data corrupts",
    "contentLower": "if smart analytics fails or doesn't work as expected, we suggest that you check the statuses of content pods to verify whether idol data is corrupted. cause a possible cause of this issue is that idol is shut down forcibly or unexpectedly. solution log in to a control plane node of the suite and then run the kubectl get pods --all-namespaces | grep smarta | grep con command to display a list of the idol content pods. for example, namespace name ready status restarts age itsma1 smarta-saw-con-0 1/1 running 0 1d itsma1 smarta-saw-con-1 1/1 running 0 1d itsma1 smarta-sawarc-con-0 0/1 running 12 2h itsma1 smarta-sawarc-con-1 1/1 running 0 1d itsma1 smarta-sawmeta-con-0 1/1 running 0 1d itsma1 smarta-sawmeta-con-1 1/1 running 0 1d ... ... ... ... ... ... take smarta-sawarc-con-0 in the above table as an example. the ready column displays 0/1. the restarts column displays 12, which indicates that the system has tried to restart this pod for many times. in this case, the pod is most likely in",
    "keywordsLower": [
      "idol",
      "data",
      "corrupts",
      "cause",
      "solution",
      "smart",
      "analytics",
      "fails",
      "doesn",
      "work",
      "expected",
      "suggest",
      "check",
      "statuses",
      "content",
      "pods",
      "verify",
      "whether",
      "corrupted.",
      "possible",
      "issue",
      "shut",
      "forcibly",
      "unexpectedly.",
      "log",
      "control",
      "plane",
      "node",
      "suite",
      "run",
      "kubectl",
      "get",
      "--all-namespaces",
      "grep",
      "smarta",
      "con",
      "command",
      "display",
      "list",
      "pods.",
      "example",
      "namespace",
      "name",
      "ready",
      "status",
      "restarts",
      "age",
      "itsma1",
      "smarta-saw-con-0",
      "running",
      "1d",
      "smarta-saw-con-1",
      "smarta-sawarc-con-0",
      "12",
      "2h",
      "smarta-sawarc-con-1",
      "smarta-sawmeta-con-0",
      "smarta-sawmeta-con-1",
      "take",
      "above",
      "table",
      "example.",
      "column",
      "displays",
      "1.",
      "indicates",
      "system",
      "tried",
      "restart",
      "pod",
      "many",
      "times.",
      "case",
      "most",
      "likely",
      "corruption.",
      "follow",
      "following",
      "steps",
      "recover",
      "encounter",
      "same",
      "mentioned",
      "above.",
      "otherwise",
      "need",
      "find",
      "another",
      "solution.",
      "warning",
      "procedure",
      "delete",
      "all",
      "files",
      "crushed",
      "reindexing.",
      "statefulset",
      "number",
      "under",
      "it."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "IDOL indexing failed due to insufficient disk space",
    "content": "IDOL indexing failed with the following error: Paused (out of disk space) Cause Smart Analytics consumes too much disk space due to big data volume. In Suite Administration (https://<EXTERNAL_ACCESS_HOST>/bo), if you navigate to Configurations > Smart Analytics > GO TO SMART ANALYTICS and double-click a content server in the Smart Analytics Components list, you'll find that the number of committed documents is more than 20% higher than the number of documents. For example: Solution For suites deployed on the cloud, you need to expand the persistent volume (PV) and then perform an IDOL index compaction. Due to the big data volume on XService Content/XService Archive Content, be sure to perform the following procedures when the system has less traffic. Expand PV Expand the PV size by following these steps: Get the persistent volume claims (PVCs): kubectl get pvc -n <itsma-namespace> Expand the PVC on each pod where this error occurs. For example: kubectl edit pvc -n itsma-xxxx  itsma-sma",
    "url": "indexfailed",
    "filename": "indexfailed",
    "headings": [
      "Cause",
      "Solution",
      "Expand PV",
      "Perform an IDOL index compaction",
      "Manual IDOL index compaction",
      "Scheduled IDOL index compaction"
    ],
    "keywords": [
      "https://smarta-saw-dih-svc:1444/DRECOMPACT?backup=false&noarchive=true&Priority=100",
      "https://smarta-sawarc-dih-svc:1444/DRECOMPACT?backup=false&noarchive=true&Priority=100",
      "https://<EXTERNAL_ACCESS_HOST>/bo",
      "idol",
      "indexing",
      "failed",
      "due",
      "insufficient",
      "disk",
      "space",
      "cause",
      "solution",
      "expand",
      "pv",
      "perform",
      "index",
      "compaction",
      "manual",
      "scheduled",
      "following",
      "error",
      "paused",
      "out",
      "smart",
      "analytics",
      "consumes",
      "too",
      "much",
      "big",
      "data",
      "volume.",
      "suite",
      "administration",
      "https",
      "bo",
      "navigate",
      "configurations",
      "go",
      "double-click",
      "content",
      "server",
      "components",
      "list",
      "ll",
      "find",
      "number",
      "committed",
      "documents",
      "20",
      "higher",
      "documents.",
      "example",
      "suites",
      "deployed",
      "cloud",
      "need",
      "persistent",
      "volume",
      "compaction.",
      "xservice",
      "archive",
      "sure",
      "procedures",
      "system",
      "less",
      "traffic.",
      "size",
      "steps",
      "get",
      "claims",
      "pvcs",
      "kubectl",
      "pvc",
      "-n",
      "pod",
      "occurs.",
      "edit",
      "itsma-xxxx",
      "itsma-smarta-saw-con-0",
      "double",
      "current",
      "storage",
      "size.",
      "spec",
      "accessmodes",
      "readwriteonce",
      "resources",
      "requests",
      "either",
      "manually",
      "schedule",
      "periodically",
      "clean",
      "optimize",
      "indexes.",
      "log",
      "administration.",
      "click",
      "analytics.",
      "assistant"
    ],
    "language": "en",
    "word_count": 113,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "idol indexing failed due to insufficient disk space",
    "contentLower": "idol indexing failed with the following error: paused (out of disk space) cause smart analytics consumes too much disk space due to big data volume. in suite administration (https://<external_access_host>/bo), if you navigate to configurations > smart analytics > go to smart analytics and double-click a content server in the smart analytics components list, you'll find that the number of committed documents is more than 20% higher than the number of documents. for example: solution for suites deployed on the cloud, you need to expand the persistent volume (pv) and then perform an idol index compaction. due to the big data volume on xservice content/xservice archive content, be sure to perform the following procedures when the system has less traffic. expand pv expand the pv size by following these steps: get the persistent volume claims (pvcs): kubectl get pvc -n <itsma-namespace> expand the pvc on each pod where this error occurs. for example: kubectl edit pvc -n itsma-xxxx  itsma-sma",
    "keywordsLower": [
      "https://smarta-saw-dih-svc:1444/drecompact?backup=false&noarchive=true&priority=100",
      "https://smarta-sawarc-dih-svc:1444/drecompact?backup=false&noarchive=true&priority=100",
      "https://<external_access_host>/bo",
      "idol",
      "indexing",
      "failed",
      "due",
      "insufficient",
      "disk",
      "space",
      "cause",
      "solution",
      "expand",
      "pv",
      "perform",
      "index",
      "compaction",
      "manual",
      "scheduled",
      "following",
      "error",
      "paused",
      "out",
      "smart",
      "analytics",
      "consumes",
      "too",
      "much",
      "big",
      "data",
      "volume.",
      "suite",
      "administration",
      "https",
      "bo",
      "navigate",
      "configurations",
      "go",
      "double-click",
      "content",
      "server",
      "components",
      "list",
      "ll",
      "find",
      "number",
      "committed",
      "documents",
      "20",
      "higher",
      "documents.",
      "example",
      "suites",
      "deployed",
      "cloud",
      "need",
      "persistent",
      "volume",
      "compaction.",
      "xservice",
      "archive",
      "sure",
      "procedures",
      "system",
      "less",
      "traffic.",
      "size",
      "steps",
      "get",
      "claims",
      "pvcs",
      "kubectl",
      "pvc",
      "-n",
      "pod",
      "occurs.",
      "edit",
      "itsma-xxxx",
      "itsma-smarta-saw-con-0",
      "double",
      "current",
      "storage",
      "size.",
      "spec",
      "accessmodes",
      "readwriteonce",
      "resources",
      "requests",
      "either",
      "manually",
      "schedule",
      "periodically",
      "clean",
      "optimize",
      "indexes.",
      "log",
      "administration.",
      "click",
      "analytics.",
      "assistant"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Import tenant fails",
    "content": "The tenant import fails, and you can find \"Calling migration user API failed.\" in the Operation history. Cause This issue happens due to the API call timeout. Follow these steps to determine the cause: Log in to a control plane node or the bastion node as root or a sudo user, and run the following command to get the pod name of xruntime-gateway: kubectl get pods -n <namespace> | grep itom-xruntime-gateway Open the log file localhost_access_log.0.txt in <suite global NFS volume>/logs/xservices/gateway/<xruntimegateway pod name>, check the job status and duration. If the value of status is 500 and the duration is larger than 18000, open the log file gateway_error.log in <suite global NFS volume>/logs/xservices/gateway/<xruntimegateway pod name>/mass, you can find \"Read timed out\" in the error. Here is a log example: 05-20-2021 18:04:36.551 ip=xx.xxx.xxx.xxx status=500 tenant=50xxxxxxx user=xxxxx@xxx.com correlationId=RID-f617e029-11a5-4c02-8871-d2c8f3cd56ae request=\"POST /rest/508284964/",
    "url": "satsimporttenantfails",
    "filename": "satsimporttenantfails",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "gateway_error.log",
      "xxx.com",
      "xxx.xxx",
      "xx.xxx",
      "1.0",
      "nio-8443",
      "0.txt",
      "exec-37",
      "36.551",
      "import",
      "tenant",
      "fails",
      "cause",
      "solution",
      "find",
      "calling",
      "migration",
      "user",
      "api",
      "failed.",
      "operation",
      "history.",
      "issue",
      "happens",
      "due",
      "call",
      "timeout.",
      "follow",
      "steps",
      "determine",
      "log",
      "control",
      "plane",
      "node",
      "bastion",
      "root",
      "sudo",
      "run",
      "following",
      "command",
      "get",
      "pod",
      "name",
      "xruntime-gateway",
      "kubectl",
      "pods",
      "-n",
      "grep",
      "itom-xruntime-gateway",
      "open",
      "file",
      "logs",
      "xservices",
      "gateway",
      "check",
      "job",
      "status",
      "duration.",
      "value",
      "500",
      "duration",
      "larger",
      "18000",
      "mass",
      "read",
      "timed",
      "out",
      "error.",
      "here",
      "example",
      "05-20-2021",
      "18",
      "04",
      "ip",
      "xx.xxx.xxx.xxx",
      "50xxxxxxx",
      "xxxxx",
      "correlationid",
      "rid-f617e029-11a5-4c02-8871-d2c8f3cd56ae",
      "request",
      "post",
      "rest",
      "508284964",
      "ums",
      "migrateusers",
      "http",
      "usertype",
      "unknown",
      "requestinfo",
      "182089",
      "bytes",
      "106",
      "thread",
      "https-jsse-nio-8443-exec-37",
      "service",
      "itemtype",
      "itemoperation",
      "n-a",
      "datalayer",
      "incomingitemcnt"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "import tenant fails",
    "contentLower": "the tenant import fails, and you can find \"calling migration user api failed.\" in the operation history. cause this issue happens due to the api call timeout. follow these steps to determine the cause: log in to a control plane node or the bastion node as root or a sudo user, and run the following command to get the pod name of xruntime-gateway: kubectl get pods -n <namespace> | grep itom-xruntime-gateway open the log file localhost_access_log.0.txt in <suite global nfs volume>/logs/xservices/gateway/<xruntimegateway pod name>, check the job status and duration. if the value of status is 500 and the duration is larger than 18000, open the log file gateway_error.log in <suite global nfs volume>/logs/xservices/gateway/<xruntimegateway pod name>/mass, you can find \"read timed out\" in the error. here is a log example: 05-20-2021 18:04:36.551 ip=xx.xxx.xxx.xxx status=500 tenant=50xxxxxxx user=xxxxx@xxx.com correlationid=rid-f617e029-11a5-4c02-8871-d2c8f3cd56ae request=\"post /rest/508284964/",
    "keywordsLower": [
      "gateway_error.log",
      "xxx.com",
      "xxx.xxx",
      "xx.xxx",
      "1.0",
      "nio-8443",
      "0.txt",
      "exec-37",
      "36.551",
      "import",
      "tenant",
      "fails",
      "cause",
      "solution",
      "find",
      "calling",
      "migration",
      "user",
      "api",
      "failed.",
      "operation",
      "history.",
      "issue",
      "happens",
      "due",
      "call",
      "timeout.",
      "follow",
      "steps",
      "determine",
      "log",
      "control",
      "plane",
      "node",
      "bastion",
      "root",
      "sudo",
      "run",
      "following",
      "command",
      "get",
      "pod",
      "name",
      "xruntime-gateway",
      "kubectl",
      "pods",
      "-n",
      "grep",
      "itom-xruntime-gateway",
      "open",
      "file",
      "logs",
      "xservices",
      "gateway",
      "check",
      "job",
      "status",
      "duration.",
      "value",
      "500",
      "duration",
      "larger",
      "18000",
      "mass",
      "read",
      "timed",
      "out",
      "error.",
      "here",
      "example",
      "05-20-2021",
      "18",
      "04",
      "ip",
      "xx.xxx.xxx.xxx",
      "50xxxxxxx",
      "xxxxx",
      "correlationid",
      "rid-f617e029-11a5-4c02-8871-d2c8f3cd56ae",
      "request",
      "post",
      "rest",
      "508284964",
      "ums",
      "migrateusers",
      "http",
      "usertype",
      "unknown",
      "requestinfo",
      "182089",
      "bytes",
      "106",
      "thread",
      "https-jsse-nio-8443-exec-37",
      "service",
      "itemtype",
      "itemoperation",
      "n-a",
      "datalayer",
      "incomingitemcnt"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration user password expires",
    "content": "The system indicates that the integration user password expires. Cause The integration user password expires and has to be reset. Solution Contact the suite admin user to reset the password for this user in Suite Administration. The system will then send an email to the configured email address of this user to set a new password.",
    "url": "satsintgruserpwexpired",
    "filename": "satsintgruserpwexpired",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "integration",
      "user",
      "password",
      "expires",
      "cause",
      "solution",
      "system",
      "indicates",
      "expires.",
      "reset.",
      "contact",
      "suite",
      "admin",
      "reset",
      "administration.",
      "send",
      "email",
      "configured",
      "address",
      "set",
      "new",
      "password."
    ],
    "language": "en",
    "word_count": 36,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration user password expires",
    "contentLower": "the system indicates that the integration user password expires. cause the integration user password expires and has to be reset. solution contact the suite admin user to reset the password for this user in suite administration. the system will then send an email to the configured email address of this user to set a new password.",
    "keywordsLower": [
      "integration",
      "user",
      "password",
      "expires",
      "cause",
      "solution",
      "system",
      "indicates",
      "expires.",
      "reset.",
      "contact",
      "suite",
      "admin",
      "reset",
      "administration.",
      "send",
      "email",
      "configured",
      "address",
      "set",
      "new",
      "password."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "License file uploading fails with error 401",
    "content": "When you attempt to upload a license file in Suite Administration, the following error occurs: License file uploading failed: 401 Cause The idm signing keys on the OMT and SMA sides are out of sync. For example, you have changed the value only on one side (OMT or SMA). As a result, you can't access the AutoPass License Manager (APLM) due to a token validation issue and the uploading of the license file then fails. Solution Perform the following steps on a control plane node to manually synchronize the idm signing keys: Run the command below to get the APLM pod name: kubectl get pods -n <suite namespace> | grep autopass Where: <suite namespace> is your suite namespace, for example, itsma-yd5az. Run the command below to enter the APLM pod: kubectl exec -it <APLM pod name> -n <suite namespace> -c autopass-lm bash Run the command below to get the secret value in OMT: get_secret VAULT_SIGNING_KEY baseinfra core The command should return the following result. Make a note of <value in core>. ",
    "url": "failtouploadlicense",
    "filename": "failtouploadlicense",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "license",
      "file",
      "uploading",
      "fails",
      "error",
      "401",
      "cause",
      "solution",
      "attempt",
      "upload",
      "suite",
      "administration",
      "following",
      "occurs",
      "failed",
      "idm",
      "signing",
      "keys",
      "omt",
      "sma",
      "sides",
      "out",
      "sync.",
      "example",
      "changed",
      "value",
      "one",
      "side",
      "result",
      "access",
      "autopass",
      "manager",
      "aplm",
      "due",
      "token",
      "validation",
      "issue",
      "fails.",
      "perform",
      "steps",
      "control",
      "plane",
      "node",
      "manually",
      "synchronize",
      "run",
      "command",
      "below",
      "get",
      "pod",
      "name",
      "kubectl",
      "pods",
      "-n",
      "grep",
      "namespace",
      "itsma-yd5az.",
      "enter",
      "exec",
      "-it",
      "-c",
      "autopass-lm",
      "bash",
      "secret",
      "baseinfra",
      "core",
      "return",
      "result.",
      "make",
      "note",
      "pass",
      "different",
      "update",
      "message",
      "displayed",
      "successfully",
      "updated.",
      "log",
      "again.",
      "able",
      "successfully."
    ],
    "language": "en",
    "word_count": 110,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "license file uploading fails with error 401",
    "contentLower": "when you attempt to upload a license file in suite administration, the following error occurs: license file uploading failed: 401 cause the idm signing keys on the omt and sma sides are out of sync. for example, you have changed the value only on one side (omt or sma). as a result, you can't access the autopass license manager (aplm) due to a token validation issue and the uploading of the license file then fails. solution perform the following steps on a control plane node to manually synchronize the idm signing keys: run the command below to get the aplm pod name: kubectl get pods -n <suite namespace> | grep autopass where: <suite namespace> is your suite namespace, for example, itsma-yd5az. run the command below to enter the aplm pod: kubectl exec -it <aplm pod name> -n <suite namespace> -c autopass-lm bash run the command below to get the secret value in omt: get_secret vault_signing_key baseinfra core the command should return the following result. make a note of <value in core>. ",
    "keywordsLower": [
      "license",
      "file",
      "uploading",
      "fails",
      "error",
      "401",
      "cause",
      "solution",
      "attempt",
      "upload",
      "suite",
      "administration",
      "following",
      "occurs",
      "failed",
      "idm",
      "signing",
      "keys",
      "omt",
      "sma",
      "sides",
      "out",
      "sync.",
      "example",
      "changed",
      "value",
      "one",
      "side",
      "result",
      "access",
      "autopass",
      "manager",
      "aplm",
      "due",
      "token",
      "validation",
      "issue",
      "fails.",
      "perform",
      "steps",
      "control",
      "plane",
      "node",
      "manually",
      "synchronize",
      "run",
      "command",
      "below",
      "get",
      "pod",
      "name",
      "kubectl",
      "pods",
      "-n",
      "grep",
      "namespace",
      "itsma-yd5az.",
      "enter",
      "exec",
      "-it",
      "-c",
      "autopass-lm",
      "bash",
      "secret",
      "baseinfra",
      "core",
      "return",
      "result.",
      "make",
      "note",
      "pass",
      "different",
      "update",
      "message",
      "displayed",
      "successfully",
      "updated.",
      "log",
      "again.",
      "able",
      "successfully."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Invalid input for UD/UCMDB",
    "content": "In the Native SACM scenario, when you try to save a Device record in Service Management, the following error occurs: Invalid input for UD/UCMDB. Cause The record has a field of the Double data type, and the system maps this field to a CMS field of the Long Integer type. If you enter a floating point number (such as 12345.66) in the SMAX field, the value is invalid on the UD/UCMDB side and therefore the error occurs. The following screenshots show an example mapping of such a SMAX field and UD/UCMDB field. SMAX field CMS field Solution Enter an integer (for example, 12345) in the SMAX field and then save the Device record.",
    "url": "invalidinput4cms",
    "filename": "invalidinput4cms",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "uducmdb",
      "12345.66",
      "invalid",
      "input",
      "ud",
      "ucmdb",
      "cause",
      "solution",
      "native",
      "sacm",
      "scenario",
      "try",
      "save",
      "device",
      "record",
      "service",
      "management",
      "following",
      "error",
      "occurs",
      "ucmdb.",
      "field",
      "double",
      "data",
      "type",
      "system",
      "maps",
      "cms",
      "long",
      "integer",
      "type.",
      "enter",
      "floating",
      "point",
      "number",
      "such",
      "smax",
      "value",
      "side",
      "therefore",
      "occurs.",
      "screenshots",
      "show",
      "example",
      "mapping",
      "field.",
      "12345",
      "record."
    ],
    "language": "en",
    "word_count": 75,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "invalid input for ud/ucmdb",
    "contentLower": "in the native sacm scenario, when you try to save a device record in service management, the following error occurs: invalid input for ud/ucmdb. cause the record has a field of the double data type, and the system maps this field to a cms field of the long integer type. if you enter a floating point number (such as 12345.66) in the smax field, the value is invalid on the ud/ucmdb side and therefore the error occurs. the following screenshots show an example mapping of such a smax field and ud/ucmdb field. smax field cms field solution enter an integer (for example, 12345) in the smax field and then save the device record.",
    "keywordsLower": [
      "uducmdb",
      "12345.66",
      "invalid",
      "input",
      "ud",
      "ucmdb",
      "cause",
      "solution",
      "native",
      "sacm",
      "scenario",
      "try",
      "save",
      "device",
      "record",
      "service",
      "management",
      "following",
      "error",
      "occurs",
      "ucmdb.",
      "field",
      "double",
      "data",
      "type",
      "system",
      "maps",
      "cms",
      "long",
      "integer",
      "type.",
      "enter",
      "floating",
      "point",
      "number",
      "such",
      "smax",
      "value",
      "side",
      "therefore",
      "occurs.",
      "screenshots",
      "show",
      "example",
      "mapping",
      "field.",
      "12345",
      "record."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Integration user's password has expired",
    "content": "UCMDB CIs are not syncing to Service Management, but Service Management CIs are syncing to UCMDB. Cause The issue occurs because the Service Management integration user's password has expired, preventing a successful connection to UCMDB. Solution Prerequisites Ensure you have the suite-admin user's password. You will need it to log in to the IdM Admin Portal and BO.Make sure the following password requirements are met: Must be between 10 to 16 characters longMust contain at least one uppercase letterMust contain at least one lowercase letterMust contain at least one numeric digitMust contain at least one special character from the following set: , : / . _ ? & % = + - [ ] ( ) UCMDB deployment with IdM Reset password Log in to IdM admin portal as suite-admin: https://<External_Access_Host>/idm-adminSelect the organization with a name equal to the Tenant ID to which the integration user is assigned.On the left navigation pane, select Users.Select the integration user (the one used to log ",
    "url": "integrationuserpasswordexpired",
    "filename": "integrationuserpasswordexpired",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "assigned.On",
      "https://<External_Access_Host>/idm-adminSelect",
      "password.To",
      "https://<External_Access_Host>/boSelect",
      "settings.Go",
      "left.On",
      "BO.Make",
      "integration",
      "user",
      "password",
      "expired",
      "cause",
      "solution",
      "ucmdb",
      "cis",
      "syncing",
      "service",
      "management",
      "ucmdb.",
      "issue",
      "occurs",
      "because",
      "preventing",
      "successful",
      "connection",
      "prerequisites",
      "ensure",
      "suite-admin",
      "password.",
      "need",
      "log",
      "idm",
      "admin",
      "portal",
      "sure",
      "following",
      "requirements",
      "met",
      "between",
      "10",
      "16",
      "characters",
      "longmust",
      "contain",
      "least",
      "one",
      "uppercase",
      "lettermust",
      "lowercase",
      "numeric",
      "digitmust",
      "special",
      "character",
      "set",
      "deployment",
      "reset",
      "https",
      "idm-adminselect",
      "organization",
      "name",
      "equal",
      "tenant",
      "id",
      "left",
      "navigation",
      "pane",
      "select",
      "users.select",
      ".scroll",
      "click",
      "password.choose",
      "new",
      "save.",
      "update",
      "native",
      "sacm",
      "configuration",
      "settings",
      "suite",
      "administration",
      "boselect",
      "tenants",
      "option.select",
      "application",
      "section.update",
      "verify",
      "test",
      "connection.",
      "pop-up",
      "message",
      "confirming",
      "appear.click",
      "again",
      "management.",
      "local",
      "client",
      "account",
      "server",
      "privileges.",
      "customer"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "integration user's password has expired",
    "contentLower": "ucmdb cis are not syncing to service management, but service management cis are syncing to ucmdb. cause the issue occurs because the service management integration user's password has expired, preventing a successful connection to ucmdb. solution prerequisites ensure you have the suite-admin user's password. you will need it to log in to the idm admin portal and bo.make sure the following password requirements are met: must be between 10 to 16 characters longmust contain at least one uppercase lettermust contain at least one lowercase lettermust contain at least one numeric digitmust contain at least one special character from the following set: , : / . _ ? & % = + - [ ] ( ) ucmdb deployment with idm reset password log in to idm admin portal as suite-admin: https://<external_access_host>/idm-adminselect the organization with a name equal to the tenant id to which the integration user is assigned.on the left navigation pane, select users.select the integration user (the one used to log ",
    "keywordsLower": [
      "assigned.on",
      "https://<external_access_host>/idm-adminselect",
      "password.to",
      "https://<external_access_host>/boselect",
      "settings.go",
      "left.on",
      "bo.make",
      "integration",
      "user",
      "password",
      "expired",
      "cause",
      "solution",
      "ucmdb",
      "cis",
      "syncing",
      "service",
      "management",
      "ucmdb.",
      "issue",
      "occurs",
      "because",
      "preventing",
      "successful",
      "connection",
      "prerequisites",
      "ensure",
      "suite-admin",
      "password.",
      "need",
      "log",
      "idm",
      "admin",
      "portal",
      "sure",
      "following",
      "requirements",
      "met",
      "between",
      "10",
      "16",
      "characters",
      "longmust",
      "contain",
      "least",
      "one",
      "uppercase",
      "lettermust",
      "lowercase",
      "numeric",
      "digitmust",
      "special",
      "character",
      "set",
      "deployment",
      "reset",
      "https",
      "idm-adminselect",
      "organization",
      "name",
      "equal",
      "tenant",
      "id",
      "left",
      "navigation",
      "pane",
      "select",
      "users.select",
      ".scroll",
      "click",
      "password.choose",
      "new",
      "save.",
      "update",
      "native",
      "sacm",
      "configuration",
      "settings",
      "suite",
      "administration",
      "boselect",
      "tenants",
      "option.select",
      "application",
      "section.update",
      "verify",
      "test",
      "connection.",
      "pop-up",
      "message",
      "confirming",
      "appear.click",
      "again",
      "management.",
      "local",
      "client",
      "account",
      "server",
      "privileges.",
      "customer"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Installation aborted, as the content isn't digitally trusted and only digitally trusted contents are allowed to install",
    "content": "The capsule installation fails with an error banner displayed on the capsule tile in the Content area: Installation aborted, as the content isn't digitally trusted and only digitally trusted contents are allowed to install. Cause This issue occurs if you are trying to install a capsule that isn't signed or if the signature of the certificate isn't trusted. Try the following solutions one by one. Solution 1 Sign the capsule with a trusted certificate as per the truststore before installing it using Content Store. Solution 2 By default, if the capsule isn't signed or if the signature isn't trusted, then the capsule installation fails. If you want to install an unsigned capsule or a capsule with a certificate that isn't trusted, then you can change the level of certificate validation for your chosen capsule to ALLOW_NOT_SIGNED or ALLOW_SIGNED. To change the level of certificate validation for your chosen capsule, tenant admin (user with CS_TENANT_ADMIN permission) should make a PUT REST A",
    "url": "unsignedcapsule",
    "filename": "unsignedcapsule",
    "headings": [
      "Cause",
      "Solution 1",
      "Solution 2",
      "Solution 3"
    ],
    "keywords": [
      "isnt",
      "https://<host>/content-store-gateway/v1/<tenant",
      "installation",
      "aborted",
      "content",
      "isn",
      "digitally",
      "trusted",
      "contents",
      "allowed",
      "install",
      "cause",
      "solution",
      "capsule",
      "fails",
      "error",
      "banner",
      "displayed",
      "tile",
      "area",
      "install.",
      "issue",
      "occurs",
      "trying",
      "signed",
      "signature",
      "certificate",
      "trusted.",
      "try",
      "following",
      "solutions",
      "one",
      "one.",
      "sign",
      "per",
      "truststore",
      "before",
      "installing",
      "store.",
      "default",
      "fails.",
      "want",
      "unsigned",
      "change",
      "level",
      "validation",
      "chosen",
      "tenant",
      "admin",
      "user",
      "permission",
      "make",
      "put",
      "rest",
      "api",
      "call",
      "parameter",
      "description",
      "request",
      "name",
      "type",
      "application",
      "json",
      "authorization",
      "url",
      "https",
      "content-store-gateway",
      "v1",
      "configure",
      "body",
      "contentsignaturelevel",
      "trust",
      "store",
      "steps",
      "log",
      "master",
      "node.",
      "command",
      "stop",
      "backend",
      "service",
      "kubectl",
      "scale",
      "deployment",
      "itom-content-store-backend",
      "--replicas",
      "-n",
      "place",
      "customized",
      "cmp",
      "source",
      "start"
    ],
    "language": "en",
    "word_count": 110,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "installation aborted, as the content isn't digitally trusted and only digitally trusted contents are allowed to install",
    "contentLower": "the capsule installation fails with an error banner displayed on the capsule tile in the content area: installation aborted, as the content isn't digitally trusted and only digitally trusted contents are allowed to install. cause this issue occurs if you are trying to install a capsule that isn't signed or if the signature of the certificate isn't trusted. try the following solutions one by one. solution 1 sign the capsule with a trusted certificate as per the truststore before installing it using content store. solution 2 by default, if the capsule isn't signed or if the signature isn't trusted, then the capsule installation fails. if you want to install an unsigned capsule or a capsule with a certificate that isn't trusted, then you can change the level of certificate validation for your chosen capsule to allow_not_signed or allow_signed. to change the level of certificate validation for your chosen capsule, tenant admin (user with cs_tenant_admin permission) should make a put rest a",
    "keywordsLower": [
      "isnt",
      "https://<host>/content-store-gateway/v1/<tenant",
      "installation",
      "aborted",
      "content",
      "isn",
      "digitally",
      "trusted",
      "contents",
      "allowed",
      "install",
      "cause",
      "solution",
      "capsule",
      "fails",
      "error",
      "banner",
      "displayed",
      "tile",
      "area",
      "install.",
      "issue",
      "occurs",
      "trying",
      "signed",
      "signature",
      "certificate",
      "trusted.",
      "try",
      "following",
      "solutions",
      "one",
      "one.",
      "sign",
      "per",
      "truststore",
      "before",
      "installing",
      "store.",
      "default",
      "fails.",
      "want",
      "unsigned",
      "change",
      "level",
      "validation",
      "chosen",
      "tenant",
      "admin",
      "user",
      "permission",
      "make",
      "put",
      "rest",
      "api",
      "call",
      "parameter",
      "description",
      "request",
      "name",
      "type",
      "application",
      "json",
      "authorization",
      "url",
      "https",
      "content-store-gateway",
      "v1",
      "configure",
      "body",
      "contentsignaturelevel",
      "trust",
      "store",
      "steps",
      "log",
      "master",
      "node.",
      "command",
      "stop",
      "backend",
      "service",
      "kubectl",
      "scale",
      "deployment",
      "itom-content-store-backend",
      "--replicas",
      "-n",
      "place",
      "customized",
      "cmp",
      "source",
      "start"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Inbox custom ENUM field mapping issue",
    "content": "When you customize the Inbox with an ENUM field, the Inbox displays the Name and not the Display name. Cause This issue occurs because of the missing metadata prior to version 25.4. Solution Following are the steps to fix the issue (post 25.4 upgarde): Go to the entity mapping section and delete the faulty mapping for the ENUM field that is causing the issue. Go to the Cross-record Settings page and delete the ENUM field from the Inbox section. Create a new ENUM field with the correct values. Add it back to the Inbox section and perform the correct entity mapping to ensure the Display Name is shown.",
    "url": "inboxenummappingissue",
    "filename": "inboxenummappingissue",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "25.4",
      "inbox",
      "custom",
      "enum",
      "field",
      "mapping",
      "issue",
      "cause",
      "solution",
      "customize",
      "displays",
      "name",
      "display",
      "name.",
      "occurs",
      "because",
      "missing",
      "metadata",
      "prior",
      "version",
      "25.4.",
      "following",
      "steps",
      "fix",
      "post",
      "upgarde",
      "go",
      "entity",
      "section",
      "delete",
      "faulty",
      "causing",
      "issue.",
      "cross-record",
      "settings",
      "page",
      "section.",
      "create",
      "new",
      "correct",
      "values.",
      "add",
      "back",
      "perform",
      "ensure",
      "shown."
    ],
    "language": "en",
    "word_count": 70,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "inbox custom enum field mapping issue",
    "contentLower": "when you customize the inbox with an enum field, the inbox displays the name and not the display name. cause this issue occurs because of the missing metadata prior to version 25.4. solution following are the steps to fix the issue (post 25.4 upgarde): go to the entity mapping section and delete the faulty mapping for the enum field that is causing the issue. go to the cross-record settings page and delete the enum field from the inbox section. create a new enum field with the correct values. add it back to the inbox section and perform the correct entity mapping to ensure the display name is shown.",
    "keywordsLower": [
      "25.4",
      "inbox",
      "custom",
      "enum",
      "field",
      "mapping",
      "issue",
      "cause",
      "solution",
      "customize",
      "displays",
      "name",
      "display",
      "name.",
      "occurs",
      "because",
      "missing",
      "metadata",
      "prior",
      "version",
      "25.4.",
      "following",
      "steps",
      "fix",
      "post",
      "upgarde",
      "go",
      "entity",
      "section",
      "delete",
      "faulty",
      "causing",
      "issue.",
      "cross-record",
      "settings",
      "page",
      "section.",
      "create",
      "new",
      "correct",
      "values.",
      "add",
      "back",
      "perform",
      "ensure",
      "shown."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Internal execution order of flows and steps",
    "content": "What happens when a flow is run? What happens when it reaches a step? What gets calculated first? Most authors ask themselves these questions sooner or later. This topic describes the OO execution engine's execution of flows and flow steps. It also gives a rich set of examples, each centered on specific cases and highlighted by application specific screenshots. The document can be read at once from start to finish or simply by identifying a specific case and focusing on it. Feel free to choose the option that better fits your needs. Introduction Each OO flow consists of steps and connections between them. When we reach a step during execution, a set of micro steps are performed at the step level. The next section describes these steps and the order in which they're performed. Flow step execution Each step is executed in the following order: The inputs are read top down. (This includes variables that are called inside constant values, using the ${var} syntax.) The worker group is assign",
    "url": "internalexecutionorder",
    "filename": "internalexecutionorder",
    "headings": [
      "Introduction",
      "Flow step execution",
      "Entire flow execution",
      "How locks are cleared",
      "Subflow execution",
      "Flows containing parallel or multi instance steps",
      "How can knowing OO’s general execution assist you?",
      "Example of mixed usage of a scriptlet in the result filter, a scriptlet in the operations, and the step",
      "Example of how multi instance step results can be merged",
      "Example of how parallel step results can be merged",
      "Example of enhanced variable manipulation",
      "Example of encapsulating steps to subflows and making use of a flow’s primary output",
      "Flow level inputs in Studio and Central",
      "Example of Single Value vs List of Values as Prompts",
      "Example of Assign From with Otherwise as Constant vs <not assigned> with Otherwise as Constant",
      "Example of Assign From when the variable set exists vs when the variable set doesn't exist",
      "Example of Required flow level inputs"
    ],
    "keywords": [
      "2.0.0",
      "1.5.0",
      "internal",
      "execution",
      "order",
      "flows",
      "steps",
      "introduction",
      "flow",
      "step",
      "entire",
      "locks",
      "cleared",
      "subflow",
      "containing",
      "parallel",
      "multi",
      "instance",
      "knowing",
      "oo",
      "general",
      "assist",
      "example",
      "mixed",
      "usage",
      "scriptlet",
      "result",
      "filter",
      "operations",
      "results",
      "merged",
      "enhanced",
      "variable",
      "manipulation",
      "encapsulating",
      "subflows",
      "making",
      "primary",
      "output",
      "level",
      "inputs",
      "studio",
      "central",
      "single",
      "value",
      "vs",
      "list",
      "values",
      "prompts",
      "assign",
      "otherwise",
      "constant",
      "set",
      "exists",
      "doesn",
      "exist",
      "required",
      "what",
      "happens",
      "run",
      "reaches",
      "gets",
      "calculated",
      "first",
      "most",
      "authors",
      "ask",
      "themselves",
      "questions",
      "sooner",
      "later.",
      "topic",
      "describes",
      "engine",
      "steps.",
      "gives",
      "rich",
      "examples",
      "centered",
      "specific",
      "cases",
      "highlighted",
      "application",
      "screenshots.",
      "document",
      "read",
      "once",
      "start",
      "finish",
      "simply",
      "identifying",
      "case",
      "focusing",
      "it.",
      "feel",
      "free",
      "choose",
      "option",
      "better",
      "fits"
    ],
    "language": "en",
    "word_count": 110,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "internal execution order of flows and steps",
    "contentLower": "what happens when a flow is run? what happens when it reaches a step? what gets calculated first? most authors ask themselves these questions sooner or later. this topic describes the oo execution engine's execution of flows and flow steps. it also gives a rich set of examples, each centered on specific cases and highlighted by application specific screenshots. the document can be read at once from start to finish or simply by identifying a specific case and focusing on it. feel free to choose the option that better fits your needs. introduction each oo flow consists of steps and connections between them. when we reach a step during execution, a set of micro steps are performed at the step level. the next section describes these steps and the order in which they're performed. flow step execution each step is executed in the following order: the inputs are read top down. (this includes variables that are called inside constant values, using the ${var} syntax.) the worker group is assign",
    "keywordsLower": [
      "2.0.0",
      "1.5.0",
      "internal",
      "execution",
      "order",
      "flows",
      "steps",
      "introduction",
      "flow",
      "step",
      "entire",
      "locks",
      "cleared",
      "subflow",
      "containing",
      "parallel",
      "multi",
      "instance",
      "knowing",
      "oo",
      "general",
      "assist",
      "example",
      "mixed",
      "usage",
      "scriptlet",
      "result",
      "filter",
      "operations",
      "results",
      "merged",
      "enhanced",
      "variable",
      "manipulation",
      "encapsulating",
      "subflows",
      "making",
      "primary",
      "output",
      "level",
      "inputs",
      "studio",
      "central",
      "single",
      "value",
      "vs",
      "list",
      "values",
      "prompts",
      "assign",
      "otherwise",
      "constant",
      "set",
      "exists",
      "doesn",
      "exist",
      "required",
      "what",
      "happens",
      "run",
      "reaches",
      "gets",
      "calculated",
      "first",
      "most",
      "authors",
      "ask",
      "themselves",
      "questions",
      "sooner",
      "later.",
      "topic",
      "describes",
      "engine",
      "steps.",
      "gives",
      "rich",
      "examples",
      "centered",
      "specific",
      "cases",
      "highlighted",
      "application",
      "screenshots.",
      "document",
      "read",
      "once",
      "start",
      "finish",
      "simply",
      "identifying",
      "case",
      "focusing",
      "it.",
      "feel",
      "free",
      "choose",
      "option",
      "better",
      "fits"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "List of flows and operations",
    "content": "The ITOM Marketplace portal contains resources that you might be able to reuse when you work with Operations Orchestration (OO). It's a good practice to check and reuse the content that's already available on the ITOM Marketplace portal. Go to the knowledge management site to view or download the latest list of flows and operations in PDF format. Related topics ITOM Marketplace portal",
    "url": "listflowsoperations",
    "filename": "listflowsoperations",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "list",
      "flows",
      "operations",
      "related",
      "topics",
      "itom",
      "marketplace",
      "portal",
      "contains",
      "resources",
      "able",
      "reuse",
      "work",
      "orchestration",
      "oo",
      "good",
      "practice",
      "check",
      "content",
      "already",
      "available",
      "portal.",
      "go",
      "knowledge",
      "management",
      "site",
      "view",
      "download",
      "latest",
      "pdf",
      "format."
    ],
    "language": "en",
    "word_count": 43,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "list of flows and operations",
    "contentLower": "the itom marketplace portal contains resources that you might be able to reuse when you work with operations orchestration (oo). it's a good practice to check and reuse the content that's already available on the itom marketplace portal. go to the knowledge management site to view or download the latest list of flows and operations in pdf format. related topics itom marketplace portal",
    "keywordsLower": [
      "list",
      "flows",
      "operations",
      "related",
      "topics",
      "itom",
      "marketplace",
      "portal",
      "contains",
      "resources",
      "able",
      "reuse",
      "work",
      "orchestration",
      "oo",
      "good",
      "practice",
      "check",
      "content",
      "already",
      "available",
      "portal.",
      "go",
      "knowledge",
      "management",
      "site",
      "view",
      "download",
      "latest",
      "pdf",
      "format."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  }
]