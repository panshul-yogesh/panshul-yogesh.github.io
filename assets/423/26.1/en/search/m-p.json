[
  {
    "title": "Problem Management",
    "content": "Problem ManagementA problem is simply the cause of one or more incidents. The aim of the problem process is to minimize the impact of known problems on consumers and prevent recurring incidents.Problem management encompasses the activities required to diagnose the root cause of incidents and determine short-term workarounds and long-term resolutions to those problems. It's also responsible for ensuring the correct resolutions are implemented through the correct control processes such as change management and release management.Problem management also maintains information about existing problems and the appropriate workarounds and resolutions to reduce the number and impact of incidents over time. This results in a close relationship between problem management and knowledge management, with the addition of a known error database adding additional functionality to the knowledge process.While incident management and problem management are separate processes, they are closely related and ",
    "url": "problemmanagement",
    "filename": "problemmanagement",
    "headings": [
      "Using Problem Management",
      "Setting up Problem Management",
      "Problem Workflow",
      "Classification",
      "Resolution",
      "Validation",
      "Problem Tasks",
      "When to Create Problems",
      "Manual Problem Analysis",
      "Guided Problem Analysis"
    ],
    "keywords": [
      "incident.Once",
      "problem.If",
      "rains.We",
      "phase.The",
      "have.Let",
      "review.When",
      "troubleshooting.Once",
      "do.Okay",
      "resolution.When",
      "problem",
      "management",
      "setting",
      "workflow",
      "classification",
      "resolution",
      "validation",
      "tasks",
      "create",
      "problems",
      "manual",
      "analysis",
      "guided",
      "managementa",
      "simply",
      "cause",
      "one",
      "incidents.",
      "aim",
      "process",
      "minimize",
      "impact",
      "known",
      "consumers",
      "prevent",
      "recurring",
      "incidents.problem",
      "encompasses",
      "activities",
      "required",
      "diagnose",
      "root",
      "incidents",
      "determine",
      "short-term",
      "workarounds",
      "long-term",
      "resolutions",
      "problems.",
      "responsible",
      "ensuring",
      "correct",
      "implemented",
      "through",
      "control",
      "processes",
      "such",
      "change",
      "release",
      "management.problem",
      "maintains",
      "information",
      "about",
      "existing",
      "appropriate",
      "reduce",
      "number",
      "over",
      "time.",
      "results",
      "close",
      "relationship",
      "between",
      "knowledge",
      "addition",
      "error",
      "database",
      "adding",
      "additional",
      "functionality",
      "process.while",
      "incident",
      "separate",
      "closely",
      "related",
      "share",
      "attributes",
      "categorization",
      "priority",
      "codes.",
      "allows",
      "easy",
      "consumption",
      "data",
      "managementone",
      "way",
      "agents",
      "deal",
      "keep",
      "happening",
      "first"
    ],
    "language": "en",
    "word_count": 89,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "problem management",
    "contentLower": "problem managementa problem is simply the cause of one or more incidents. the aim of the problem process is to minimize the impact of known problems on consumers and prevent recurring incidents.problem management encompasses the activities required to diagnose the root cause of incidents and determine short-term workarounds and long-term resolutions to those problems. it's also responsible for ensuring the correct resolutions are implemented through the correct control processes such as change management and release management.problem management also maintains information about existing problems and the appropriate workarounds and resolutions to reduce the number and impact of incidents over time. this results in a close relationship between problem management and knowledge management, with the addition of a known error database adding additional functionality to the knowledge process.while incident management and problem management are separate processes, they are closely related and ",
    "keywordsLower": [
      "incident.once",
      "problem.if",
      "rains.we",
      "phase.the",
      "have.let",
      "review.when",
      "troubleshooting.once",
      "do.okay",
      "resolution.when",
      "problem",
      "management",
      "setting",
      "workflow",
      "classification",
      "resolution",
      "validation",
      "tasks",
      "create",
      "problems",
      "manual",
      "analysis",
      "guided",
      "managementa",
      "simply",
      "cause",
      "one",
      "incidents.",
      "aim",
      "process",
      "minimize",
      "impact",
      "known",
      "consumers",
      "prevent",
      "recurring",
      "incidents.problem",
      "encompasses",
      "activities",
      "required",
      "diagnose",
      "root",
      "incidents",
      "determine",
      "short-term",
      "workarounds",
      "long-term",
      "resolutions",
      "problems.",
      "responsible",
      "ensuring",
      "correct",
      "implemented",
      "through",
      "control",
      "processes",
      "such",
      "change",
      "release",
      "management.problem",
      "maintains",
      "information",
      "about",
      "existing",
      "appropriate",
      "reduce",
      "number",
      "over",
      "time.",
      "results",
      "close",
      "relationship",
      "between",
      "knowledge",
      "addition",
      "error",
      "database",
      "adding",
      "additional",
      "functionality",
      "process.while",
      "incident",
      "separate",
      "closely",
      "related",
      "share",
      "attributes",
      "categorization",
      "priority",
      "codes.",
      "allows",
      "easy",
      "consumption",
      "data",
      "managementone",
      "way",
      "agents",
      "deal",
      "keep",
      "happening",
      "first"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Managed Service Provider (MSP) Mode",
    "content": "Service Management provides the ability for shared services providers to manage their business units' and/or clients' request and incident records in a single interface using the multi-tenant (MT) console. Each customer/business unit can use their own tenant but allow the “parent” to have direct access to the tickets from a consolidated view. The MT console requires the Tenants to be set up in a specific way, and then Agents to be given appropriate permission to the MT console and “child” tenants. If an agent has been granted the permissions on a specific managed tenant, when the agent clicks on an Individual record in the MT console, the record opens in a separate tab, and the agent is then inside that specific tenant. Suite Administration Configuration for MT Console Create an Account in the Suite Administration and set it with a Shared Service type of Provider. Add users to the provider account. Two additional roles are now available for users. Shared service admin: Assigned with MT",
    "url": "mspmode",
    "filename": "mspmode",
    "headings": [
      "Suite Administration Configuration for MT Console",
      "Using the MT Console",
      "Managed Tenant",
      "Provider Tenant",
      "MT Console Tabs",
      "Dashboard",
      "Requests",
      "Incidents"
    ],
    "keywords": [
      "managed",
      "service",
      "provider",
      "msp",
      "mode",
      "suite",
      "administration",
      "configuration",
      "mt",
      "console",
      "tenant",
      "tabs",
      "dashboard",
      "requests",
      "incidents",
      "management",
      "provides",
      "ability",
      "shared",
      "services",
      "providers",
      "manage",
      "business",
      "units",
      "clients",
      "request",
      "incident",
      "records",
      "single",
      "interface",
      "multi-tenant",
      "console.",
      "customer",
      "unit",
      "own",
      "allow",
      "parent",
      "direct",
      "access",
      "tickets",
      "consolidated",
      "view.",
      "requires",
      "tenants",
      "set",
      "specific",
      "way",
      "agents",
      "given",
      "appropriate",
      "permission",
      "child",
      "tenants.",
      "agent",
      "granted",
      "permissions",
      "clicks",
      "individual",
      "record",
      "opens",
      "separate",
      "tab",
      "inside",
      "tenant.",
      "create",
      "account",
      "type",
      "provider.",
      "add",
      "users",
      "account.",
      "two",
      "additional",
      "roles",
      "now",
      "available",
      "users.",
      "admin",
      "assigned",
      "administrator",
      "role",
      "automatically.",
      "normal",
      "field",
      "need",
      "assign",
      "licenses",
      "deploy",
      "normal.",
      "show",
      "icon",
      "identify",
      "managed.",
      "except",
      "see",
      "associated",
      "it.",
      "after",
      "deployed",
      "go"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "managed service provider (msp) mode",
    "contentLower": "service management provides the ability for shared services providers to manage their business units' and/or clients' request and incident records in a single interface using the multi-tenant (mt) console. each customer/business unit can use their own tenant but allow the “parent” to have direct access to the tickets from a consolidated view. the mt console requires the tenants to be set up in a specific way, and then agents to be given appropriate permission to the mt console and “child” tenants. if an agent has been granted the permissions on a specific managed tenant, when the agent clicks on an individual record in the mt console, the record opens in a separate tab, and the agent is then inside that specific tenant. suite administration configuration for mt console create an account in the suite administration and set it with a shared service type of provider. add users to the provider account. two additional roles are now available for users. shared service admin: assigned with mt",
    "keywordsLower": [
      "managed",
      "service",
      "provider",
      "msp",
      "mode",
      "suite",
      "administration",
      "configuration",
      "mt",
      "console",
      "tenant",
      "tabs",
      "dashboard",
      "requests",
      "incidents",
      "management",
      "provides",
      "ability",
      "shared",
      "services",
      "providers",
      "manage",
      "business",
      "units",
      "clients",
      "request",
      "incident",
      "records",
      "single",
      "interface",
      "multi-tenant",
      "console.",
      "customer",
      "unit",
      "own",
      "allow",
      "parent",
      "direct",
      "access",
      "tickets",
      "consolidated",
      "view.",
      "requires",
      "tenants",
      "set",
      "specific",
      "way",
      "agents",
      "given",
      "appropriate",
      "permission",
      "child",
      "tenants.",
      "agent",
      "granted",
      "permissions",
      "clicks",
      "individual",
      "record",
      "opens",
      "separate",
      "tab",
      "inside",
      "tenant.",
      "create",
      "account",
      "type",
      "provider.",
      "add",
      "users",
      "account.",
      "two",
      "additional",
      "roles",
      "now",
      "available",
      "users.",
      "admin",
      "assigned",
      "administrator",
      "role",
      "automatically.",
      "normal",
      "field",
      "need",
      "assign",
      "licenses",
      "deploy",
      "normal.",
      "show",
      "icon",
      "identify",
      "managed.",
      "except",
      "see",
      "associated",
      "it.",
      "after",
      "deployed",
      "go"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OPB Certificates",
    "content": "Introduction When creating an integration to Service Management with a remote system that has an SSL address, it is possible that the certificate of the remote server must be imported into the trusted keystore file of the On-Premises Bridge (OPB). The cacerts file stores public certificates of root Certificate Authority (CA). If there is a problem with the connection between the OPB and the remote system, check the controller.log file of the OPB for the error defined below. If the error exists, then you may be required to follow the procedure described in this document. javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target This chapter will include: Finding the location of the Service Management OPB controller log Finding the location of the Service Management OPB trusted keystore Obtaining the certificate of the re",
    "url": "wow_opb_certificates",
    "filename": "wow_opb_certificates",
    "headings": [
      "Introduction",
      "Finding the location of the Service Management OPB controller log",
      "Finding the location of the Service Management OPB trusted keystore",
      "Obtaining the certificate of the remote server",
      "Adding the certificate to the trusted keystore",
      "How to identify if the certificate is present"
    ],
    "keywords": [
      "contents.txt",
      "myCert.crt",
      "javax.net",
      "controller.log",
      "opb",
      "certificates",
      "introduction",
      "finding",
      "location",
      "service",
      "management",
      "controller",
      "log",
      "trusted",
      "keystore",
      "obtaining",
      "certificate",
      "remote",
      "server",
      "adding",
      "identify",
      "present",
      "creating",
      "integration",
      "system",
      "ssl",
      "address",
      "possible",
      "imported",
      "file",
      "on-premises",
      "bridge",
      "cacerts",
      "stores",
      "public",
      "root",
      "authority",
      "ca",
      "there",
      "problem",
      "connection",
      "between",
      "check",
      "error",
      "defined",
      "below.",
      "exists",
      "required",
      "follow",
      "procedure",
      "described",
      "document.",
      "javax.net.ssl.sslhandshakeexception",
      "sun.security.validator.validatorexception",
      "pkix",
      "path",
      "building",
      "failed",
      "sun.security.provider.certpath.suncertpathbuilderexception",
      "unable",
      "find",
      "valid",
      "certification",
      "requested",
      "target",
      "chapter",
      "include",
      "default",
      "program",
      "files",
      "hp",
      "agent",
      "product",
      "directory.",
      "once",
      "located",
      "search",
      "handshakeexception",
      "error.",
      "appears",
      "fully",
      "qualified",
      "domain",
      "name",
      "fqdn",
      "logged",
      "allow",
      "verify",
      "endpoint",
      "question",
      "indeed",
      "one",
      "giving",
      "named",
      "util",
      "3rd-party",
      "jre",
      "lib",
      "security",
      "own"
    ],
    "language": "en",
    "word_count": 85,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "opb certificates",
    "contentLower": "introduction when creating an integration to service management with a remote system that has an ssl address, it is possible that the certificate of the remote server must be imported into the trusted keystore file of the on-premises bridge (opb). the cacerts file stores public certificates of root certificate authority (ca). if there is a problem with the connection between the opb and the remote system, check the controller.log file of the opb for the error defined below. if the error exists, then you may be required to follow the procedure described in this document. javax.net.ssl.sslhandshakeexception: sun.security.validator.validatorexception: pkix path building failed: sun.security.provider.certpath.suncertpathbuilderexception: unable to find valid certification path to requested target this chapter will include: finding the location of the service management opb controller log finding the location of the service management opb trusted keystore obtaining the certificate of the re",
    "keywordsLower": [
      "contents.txt",
      "mycert.crt",
      "javax.net",
      "controller.log",
      "opb",
      "certificates",
      "introduction",
      "finding",
      "location",
      "service",
      "management",
      "controller",
      "log",
      "trusted",
      "keystore",
      "obtaining",
      "certificate",
      "remote",
      "server",
      "adding",
      "identify",
      "present",
      "creating",
      "integration",
      "system",
      "ssl",
      "address",
      "possible",
      "imported",
      "file",
      "on-premises",
      "bridge",
      "cacerts",
      "stores",
      "public",
      "root",
      "authority",
      "ca",
      "there",
      "problem",
      "connection",
      "between",
      "check",
      "error",
      "defined",
      "below.",
      "exists",
      "required",
      "follow",
      "procedure",
      "described",
      "document.",
      "javax.net.ssl.sslhandshakeexception",
      "sun.security.validator.validatorexception",
      "pkix",
      "path",
      "building",
      "failed",
      "sun.security.provider.certpath.suncertpathbuilderexception",
      "unable",
      "find",
      "valid",
      "certification",
      "requested",
      "target",
      "chapter",
      "include",
      "default",
      "program",
      "files",
      "hp",
      "agent",
      "product",
      "directory.",
      "once",
      "located",
      "search",
      "handshakeexception",
      "error.",
      "appears",
      "fully",
      "qualified",
      "domain",
      "name",
      "fqdn",
      "logged",
      "allow",
      "verify",
      "endpoint",
      "question",
      "indeed",
      "one",
      "giving",
      "named",
      "util",
      "3rd-party",
      "jre",
      "lib",
      "security",
      "own"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OO capabilities",
    "content": "The key features of Operations Orchestration (OO) are the following: Helps you keep, troubleshoot, and repair your resources. Checks the health of, diagnose, and repair the networks, servers, software applications, and workstations. Helps you to perform repetitive tasks, such as checking status on internal or external website pages. Enables element automation, run book automation, integrated automation, and automation of the business services. Offers an intuitive drag and wire capability to design, create, share, and customize flows. Powerful execution mechanism, which enables high performance. You can scale OO Central to reach high numbers of steps per second. Offers interactive execution and user interface (UI) embedding capabilities, which enable you to interact with the system in a much more usable fashion. OO Central OO Central is the runtime environment of OO. You can deploy it using the OO Helm Chart. It has a web based user interface (UI), that the administrators and end users ",
    "url": "oocapabilities",
    "filename": "oocapabilities",
    "headings": [
      "OO Central",
      "Internal OO RAS",
      "OO Workflow Designer",
      "External OO Remote Action Service (RAS)"
    ],
    "keywords": [
      "oo",
      "capabilities",
      "central",
      "internal",
      "ras",
      "workflow",
      "designer",
      "external",
      "remote",
      "action",
      "service",
      "key",
      "features",
      "operations",
      "orchestration",
      "following",
      "helps",
      "keep",
      "troubleshoot",
      "repair",
      "resources.",
      "checks",
      "health",
      "diagnose",
      "networks",
      "servers",
      "software",
      "applications",
      "workstations.",
      "perform",
      "repetitive",
      "tasks",
      "such",
      "checking",
      "status",
      "website",
      "pages.",
      "enables",
      "element",
      "automation",
      "run",
      "book",
      "integrated",
      "business",
      "services.",
      "offers",
      "intuitive",
      "drag",
      "wire",
      "capability",
      "design",
      "create",
      "share",
      "customize",
      "flows.",
      "powerful",
      "execution",
      "mechanism",
      "high",
      "performance.",
      "scale",
      "reach",
      "numbers",
      "steps",
      "per",
      "second.",
      "interactive",
      "user",
      "interface",
      "ui",
      "embedding",
      "enable",
      "interact",
      "system",
      "much",
      "usable",
      "fashion.",
      "runtime",
      "environment",
      "oo.",
      "deploy",
      "helm",
      "chart.",
      "web",
      "based",
      "administrators",
      "end",
      "users",
      "access.",
      "running",
      "flows",
      "monitoring",
      "various",
      "runs",
      "generating",
      "reports.",
      "flow",
      "deployed",
      "executing",
      "include"
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "oo capabilities",
    "contentLower": "the key features of operations orchestration (oo) are the following: helps you keep, troubleshoot, and repair your resources. checks the health of, diagnose, and repair the networks, servers, software applications, and workstations. helps you to perform repetitive tasks, such as checking status on internal or external website pages. enables element automation, run book automation, integrated automation, and automation of the business services. offers an intuitive drag and wire capability to design, create, share, and customize flows. powerful execution mechanism, which enables high performance. you can scale oo central to reach high numbers of steps per second. offers interactive execution and user interface (ui) embedding capabilities, which enable you to interact with the system in a much more usable fashion. oo central oo central is the runtime environment of oo. you can deploy it using the oo helm chart. it has a web based user interface (ui), that the administrators and end users ",
    "keywordsLower": [
      "oo",
      "capabilities",
      "central",
      "internal",
      "ras",
      "workflow",
      "designer",
      "external",
      "remote",
      "action",
      "service",
      "key",
      "features",
      "operations",
      "orchestration",
      "following",
      "helps",
      "keep",
      "troubleshoot",
      "repair",
      "resources.",
      "checks",
      "health",
      "diagnose",
      "networks",
      "servers",
      "software",
      "applications",
      "workstations.",
      "perform",
      "repetitive",
      "tasks",
      "such",
      "checking",
      "status",
      "website",
      "pages.",
      "enables",
      "element",
      "automation",
      "run",
      "book",
      "integrated",
      "business",
      "services.",
      "offers",
      "intuitive",
      "drag",
      "wire",
      "capability",
      "design",
      "create",
      "share",
      "customize",
      "flows.",
      "powerful",
      "execution",
      "mechanism",
      "high",
      "performance.",
      "scale",
      "reach",
      "numbers",
      "steps",
      "per",
      "second.",
      "interactive",
      "user",
      "interface",
      "ui",
      "embedding",
      "enable",
      "interact",
      "system",
      "much",
      "usable",
      "fashion.",
      "runtime",
      "environment",
      "oo.",
      "deploy",
      "helm",
      "chart.",
      "web",
      "based",
      "administrators",
      "end",
      "users",
      "access.",
      "running",
      "flows",
      "monitoring",
      "various",
      "runs",
      "generating",
      "reports.",
      "flow",
      "deployed",
      "executing",
      "include"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Overview",
    "content": "Introduction At its core, CSM involves managing the interactions between the organization and its customers through multiple channels—such as phone, email, live chat, and in-person support. The goal is to ensure that customers receive timely, accurate, and helpful assistance, which leads to improved loyalty and retention. The broader goal of CSM is not only to solve problems but also to create positive experiences that encourage customers to remain loyal and recommend the company to others. As businesses increasingly prioritize customer experience, CSM has become a competitive differentiator, influencing both customer retention and brand reputation. By optimizing service delivery, organizations can enhance customer satisfaction, reduce churn, and ultimately, drive business success. Traditionally, OpenText Service Management has focused on enabling IT and business units to support their own employees within an organization. CSM extends this capability to support entire external organiza",
    "url": "csmoverview",
    "filename": "csmoverview",
    "headings": [
      "Introduction",
      "CSM vs. ITSM",
      "Why should ITSM and CSM integrate?",
      "Key concepts and features"
    ],
    "keywords": [
      "overview",
      "introduction",
      "csm",
      "vs.",
      "itsm",
      "integrate",
      "key",
      "concepts",
      "features",
      "core",
      "involves",
      "managing",
      "interactions",
      "between",
      "organization",
      "customers",
      "through",
      "multiple",
      "channels",
      "such",
      "phone",
      "email",
      "live",
      "chat",
      "in-person",
      "support.",
      "goal",
      "ensure",
      "receive",
      "timely",
      "accurate",
      "helpful",
      "assistance",
      "leads",
      "improved",
      "loyalty",
      "retention.",
      "broader",
      "solve",
      "problems",
      "create",
      "positive",
      "experiences",
      "encourage",
      "remain",
      "loyal",
      "recommend",
      "company",
      "others.",
      "businesses",
      "increasingly",
      "prioritize",
      "customer",
      "experience",
      "become",
      "competitive",
      "differentiator",
      "influencing",
      "both",
      "retention",
      "brand",
      "reputation.",
      "optimizing",
      "service",
      "delivery",
      "organizations",
      "enhance",
      "satisfaction",
      "reduce",
      "churn",
      "ultimately",
      "drive",
      "business",
      "success.",
      "traditionally",
      "opentext",
      "management",
      "focused",
      "enabling",
      "units",
      "support",
      "own",
      "employees",
      "organization.",
      "extends",
      "capability",
      "entire",
      "external",
      "b2b",
      "rather",
      "just",
      "individual",
      "users.",
      "components",
      "include",
      "requests",
      "cases",
      "fulfillment",
      "often",
      "contact"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "overview",
    "contentLower": "introduction at its core, csm involves managing the interactions between the organization and its customers through multiple channels—such as phone, email, live chat, and in-person support. the goal is to ensure that customers receive timely, accurate, and helpful assistance, which leads to improved loyalty and retention. the broader goal of csm is not only to solve problems but also to create positive experiences that encourage customers to remain loyal and recommend the company to others. as businesses increasingly prioritize customer experience, csm has become a competitive differentiator, influencing both customer retention and brand reputation. by optimizing service delivery, organizations can enhance customer satisfaction, reduce churn, and ultimately, drive business success. traditionally, opentext service management has focused on enabling it and business units to support their own employees within an organization. csm extends this capability to support entire external organiza",
    "keywordsLower": [
      "overview",
      "introduction",
      "csm",
      "vs.",
      "itsm",
      "integrate",
      "key",
      "concepts",
      "features",
      "core",
      "involves",
      "managing",
      "interactions",
      "between",
      "organization",
      "customers",
      "through",
      "multiple",
      "channels",
      "such",
      "phone",
      "email",
      "live",
      "chat",
      "in-person",
      "support.",
      "goal",
      "ensure",
      "receive",
      "timely",
      "accurate",
      "helpful",
      "assistance",
      "leads",
      "improved",
      "loyalty",
      "retention.",
      "broader",
      "solve",
      "problems",
      "create",
      "positive",
      "experiences",
      "encourage",
      "remain",
      "loyal",
      "recommend",
      "company",
      "others.",
      "businesses",
      "increasingly",
      "prioritize",
      "customer",
      "experience",
      "become",
      "competitive",
      "differentiator",
      "influencing",
      "both",
      "retention",
      "brand",
      "reputation.",
      "optimizing",
      "service",
      "delivery",
      "organizations",
      "enhance",
      "satisfaction",
      "reduce",
      "churn",
      "ultimately",
      "drive",
      "business",
      "success.",
      "traditionally",
      "opentext",
      "management",
      "focused",
      "enabling",
      "units",
      "support",
      "own",
      "employees",
      "organization.",
      "extends",
      "capability",
      "entire",
      "external",
      "b2b",
      "rather",
      "just",
      "individual",
      "users.",
      "components",
      "include",
      "requests",
      "cases",
      "fulfillment",
      "often",
      "contact"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage software compliance using SAM",
    "content": "Knowing the software license position is critical to any organization aiming to manage software spending and prevent audit issues. The Software Asset Management (SAM) capability allows you to create and analyze license compliance reports so that you can systematically manage, control your software assets, and make insightful business decisions. This use case describes how to navigate SAM, and how elements such as license, license rule, and license metric can be consumed to create and analyze compliance reports. This topic assumes that you have installed and enabled the SAM capability, and have deployed a Universal CMDB (UCMDB) environment with Universal Discovery (UD). If this isn’t the case, see Install add-on capabilities and Enable SAM to deploy SAM. Because the configuration items (CIs) discovered will vary between environments and with the introduction of new SAM features in each release, the screenshots in this document might slightly differ from yours. Key concepts and their rel",
    "url": "managecompliancewithsam",
    "filename": "managecompliancewithsam",
    "headings": [
      "Key concepts and their relationships",
      "Overview and navigation",
      "Workflow",
      "Preparation",
      "Configurations",
      "Configure the license",
      "Create a license model",
      "Create a license",
      "Configure the product",
      "Maintain license compatibility matrix",
      "Link a license rule to a product",
      "Configurations required for user-based licensing",
      "Calculation",
      "Compliance",
      "View compliance report",
      "Analyze license compliance",
      "Advanced use",
      "License metrics",
      "License rules",
      "Troubleshoot"
    ],
    "keywords": [
      "Spring.log",
      "Full_calc.log",
      "Full.log",
      "24.211",
      "VIRUSSCAN_calc.log",
      "24.179",
      "21.427",
      "25.258",
      "Full_adapter.log",
      "24.195",
      "30.356",
      "30.528",
      "24.164",
      "30.262",
      "30.559",
      "manage",
      "software",
      "compliance",
      "sam",
      "key",
      "concepts",
      "relationships",
      "overview",
      "navigation",
      "workflow",
      "preparation",
      "configurations",
      "configure",
      "license",
      "create",
      "model",
      "product",
      "maintain",
      "compatibility",
      "matrix",
      "link",
      "rule",
      "required",
      "user-based",
      "licensing",
      "calculation",
      "view",
      "report",
      "analyze",
      "advanced",
      "metrics",
      "rules",
      "troubleshoot",
      "toubleshoot",
      "based",
      "documentations",
      "logs",
      "example",
      "troubleshooting",
      "related",
      "topics",
      "knowing",
      "position",
      "critical",
      "any",
      "organization",
      "aiming",
      "spending",
      "prevent",
      "audit",
      "issues.",
      "asset",
      "management",
      "capability",
      "allows",
      "reports",
      "systematically",
      "control",
      "assets",
      "make",
      "insightful",
      "business",
      "decisions.",
      "case",
      "describes",
      "navigate",
      "elements",
      "such",
      "metric",
      "consumed",
      "reports.",
      "topic",
      "assumes",
      "installed",
      "enabled",
      "deployed",
      "universal",
      "cmdb",
      "ucmdb",
      "environment",
      "discovery",
      "ud",
      "isn",
      "see",
      "install"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage software compliance using sam",
    "contentLower": "knowing the software license position is critical to any organization aiming to manage software spending and prevent audit issues. the software asset management (sam) capability allows you to create and analyze license compliance reports so that you can systematically manage, control your software assets, and make insightful business decisions. this use case describes how to navigate sam, and how elements such as license, license rule, and license metric can be consumed to create and analyze compliance reports. this topic assumes that you have installed and enabled the sam capability, and have deployed a universal cmdb (ucmdb) environment with universal discovery (ud). if this isn’t the case, see install add-on capabilities and enable sam to deploy sam. because the configuration items (cis) discovered will vary between environments and with the introduction of new sam features in each release, the screenshots in this document might slightly differ from yours. key concepts and their rel",
    "keywordsLower": [
      "spring.log",
      "full_calc.log",
      "full.log",
      "24.211",
      "virusscan_calc.log",
      "24.179",
      "21.427",
      "25.258",
      "full_adapter.log",
      "24.195",
      "30.356",
      "30.528",
      "24.164",
      "30.262",
      "30.559",
      "manage",
      "software",
      "compliance",
      "sam",
      "key",
      "concepts",
      "relationships",
      "overview",
      "navigation",
      "workflow",
      "preparation",
      "configurations",
      "configure",
      "license",
      "create",
      "model",
      "product",
      "maintain",
      "compatibility",
      "matrix",
      "link",
      "rule",
      "required",
      "user-based",
      "licensing",
      "calculation",
      "view",
      "report",
      "analyze",
      "advanced",
      "metrics",
      "rules",
      "troubleshoot",
      "toubleshoot",
      "based",
      "documentations",
      "logs",
      "example",
      "troubleshooting",
      "related",
      "topics",
      "knowing",
      "position",
      "critical",
      "any",
      "organization",
      "aiming",
      "spending",
      "prevent",
      "audit",
      "issues.",
      "asset",
      "management",
      "capability",
      "allows",
      "reports",
      "systematically",
      "control",
      "assets",
      "make",
      "insightful",
      "business",
      "decisions.",
      "case",
      "describes",
      "navigate",
      "elements",
      "such",
      "metric",
      "consumed",
      "reports.",
      "topic",
      "assumes",
      "installed",
      "enabled",
      "deployed",
      "universal",
      "cmdb",
      "ucmdb",
      "environment",
      "discovery",
      "ud",
      "isn",
      "see",
      "install"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan for on-premises",
    "content": "The following table provides an overview of the topics covered in the Plan section: Topic Description Plan the deployment Collect and evaluate data from your current environment to decide the following: Suite deployment option that suits your environment the best Size and complexity of your suite deployment System requirements Based on your deployment plan, review the system requirements for the installation of suite. At the conclusion of this phase, you must have the following resources in place: Control plane node(s) required to control the logic of service deployment Worker node(s) to run suite services NFS server to store OMT and suite data Relational database Check the file system requirements Lists the required directories and free space needed in each directory Related topics Next topic: Plan the deployment",
    "url": "plandeploymentoverview",
    "filename": "plandeploymentoverview",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "plan",
      "on-premises",
      "related",
      "topics",
      "following",
      "table",
      "provides",
      "overview",
      "covered",
      "section",
      "topic",
      "description",
      "deployment",
      "collect",
      "evaluate",
      "data",
      "current",
      "environment",
      "decide",
      "suite",
      "option",
      "suits",
      "best",
      "size",
      "complexity",
      "system",
      "requirements",
      "based",
      "review",
      "installation",
      "suite.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "control",
      "plane",
      "node",
      "required",
      "logic",
      "service",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "relational",
      "database",
      "check",
      "file",
      "lists",
      "directories",
      "free",
      "space",
      "needed",
      "directory",
      "next"
    ],
    "language": "en",
    "word_count": 86,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan for on-premises",
    "contentLower": "the following table provides an overview of the topics covered in the plan section: topic description plan the deployment collect and evaluate data from your current environment to decide the following: suite deployment option that suits your environment the best size and complexity of your suite deployment system requirements based on your deployment plan, review the system requirements for the installation of suite. at the conclusion of this phase, you must have the following resources in place: control plane node(s) required to control the logic of service deployment worker node(s) to run suite services nfs server to store omt and suite data relational database check the file system requirements lists the required directories and free space needed in each directory related topics next topic: plan the deployment",
    "keywordsLower": [
      "plan",
      "on-premises",
      "related",
      "topics",
      "following",
      "table",
      "provides",
      "overview",
      "covered",
      "section",
      "topic",
      "description",
      "deployment",
      "collect",
      "evaluate",
      "data",
      "current",
      "environment",
      "decide",
      "suite",
      "option",
      "suits",
      "best",
      "size",
      "complexity",
      "system",
      "requirements",
      "based",
      "review",
      "installation",
      "suite.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "control",
      "plane",
      "node",
      "required",
      "logic",
      "service",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "relational",
      "database",
      "check",
      "file",
      "lists",
      "directories",
      "free",
      "space",
      "needed",
      "directory",
      "next"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan the deployment",
    "content": "Your deployment plan varies depending on the product components (\"capabilities\") that you choose to deploy and the size of your environment. To create a deployment plan: Choose a deployment optionChoose a deployment sizeCalculate the hardware resource requirements The sizing recommendations are the minimum dedicated resources required for running the suite (lab tests indicate the average CPU usage is 40 ~ 60%, and memory usage is 75 ~ 85%). The sizing recommendations below don't consider the following: Any other applications that you may run in the environment, especially high resource-consuming software such as antivirus or mining software The extra load as the volume of tickets in the system increases over time For node high availability, better system performance, and stability in production, you can add one or two extra worker nodes. This is important especially if you have a different user load, business throughput, and data volume size or if you're uncertain about your business n",
    "url": "plandeployment",
    "filename": "plandeployment",
    "headings": [
      "Choose a deployment option",
      "Capability installation considerations",
      "UCMDB installation considerations",
      "Typical deployment options",
      "Option 1. Install Service Management only",
      "Option 2. Install Service Management + SAM  + UCMDB",
      "Service Management + SAM + classic UCMDB",
      "Service Management + SAM + Containerized UCMDB",
      "Option 3. Install Service Management +DND + OO Containerized + Cloud Cost Reporting",
      "Calculate the capability deployment sizes",
      "Service Management size definitions",
      "SAM size definitions",
      "DND size definitions",
      "Cloud Cost Reporting size definitions",
      "Operations Orchestration Containerized size definition",
      "Calculate your hardware requirements",
      "Hardware requirements for option 1  (Service Management)",
      "Hardware requirements for option 2 (Service Management + SAM + UCMDB)",
      "Hardware requirements for option 3 (Service Management + DND + OO Containerized + Cloud Cost Reporting)",
      "Hardware requirements for other options"
    ],
    "keywords": [
      "0.5",
      "manually.If",
      "requests.IT",
      "1.5",
      "Memory.In",
      "Settings.In",
      "2.5",
      "needs.You",
      "0.3",
      "0.7",
      "performance.In",
      "space.the",
      "here.To",
      "machine.In",
      "oo_size_values.yaml",
      "0.9",
      "plan",
      "deployment",
      "choose",
      "option",
      "capability",
      "installation",
      "considerations",
      "ucmdb",
      "typical",
      "options",
      "1.",
      "install",
      "service",
      "management",
      "2.",
      "sam",
      "classic",
      "containerized",
      "3.",
      "dnd",
      "oo",
      "cloud",
      "cost",
      "reporting",
      "calculate",
      "sizes",
      "size",
      "definitions",
      "operations",
      "orchestration",
      "definition",
      "hardware",
      "requirements",
      "system",
      "configuration",
      "recommendations",
      "cpu",
      "storage",
      "network",
      "latency",
      "virtual",
      "machines",
      "configure",
      "drs",
      "hosting",
      "suite",
      "reserve",
      "memory",
      "resources",
      "odata",
      "integration",
      "related",
      "topics",
      "varies",
      "depending",
      "product",
      "components",
      "capabilities",
      "deploy",
      "environment.",
      "create",
      "optionchoose",
      "sizecalculate",
      "resource",
      "sizing",
      "minimum",
      "dedicated",
      "required",
      "running",
      "lab",
      "tests",
      "indicate",
      "average",
      "usage",
      "40",
      "60",
      "75",
      "85",
      "below",
      "don",
      "consider",
      "following",
      "any",
      "applications"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan the deployment",
    "contentLower": "your deployment plan varies depending on the product components (\"capabilities\") that you choose to deploy and the size of your environment. to create a deployment plan: choose a deployment optionchoose a deployment sizecalculate the hardware resource requirements the sizing recommendations are the minimum dedicated resources required for running the suite (lab tests indicate the average cpu usage is 40 ~ 60%, and memory usage is 75 ~ 85%). the sizing recommendations below don't consider the following: any other applications that you may run in the environment, especially high resource-consuming software such as antivirus or mining software the extra load as the volume of tickets in the system increases over time for node high availability, better system performance, and stability in production, you can add one or two extra worker nodes. this is important especially if you have a different user load, business throughput, and data volume size or if you're uncertain about your business n",
    "keywordsLower": [
      "0.5",
      "manually.if",
      "requests.it",
      "1.5",
      "memory.in",
      "settings.in",
      "2.5",
      "needs.you",
      "0.3",
      "0.7",
      "performance.in",
      "space.the",
      "here.to",
      "machine.in",
      "oo_size_values.yaml",
      "0.9",
      "plan",
      "deployment",
      "choose",
      "option",
      "capability",
      "installation",
      "considerations",
      "ucmdb",
      "typical",
      "options",
      "1.",
      "install",
      "service",
      "management",
      "2.",
      "sam",
      "classic",
      "containerized",
      "3.",
      "dnd",
      "oo",
      "cloud",
      "cost",
      "reporting",
      "calculate",
      "sizes",
      "size",
      "definitions",
      "operations",
      "orchestration",
      "definition",
      "hardware",
      "requirements",
      "system",
      "configuration",
      "recommendations",
      "cpu",
      "storage",
      "network",
      "latency",
      "virtual",
      "machines",
      "configure",
      "drs",
      "hosting",
      "suite",
      "reserve",
      "memory",
      "resources",
      "odata",
      "integration",
      "related",
      "topics",
      "varies",
      "depending",
      "product",
      "components",
      "capabilities",
      "deploy",
      "environment.",
      "create",
      "optionchoose",
      "sizecalculate",
      "resource",
      "sizing",
      "minimum",
      "dedicated",
      "required",
      "running",
      "lab",
      "tests",
      "indicate",
      "average",
      "usage",
      "40",
      "60",
      "75",
      "85",
      "below",
      "don",
      "consider",
      "following",
      "any",
      "applications"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare external databases",
    "content": "You must use external databases for OPTIC Management Toolkit (OMT) and the suite in a production environment. For information about supported external databases, see Support matrix for on-premises deployment. For instructions on OMT database configuration, see the OMT documentation. This topic includes steps to configure the pg_hba.conf file for PostgreSQL and AlloyDB Omni databases. For more information about how to configure other parameters such as connection type, database name, user name, client IP address range, and authentication method, refer to the PostgreSQL documentation. For example, the PostgreSQL 14 documentation. In this release, support for AlloyDB Omni is under controlled availability. Please contact our product management team before you begin the implementation in your production environment. General requirements for external databases To set up the PostgreSQL or AlloyDB Omni database, you must meet the following requirements: All configurations described in this top",
    "url": "prepareexternaldbs",
    "filename": "prepareexternaldbs",
    "headings": [
      "General requirements for external databases",
      "Install a database server",
      "Install PostgreSQL",
      "Install AlloyDB Omni",
      "Check the template0 database",
      "Prepare external database for the suite",
      "Check your database instance for duplicate database names",
      "Configure the postgresql.conf file",
      "Update the pg_hba.conf file",
      "Restart the database server",
      "Install and configure Vertica for FinOps"
    ],
    "keywords": [
      "0.5",
      "0.70",
      "0.65",
      "172.0.0",
      "postgresql.conf",
      "lifecycle.log",
      "server.crt",
      "0.75",
      "en_US.utf8",
      "server.key",
      "pg_hba.conf",
      "sha-256",
      "172.0.0.0",
      "1.1.1",
      "prepare",
      "external",
      "databases",
      "general",
      "requirements",
      "install",
      "database",
      "server",
      "postgresql",
      "alloydb",
      "omni",
      "check",
      "template0",
      "suite",
      "instance",
      "duplicate",
      "names",
      "configure",
      "file",
      "update",
      "restart",
      "vertica",
      "finops",
      "optic",
      "management",
      "toolkit",
      "omt",
      "production",
      "environment.",
      "information",
      "about",
      "supported",
      "see",
      "support",
      "matrix",
      "on-premises",
      "deployment.",
      "instructions",
      "configuration",
      "documentation.",
      "topic",
      "includes",
      "steps",
      "databases.",
      "parameters",
      "such",
      "connection",
      "type",
      "name",
      "user",
      "client",
      "ip",
      "address",
      "range",
      "authentication",
      "method",
      "refer",
      "example",
      "14",
      "release",
      "under",
      "controlled",
      "availability.",
      "please",
      "contact",
      "product",
      "team",
      "before",
      "begin",
      "implementation",
      "set",
      "meet",
      "following",
      "all",
      "configurations",
      "described",
      "including",
      "per",
      "profile",
      "applied.",
      "encoding",
      "utf8",
      "document.",
      "versions",
      "version",
      "matrix."
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare external databases",
    "contentLower": "you must use external databases for optic management toolkit (omt) and the suite in a production environment. for information about supported external databases, see support matrix for on-premises deployment. for instructions on omt database configuration, see the omt documentation. this topic includes steps to configure the pg_hba.conf file for postgresql and alloydb omni databases. for more information about how to configure other parameters such as connection type, database name, user name, client ip address range, and authentication method, refer to the postgresql documentation. for example, the postgresql 14 documentation. in this release, support for alloydb omni is under controlled availability. please contact our product management team before you begin the implementation in your production environment. general requirements for external databases to set up the postgresql or alloydb omni database, you must meet the following requirements: all configurations described in this top",
    "keywordsLower": [
      "0.5",
      "0.70",
      "0.65",
      "172.0.0",
      "postgresql.conf",
      "lifecycle.log",
      "server.crt",
      "0.75",
      "en_us.utf8",
      "server.key",
      "pg_hba.conf",
      "sha-256",
      "172.0.0.0",
      "1.1.1",
      "prepare",
      "external",
      "databases",
      "general",
      "requirements",
      "install",
      "database",
      "server",
      "postgresql",
      "alloydb",
      "omni",
      "check",
      "template0",
      "suite",
      "instance",
      "duplicate",
      "names",
      "configure",
      "file",
      "update",
      "restart",
      "vertica",
      "finops",
      "optic",
      "management",
      "toolkit",
      "omt",
      "production",
      "environment.",
      "information",
      "about",
      "supported",
      "see",
      "support",
      "matrix",
      "on-premises",
      "deployment.",
      "instructions",
      "configuration",
      "documentation.",
      "topic",
      "includes",
      "steps",
      "databases.",
      "parameters",
      "such",
      "connection",
      "type",
      "name",
      "user",
      "client",
      "ip",
      "address",
      "range",
      "authentication",
      "method",
      "refer",
      "example",
      "14",
      "release",
      "under",
      "controlled",
      "availability.",
      "please",
      "contact",
      "product",
      "team",
      "before",
      "begin",
      "implementation",
      "set",
      "meet",
      "following",
      "all",
      "configurations",
      "described",
      "including",
      "per",
      "profile",
      "applied.",
      "encoding",
      "utf8",
      "document.",
      "versions",
      "version",
      "matrix."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for UD/UCMDB",
    "content": "UD/UCMDB installation in Helm mode requires the following Persistent Volumes (PVs). PVs are relevant to NFS volumes. Component Persistent volume name Description UD/UCMDB ucmdb-config-volume PV for configuration UD/UCMDB ucmdb-data-volume PV for data UD/UCMDB ucmdb-log-volume PV for logs If you already have the PVs available for use, you need to ensure that the storageClassName attribute of the PVs is the same as the storage class name in the values.yaml file. In the later step, you will need to specify the name of PV to be bound with one of the persistent volume claims (PVCs). If you want to create the required PVs from scratch, perform the following tasks: Navigate to the chart package directory. For example: cd UCMDB_Helm_Chart-2x.x/ucmdb-helm-charts/samples/ Open the ucmdb-pv.yaml file with a text editor, provide the FQDN of the NFS server and the NFS directory path that you provided during the creation of the NFS volumes, and then save the file. By default, the storageClassName at",
    "url": "cmswithsmaxpreparepvs",
    "filename": "cmswithsmaxpreparepvs",
    "headings": [],
    "keywords": [
      "uducmdb",
      "values.yaml",
      "pv.yaml",
      "prepare",
      "persistent",
      "volumes",
      "ud",
      "ucmdb",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "nfs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "ucmdb-config-volume",
      "pv",
      "configuration",
      "ucmdb-data-volume",
      "data",
      "ucmdb-log-volume",
      "logs",
      "already",
      "available",
      "need",
      "ensure",
      "storageclassname",
      "attribute",
      "same",
      "storage",
      "class",
      "file.",
      "later",
      "step",
      "specify",
      "bound",
      "one",
      "claims",
      "pvcs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "navigate",
      "chart",
      "package",
      "directory.",
      "example",
      "cd",
      "ucmdb-helm-charts",
      "samples",
      "open",
      "ucmdb-pv.yaml",
      "file",
      "text",
      "editor",
      "provide",
      "fqdn",
      "server",
      "directory",
      "path",
      "provided",
      "during",
      "creation",
      "save",
      "default",
      "empty.",
      "change",
      "value",
      "make",
      "sure",
      "together.",
      "run",
      "command",
      "kubectl",
      "-f",
      "verify",
      "get",
      "grep"
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for ud/ucmdb",
    "contentLower": "ud/ucmdb installation in helm mode requires the following persistent volumes (pvs). pvs are relevant to nfs volumes. component persistent volume name description ud/ucmdb ucmdb-config-volume pv for configuration ud/ucmdb ucmdb-data-volume pv for data ud/ucmdb ucmdb-log-volume pv for logs if you already have the pvs available for use, you need to ensure that the storageclassname attribute of the pvs is the same as the storage class name in the values.yaml file. in the later step, you will need to specify the name of pv to be bound with one of the persistent volume claims (pvcs). if you want to create the required pvs from scratch, perform the following tasks: navigate to the chart package directory. for example: cd ucmdb_helm_chart-2x.x/ucmdb-helm-charts/samples/ open the ucmdb-pv.yaml file with a text editor, provide the fqdn of the nfs server and the nfs directory path that you provided during the creation of the nfs volumes, and then save the file. by default, the storageclassname at",
    "keywordsLower": [
      "uducmdb",
      "values.yaml",
      "pv.yaml",
      "prepare",
      "persistent",
      "volumes",
      "ud",
      "ucmdb",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "nfs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "ucmdb-config-volume",
      "pv",
      "configuration",
      "ucmdb-data-volume",
      "data",
      "ucmdb-log-volume",
      "logs",
      "already",
      "available",
      "need",
      "ensure",
      "storageclassname",
      "attribute",
      "same",
      "storage",
      "class",
      "file.",
      "later",
      "step",
      "specify",
      "bound",
      "one",
      "claims",
      "pvcs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "navigate",
      "chart",
      "package",
      "directory.",
      "example",
      "cd",
      "ucmdb-helm-charts",
      "samples",
      "open",
      "ucmdb-pv.yaml",
      "file",
      "text",
      "editor",
      "provide",
      "fqdn",
      "server",
      "directory",
      "path",
      "provided",
      "during",
      "creation",
      "save",
      "default",
      "empty.",
      "change",
      "value",
      "make",
      "sure",
      "together.",
      "run",
      "command",
      "kubectl",
      "-f",
      "verify",
      "get",
      "grep"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare external databases for UD/UCMDB",
    "content": "For this release, you must use separate PostgreSQL databases instances for Service Management and the containerized UD/UCMDB for these reasons: Because of the different dimensions of Service Management and UD/UCMDB database sizing, if Service Management and UD/UCMDB share the same PostgreSQL database, the PostgreSQL instance size will be much larger than the size of separate ones. There are unknown issues and risks if Service Management and UD/UCMDB share the same PostgreSQL database. To prepare PostgreSQL for UD/UCMDB, perform the following tasks. Install PostgreSQL Follow the official PostgreSQL documentation for installation instructions to create a separate PostgreSQL database server for UD/UCMDB. Note It is strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it. Also, if the database server machine is a virtual machine, the resource MUST be dedicated to the database server. Database se",
    "url": "cmswithsmaxprepareexternaldb",
    "filename": "cmswithsmaxprepareexternaldb",
    "headings": [
      "Install PostgreSQL",
      "Configure database server",
      "Create database users and databases for UD/UCMDB"
    ],
    "keywords": [
      "uducmdb",
      "en_US.UTF",
      "postgresql.conf",
      "prepare",
      "external",
      "databases",
      "ud",
      "ucmdb",
      "install",
      "postgresql",
      "configure",
      "database",
      "server",
      "create",
      "users",
      "release",
      "separate",
      "instances",
      "service",
      "management",
      "containerized",
      "reasons",
      "because",
      "different",
      "dimensions",
      "sizing",
      "share",
      "same",
      "instance",
      "size",
      "much",
      "larger",
      "ones.",
      "there",
      "unknown",
      "issues",
      "risks",
      "database.",
      "perform",
      "following",
      "tasks.",
      "follow",
      "official",
      "documentation",
      "installation",
      "instructions",
      "ucmdb.",
      "note",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "it.",
      "virtual",
      "resource",
      "dedicated",
      "server.",
      "servers",
      "set",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "universal",
      "cmdb",
      "servers.",
      "located",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "system",
      "performance",
      "impacted.",
      "optionally",
      "enable",
      "tls",
      "connections",
      "see",
      "postgresql.",
      "both",
      "ssl",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "details",
      "documentation.",
      "file",
      "parameters",
      "depending",
      "deployment",
      "small",
      "medium"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare external databases for ud/ucmdb",
    "contentLower": "for this release, you must use separate postgresql databases instances for service management and the containerized ud/ucmdb for these reasons: because of the different dimensions of service management and ud/ucmdb database sizing, if service management and ud/ucmdb share the same postgresql database, the postgresql instance size will be much larger than the size of separate ones. there are unknown issues and risks if service management and ud/ucmdb share the same postgresql database. to prepare postgresql for ud/ucmdb, perform the following tasks. install postgresql follow the official postgresql documentation for installation instructions to create a separate postgresql database server for ud/ucmdb. note it is strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it. also, if the database server machine is a virtual machine, the resource must be dedicated to the database server. database se",
    "keywordsLower": [
      "uducmdb",
      "en_us.utf",
      "postgresql.conf",
      "prepare",
      "external",
      "databases",
      "ud",
      "ucmdb",
      "install",
      "postgresql",
      "configure",
      "database",
      "server",
      "create",
      "users",
      "release",
      "separate",
      "instances",
      "service",
      "management",
      "containerized",
      "reasons",
      "because",
      "different",
      "dimensions",
      "sizing",
      "share",
      "same",
      "instance",
      "size",
      "much",
      "larger",
      "ones.",
      "there",
      "unknown",
      "issues",
      "risks",
      "database.",
      "perform",
      "following",
      "tasks.",
      "follow",
      "official",
      "documentation",
      "installation",
      "instructions",
      "ucmdb.",
      "note",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "it.",
      "virtual",
      "resource",
      "dedicated",
      "server.",
      "servers",
      "set",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "universal",
      "cmdb",
      "servers.",
      "located",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "system",
      "performance",
      "impacted.",
      "optionally",
      "enable",
      "tls",
      "connections",
      "see",
      "postgresql.",
      "both",
      "ssl",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "details",
      "documentation.",
      "file",
      "parameters",
      "depending",
      "deployment",
      "small",
      "medium"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for OO Containerized",
    "content": "Operations Orchestration (OO) Containerized installation requires below Persistent Volumes (PVs) and Persistent Volume Claims (PVC). PVs are pieces of storage relevant to NFS volumes, and PVCs are requests for a piece of storage on a volume. Currently, the way OO Containerized uses persistent storage is by providing the names of the required PVCs at OO install time via Helm parameters, which means that both PVs and PVCs must be created upfront. Component Persistent volume name Persistent volume claim name Description OO oo-config-vol oo-config-pvc Used for storing configuration files. OO oo-data-vol oo-data-pvc Used for storing data files. OO oo-logs-vol oo-logs-pvc Used for storing all logs except OO RAS logs. OO oo-data-export-vol oo-data-export-pvc Used for storing database exported data before purging. OO oo-ras-logs-vol oo-ras-logs-pvc Used for storing OO RAS logs. Create persistent volumes Navigate to the OO chart package directory. For example: oo-helm-charts-1.x.x+26.x.x/oo-1.x",
    "url": "399-oocpreparecreatepv",
    "filename": "399-oocpreparecreatepv",
    "headings": [
      "Create persistent volumes",
      "Create persistent volume claims"
    ],
    "keywords": [
      "volumes.yaml",
      "claims.yaml",
      "prepare",
      "persistent",
      "volumes",
      "oo",
      "containerized",
      "create",
      "volume",
      "claims",
      "operations",
      "orchestration",
      "installation",
      "requires",
      "below",
      "pvs",
      "pvc",
      "pieces",
      "storage",
      "relevant",
      "nfs",
      "pvcs",
      "requests",
      "piece",
      "volume.",
      "currently",
      "way",
      "uses",
      "providing",
      "names",
      "required",
      "install",
      "time",
      "via",
      "helm",
      "parameters",
      "means",
      "both",
      "created",
      "upfront.",
      "component",
      "name",
      "claim",
      "description",
      "oo-config-vol",
      "oo-config-pvc",
      "storing",
      "configuration",
      "files.",
      "oo-data-vol",
      "oo-data-pvc",
      "data",
      "oo-logs-vol",
      "oo-logs-pvc",
      "all",
      "logs",
      "except",
      "ras",
      "logs.",
      "oo-data-export-vol",
      "oo-data-export-pvc",
      "database",
      "exported",
      "before",
      "purging.",
      "oo-ras-logs-vol",
      "oo-ras-logs-pvc",
      "navigate",
      "chart",
      "package",
      "directory.",
      "example",
      "oo-helm-charts-1.x.x",
      "26.x.x",
      "oo-1.x.x",
      "oo-helm-charts",
      "samples",
      "open",
      "file",
      "text",
      "editor",
      "add",
      "information",
      "template",
      "save",
      "fqdn",
      "server.",
      "directory",
      "path",
      "provided",
      "during",
      "creation",
      "volumes.",
      "class",
      "needs",
      "default",
      "storageclassname",
      "attribute",
      "empty.",
      "one"
    ],
    "language": "en",
    "word_count": 112,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for oo containerized",
    "contentLower": "operations orchestration (oo) containerized installation requires below persistent volumes (pvs) and persistent volume claims (pvc). pvs are pieces of storage relevant to nfs volumes, and pvcs are requests for a piece of storage on a volume. currently, the way oo containerized uses persistent storage is by providing the names of the required pvcs at oo install time via helm parameters, which means that both pvs and pvcs must be created upfront. component persistent volume name persistent volume claim name description oo oo-config-vol oo-config-pvc used for storing configuration files. oo oo-data-vol oo-data-pvc used for storing data files. oo oo-logs-vol oo-logs-pvc used for storing all logs except oo ras logs. oo oo-data-export-vol oo-data-export-pvc used for storing database exported data before purging. oo oo-ras-logs-vol oo-ras-logs-pvc used for storing oo ras logs. create persistent volumes navigate to the oo chart package directory. for example: oo-helm-charts-1.x.x+26.x.x/oo-1.x",
    "keywordsLower": [
      "volumes.yaml",
      "claims.yaml",
      "prepare",
      "persistent",
      "volumes",
      "oo",
      "containerized",
      "create",
      "volume",
      "claims",
      "operations",
      "orchestration",
      "installation",
      "requires",
      "below",
      "pvs",
      "pvc",
      "pieces",
      "storage",
      "relevant",
      "nfs",
      "pvcs",
      "requests",
      "piece",
      "volume.",
      "currently",
      "way",
      "uses",
      "providing",
      "names",
      "required",
      "install",
      "time",
      "via",
      "helm",
      "parameters",
      "means",
      "both",
      "created",
      "upfront.",
      "component",
      "name",
      "claim",
      "description",
      "oo-config-vol",
      "oo-config-pvc",
      "storing",
      "configuration",
      "files.",
      "oo-data-vol",
      "oo-data-pvc",
      "data",
      "oo-logs-vol",
      "oo-logs-pvc",
      "all",
      "logs",
      "except",
      "ras",
      "logs.",
      "oo-data-export-vol",
      "oo-data-export-pvc",
      "database",
      "exported",
      "before",
      "purging.",
      "oo-ras-logs-vol",
      "oo-ras-logs-pvc",
      "navigate",
      "chart",
      "package",
      "directory.",
      "example",
      "oo-helm-charts-1.x.x",
      "26.x.x",
      "oo-1.x.x",
      "oo-helm-charts",
      "samples",
      "open",
      "file",
      "text",
      "editor",
      "add",
      "information",
      "template",
      "save",
      "fqdn",
      "server.",
      "directory",
      "path",
      "provided",
      "during",
      "creation",
      "volumes.",
      "class",
      "needs",
      "default",
      "storageclassname",
      "attribute",
      "empty.",
      "one"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare external database for OO",
    "content": "To prepare the PostgreSQL database for Operations Orchestration Containerized (OO Containerized), perform the following tasks: Install the PostgreSQL database You can use a PostgreSQL database server for OO Containerized. Please see the PostgreSQL documentation for installation details. It's strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it.If the database server machine is a virtual machine, you must have a dedicated resource as a database server.Set the database servers to the same time zone, daylight savings settings, and time as the OO servers.Install OO in the same LAN as the database servers (without a proxy and firewalls between them). Otherwise, your system's performance may get impacted.Set the machine's system resources where you install the PostgreSQL database according to the on-premises deployment sizing guide. Configure the database server Configure the PostgreSQL databas",
    "url": "oocprepareexternaldb",
    "filename": "oocprepareexternaldb",
    "headings": [
      "Install the PostgreSQL database",
      "Configure the database server",
      "Configure the PostgreSQL database server​​​​​​",
      "Create OO database users and database names"
    ],
    "keywords": [
      "en_US.UTF",
      "impacted.Set",
      "postgresql.conf",
      "7.5",
      "server.Set",
      "4.5",
      "values.yaml",
      "it.If",
      "prepare",
      "external",
      "database",
      "oo",
      "install",
      "postgresql",
      "configure",
      "server",
      "create",
      "users",
      "names",
      "operations",
      "orchestration",
      "containerized",
      "perform",
      "following",
      "tasks",
      "containerized.",
      "please",
      "see",
      "documentation",
      "installation",
      "details.",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "virtual",
      "dedicated",
      "resource",
      "servers",
      "same",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "servers.install",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "system",
      "performance",
      "get",
      "resources",
      "according",
      "on-premises",
      "deployment",
      "sizing",
      "guide.",
      "optional",
      "enable",
      "tls",
      "connections",
      "server.",
      "details",
      "postgresql.",
      "both",
      "ssl",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "documentation.",
      "file",
      "parameters",
      "parameter",
      "value",
      "gb",
      "256",
      "mb",
      "1400",
      "300",
      "default",
      "localhost",
      "means",
      "connect",
      "instance.",
      "set",
      "allow",
      "remote",
      "connections.",
      "share",
      "suite",
      "components",
      "make"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare external database for oo",
    "contentLower": "to prepare the postgresql database for operations orchestration containerized (oo containerized), perform the following tasks: install the postgresql database you can use a postgresql database server for oo containerized. please see the postgresql documentation for installation details. it's strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it.if the database server machine is a virtual machine, you must have a dedicated resource as a database server.set the database servers to the same time zone, daylight savings settings, and time as the oo servers.install oo in the same lan as the database servers (without a proxy and firewalls between them). otherwise, your system's performance may get impacted.set the machine's system resources where you install the postgresql database according to the on-premises deployment sizing guide. configure the database server configure the postgresql databas",
    "keywordsLower": [
      "en_us.utf",
      "impacted.set",
      "postgresql.conf",
      "7.5",
      "server.set",
      "4.5",
      "values.yaml",
      "it.if",
      "prepare",
      "external",
      "database",
      "oo",
      "install",
      "postgresql",
      "configure",
      "server",
      "create",
      "users",
      "names",
      "operations",
      "orchestration",
      "containerized",
      "perform",
      "following",
      "tasks",
      "containerized.",
      "please",
      "see",
      "documentation",
      "installation",
      "details.",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "virtual",
      "dedicated",
      "resource",
      "servers",
      "same",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "servers.install",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "system",
      "performance",
      "get",
      "resources",
      "according",
      "on-premises",
      "deployment",
      "sizing",
      "guide.",
      "optional",
      "enable",
      "tls",
      "connections",
      "server.",
      "details",
      "postgresql.",
      "both",
      "ssl",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "documentation.",
      "file",
      "parameters",
      "parameter",
      "value",
      "gb",
      "256",
      "mb",
      "1400",
      "300",
      "default",
      "localhost",
      "means",
      "connect",
      "instance.",
      "set",
      "allow",
      "remote",
      "connections.",
      "share",
      "suite",
      "components",
      "make"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Post installation tasks for OO Containerized",
    "content": "After successful installation of OO Containerized, perform the following task: Import OO certificate into the suite.",
    "url": "postinstalltasksoo",
    "filename": "postinstalltasksoo",
    "headings": [],
    "keywords": [
      "post",
      "installation",
      "tasks",
      "oo",
      "containerized",
      "after",
      "successful",
      "perform",
      "following",
      "task",
      "import",
      "certificate",
      "suite."
    ],
    "language": "en",
    "word_count": 17,
    "importance_score": 1.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "post installation tasks for oo containerized",
    "contentLower": "after successful installation of oo containerized, perform the following task: import oo certificate into the suite.",
    "keywordsLower": [
      "post",
      "installation",
      "tasks",
      "oo",
      "containerized",
      "after",
      "successful",
      "perform",
      "following",
      "task",
      "import",
      "certificate",
      "suite."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for Audit",
    "content": "Audit service installation in Helm mode requires the following Persistent Volumes (PVs). PVs are relevant to NFS volumes. Component Persistent volume name Description ITOM Audit as-vault-volume-<Audit Namespace> PV for configuration ITOM Audit as-log-volume-<Audit Namespace> PV for logs If you want to create the required PVs from scratch, perform the following tasks: Navigate to the audit install directory audit-<version> that you created when downloading the audit Helm chart. For example: cd <audit_install-dir>/audit-2x.x Open the itom-audit-pv.yaml file with a text editor, give the FQDN of the NFS server and the NFS directory path that you provided during the creation of the NFS volumes, and then save the file. Run the following command to create persistent volumes: kubectl create -f itom-audit-pv.yaml Output: persistentvolume/as-vault-volume-auditns created persistentvolume/as-log-volume-auditns created Run the following command to verify the volume creation: kubectl get pv |grep au",
    "url": "preparepv4audit",
    "filename": "preparepv4audit",
    "headings": [],
    "keywords": [
      "pv.yaml",
      "prepare",
      "persistent",
      "volumes",
      "audit",
      "service",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "nfs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "itom",
      "as-vault-volume-",
      "pv",
      "configuration",
      "as-log-volume-",
      "logs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "navigate",
      "install",
      "directory",
      "audit-",
      "created",
      "downloading",
      "chart.",
      "example",
      "cd",
      "audit-2x.x",
      "open",
      "itom-audit-pv.yaml",
      "file",
      "text",
      "editor",
      "give",
      "fqdn",
      "server",
      "path",
      "provided",
      "during",
      "creation",
      "save",
      "file.",
      "run",
      "command",
      "kubectl",
      "-f",
      "output",
      "persistentvolume",
      "as-vault-volume-auditns",
      "as-log-volume-auditns",
      "verify",
      "get",
      "grep",
      "5gi",
      "rwx",
      "retain",
      "available",
      "itom-audit-sc",
      "17s"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for audit",
    "contentLower": "audit service installation in helm mode requires the following persistent volumes (pvs). pvs are relevant to nfs volumes. component persistent volume name description itom audit as-vault-volume-<audit namespace> pv for configuration itom audit as-log-volume-<audit namespace> pv for logs if you want to create the required pvs from scratch, perform the following tasks: navigate to the audit install directory audit-<version> that you created when downloading the audit helm chart. for example: cd <audit_install-dir>/audit-2x.x open the itom-audit-pv.yaml file with a text editor, give the fqdn of the nfs server and the nfs directory path that you provided during the creation of the nfs volumes, and then save the file. run the following command to create persistent volumes: kubectl create -f itom-audit-pv.yaml output: persistentvolume/as-vault-volume-auditns created persistentvolume/as-log-volume-auditns created run the following command to verify the volume creation: kubectl get pv |grep au",
    "keywordsLower": [
      "pv.yaml",
      "prepare",
      "persistent",
      "volumes",
      "audit",
      "service",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "nfs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "itom",
      "as-vault-volume-",
      "pv",
      "configuration",
      "as-log-volume-",
      "logs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "navigate",
      "install",
      "directory",
      "audit-",
      "created",
      "downloading",
      "chart.",
      "example",
      "cd",
      "audit-2x.x",
      "open",
      "itom-audit-pv.yaml",
      "file",
      "text",
      "editor",
      "give",
      "fqdn",
      "server",
      "path",
      "provided",
      "during",
      "creation",
      "save",
      "file.",
      "run",
      "command",
      "kubectl",
      "-f",
      "output",
      "persistentvolume",
      "as-vault-volume-auditns",
      "as-log-volume-auditns",
      "verify",
      "get",
      "grep",
      "5gi",
      "rwx",
      "retain",
      "available",
      "itom-audit-sc",
      "17s"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare an external database for Audit",
    "content": "To prepare PostgreSQL, perform the following tasks. Install PostgreSQL You must create a separate PostgreSQL database for Audit service. This database can share the same PostgreSQL instance as Service Management. Make sure you enable SSL on the PostgreSQL because Audit uses SSL to communicate with PostgreSQL. Instructions on how to create a user and a database for a service for PostgreSQL are provided in the PostgreSQL section below. Note It is strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it. Also, if the database server machine is a virtual machine, the resource must be dedicated to the database server. Database servers must be set to the same time zone, daylight savings settings, and time as the Audit server. The Audit server should be located in the same LAN as the database servers (without a proxy and firewalls between them). Otherwise, your system's performance may be impacted. ",
    "url": "prepareexternaldb4audit",
    "filename": "prepareexternaldb4audit",
    "headings": [
      "Install PostgreSQL",
      "Configure PostgreSQL database server",
      "PostgreSQL"
    ],
    "keywords": [
      "en_US.UTF",
      "postgresql.conf",
      "prepare",
      "external",
      "database",
      "audit",
      "install",
      "postgresql",
      "configure",
      "server",
      "perform",
      "following",
      "tasks.",
      "create",
      "separate",
      "service.",
      "share",
      "same",
      "instance",
      "service",
      "management.",
      "make",
      "sure",
      "enable",
      "ssl",
      "because",
      "uses",
      "communicate",
      "postgresql.",
      "instructions",
      "user",
      "provided",
      "section",
      "below.",
      "note",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "it.",
      "virtual",
      "resource",
      "dedicated",
      "server.",
      "servers",
      "set",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "located",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "system",
      "performance",
      "impacted.",
      "creating",
      "follow",
      "steps",
      "tls",
      "connections",
      "both",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "details",
      "see",
      "documentation",
      "file",
      "parameters",
      "depending",
      "itom",
      "deployment",
      "size",
      "small",
      "medium",
      "large",
      "default",
      "value",
      "parameter",
      "localhost",
      "means",
      "connect",
      "instance.",
      "need",
      "allow",
      "remote",
      "connections.",
      "table",
      "provides",
      "example"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare an external database for audit",
    "contentLower": "to prepare postgresql, perform the following tasks. install postgresql you must create a separate postgresql database for audit service. this database can share the same postgresql instance as service management. make sure you enable ssl on the postgresql because audit uses ssl to communicate with postgresql. instructions on how to create a user and a database for a service for postgresql are provided in the postgresql section below. note it is strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it. also, if the database server machine is a virtual machine, the resource must be dedicated to the database server. database servers must be set to the same time zone, daylight savings settings, and time as the audit server. the audit server should be located in the same lan as the database servers (without a proxy and firewalls between them). otherwise, your system's performance may be impacted. ",
    "keywordsLower": [
      "en_us.utf",
      "postgresql.conf",
      "prepare",
      "external",
      "database",
      "audit",
      "install",
      "postgresql",
      "configure",
      "server",
      "perform",
      "following",
      "tasks.",
      "create",
      "separate",
      "service.",
      "share",
      "same",
      "instance",
      "service",
      "management.",
      "make",
      "sure",
      "enable",
      "ssl",
      "because",
      "uses",
      "communicate",
      "postgresql.",
      "instructions",
      "user",
      "provided",
      "section",
      "below.",
      "note",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "it.",
      "virtual",
      "resource",
      "dedicated",
      "server.",
      "servers",
      "set",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "located",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "system",
      "performance",
      "impacted.",
      "creating",
      "follow",
      "steps",
      "tls",
      "connections",
      "both",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "details",
      "see",
      "documentation",
      "file",
      "parameters",
      "depending",
      "itom",
      "deployment",
      "size",
      "small",
      "medium",
      "large",
      "default",
      "value",
      "parameter",
      "localhost",
      "means",
      "connect",
      "instance.",
      "need",
      "allow",
      "remote",
      "connections.",
      "table",
      "provides",
      "example"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan for AWS",
    "content": "The following table provides an overview of the topics covered in the Plan section: Topic Description Before you begin Review the suite deployment architecture on AWS, including the benefits of using EKS, deployment components, inter-communication between components, and the design considerations like security and performance. Support matrix Based on your deployment plan, review the system requirements for the suite installation. At the conclusion of this phase, you must have the following resources in place: Bastion node required to communicate with the Kubernetes cluster Worker node(s) to run suite services NFS server to store OMT and suite data Relational database Sizing consideration Collect and evaluate data from your current environment to determine the following: Suite deployment option that suits your environment the best Size and complexity of your suite deployment FAQs List the most frequently asked questions about deploying the suite on AWS with EKS, including generic cloud ",
    "url": "planforawseks",
    "filename": "planforawseks",
    "headings": [],
    "keywords": [
      "plan",
      "aws",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "before",
      "begin",
      "review",
      "suite",
      "deployment",
      "architecture",
      "including",
      "benefits",
      "eks",
      "components",
      "inter-communication",
      "between",
      "design",
      "considerations",
      "like",
      "security",
      "performance.",
      "support",
      "matrix",
      "based",
      "system",
      "requirements",
      "installation.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "data",
      "relational",
      "database",
      "sizing",
      "consideration",
      "collect",
      "evaluate",
      "current",
      "environment",
      "determine",
      "option",
      "suits",
      "best",
      "size",
      "complexity",
      "faqs",
      "list",
      "most",
      "frequently",
      "asked",
      "questions",
      "about",
      "deploying",
      "generic",
      "cloud",
      "specific",
      "only."
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan for aws",
    "contentLower": "the following table provides an overview of the topics covered in the plan section: topic description before you begin review the suite deployment architecture on aws, including the benefits of using eks, deployment components, inter-communication between components, and the design considerations like security and performance. support matrix based on your deployment plan, review the system requirements for the suite installation. at the conclusion of this phase, you must have the following resources in place: bastion node required to communicate with the kubernetes cluster worker node(s) to run suite services nfs server to store omt and suite data relational database sizing consideration collect and evaluate data from your current environment to determine the following: suite deployment option that suits your environment the best size and complexity of your suite deployment faqs list the most frequently asked questions about deploying the suite on aws with eks, including generic cloud ",
    "keywordsLower": [
      "plan",
      "aws",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "before",
      "begin",
      "review",
      "suite",
      "deployment",
      "architecture",
      "including",
      "benefits",
      "eks",
      "components",
      "inter-communication",
      "between",
      "design",
      "considerations",
      "like",
      "security",
      "performance.",
      "support",
      "matrix",
      "based",
      "system",
      "requirements",
      "installation.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "data",
      "relational",
      "database",
      "sizing",
      "consideration",
      "collect",
      "evaluate",
      "current",
      "environment",
      "determine",
      "option",
      "suits",
      "best",
      "size",
      "complexity",
      "faqs",
      "list",
      "most",
      "frequently",
      "asked",
      "questions",
      "about",
      "deploying",
      "generic",
      "cloud",
      "specific",
      "only."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare VPC (FIPS mode)",
    "content": "Role Location IT / Network administrator AWS Management Console To deploy OMT and the suite on AWS with EKS, you must prepare a Virtual Private Cloud (VPC) first and place the other resources in this prepared VPC. There are three options as described below. The first two options are for the scenario where your IT policy allows you to deploy public subnets with private subnets in a single VPC and you need to follow the architecture mentioned in Before you begin. The third option is for the scenarios where your IT policy doesn't allow public subnets to work with private subnets in a single VPC and you have the flexibility to deploy on different cloud deployments on AWS. However, you need to work with your IT/Network administrator to figure out the detailed solutions for this deployment. The below diagram is one example for your reference. You need to consult your IT/Network administrator in the cloud to choose which option to start with. Option 1: Deploy in new VPC To create a new VPC by",
    "url": "eksvpcfips",
    "filename": "eksvpcfips",
    "headings": [
      "Option 1: Deploy in new VPC",
      "Option 2: Deploy in the existing VPC",
      "Option 3: Deploy in existing VPC and subnets",
      "Step 1. Check the requirements for the VPC and subnets",
      "Step 2. Prepare the other network requirements listed in the EKS guide"
    ],
    "keywords": [
      "0.40",
      "10.0.30.0",
      "10.0.10",
      "10.0.50",
      "10.0.10.0",
      "0.50",
      "10.0.40.0",
      "10.0.0",
      "10.0.40",
      "0.20",
      "10.0.20",
      "10.0.20.0",
      "0.30",
      "0.10",
      "10.0.50.0",
      "10.0.30",
      "10.0.0.0",
      "prepare",
      "vpc",
      "fips",
      "mode",
      "option",
      "deploy",
      "new",
      "existing",
      "subnets",
      "step",
      "1.",
      "check",
      "requirements",
      "2.",
      "network",
      "listed",
      "eks",
      "guide",
      "role",
      "location",
      "administrator",
      "aws",
      "management",
      "console",
      "omt",
      "suite",
      "virtual",
      "private",
      "cloud",
      "first",
      "place",
      "resources",
      "prepared",
      "vpc.",
      "there",
      "three",
      "options",
      "described",
      "below.",
      "two",
      "scenario",
      "policy",
      "allows",
      "public",
      "single",
      "need",
      "follow",
      "architecture",
      "mentioned",
      "before",
      "begin.",
      "third",
      "scenarios",
      "doesn",
      "allow",
      "work",
      "flexibility",
      "different",
      "deployments",
      "aws.",
      "however",
      "figure",
      "out",
      "detailed",
      "solutions",
      "deployment.",
      "below",
      "diagram",
      "one",
      "example",
      "reference.",
      "consult",
      "choose",
      "start",
      "with.",
      "create",
      "cloudformation",
      "steps",
      "log",
      "switch",
      "selected",
      "region.",
      "go"
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare vpc (fips mode)",
    "contentLower": "role location it / network administrator aws management console to deploy omt and the suite on aws with eks, you must prepare a virtual private cloud (vpc) first and place the other resources in this prepared vpc. there are three options as described below. the first two options are for the scenario where your it policy allows you to deploy public subnets with private subnets in a single vpc and you need to follow the architecture mentioned in before you begin. the third option is for the scenarios where your it policy doesn't allow public subnets to work with private subnets in a single vpc and you have the flexibility to deploy on different cloud deployments on aws. however, you need to work with your it/network administrator to figure out the detailed solutions for this deployment. the below diagram is one example for your reference. you need to consult your it/network administrator in the cloud to choose which option to start with. option 1: deploy in new vpc to create a new vpc by",
    "keywordsLower": [
      "0.40",
      "10.0.30.0",
      "10.0.10",
      "10.0.50",
      "10.0.10.0",
      "0.50",
      "10.0.40.0",
      "10.0.0",
      "10.0.40",
      "0.20",
      "10.0.20",
      "10.0.20.0",
      "0.30",
      "0.10",
      "10.0.50.0",
      "10.0.30",
      "10.0.0.0",
      "prepare",
      "vpc",
      "fips",
      "mode",
      "option",
      "deploy",
      "new",
      "existing",
      "subnets",
      "step",
      "1.",
      "check",
      "requirements",
      "2.",
      "network",
      "listed",
      "eks",
      "guide",
      "role",
      "location",
      "administrator",
      "aws",
      "management",
      "console",
      "omt",
      "suite",
      "virtual",
      "private",
      "cloud",
      "first",
      "place",
      "resources",
      "prepared",
      "vpc.",
      "there",
      "three",
      "options",
      "described",
      "below.",
      "two",
      "scenario",
      "policy",
      "allows",
      "public",
      "single",
      "need",
      "follow",
      "architecture",
      "mentioned",
      "before",
      "begin.",
      "third",
      "scenarios",
      "doesn",
      "allow",
      "work",
      "flexibility",
      "different",
      "deployments",
      "aws.",
      "however",
      "figure",
      "out",
      "detailed",
      "solutions",
      "deployment.",
      "below",
      "diagram",
      "one",
      "example",
      "reference.",
      "consult",
      "choose",
      "start",
      "with.",
      "create",
      "cloudformation",
      "steps",
      "log",
      "switch",
      "selected",
      "region.",
      "go"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare VPC",
    "content": "Role Location IT / Network administrator AWS Management Console To deploy OMT and the suite on AWS with EKS, you must prepare a Virtual Private Cloud (VPC) first and place the other resources in this prepared VPC. There are three options when deploying a VPC, this section describes these options. The first two options are for the scenario where your IT policy allows you to deploy public subnets with private subnets in a single VPC and you need to follow the architecture mentioned in Before you begin. The third option is for the scenarios where your IT policy doesn't allow public subnets to work with private subnets in a single VPC and you have the flexibility to deploy on different cloud deployments on AWS. However, you need to work with your IT/Network administrator to figure out the detailed solutions for this deployment. The below diagram is one example for your reference. You need to consult your IT/Network administrator in the cloud to choose which option to start with. Option 1: ",
    "url": "eksvpc",
    "filename": "eksvpc",
    "headings": [
      "Option 1: Deploy in new VPC",
      "Option 2: Deploy in the existing VPC",
      "Option 3: Deploy in existing VPC and subnets",
      "Step 1. Check the requirements for the VPC and subnets",
      "Step 2. Prepare the other network requirements listed in the EKS guide"
    ],
    "keywords": [
      "reqs.html",
      "network_reqs.html",
      "10.0.40",
      "10.0.0.0",
      "10.0.50",
      "amazon.com",
      "https://docs.aws.amazon.com/eks/latest/userguide/network_reqs.html",
      "0.10",
      "10.0.30",
      "https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html",
      "0.40",
      "10.0.30.0",
      "10.0.10.0",
      "https://console.aws.amazon.com/vpc/home",
      "0.50",
      "10.0.40.0",
      "10.0.20.0",
      "docs.aws",
      "10.0.50.0",
      "console.aws",
      "10.0.10",
      "10.0.0",
      "0.20",
      "10.0.20",
      "0.30",
      "prepare",
      "vpc",
      "option",
      "deploy",
      "new",
      "existing",
      "subnets",
      "step",
      "1.",
      "check",
      "requirements",
      "2.",
      "network",
      "listed",
      "eks",
      "guide",
      "role",
      "location",
      "administrator",
      "aws",
      "management",
      "console",
      "omt",
      "suite",
      "virtual",
      "private",
      "cloud",
      "first",
      "place",
      "resources",
      "prepared",
      "vpc.",
      "there",
      "three",
      "options",
      "deploying",
      "section",
      "describes",
      "options.",
      "two",
      "scenario",
      "policy",
      "allows",
      "public",
      "single",
      "need",
      "follow",
      "architecture",
      "mentioned",
      "before",
      "begin.",
      "third",
      "scenarios",
      "doesn",
      "allow",
      "work",
      "flexibility",
      "different",
      "deployments",
      "aws.",
      "however",
      "figure",
      "out",
      "detailed",
      "solutions",
      "deployment.",
      "below",
      "diagram",
      "one",
      "example",
      "reference.",
      "consult",
      "choose",
      "start",
      "with."
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare vpc",
    "contentLower": "role location it / network administrator aws management console to deploy omt and the suite on aws with eks, you must prepare a virtual private cloud (vpc) first and place the other resources in this prepared vpc. there are three options when deploying a vpc, this section describes these options. the first two options are for the scenario where your it policy allows you to deploy public subnets with private subnets in a single vpc and you need to follow the architecture mentioned in before you begin. the third option is for the scenarios where your it policy doesn't allow public subnets to work with private subnets in a single vpc and you have the flexibility to deploy on different cloud deployments on aws. however, you need to work with your it/network administrator to figure out the detailed solutions for this deployment. the below diagram is one example for your reference. you need to consult your it/network administrator in the cloud to choose which option to start with. option 1: ",
    "keywordsLower": [
      "reqs.html",
      "network_reqs.html",
      "10.0.40",
      "10.0.0.0",
      "10.0.50",
      "amazon.com",
      "https://docs.aws.amazon.com/eks/latest/userguide/network_reqs.html",
      "0.10",
      "10.0.30",
      "https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html",
      "0.40",
      "10.0.30.0",
      "10.0.10.0",
      "https://console.aws.amazon.com/vpc/home",
      "0.50",
      "10.0.40.0",
      "10.0.20.0",
      "docs.aws",
      "10.0.50.0",
      "console.aws",
      "10.0.10",
      "10.0.0",
      "0.20",
      "10.0.20",
      "0.30",
      "prepare",
      "vpc",
      "option",
      "deploy",
      "new",
      "existing",
      "subnets",
      "step",
      "1.",
      "check",
      "requirements",
      "2.",
      "network",
      "listed",
      "eks",
      "guide",
      "role",
      "location",
      "administrator",
      "aws",
      "management",
      "console",
      "omt",
      "suite",
      "virtual",
      "private",
      "cloud",
      "first",
      "place",
      "resources",
      "prepared",
      "vpc.",
      "there",
      "three",
      "options",
      "deploying",
      "section",
      "describes",
      "options.",
      "two",
      "scenario",
      "policy",
      "allows",
      "public",
      "single",
      "need",
      "follow",
      "architecture",
      "mentioned",
      "before",
      "begin.",
      "third",
      "scenarios",
      "doesn",
      "allow",
      "work",
      "flexibility",
      "different",
      "deployments",
      "aws.",
      "however",
      "figure",
      "out",
      "detailed",
      "solutions",
      "deployment.",
      "below",
      "diagram",
      "one",
      "example",
      "reference.",
      "consult",
      "choose",
      "start",
      "with."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for UD/UCMDB",
    "content": "UD/UCMDB installation in Helm mode requires the following Persistent Volumes (PVs). PVs are relevant to EFS volumes. Component EFS volume name Description Example directory path UD/UCMDB ucmdb-data-volume Stores data that's generated by UD/UCMDB. /var/vols/itom/ucmdb/data_volume UD/UCMDB ucmdb-config-volume Stores UD/UCMDB configuration files. /var/vols/itom/ucmdb/conf_volume UD/UCMDB ucmdb-log-volume Stores logs generated by UD/UCMDB. /var/vols/itom/ucmdb/log_volume If you already have the PVs available for use, you need to ensure that the storageClassName attribute of the PVs is the same as the storage class name in the my-values.yaml file. In the later step, you will need to specify the name of PV to be bound with one of the persistent volume claims (PVCs). If you want to create the required PVs from scratch, perform the following tasks: Log in to the bastion node and navigate to the UD/UCMDB chart package directory. For example: cd UCMDB_Helm_Chart-2x.x/ucmdb-helm-charts/samples/ O",
    "url": "preparepersistentvolumesekscms",
    "filename": "preparepersistentvolumesekscms",
    "headings": [],
    "keywords": [
      "uducmdb",
      "pv.yaml",
      "amazonaws.com",
      "configureCMSEFS.sh",
      "58c3d35d.efs",
      "values.yaml",
      "prepare",
      "persistent",
      "volumes",
      "ud",
      "ucmdb",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "efs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "ucmdb-data-volume",
      "stores",
      "data",
      "generated",
      "ucmdb.",
      "var",
      "vols",
      "itom",
      "ucmdb-config-volume",
      "configuration",
      "files.",
      "ucmdb-log-volume",
      "logs",
      "already",
      "available",
      "need",
      "ensure",
      "storageclassname",
      "attribute",
      "same",
      "storage",
      "class",
      "my-values.yaml",
      "file.",
      "later",
      "step",
      "specify",
      "pv",
      "bound",
      "one",
      "claims",
      "pvcs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "log",
      "bastion",
      "node",
      "navigate",
      "chart",
      "package",
      "directory.",
      "cd",
      "ucmdb-helm-charts",
      "samples",
      "open",
      "ucmdb-pv.yaml",
      "file",
      "text",
      "editor",
      "configure",
      "parameters",
      "save",
      "dns",
      "get",
      "navigating",
      "aws",
      "all",
      "services",
      "cloudformation",
      "stacks",
      "stack",
      "output",
      "find",
      "fileserverdnsname.",
      "paths",
      "three",
      "provided",
      "creating",
      "see"
    ],
    "language": "en",
    "word_count": 116,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for ud/ucmdb",
    "contentLower": "ud/ucmdb installation in helm mode requires the following persistent volumes (pvs). pvs are relevant to efs volumes. component efs volume name description example directory path ud/ucmdb ucmdb-data-volume stores data that's generated by ud/ucmdb. /var/vols/itom/ucmdb/data_volume ud/ucmdb ucmdb-config-volume stores ud/ucmdb configuration files. /var/vols/itom/ucmdb/conf_volume ud/ucmdb ucmdb-log-volume stores logs generated by ud/ucmdb. /var/vols/itom/ucmdb/log_volume if you already have the pvs available for use, you need to ensure that the storageclassname attribute of the pvs is the same as the storage class name in the my-values.yaml file. in the later step, you will need to specify the name of pv to be bound with one of the persistent volume claims (pvcs). if you want to create the required pvs from scratch, perform the following tasks: log in to the bastion node and navigate to the ud/ucmdb chart package directory. for example: cd ucmdb_helm_chart-2x.x/ucmdb-helm-charts/samples/ o",
    "keywordsLower": [
      "uducmdb",
      "pv.yaml",
      "amazonaws.com",
      "configurecmsefs.sh",
      "58c3d35d.efs",
      "values.yaml",
      "prepare",
      "persistent",
      "volumes",
      "ud",
      "ucmdb",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "efs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "ucmdb-data-volume",
      "stores",
      "data",
      "generated",
      "ucmdb.",
      "var",
      "vols",
      "itom",
      "ucmdb-config-volume",
      "configuration",
      "files.",
      "ucmdb-log-volume",
      "logs",
      "already",
      "available",
      "need",
      "ensure",
      "storageclassname",
      "attribute",
      "same",
      "storage",
      "class",
      "my-values.yaml",
      "file.",
      "later",
      "step",
      "specify",
      "pv",
      "bound",
      "one",
      "claims",
      "pvcs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "log",
      "bastion",
      "node",
      "navigate",
      "chart",
      "package",
      "directory.",
      "cd",
      "ucmdb-helm-charts",
      "samples",
      "open",
      "ucmdb-pv.yaml",
      "file",
      "text",
      "editor",
      "configure",
      "parameters",
      "save",
      "dns",
      "get",
      "navigating",
      "aws",
      "all",
      "services",
      "cloudformation",
      "stacks",
      "stack",
      "output",
      "find",
      "fileserverdnsname.",
      "paths",
      "three",
      "provided",
      "creating",
      "see"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes and claims for OO (EKS)",
    "content": "OO Containerized installation requires below Persistent Volumes (PVs) and Persistent Volume Claims (PVCs). PVs are pieces of storage relevant to Amazon Elastic File System volumes, and PVCs are requests for a piece of storage on a volume. Currently, OO uses persistent storage by providing the names of the required PVCs at OO install time via Helm parameters. This means that you must create both PVs and PVCs upfront. Component Amazon Elastic File System volume name Description Example Directory path Size OO oo-config-vol Stores OO configuration files. /var/vols/itom/oo/oo_config_vol ~ 1 GB OO oo-data-vol Stores OO, OO deployed AutoPass, and OO deployed Vault-generated data. /var/vols/itom/oo/oo_data_vol ~ 1 GB OO oo-logs-vol Stores logs generated by the OO components, except RAS logs which are stored in a different volume. /var/vols/itom/oo/oo_logs_vol 1-5 GB OO oo-data-export-vol OO runs a Kubernetes cron job for purging OO database with a retention period set to 375 days (default valu",
    "url": "399-preparepvooceks",
    "filename": "399-preparepvooceks",
    "headings": [
      "Create persistent volumes",
      "Create persistent volume claims"
    ],
    "keywords": [
      "volumes.yaml",
      "claims.yaml",
      "encrypted.yaml",
      "prepare",
      "persistent",
      "volumes",
      "claims",
      "oo",
      "eks",
      "create",
      "volume",
      "containerized",
      "installation",
      "requires",
      "below",
      "pvs",
      "pvcs",
      "pieces",
      "storage",
      "relevant",
      "amazon",
      "elastic",
      "file",
      "system",
      "requests",
      "piece",
      "volume.",
      "currently",
      "uses",
      "providing",
      "names",
      "required",
      "install",
      "time",
      "via",
      "helm",
      "parameters.",
      "means",
      "both",
      "upfront.",
      "component",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "size",
      "oo-config-vol",
      "stores",
      "configuration",
      "files.",
      "var",
      "vols",
      "itom",
      "gb",
      "oo-data-vol",
      "deployed",
      "autopass",
      "vault-generated",
      "data.",
      "oo-logs-vol",
      "logs",
      "generated",
      "components",
      "except",
      "ras",
      "stored",
      "different",
      "1-5",
      "oo-data-export-vol",
      "runs",
      "kubernetes",
      "cron",
      "job",
      "purging",
      "database",
      "retention",
      "period",
      "set",
      "375",
      "days",
      "default",
      "value",
      "all",
      "data",
      "marked",
      "purge",
      "gets",
      "exported",
      "first",
      "exposed",
      "before",
      "purging.",
      "50-90",
      "see",
      "notes",
      "oo-ras-logs-vol",
      "logs.",
      "maintenance",
      "export"
    ],
    "language": "en",
    "word_count": 133,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes and claims for oo (eks)",
    "contentLower": "oo containerized installation requires below persistent volumes (pvs) and persistent volume claims (pvcs). pvs are pieces of storage relevant to amazon elastic file system volumes, and pvcs are requests for a piece of storage on a volume. currently, oo uses persistent storage by providing the names of the required pvcs at oo install time via helm parameters. this means that you must create both pvs and pvcs upfront. component amazon elastic file system volume name description example directory path size oo oo-config-vol stores oo configuration files. /var/vols/itom/oo/oo_config_vol ~ 1 gb oo oo-data-vol stores oo, oo deployed autopass, and oo deployed vault-generated data. /var/vols/itom/oo/oo_data_vol ~ 1 gb oo oo-logs-vol stores logs generated by the oo components, except ras logs which are stored in a different volume. /var/vols/itom/oo/oo_logs_vol 1-5 gb oo oo-data-export-vol oo runs a kubernetes cron job for purging oo database with a retention period set to 375 days (default valu",
    "keywordsLower": [
      "volumes.yaml",
      "claims.yaml",
      "encrypted.yaml",
      "prepare",
      "persistent",
      "volumes",
      "claims",
      "oo",
      "eks",
      "create",
      "volume",
      "containerized",
      "installation",
      "requires",
      "below",
      "pvs",
      "pvcs",
      "pieces",
      "storage",
      "relevant",
      "amazon",
      "elastic",
      "file",
      "system",
      "requests",
      "piece",
      "volume.",
      "currently",
      "uses",
      "providing",
      "names",
      "required",
      "install",
      "time",
      "via",
      "helm",
      "parameters.",
      "means",
      "both",
      "upfront.",
      "component",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "size",
      "oo-config-vol",
      "stores",
      "configuration",
      "files.",
      "var",
      "vols",
      "itom",
      "gb",
      "oo-data-vol",
      "deployed",
      "autopass",
      "vault-generated",
      "data.",
      "oo-logs-vol",
      "logs",
      "generated",
      "components",
      "except",
      "ras",
      "stored",
      "different",
      "1-5",
      "oo-data-export-vol",
      "runs",
      "kubernetes",
      "cron",
      "job",
      "purging",
      "database",
      "retention",
      "period",
      "set",
      "375",
      "days",
      "default",
      "value",
      "all",
      "data",
      "marked",
      "purge",
      "gets",
      "exported",
      "first",
      "exposed",
      "before",
      "purging.",
      "50-90",
      "see",
      "notes",
      "oo-ras-logs-vol",
      "logs.",
      "maintenance",
      "export"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Post installation task for OO Containerized on AWS (EKS)",
    "content": "After successful installation of OO Containerized, perform the following tasks: Configure load balancers for OO Import OO certificate into the suite",
    "url": "postooinstalleks",
    "filename": "postooinstalleks",
    "headings": [],
    "keywords": [
      "post",
      "installation",
      "task",
      "oo",
      "containerized",
      "aws",
      "eks",
      "after",
      "successful",
      "perform",
      "following",
      "tasks",
      "configure",
      "load",
      "balancers",
      "import",
      "certificate",
      "suite"
    ],
    "language": "en",
    "word_count": 23,
    "importance_score": 1.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "post installation task for oo containerized on aws (eks)",
    "contentLower": "after successful installation of oo containerized, perform the following tasks: configure load balancers for oo import oo certificate into the suite",
    "keywordsLower": [
      "post",
      "installation",
      "task",
      "oo",
      "containerized",
      "aws",
      "eks",
      "after",
      "successful",
      "perform",
      "following",
      "tasks",
      "configure",
      "load",
      "balancers",
      "import",
      "certificate",
      "suite"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for Audit service",
    "content": "Audit service installation in Helm mode requires the following Persistent Volumes (PVs). PVs are relevant to Elastic File System (EFS) volumes. Component Persistent volume name Description ITOM Audit as-vault-volume-<Audit Namespace> PV for configuration ITOM Audit as-log-volume-<Audit Namespace> PV for logs If you want to create the required PVs from scratch, perform the following tasks: Navigate to the audit install directory (audit-<version>) that you created when downloading the audit Helm chart. For example: cd <audit_install-dir>/audit-2x.x To find or verify the FQDN of the EFS server and the EFS directory path, run the following two commands. To find the name of the config volume, run the following command: kubectl get pv | grep 'config' To describe the PV, run this command: kubectl describe pv <pv name of config volume that you found from the previous step> Part of the output: ... Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: aws-efs.example.net Path",
    "url": "preparepveks4audit",
    "filename": "preparepveks4audit",
    "headings": [],
    "keywords": [
      "pv.yaml",
      "example.net",
      "prepare",
      "persistent",
      "volumes",
      "audit",
      "service",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "elastic",
      "file",
      "system",
      "efs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "itom",
      "as-vault-volume-",
      "pv",
      "configuration",
      "as-log-volume-",
      "logs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "navigate",
      "install",
      "directory",
      "audit-",
      "created",
      "downloading",
      "chart.",
      "example",
      "cd",
      "audit-2x.x",
      "find",
      "verify",
      "fqdn",
      "server",
      "path",
      "run",
      "two",
      "commands.",
      "config",
      "command",
      "kubectl",
      "get",
      "grep",
      "describe",
      "part",
      "output",
      "source",
      "type",
      "nfs",
      "mount",
      "lasts",
      "lifetime",
      "pod",
      "aws-efs.example.net",
      "var",
      "vols",
      "itsma",
      "config-volume",
      "readonly",
      "false",
      "gives",
      "information",
      "need",
      "give",
      "itom-audit-pv.yaml",
      "start",
      "same",
      "till",
      "directory.",
      "shown",
      "attribute",
      "config-volume.",
      "open",
      "text",
      "editor",
      "provided",
      "during",
      "creation",
      "save",
      "file.",
      "-f",
      "persistentvolume",
      "as-vault-volume-auditns",
      "as-log-volume-auditns",
      "5gi"
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for audit service",
    "contentLower": "audit service installation in helm mode requires the following persistent volumes (pvs). pvs are relevant to elastic file system (efs) volumes. component persistent volume name description itom audit as-vault-volume-<audit namespace> pv for configuration itom audit as-log-volume-<audit namespace> pv for logs if you want to create the required pvs from scratch, perform the following tasks: navigate to the audit install directory (audit-<version>) that you created when downloading the audit helm chart. for example: cd <audit_install-dir>/audit-2x.x to find or verify the fqdn of the efs server and the efs directory path, run the following two commands. to find the name of the config volume, run the following command: kubectl get pv | grep 'config' to describe the pv, run this command: kubectl describe pv <pv name of config volume that you found from the previous step> part of the output: ... source: type: nfs (an nfs mount that lasts the lifetime of a pod) server: aws-efs.example.net path",
    "keywordsLower": [
      "pv.yaml",
      "example.net",
      "prepare",
      "persistent",
      "volumes",
      "audit",
      "service",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "elastic",
      "file",
      "system",
      "efs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "itom",
      "as-vault-volume-",
      "pv",
      "configuration",
      "as-log-volume-",
      "logs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "navigate",
      "install",
      "directory",
      "audit-",
      "created",
      "downloading",
      "chart.",
      "example",
      "cd",
      "audit-2x.x",
      "find",
      "verify",
      "fqdn",
      "server",
      "path",
      "run",
      "two",
      "commands.",
      "config",
      "command",
      "kubectl",
      "get",
      "grep",
      "describe",
      "part",
      "output",
      "source",
      "type",
      "nfs",
      "mount",
      "lasts",
      "lifetime",
      "pod",
      "aws-efs.example.net",
      "var",
      "vols",
      "itsma",
      "config-volume",
      "readonly",
      "false",
      "gives",
      "information",
      "need",
      "give",
      "itom-audit-pv.yaml",
      "start",
      "same",
      "till",
      "directory.",
      "shown",
      "attribute",
      "config-volume.",
      "open",
      "text",
      "editor",
      "provided",
      "during",
      "creation",
      "save",
      "file.",
      "-f",
      "persistentvolume",
      "as-vault-volume-auditns",
      "as-log-volume-auditns",
      "5gi"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan for Azure",
    "content": "The following table provides an overview of the topics covered in the Plan section: Topic Description Before you begin Review the suite deployment architecture on Azure, including the benefits of using AKS, deployment components, inter-communication between components, and the design considerations like security and performance. Support matrix Based on your deployment plan, review the system requirements for the installation of suite. At the conclusion of this phase, you must have the following resources in place: Bastion node required to communicate with the Kubernetes cluster Worker node(s) to run suite services NFS server to store OMT and suite data Relational database Sizing consideration Collect and evaluate data from your current environment to determine the following: Suite deployment option that suits your environment the best Size and complexity of your suite deployment FAQs List the most frequently asked questions about deploying the suite on Azure with AKS, including generic",
    "url": "planforazureaks",
    "filename": "planforazureaks",
    "headings": [],
    "keywords": [
      "plan",
      "azure",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "before",
      "begin",
      "review",
      "suite",
      "deployment",
      "architecture",
      "including",
      "benefits",
      "aks",
      "components",
      "inter-communication",
      "between",
      "design",
      "considerations",
      "like",
      "security",
      "performance.",
      "support",
      "matrix",
      "based",
      "system",
      "requirements",
      "installation",
      "suite.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "data",
      "relational",
      "database",
      "sizing",
      "consideration",
      "collect",
      "evaluate",
      "current",
      "environment",
      "determine",
      "option",
      "suits",
      "best",
      "size",
      "complexity",
      "faqs",
      "list",
      "most",
      "frequently",
      "asked",
      "questions",
      "about",
      "deploying",
      "generic",
      "cloud",
      "specific",
      "only."
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan for azure",
    "contentLower": "the following table provides an overview of the topics covered in the plan section: topic description before you begin review the suite deployment architecture on azure, including the benefits of using aks, deployment components, inter-communication between components, and the design considerations like security and performance. support matrix based on your deployment plan, review the system requirements for the installation of suite. at the conclusion of this phase, you must have the following resources in place: bastion node required to communicate with the kubernetes cluster worker node(s) to run suite services nfs server to store omt and suite data relational database sizing consideration collect and evaluate data from your current environment to determine the following: suite deployment option that suits your environment the best size and complexity of your suite deployment faqs list the most frequently asked questions about deploying the suite on azure with aks, including generic",
    "keywordsLower": [
      "plan",
      "azure",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "before",
      "begin",
      "review",
      "suite",
      "deployment",
      "architecture",
      "including",
      "benefits",
      "aks",
      "components",
      "inter-communication",
      "between",
      "design",
      "considerations",
      "like",
      "security",
      "performance.",
      "support",
      "matrix",
      "based",
      "system",
      "requirements",
      "installation",
      "suite.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "data",
      "relational",
      "database",
      "sizing",
      "consideration",
      "collect",
      "evaluate",
      "current",
      "environment",
      "determine",
      "option",
      "suits",
      "best",
      "size",
      "complexity",
      "faqs",
      "list",
      "most",
      "frequently",
      "asked",
      "questions",
      "about",
      "deploying",
      "generic",
      "cloud",
      "specific",
      "only."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare File Storage - Subscribe NetApp Files (Helm)",
    "content": "This release supports NetApp Files or Azure Files and Azure Disks as the storage service on Azure. To deploy the suite using Azure Files and Azure Disks, refer to Subscribe premium Azure Files and Azure Disks. To use Azure NetApp Files as the storage service, you can use either of the following methods. Option 1: Azure NetApp Files static provision. Option 2: Azure NetApp Files dynamic provision. You must set up Azure NetApp file for your deployment before you create the storage volume according to the prefered method. Set up Azure NetApp Files Role Location Storage administrator Bastion node and Azure portal You must first register for NetApp Files and NetApp Resource Provider with your subscription details before setting up the Azure NetApp files. To registered, complete the following steps. Run the following commands on the bastion: az login #Specify the subscription that has been allowlisted for Azure NetApp Files az account set --subscription <subscription_id> #Register the Azure ",
    "url": "aksnfshelm",
    "filename": "aksnfshelm",
    "headings": [
      "Set up Azure NetApp Files",
      "Azure NetApp Files static provision",
      "Azure NetApp Files dynamic provision."
    ],
    "keywords": [
      "azure.com",
      "192.168.0",
      "NFSv4.xx",
      "192.168",
      "192.168.0.0",
      "createNFS.sh",
      "10.1.3",
      "https://portal.azure.com",
      "10.1.3.4",
      "prepare",
      "file",
      "storage",
      "subscribe",
      "netapp",
      "files",
      "helm",
      "set",
      "azure",
      "static",
      "provision",
      "dynamic",
      "provision.",
      "release",
      "supports",
      "disks",
      "service",
      "azure.",
      "deploy",
      "suite",
      "refer",
      "premium",
      "disks.",
      "either",
      "following",
      "methods.",
      "option",
      "deployment",
      "before",
      "create",
      "volume",
      "according",
      "prefered",
      "method.",
      "role",
      "location",
      "administrator",
      "bastion",
      "node",
      "portal",
      "first",
      "register",
      "resource",
      "provider",
      "subscription",
      "details",
      "setting",
      "files.",
      "registered",
      "complete",
      "steps.",
      "run",
      "commands",
      "az",
      "login",
      "specify",
      "allowlisted",
      "account",
      "--subscription",
      "--namespace",
      "microsoft.netapp",
      "--wait",
      "id.",
      "find",
      "id",
      "home",
      "page",
      "subscriptions.",
      "after",
      "registration",
      "completet",
      "steps",
      "instance",
      "log",
      "https",
      "portal.azure.com.",
      "click",
      "create.",
      "give",
      "name",
      "new",
      "populate",
      "fields",
      "select",
      "drop-down",
      "list.",
      "group",
      "created",
      "build",
      "aks",
      "cluster"
    ],
    "language": "en",
    "word_count": 113,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare file storage - subscribe netapp files (helm)",
    "contentLower": "this release supports netapp files or azure files and azure disks as the storage service on azure. to deploy the suite using azure files and azure disks, refer to subscribe premium azure files and azure disks. to use azure netapp files as the storage service, you can use either of the following methods. option 1: azure netapp files static provision. option 2: azure netapp files dynamic provision. you must set up azure netapp file for your deployment before you create the storage volume according to the prefered method. set up azure netapp files role location storage administrator bastion node and azure portal you must first register for netapp files and netapp resource provider with your subscription details before setting up the azure netapp files. to registered, complete the following steps. run the following commands on the bastion: az login #specify the subscription that has been allowlisted for azure netapp files az account set --subscription <subscription_id> #register the azure ",
    "keywordsLower": [
      "azure.com",
      "192.168.0",
      "nfsv4.xx",
      "192.168",
      "192.168.0.0",
      "createnfs.sh",
      "10.1.3",
      "https://portal.azure.com",
      "10.1.3.4",
      "prepare",
      "file",
      "storage",
      "subscribe",
      "netapp",
      "files",
      "helm",
      "set",
      "azure",
      "static",
      "provision",
      "dynamic",
      "provision.",
      "release",
      "supports",
      "disks",
      "service",
      "azure.",
      "deploy",
      "suite",
      "refer",
      "premium",
      "disks.",
      "either",
      "following",
      "methods.",
      "option",
      "deployment",
      "before",
      "create",
      "volume",
      "according",
      "prefered",
      "method.",
      "role",
      "location",
      "administrator",
      "bastion",
      "node",
      "portal",
      "first",
      "register",
      "resource",
      "provider",
      "subscription",
      "details",
      "setting",
      "files.",
      "registered",
      "complete",
      "steps.",
      "run",
      "commands",
      "az",
      "login",
      "specify",
      "allowlisted",
      "account",
      "--subscription",
      "--namespace",
      "microsoft.netapp",
      "--wait",
      "id.",
      "find",
      "id",
      "home",
      "page",
      "subscriptions.",
      "after",
      "registration",
      "completet",
      "steps",
      "instance",
      "log",
      "https",
      "portal.azure.com.",
      "click",
      "create.",
      "give",
      "name",
      "new",
      "populate",
      "fields",
      "select",
      "drop-down",
      "list.",
      "group",
      "created",
      "build",
      "aks",
      "cluster"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare File Storage - Subscribe premium Azure Files and Azure Disks(Helm)",
    "content": "This release supports NetApp Files or Azure Files and Azure Disks as the storage service on Azure. Due to the unstable performance of Azure Files, it's highly recommended that you deploy the suite using NetApp Files (see Subscribe NetApp Files). To use Azure Files and Azure Disks as the storage service, you can use either of the following methods: Option 1: Azure File static provision and Azure Disk dynamic provisionOption 2: Azure File dynamic provision and Azure Disk dynamic provision You will need to create a premium Azure file share for your deployment first, and then refer to the appropriate section below for the method you want to use. Create premium Azure file share Role Location Storage administrator Bastion node To create and configure premium Azure file share, log on to the bastion and follow these steps: Add network information Go to Security > Networking > Network connectivity Select Enable public access from selected networks and IPSelect IP address, Bastion public IP In t",
    "url": "akspremiumnfshelm",
    "filename": "akspremiumnfshelm",
    "headings": [
      "Create premium Azure file share",
      "Add network information",
      "Create private endpoint",
      "Update AKS Cluster Private Subnet",
      "Create log analytics workspace",
      "Create Azure files in storage account",
      "Using Static provision Azure File and Azure Disk dynamic provision",
      "Using Dynamic provision for both Azure File and Azure Disk"
    ],
    "keywords": [
      "diskshelm",
      "71.8",
      "Microsoft.sql",
      "windows.net",
      "3.0",
      "1.cred",
      "1.file",
      "5.0",
      "0.0713496",
      "5.1",
      "small.img",
      "mountazurefile.sh",
      "prepare",
      "file",
      "storage",
      "subscribe",
      "premium",
      "azure",
      "files",
      "disks",
      "helm",
      "create",
      "share",
      "add",
      "network",
      "information",
      "private",
      "endpoint",
      "update",
      "aks",
      "cluster",
      "subnet",
      "log",
      "analytics",
      "workspace",
      "account",
      "static",
      "provision",
      "disk",
      "dynamic",
      "both",
      "release",
      "supports",
      "netapp",
      "service",
      "azure.",
      "due",
      "unstable",
      "performance",
      "highly",
      "recommended",
      "deploy",
      "suite",
      "see",
      "either",
      "following",
      "methods",
      "option",
      "provisionoption",
      "need",
      "deployment",
      "first",
      "refer",
      "appropriate",
      "section",
      "below",
      "method",
      "want",
      "use.",
      "role",
      "location",
      "administrator",
      "bastion",
      "node",
      "configure",
      "follow",
      "steps",
      "go",
      "security",
      "networking",
      "connectivity",
      "select",
      "enable",
      "public",
      "access",
      "selected",
      "networks",
      "ipselect",
      "ip",
      "address",
      "left",
      "navigation",
      "bar",
      "datastorage",
      "share.create",
      "entry",
      "named",
      "itomfileshare",
      "100",
      "gb.leave"
    ],
    "language": "en",
    "word_count": 122,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare file storage - subscribe premium azure files and azure disks(helm)",
    "contentLower": "this release supports netapp files or azure files and azure disks as the storage service on azure. due to the unstable performance of azure files, it's highly recommended that you deploy the suite using netapp files (see subscribe netapp files). to use azure files and azure disks as the storage service, you can use either of the following methods: option 1: azure file static provision and azure disk dynamic provisionoption 2: azure file dynamic provision and azure disk dynamic provision you will need to create a premium azure file share for your deployment first, and then refer to the appropriate section below for the method you want to use. create premium azure file share role location storage administrator bastion node to create and configure premium azure file share, log on to the bastion and follow these steps: add network information go to security > networking > network connectivity select enable public access from selected networks and ipselect ip address, bastion public ip in t",
    "keywordsLower": [
      "diskshelm",
      "71.8",
      "microsoft.sql",
      "windows.net",
      "3.0",
      "1.cred",
      "1.file",
      "5.0",
      "0.0713496",
      "5.1",
      "small.img",
      "mountazurefile.sh",
      "prepare",
      "file",
      "storage",
      "subscribe",
      "premium",
      "azure",
      "files",
      "disks",
      "helm",
      "create",
      "share",
      "add",
      "network",
      "information",
      "private",
      "endpoint",
      "update",
      "aks",
      "cluster",
      "subnet",
      "log",
      "analytics",
      "workspace",
      "account",
      "static",
      "provision",
      "disk",
      "dynamic",
      "both",
      "release",
      "supports",
      "netapp",
      "service",
      "azure.",
      "due",
      "unstable",
      "performance",
      "highly",
      "recommended",
      "deploy",
      "suite",
      "see",
      "either",
      "following",
      "methods",
      "option",
      "provisionoption",
      "need",
      "deployment",
      "first",
      "refer",
      "appropriate",
      "section",
      "below",
      "method",
      "want",
      "use.",
      "role",
      "location",
      "administrator",
      "bastion",
      "node",
      "configure",
      "follow",
      "steps",
      "go",
      "security",
      "networking",
      "connectivity",
      "select",
      "enable",
      "public",
      "access",
      "selected",
      "networks",
      "ipselect",
      "ip",
      "address",
      "left",
      "navigation",
      "bar",
      "datastorage",
      "share.create",
      "entry",
      "named",
      "itomfileshare",
      "100",
      "gb.leave"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for ESM on Azure",
    "content": "To install the application in the Helm mode, you must create PVs. You can create PVs either using static volume provisioning or dynamic volume provisioning. To learn more about the two methods, see Dynamic vs. static volume provisioning. Create the secret to access Azure Files share You must create the secret under the ESM namespace to access the Azure Files share. Perform these steps only if you choose Azure Files and Azure Disk as the storage service: Log in to the bastion node. Check if a file named /<root>/.kube/config with cluster credentials exists. If the file does not exist, run the sudo az login command to log in to the Azure CLI and follow the instructions in the command output. Run the following command to get the required permission to run the Kubernetes commands. sudo az aks get-credentials --resource-group <resource_group_name> --name <kubernetes_cluster_name> --subscription <subscription_name_or_id> The following table lists the parameters used in this command: AZ aks pa",
    "url": "preparepvsaks",
    "filename": "preparepvsaks",
    "headings": [
      "Create the secret to access Azure Files share",
      "Create PVs using static volume provisioning",
      "Create PVs using dynamic volume provisioning",
      "Azure Disks",
      "Azure Files",
      "Azure NetApp Files",
      "External references"
    ],
    "keywords": [
      "azure.com",
      "1.50",
      "10.1.50",
      "createPv.sh",
      "netapp.io",
      "disk.csi",
      "createPV.sh",
      "storageclass.yaml",
      "10.1.50.4",
      "sc.yaml",
      "storage.k8s",
      "file.csi",
      "anf.yaml",
      "values.yaml",
      "prepare",
      "persistent",
      "volumes",
      "esm",
      "azure",
      "create",
      "secret",
      "access",
      "files",
      "share",
      "pvs",
      "static",
      "volume",
      "provisioning",
      "dynamic",
      "disks",
      "netapp",
      "external",
      "references",
      "install",
      "application",
      "helm",
      "mode",
      "pvs.",
      "either",
      "provisioning.",
      "learn",
      "about",
      "two",
      "methods",
      "see",
      "vs.",
      "under",
      "namespace",
      "share.",
      "perform",
      "steps",
      "choose",
      "disk",
      "storage",
      "service",
      "log",
      "bastion",
      "node.",
      "check",
      "file",
      "named",
      ".kube",
      "config",
      "cluster",
      "credentials",
      "exists.",
      "exist",
      "run",
      "sudo",
      "az",
      "login",
      "command",
      "cli",
      "follow",
      "instructions",
      "output.",
      "following",
      "get",
      "required",
      "permission",
      "kubernetes",
      "commands.",
      "aks",
      "get-credentials",
      "--resource-group",
      "--name",
      "--subscription",
      "table",
      "lists",
      "parameters",
      "description",
      "name",
      "resource",
      "group",
      "created",
      "build",
      "page.",
      "format",
      "cluster.",
      "subscription"
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for esm on azure",
    "contentLower": "to install the application in the helm mode, you must create pvs. you can create pvs either using static volume provisioning or dynamic volume provisioning. to learn more about the two methods, see dynamic vs. static volume provisioning. create the secret to access azure files share you must create the secret under the esm namespace to access the azure files share. perform these steps only if you choose azure files and azure disk as the storage service: log in to the bastion node. check if a file named /<root>/.kube/config with cluster credentials exists. if the file does not exist, run the sudo az login command to log in to the azure cli and follow the instructions in the command output. run the following command to get the required permission to run the kubernetes commands. sudo az aks get-credentials --resource-group <resource_group_name> --name <kubernetes_cluster_name> --subscription <subscription_name_or_id> the following table lists the parameters used in this command: az aks pa",
    "keywordsLower": [
      "azure.com",
      "1.50",
      "10.1.50",
      "createpv.sh",
      "netapp.io",
      "disk.csi",
      "createpv.sh",
      "storageclass.yaml",
      "10.1.50.4",
      "sc.yaml",
      "storage.k8s",
      "file.csi",
      "anf.yaml",
      "values.yaml",
      "prepare",
      "persistent",
      "volumes",
      "esm",
      "azure",
      "create",
      "secret",
      "access",
      "files",
      "share",
      "pvs",
      "static",
      "volume",
      "provisioning",
      "dynamic",
      "disks",
      "netapp",
      "external",
      "references",
      "install",
      "application",
      "helm",
      "mode",
      "pvs.",
      "either",
      "provisioning.",
      "learn",
      "about",
      "two",
      "methods",
      "see",
      "vs.",
      "under",
      "namespace",
      "share.",
      "perform",
      "steps",
      "choose",
      "disk",
      "storage",
      "service",
      "log",
      "bastion",
      "node.",
      "check",
      "file",
      "named",
      ".kube",
      "config",
      "cluster",
      "credentials",
      "exists.",
      "exist",
      "run",
      "sudo",
      "az",
      "login",
      "command",
      "cli",
      "follow",
      "instructions",
      "output.",
      "following",
      "get",
      "required",
      "permission",
      "kubernetes",
      "commands.",
      "aks",
      "get-credentials",
      "--resource-group",
      "--name",
      "--subscription",
      "table",
      "lists",
      "parameters",
      "description",
      "name",
      "resource",
      "group",
      "created",
      "build",
      "page.",
      "format",
      "cluster.",
      "subscription"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare external databases for UD/UCMDB (AKS)",
    "content": "UD/UCMDB contains an internal (embedded) PostgreSQL database instance, which should be used for demonstration purposes only. You must use external databases in a production environment. In addition, you must use separate PostgreSQL databases instances for Service Management and the containerized UD/UCMDB. For the UD/UCMDB deployed on Azure, you need to set up an Azure flexible server as the database engine. This flexible server instance will be used as the external databases for the following UD/UCMDB components: UCMDB Server AutoPass License Service (APLS) This release no longer supports the method of installing PostgreSQL on a virtual machine on Azure. You must Migrate from self-hosted PostgreSQL to Azure Flexible Server. Set up an Azure flexible server To set up an Azure flexible server, follow these steps. Go to the Azure portal (https://portal.azure.com). Search for \"Azure Database for PostgreSQL flexible servers\" and open it. Click Create. On the creation page, enter the followin",
    "url": "prepareexternaldbakscms",
    "filename": "prepareexternaldbakscms",
    "headings": [
      "Set up an Azure flexible server",
      "Set up PostgreSQL databases for UD/UCMDB"
    ],
    "keywords": [
      "uducmdb",
      "azure.com",
      "values.yaml",
      "6.5",
      "https://portal.azure.com",
      "prepare",
      "external",
      "databases",
      "ud",
      "ucmdb",
      "aks",
      "set",
      "azure",
      "flexible",
      "server",
      "postgresql",
      "contains",
      "internal",
      "embedded",
      "database",
      "instance",
      "demonstration",
      "purposes",
      "only.",
      "production",
      "environment.",
      "addition",
      "separate",
      "instances",
      "service",
      "management",
      "containerized",
      "ucmdb.",
      "deployed",
      "need",
      "engine.",
      "following",
      "components",
      "autopass",
      "license",
      "apls",
      "release",
      "longer",
      "supports",
      "method",
      "installing",
      "virtual",
      "machine",
      "azure.",
      "migrate",
      "self-hosted",
      "server.",
      "follow",
      "steps.",
      "go",
      "portal",
      "https",
      "portal.azure.com",
      "search",
      "servers",
      "open",
      "it.",
      "click",
      "create.",
      "creation",
      "page",
      "enter",
      "information.",
      "project",
      "details",
      "subscriptions",
      "subscription",
      "id.",
      "get",
      "home",
      "subscriptions.",
      "resource",
      "group",
      "name",
      "created",
      "prerequisite",
      "tasks.",
      "example",
      "myresourcegroup.",
      "cmspghost.",
      "region",
      "choose",
      "region.",
      "ideally",
      "same",
      "workload",
      "type",
      "select",
      "according",
      "needs.",
      "compute",
      "storage",
      "configure",
      "size",
      "sizing"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare external databases for ud/ucmdb (aks)",
    "contentLower": "ud/ucmdb contains an internal (embedded) postgresql database instance, which should be used for demonstration purposes only. you must use external databases in a production environment. in addition, you must use separate postgresql databases instances for service management and the containerized ud/ucmdb. for the ud/ucmdb deployed on azure, you need to set up an azure flexible server as the database engine. this flexible server instance will be used as the external databases for the following ud/ucmdb components: ucmdb server autopass license service (apls) this release no longer supports the method of installing postgresql on a virtual machine on azure. you must migrate from self-hosted postgresql to azure flexible server. set up an azure flexible server to set up an azure flexible server, follow these steps. go to the azure portal (https://portal.azure.com). search for \"azure database for postgresql flexible servers\" and open it. click create. on the creation page, enter the followin",
    "keywordsLower": [
      "uducmdb",
      "azure.com",
      "values.yaml",
      "6.5",
      "https://portal.azure.com",
      "prepare",
      "external",
      "databases",
      "ud",
      "ucmdb",
      "aks",
      "set",
      "azure",
      "flexible",
      "server",
      "postgresql",
      "contains",
      "internal",
      "embedded",
      "database",
      "instance",
      "demonstration",
      "purposes",
      "only.",
      "production",
      "environment.",
      "addition",
      "separate",
      "instances",
      "service",
      "management",
      "containerized",
      "ucmdb.",
      "deployed",
      "need",
      "engine.",
      "following",
      "components",
      "autopass",
      "license",
      "apls",
      "release",
      "longer",
      "supports",
      "method",
      "installing",
      "virtual",
      "machine",
      "azure.",
      "migrate",
      "self-hosted",
      "server.",
      "follow",
      "steps.",
      "go",
      "portal",
      "https",
      "portal.azure.com",
      "search",
      "servers",
      "open",
      "it.",
      "click",
      "create.",
      "creation",
      "page",
      "enter",
      "information.",
      "project",
      "details",
      "subscriptions",
      "subscription",
      "id.",
      "get",
      "home",
      "subscriptions.",
      "resource",
      "group",
      "name",
      "created",
      "prerequisite",
      "tasks.",
      "example",
      "myresourcegroup.",
      "cmspghost.",
      "region",
      "choose",
      "region.",
      "ideally",
      "same",
      "workload",
      "type",
      "select",
      "according",
      "needs.",
      "compute",
      "storage",
      "configure",
      "size",
      "sizing"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes and claims for OO (AKS)",
    "content": "OO Containerized installation requires below Persistent Volumes (PVs) and Persistent Volume Claims (PVCs). PVs are pieces of storage relevant to Azure File Share volumes, and PVCs are requests for a piece of storage on a volume. Currently, OO uses persistent storage by providing the names of the required PVCs at OO install time via Helm parameters. This means that you must create both PVs and PVCs upfront. Component Azure File Share volume name Description Example Directory path Size OO oo-config-vol Stores OO configuration files. /var/vols/itom/oo/oo_config_vol ~ 1 GB OO oo-data-vol Stores OO, OO deployed AutoPass, and OO deployed Vault generated data. /var/vols/itom/oo/oo_data_vol ~ 1 GB OO oo-logs-vol Stores logs generated by the OO components, except RAS logs which are stored in a different volume. /var/vols/itom/oo/oo_logs_vol 1-5 GB OO oo-data-export-vol OO runs a Kubernetes cron job for purging OO database with a retention period set to 375 days (default value). All the database",
    "url": "preparepvooaks",
    "filename": "preparepvooaks",
    "headings": [
      "Configure persistent volumes",
      "Create OO PVC"
    ],
    "keywords": [
      "run.The",
      "sample_template.yaml",
      "prepare",
      "persistent",
      "volumes",
      "claims",
      "oo",
      "aks",
      "configure",
      "create",
      "pvc",
      "containerized",
      "installation",
      "requires",
      "below",
      "pvs",
      "volume",
      "pvcs",
      "pieces",
      "storage",
      "relevant",
      "azure",
      "file",
      "share",
      "requests",
      "piece",
      "volume.",
      "currently",
      "uses",
      "providing",
      "names",
      "required",
      "install",
      "time",
      "via",
      "helm",
      "parameters.",
      "means",
      "both",
      "upfront.",
      "component",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "size",
      "oo-config-vol",
      "stores",
      "configuration",
      "files.",
      "var",
      "vols",
      "itom",
      "gb",
      "oo-data-vol",
      "deployed",
      "autopass",
      "vault",
      "generated",
      "data.",
      "oo-logs-vol",
      "logs",
      "components",
      "except",
      "ras",
      "stored",
      "different",
      "1-5",
      "oo-data-export-vol",
      "runs",
      "kubernetes",
      "cron",
      "job",
      "purging",
      "database",
      "retention",
      "period",
      "set",
      "375",
      "days",
      "default",
      "value",
      "all",
      "data",
      "marked",
      "purge",
      "gets",
      "exported",
      "first",
      "exposed",
      "before",
      "purging.",
      "50-90",
      "see",
      "note",
      "oo-ras-logs-vol",
      "logs.",
      "maintenance",
      "export"
    ],
    "language": "en",
    "word_count": 134,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes and claims for oo (aks)",
    "contentLower": "oo containerized installation requires below persistent volumes (pvs) and persistent volume claims (pvcs). pvs are pieces of storage relevant to azure file share volumes, and pvcs are requests for a piece of storage on a volume. currently, oo uses persistent storage by providing the names of the required pvcs at oo install time via helm parameters. this means that you must create both pvs and pvcs upfront. component azure file share volume name description example directory path size oo oo-config-vol stores oo configuration files. /var/vols/itom/oo/oo_config_vol ~ 1 gb oo oo-data-vol stores oo, oo deployed autopass, and oo deployed vault generated data. /var/vols/itom/oo/oo_data_vol ~ 1 gb oo oo-logs-vol stores logs generated by the oo components, except ras logs which are stored in a different volume. /var/vols/itom/oo/oo_logs_vol 1-5 gb oo oo-data-export-vol oo runs a kubernetes cron job for purging oo database with a retention period set to 375 days (default value). all the database",
    "keywordsLower": [
      "run.the",
      "sample_template.yaml",
      "prepare",
      "persistent",
      "volumes",
      "claims",
      "oo",
      "aks",
      "configure",
      "create",
      "pvc",
      "containerized",
      "installation",
      "requires",
      "below",
      "pvs",
      "volume",
      "pvcs",
      "pieces",
      "storage",
      "relevant",
      "azure",
      "file",
      "share",
      "requests",
      "piece",
      "volume.",
      "currently",
      "uses",
      "providing",
      "names",
      "required",
      "install",
      "time",
      "via",
      "helm",
      "parameters.",
      "means",
      "both",
      "upfront.",
      "component",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "size",
      "oo-config-vol",
      "stores",
      "configuration",
      "files.",
      "var",
      "vols",
      "itom",
      "gb",
      "oo-data-vol",
      "deployed",
      "autopass",
      "vault",
      "generated",
      "data.",
      "oo-logs-vol",
      "logs",
      "components",
      "except",
      "ras",
      "stored",
      "different",
      "1-5",
      "oo-data-export-vol",
      "runs",
      "kubernetes",
      "cron",
      "job",
      "purging",
      "database",
      "retention",
      "period",
      "set",
      "375",
      "days",
      "default",
      "value",
      "all",
      "data",
      "marked",
      "purge",
      "gets",
      "exported",
      "first",
      "exposed",
      "before",
      "purging.",
      "50-90",
      "see",
      "note",
      "oo-ras-logs-vol",
      "logs.",
      "maintenance",
      "export"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare databases for OO (AKS)",
    "content": "To prepare the database for Operations Orchestration Containerized (OO Containerized), review the sizing considerations and identify the targeted tenant(s) that OO has to support. It's strongly recommended to use a dedicated database server for OO to ensure a smooth and fluent experience. However, if you connect OO to the same database that the suite uses, resize the database to be capable of supporting the added requirements of both the suite and OO. Set up an Azure flexible server To set up an Azure flexible server, follow these steps. Go to the Azure portal.Search for Azure Database for PostgreSQL flexible servers and open it.Click Create.On the creation page, enter the following information. Project details Subscriptions: Enter your subscription ID. You can get it from the Azure portal Home page > Subscriptions.Resource group: Enter the name of the resource group created in the prerequisite tasks. For example, myResourceGroup. Server details Server name: Enter a name for the flexib",
    "url": "prepareexternaldboo",
    "filename": "prepareexternaldboo",
    "headings": [
      "Set up an Azure flexible server",
      "Set up PostgreSQL databases for OO"
    ],
    "keywords": [
      "Create.On",
      "6.5",
      "values.yaml",
      "prepare",
      "databases",
      "oo",
      "aks",
      "set",
      "azure",
      "flexible",
      "server",
      "postgresql",
      "database",
      "operations",
      "orchestration",
      "containerized",
      "review",
      "sizing",
      "considerations",
      "identify",
      "targeted",
      "tenant",
      "support.",
      "strongly",
      "recommended",
      "dedicated",
      "ensure",
      "smooth",
      "fluent",
      "experience.",
      "however",
      "connect",
      "same",
      "suite",
      "uses",
      "resize",
      "capable",
      "supporting",
      "added",
      "requirements",
      "both",
      "oo.",
      "follow",
      "steps.",
      "go",
      "portal.search",
      "servers",
      "open",
      "it.click",
      "creation",
      "page",
      "enter",
      "following",
      "information.",
      "project",
      "details",
      "subscriptions",
      "subscription",
      "id.",
      "get",
      "portal",
      "home",
      "subscriptions.resource",
      "group",
      "name",
      "resource",
      "created",
      "prerequisite",
      "tasks.",
      "example",
      "myresourcegroup.",
      "server.",
      "oopg.region",
      "choose",
      "region.",
      "ideally",
      "region",
      "production",
      "environment.workload",
      "type",
      "select",
      "workload",
      "according",
      "needs.compute",
      "storage",
      "click",
      "configure",
      "compute",
      "size",
      "amount",
      "see",
      "plan",
      "deployment",
      "save.availability",
      "zone",
      "preference.",
      "high",
      "availability",
      "check",
      "enable"
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare databases for oo (aks)",
    "contentLower": "to prepare the database for operations orchestration containerized (oo containerized), review the sizing considerations and identify the targeted tenant(s) that oo has to support. it's strongly recommended to use a dedicated database server for oo to ensure a smooth and fluent experience. however, if you connect oo to the same database that the suite uses, resize the database to be capable of supporting the added requirements of both the suite and oo. set up an azure flexible server to set up an azure flexible server, follow these steps. go to the azure portal.search for azure database for postgresql flexible servers and open it.click create.on the creation page, enter the following information. project details subscriptions: enter your subscription id. you can get it from the azure portal home page > subscriptions.resource group: enter the name of the resource group created in the prerequisite tasks. for example, myresourcegroup. server details server name: enter a name for the flexib",
    "keywordsLower": [
      "create.on",
      "6.5",
      "values.yaml",
      "prepare",
      "databases",
      "oo",
      "aks",
      "set",
      "azure",
      "flexible",
      "server",
      "postgresql",
      "database",
      "operations",
      "orchestration",
      "containerized",
      "review",
      "sizing",
      "considerations",
      "identify",
      "targeted",
      "tenant",
      "support.",
      "strongly",
      "recommended",
      "dedicated",
      "ensure",
      "smooth",
      "fluent",
      "experience.",
      "however",
      "connect",
      "same",
      "suite",
      "uses",
      "resize",
      "capable",
      "supporting",
      "added",
      "requirements",
      "both",
      "oo.",
      "follow",
      "steps.",
      "go",
      "portal.search",
      "servers",
      "open",
      "it.click",
      "creation",
      "page",
      "enter",
      "following",
      "information.",
      "project",
      "details",
      "subscriptions",
      "subscription",
      "id.",
      "get",
      "portal",
      "home",
      "subscriptions.resource",
      "group",
      "name",
      "resource",
      "created",
      "prerequisite",
      "tasks.",
      "example",
      "myresourcegroup.",
      "server.",
      "oopg.region",
      "choose",
      "region.",
      "ideally",
      "region",
      "production",
      "environment.workload",
      "type",
      "select",
      "workload",
      "according",
      "needs.compute",
      "storage",
      "click",
      "configure",
      "compute",
      "size",
      "amount",
      "see",
      "plan",
      "deployment",
      "save.availability",
      "zone",
      "preference.",
      "high",
      "availability",
      "check",
      "enable"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Post installation task for OO Containerized on Azure (AKS)",
    "content": "After successful installation of OO Containerized, perform the following task: Import OO certificate into the suite.",
    "url": "postinstallooazure",
    "filename": "postinstallooazure",
    "headings": [],
    "keywords": [
      "post",
      "installation",
      "task",
      "oo",
      "containerized",
      "azure",
      "aks",
      "after",
      "successful",
      "perform",
      "following",
      "import",
      "certificate",
      "suite."
    ],
    "language": "en",
    "word_count": 19,
    "importance_score": 1.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "post installation task for oo containerized on azure (aks)",
    "contentLower": "after successful installation of oo containerized, perform the following task: import oo certificate into the suite.",
    "keywordsLower": [
      "post",
      "installation",
      "task",
      "oo",
      "containerized",
      "azure",
      "aks",
      "after",
      "successful",
      "perform",
      "following",
      "import",
      "certificate",
      "suite."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan for GCP",
    "content": "The following table provides an overview of the topics covered in the Plan section: Topic Description Before you begin Review the suite deployment architecture on GCP, including the benefits of using GKE, deployment components, inter-communication between components, and the design considerations like security and performance. Support matrix Based on your deployment plan, review the system requirements for the suite installation. At the conclusion of this phase, you must have the following resources in place: Bastion node required to communicate with the Kubernetes cluster Worker node(s) to run suite services NFS server to store OMT and suite data Relational database Sizing consideration Collect and evaluate data from your current environment to determine the following: Suite deployment option that suits your environment the best Size and complexity of your suite deployment FAQs List the most frequently asked questions about deploying the suite on GCP with GKE, including generic cloud ",
    "url": "planforgcp",
    "filename": "planforgcp",
    "headings": [],
    "keywords": [
      "plan",
      "gcp",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "before",
      "begin",
      "review",
      "suite",
      "deployment",
      "architecture",
      "including",
      "benefits",
      "gke",
      "components",
      "inter-communication",
      "between",
      "design",
      "considerations",
      "like",
      "security",
      "performance.",
      "support",
      "matrix",
      "based",
      "system",
      "requirements",
      "installation.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "data",
      "relational",
      "database",
      "sizing",
      "consideration",
      "collect",
      "evaluate",
      "current",
      "environment",
      "determine",
      "option",
      "suits",
      "best",
      "size",
      "complexity",
      "faqs",
      "list",
      "most",
      "frequently",
      "asked",
      "questions",
      "about",
      "deploying",
      "generic",
      "cloud",
      "specific",
      "only."
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan for gcp",
    "contentLower": "the following table provides an overview of the topics covered in the plan section: topic description before you begin review the suite deployment architecture on gcp, including the benefits of using gke, deployment components, inter-communication between components, and the design considerations like security and performance. support matrix based on your deployment plan, review the system requirements for the suite installation. at the conclusion of this phase, you must have the following resources in place: bastion node required to communicate with the kubernetes cluster worker node(s) to run suite services nfs server to store omt and suite data relational database sizing consideration collect and evaluate data from your current environment to determine the following: suite deployment option that suits your environment the best size and complexity of your suite deployment faqs list the most frequently asked questions about deploying the suite on gcp with gke, including generic cloud ",
    "keywordsLower": [
      "plan",
      "gcp",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "before",
      "begin",
      "review",
      "suite",
      "deployment",
      "architecture",
      "including",
      "benefits",
      "gke",
      "components",
      "inter-communication",
      "between",
      "design",
      "considerations",
      "like",
      "security",
      "performance.",
      "support",
      "matrix",
      "based",
      "system",
      "requirements",
      "installation.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "data",
      "relational",
      "database",
      "sizing",
      "consideration",
      "collect",
      "evaluate",
      "current",
      "environment",
      "determine",
      "option",
      "suits",
      "best",
      "size",
      "complexity",
      "faqs",
      "list",
      "most",
      "frequently",
      "asked",
      "questions",
      "about",
      "deploying",
      "generic",
      "cloud",
      "specific",
      "only."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare GCP project and roles",
    "content": "Role Location Cloud administrator GCP Console All GCP resources belong to a project. Before deploying the suite on GCP, you need to either select an existing project or create a new one. For the detailed information of a specific project including Project name, Project ID, and Project number, navigate to the GCP Console Home > Dashboard > Project info. Roles and permissions on GCP To deploy OMT and the suite on GCP, prepare the following accounts and resources with the required permissions. On GCP, permissions change with roles. Compute Engine default service account When you create a project, the Compute Engine default service account using email address [PROJECT_NUMBER]-compute@developer.gserviceaccount.com comes along with it automatically. By default, you can use this service account to access all instances under the project with the project Editor role. To grant more required permissions to Compute Engine default service account, follow these steps: Log in to the Google Cloud Plat",
    "url": "gcpproject",
    "filename": "gcpproject",
    "headings": [
      "Roles and permissions on GCP",
      "Compute Engine default service account",
      "User account",
      "External references"
    ],
    "keywords": [
      "gcr.io",
      "gserviceaccount.com",
      "https://console.cloud.google.com",
      "google.com",
      "prepare",
      "gcp",
      "project",
      "roles",
      "permissions",
      "compute",
      "engine",
      "default",
      "service",
      "account",
      "user",
      "external",
      "references",
      "role",
      "location",
      "cloud",
      "administrator",
      "console",
      "all",
      "resources",
      "belong",
      "project.",
      "before",
      "deploying",
      "suite",
      "need",
      "either",
      "select",
      "existing",
      "create",
      "new",
      "one.",
      "detailed",
      "information",
      "specific",
      "including",
      "name",
      "id",
      "number",
      "navigate",
      "home",
      "dashboard",
      "info.",
      "deploy",
      "omt",
      "following",
      "accounts",
      "required",
      "permissions.",
      "change",
      "roles.",
      "email",
      "address",
      "-compute",
      "developer.gserviceaccount.com",
      "comes",
      "along",
      "automatically.",
      "access",
      "instances",
      "under",
      "editor",
      "role.",
      "grant",
      "follow",
      "steps",
      "log",
      "google",
      "platform",
      "https",
      "console.cloud.google.com",
      "iam",
      "admin",
      "iam.",
      "locate",
      "click",
      "pencil",
      "icon",
      "edit",
      "add",
      "another",
      "kubernetes",
      "account.",
      "container",
      "registry",
      "gcr",
      "accessible",
      "get",
      "ready",
      "installation",
      "administration",
      "necessary",
      "check",
      "double-check",
      "networking",
      "missing"
    ],
    "language": "en",
    "word_count": 107,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare gcp project and roles",
    "contentLower": "role location cloud administrator gcp console all gcp resources belong to a project. before deploying the suite on gcp, you need to either select an existing project or create a new one. for the detailed information of a specific project including project name, project id, and project number, navigate to the gcp console home > dashboard > project info. roles and permissions on gcp to deploy omt and the suite on gcp, prepare the following accounts and resources with the required permissions. on gcp, permissions change with roles. compute engine default service account when you create a project, the compute engine default service account using email address [project_number]-compute@developer.gserviceaccount.com comes along with it automatically. by default, you can use this service account to access all instances under the project with the project editor role. to grant more required permissions to compute engine default service account, follow these steps: log in to the google cloud plat",
    "keywordsLower": [
      "gcr.io",
      "gserviceaccount.com",
      "https://console.cloud.google.com",
      "google.com",
      "prepare",
      "gcp",
      "project",
      "roles",
      "permissions",
      "compute",
      "engine",
      "default",
      "service",
      "account",
      "user",
      "external",
      "references",
      "role",
      "location",
      "cloud",
      "administrator",
      "console",
      "all",
      "resources",
      "belong",
      "project.",
      "before",
      "deploying",
      "suite",
      "need",
      "either",
      "select",
      "existing",
      "create",
      "new",
      "one.",
      "detailed",
      "information",
      "specific",
      "including",
      "name",
      "id",
      "number",
      "navigate",
      "home",
      "dashboard",
      "info.",
      "deploy",
      "omt",
      "following",
      "accounts",
      "required",
      "permissions.",
      "change",
      "roles.",
      "email",
      "address",
      "-compute",
      "developer.gserviceaccount.com",
      "comes",
      "along",
      "automatically.",
      "access",
      "instances",
      "under",
      "editor",
      "role.",
      "grant",
      "follow",
      "steps",
      "log",
      "google",
      "platform",
      "https",
      "console.cloud.google.com",
      "iam",
      "admin",
      "iam.",
      "locate",
      "click",
      "pencil",
      "icon",
      "edit",
      "add",
      "another",
      "kubernetes",
      "account.",
      "container",
      "registry",
      "gcr",
      "accessible",
      "get",
      "ready",
      "installation",
      "administration",
      "necessary",
      "check",
      "double-check",
      "networking",
      "missing"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for ESM on GCP",
    "content": "To install the application in the Helm mode, you must create PVs. You can create PVs either using static volume provisioning or dynamic volume provisioning. To learn more about the two methods, see Dynamic vs. static volume provisioning. As you should already choose one of the following methods: Option 1:  Cloud  Filestore static provision and Peristent Disk dynamic provision Option 2:  Cloud  Filestore dynamic provision and Peristent Disk dynamic provision You need choose the steps according to your option. Prerequisite Make you have the Filestore CSI driver deployed to your GKE cluster. For details, see Enable the Filestore CSI driver on an existing cluster. This is required for both option 1 and option 2. Create PVs using static volume provisioning Only applied to option 1. To create PVs for the suite using static volume provisioning, run the createPV.sh script. The following examples show how to use the createPv.sh script: Example for File Storage: ./createPV.sh -n itsma -gcp -s 10",
    "url": "preparepvsgcp",
    "filename": "preparepvsgcp",
    "headings": [
      "Prerequisite",
      "Create PVs using static volume provisioning",
      "Create PVs using dynamic volume provisioning",
      "Persistent Disks",
      "Cloud  Filestore"
    ],
    "keywords": [
      "filestore.csi",
      "createPv.sh",
      "createPV.sh",
      "pd.csi",
      "10.xx",
      "sc.yaml",
      "storage.k8s",
      "97.2",
      "values.yaml",
      "storage.gke",
      "prepare",
      "persistent",
      "volumes",
      "esm",
      "gcp",
      "prerequisite",
      "create",
      "pvs",
      "static",
      "volume",
      "provisioning",
      "dynamic",
      "disks",
      "cloud",
      "filestore",
      "install",
      "application",
      "helm",
      "mode",
      "pvs.",
      "either",
      "provisioning.",
      "learn",
      "about",
      "two",
      "methods",
      "see",
      "vs.",
      "already",
      "choose",
      "one",
      "following",
      "option",
      "provision",
      "peristent",
      "disk",
      "need",
      "steps",
      "according",
      "option.",
      "make",
      "csi",
      "driver",
      "deployed",
      "gke",
      "cluster.",
      "details",
      "enable",
      "existing",
      "required",
      "both",
      "2.",
      "applied",
      "1.",
      "suite",
      "run",
      "script.",
      "examples",
      "show",
      "script",
      "example",
      "file",
      "storage",
      "-n",
      "itsma",
      "-gcp",
      "-s",
      "10.xx.97.2",
      "-p",
      "smax",
      "var",
      "vols",
      "itom",
      "--location",
      "us-west2",
      "esm-verify",
      "-u",
      "1999",
      "-g",
      "--size",
      "small",
      "add",
      "storageclass",
      "name",
      "defined",
      "pvc",
      "my-values.yaml",
      "cli",
      "installation",
      "specify"
    ],
    "language": "en",
    "word_count": 113,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for esm on gcp",
    "contentLower": "to install the application in the helm mode, you must create pvs. you can create pvs either using static volume provisioning or dynamic volume provisioning. to learn more about the two methods, see dynamic vs. static volume provisioning. as you should already choose one of the following methods: option 1:  cloud  filestore static provision and peristent disk dynamic provision option 2:  cloud  filestore dynamic provision and peristent disk dynamic provision you need choose the steps according to your option. prerequisite make you have the filestore csi driver deployed to your gke cluster. for details, see enable the filestore csi driver on an existing cluster. this is required for both option 1 and option 2. create pvs using static volume provisioning only applied to option 1. to create pvs for the suite using static volume provisioning, run the createpv.sh script. the following examples show how to use the createpv.sh script: example for file storage: ./createpv.sh -n itsma -gcp -s 10",
    "keywordsLower": [
      "filestore.csi",
      "createpv.sh",
      "createpv.sh",
      "pd.csi",
      "10.xx",
      "sc.yaml",
      "storage.k8s",
      "97.2",
      "values.yaml",
      "storage.gke",
      "prepare",
      "persistent",
      "volumes",
      "esm",
      "gcp",
      "prerequisite",
      "create",
      "pvs",
      "static",
      "volume",
      "provisioning",
      "dynamic",
      "disks",
      "cloud",
      "filestore",
      "install",
      "application",
      "helm",
      "mode",
      "pvs.",
      "either",
      "provisioning.",
      "learn",
      "about",
      "two",
      "methods",
      "see",
      "vs.",
      "already",
      "choose",
      "one",
      "following",
      "option",
      "provision",
      "peristent",
      "disk",
      "need",
      "steps",
      "according",
      "option.",
      "make",
      "csi",
      "driver",
      "deployed",
      "gke",
      "cluster.",
      "details",
      "enable",
      "existing",
      "required",
      "both",
      "2.",
      "applied",
      "1.",
      "suite",
      "run",
      "script.",
      "examples",
      "show",
      "script",
      "example",
      "file",
      "storage",
      "-n",
      "itsma",
      "-gcp",
      "-s",
      "10.xx.97.2",
      "-p",
      "smax",
      "var",
      "vols",
      "itom",
      "--location",
      "us-west2",
      "esm-verify",
      "-u",
      "1999",
      "-g",
      "--size",
      "small",
      "add",
      "storageclass",
      "name",
      "defined",
      "pvc",
      "my-values.yaml",
      "cli",
      "installation",
      "specify"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan for OO Containerized on GKE",
    "content": "The following table provides an overview of the topics covered in the Plan section: Topic Description Support matrix Based on your deployment plan, review the system requirements for the OO Containerized installation. Then, you must have the following resources in place: Bastion node required to communicate with the Kubernetes cluster Worker node(s) to run suite services NFS server to store OO Containerized and suite data Relational database Sizing considerations Collect and evaluate data from your current environment to determine the following: Suite deployment option that best suits your environment Size and complexity of your suite deployment",
    "url": "399-planoocgcp",
    "filename": "399-planoocgcp",
    "headings": [],
    "keywords": [
      "plan",
      "oo",
      "containerized",
      "gke",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "support",
      "matrix",
      "based",
      "deployment",
      "review",
      "system",
      "requirements",
      "installation.",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "suite",
      "services",
      "nfs",
      "server",
      "store",
      "data",
      "relational",
      "database",
      "sizing",
      "considerations",
      "collect",
      "evaluate",
      "current",
      "environment",
      "determine",
      "option",
      "best",
      "suits",
      "size",
      "complexity"
    ],
    "language": "en",
    "word_count": 68,
    "importance_score": 1.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan for oo containerized on gke",
    "contentLower": "the following table provides an overview of the topics covered in the plan section: topic description support matrix based on your deployment plan, review the system requirements for the oo containerized installation. then, you must have the following resources in place: bastion node required to communicate with the kubernetes cluster worker node(s) to run suite services nfs server to store oo containerized and suite data relational database sizing considerations collect and evaluate data from your current environment to determine the following: suite deployment option that best suits your environment size and complexity of your suite deployment",
    "keywordsLower": [
      "plan",
      "oo",
      "containerized",
      "gke",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "support",
      "matrix",
      "based",
      "deployment",
      "review",
      "system",
      "requirements",
      "installation.",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "suite",
      "services",
      "nfs",
      "server",
      "store",
      "data",
      "relational",
      "database",
      "sizing",
      "considerations",
      "collect",
      "evaluate",
      "current",
      "environment",
      "determine",
      "option",
      "best",
      "suits",
      "size",
      "complexity"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan OO Containerized deployment on GKE",
    "content": "By default, an OO Containerized installation supports up to 5 tenants. The following sizing requirements are based on running 5 tenants. Nodes configuration GKE GCP Cloud SQL Instance Type Configuration Storage Quantity Configuration Additional storage Instance Type Core Count vCPU Memory Storage Engine max_connections n2-standard-8 8 vCPU 32 GB RAM 100 GB 2 OO shares the same GKE server with the suite. 500 GB n2-highmem-2 1 2 16 GB 750 GB ~ 5 TB PG 13.x, 14.x, 15.x, 16.x 1400 OO Containerized also supports up to 20 tenants. The following sizing requirements are based on running 20 tenants. Nodes configuration GKE GCP Cloud SQL Instance Type Configuration Storage Quantity Configuration Additional storage Instance Type Core Count vCPU Memory Storage Engine max_connections n2-standard-8 8 vCPU 32 GB RAM 100 GB 8 OO shares the same GKE server with the suite. 500 GB n2-highmem-2 4 8 64 GB 750 GB ~ 5 TB PG 13.x, 14.x, 15.x, 16.x 5710 It's recommended to use a dedicated Cloud SQL instance fo",
    "url": "399-gcpoocsizing",
    "filename": "399-gcpoocsizing",
    "headings": [],
    "keywords": [
      "plan",
      "oo",
      "containerized",
      "deployment",
      "gke",
      "default",
      "installation",
      "supports",
      "tenants.",
      "following",
      "sizing",
      "requirements",
      "based",
      "running",
      "nodes",
      "configuration",
      "gcp",
      "cloud",
      "sql",
      "instance",
      "type",
      "storage",
      "quantity",
      "additional",
      "core",
      "count",
      "vcpu",
      "memory",
      "engine",
      "n2-standard-8",
      "32",
      "gb",
      "ram",
      "100",
      "shares",
      "same",
      "server",
      "suite.",
      "500",
      "n2-highmem-2",
      "16",
      "750",
      "tb",
      "pg",
      "13.x",
      "14.x",
      "15.x",
      "16.x",
      "1400",
      "20",
      "64",
      "5710",
      "recommended",
      "dedicated",
      "google",
      "platform",
      "choose",
      "connect",
      "rest",
      "suite",
      "components",
      "need",
      "resize",
      "machine",
      "support",
      "workload",
      "both",
      "containerized.",
      "persistent",
      "disk",
      "pd-ssd",
      "pd-hdd",
      "modify",
      "parameter.",
      "details",
      "see",
      "documentation.",
      "set",
      "parameter",
      "follow",
      "steps",
      "settings",
      "go",
      "console",
      "sql.",
      "select",
      "instance.",
      "click",
      "edit",
      "scroll",
      "flags.",
      "add",
      "required",
      "value.",
      "restart",
      "created",
      "before",
      "change",
      "new",
      "take"
    ],
    "language": "en",
    "word_count": 148,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan oo containerized deployment on gke",
    "contentLower": "by default, an oo containerized installation supports up to 5 tenants. the following sizing requirements are based on running 5 tenants. nodes configuration gke gcp cloud sql instance type configuration storage quantity configuration additional storage instance type core count vcpu memory storage engine max_connections n2-standard-8 8 vcpu 32 gb ram 100 gb 2 oo shares the same gke server with the suite. 500 gb n2-highmem-2 1 2 16 gb 750 gb ~ 5 tb pg 13.x, 14.x, 15.x, 16.x 1400 oo containerized also supports up to 20 tenants. the following sizing requirements are based on running 20 tenants. nodes configuration gke gcp cloud sql instance type configuration storage quantity configuration additional storage instance type core count vcpu memory storage engine max_connections n2-standard-8 8 vcpu 32 gb ram 100 gb 8 oo shares the same gke server with the suite. 500 gb n2-highmem-2 4 8 64 gb 750 gb ~ 5 tb pg 13.x, 14.x, 15.x, 16.x 5710 it's recommended to use a dedicated cloud sql instance fo",
    "keywordsLower": [
      "plan",
      "oo",
      "containerized",
      "deployment",
      "gke",
      "default",
      "installation",
      "supports",
      "tenants.",
      "following",
      "sizing",
      "requirements",
      "based",
      "running",
      "nodes",
      "configuration",
      "gcp",
      "cloud",
      "sql",
      "instance",
      "type",
      "storage",
      "quantity",
      "additional",
      "core",
      "count",
      "vcpu",
      "memory",
      "engine",
      "n2-standard-8",
      "32",
      "gb",
      "ram",
      "100",
      "shares",
      "same",
      "server",
      "suite.",
      "500",
      "n2-highmem-2",
      "16",
      "750",
      "tb",
      "pg",
      "13.x",
      "14.x",
      "15.x",
      "16.x",
      "1400",
      "20",
      "64",
      "5710",
      "recommended",
      "dedicated",
      "google",
      "platform",
      "choose",
      "connect",
      "rest",
      "suite",
      "components",
      "need",
      "resize",
      "machine",
      "support",
      "workload",
      "both",
      "containerized.",
      "persistent",
      "disk",
      "pd-ssd",
      "pd-hdd",
      "modify",
      "parameter.",
      "details",
      "see",
      "documentation.",
      "set",
      "parameter",
      "follow",
      "steps",
      "settings",
      "go",
      "console",
      "sql.",
      "select",
      "instance.",
      "click",
      "edit",
      "scroll",
      "flags.",
      "add",
      "required",
      "value.",
      "restart",
      "created",
      "before",
      "change",
      "new",
      "take"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare for OO Containerized on GKE",
    "content": "This section contains all the mandatory and optional tasks for installing the suite in a GCP environment. Prepare GCP project and roles Create Virtual Private Cloud Set up IP and external access hostname Download OO Containerized chart package (GKE) Download and upload container images for OO Configure Google Cloud Filestore for OO Create a new deployment for OO on GKE Prepare persistent volumes and claims for OO (GKE) Launch Google Cloud SQL for OO Create OO integration admin user in the Service Management IdM (GKE) Subscribe NFS Filestore Create external databases Create values.yaml for OO (GKE) Configure load balancers for OO (GKE)",
    "url": "399-prepareoocgcp",
    "filename": "399-prepareoocgcp",
    "headings": [],
    "keywords": [
      "values.yaml",
      "prepare",
      "oo",
      "containerized",
      "gke",
      "section",
      "contains",
      "all",
      "mandatory",
      "optional",
      "tasks",
      "installing",
      "suite",
      "gcp",
      "environment.",
      "project",
      "roles",
      "create",
      "virtual",
      "private",
      "cloud",
      "set",
      "ip",
      "external",
      "access",
      "hostname",
      "download",
      "chart",
      "package",
      "upload",
      "container",
      "images",
      "configure",
      "google",
      "filestore",
      "new",
      "deployment",
      "persistent",
      "volumes",
      "claims",
      "launch",
      "sql",
      "integration",
      "admin",
      "user",
      "service",
      "management",
      "idm",
      "subscribe",
      "nfs",
      "databases",
      "load",
      "balancers"
    ],
    "language": "en",
    "word_count": 83,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare for oo containerized on gke",
    "contentLower": "this section contains all the mandatory and optional tasks for installing the suite in a gcp environment. prepare gcp project and roles create virtual private cloud set up ip and external access hostname download oo containerized chart package (gke) download and upload container images for oo configure google cloud filestore for oo create a new deployment for oo on gke prepare persistent volumes and claims for oo (gke) launch google cloud sql for oo create oo integration admin user in the service management idm (gke) subscribe nfs filestore create external databases create values.yaml for oo (gke) configure load balancers for oo (gke)",
    "keywordsLower": [
      "values.yaml",
      "prepare",
      "oo",
      "containerized",
      "gke",
      "section",
      "contains",
      "all",
      "mandatory",
      "optional",
      "tasks",
      "installing",
      "suite",
      "gcp",
      "environment.",
      "project",
      "roles",
      "create",
      "virtual",
      "private",
      "cloud",
      "set",
      "ip",
      "external",
      "access",
      "hostname",
      "download",
      "chart",
      "package",
      "upload",
      "container",
      "images",
      "configure",
      "google",
      "filestore",
      "new",
      "deployment",
      "persistent",
      "volumes",
      "claims",
      "launch",
      "sql",
      "integration",
      "admin",
      "user",
      "service",
      "management",
      "idm",
      "subscribe",
      "nfs",
      "databases",
      "load",
      "balancers"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare GCP project and roles",
    "content": "Role Location Cloud administrator GCP Console All GCP resources belong to a project. Before deploying the suite on GCP, you must either select an existing project or create a new one. For detailed information on a specific project including Project name, Project ID, and Project number, navigate to the GCP Console Home > Dashboard > Project info. Roles and permissions on GCP To deploy OMT and the suite on GCP, prepare the following accounts and resources with the required permissions. On GCP, permissions change with roles. Compute Engine default service account When you create a project, the Compute Engine default service account using email address [PROJECT_NUMBER]-compute@developer.gserviceaccount.com comes along with it automatically. By default, you can use this service account to access all instances under the project with the project Editor role. To grant more required permissions to Compute Engine default service account, follow these steps: Log in to the Google Cloud Platform Co",
    "url": "399-gcpoocprojectroles",
    "filename": "399-gcpoocprojectroles",
    "headings": [
      "Roles and permissions on GCP",
      "Compute Engine default service account",
      "User account",
      "External references"
    ],
    "keywords": [
      "gcr.io",
      "gserviceaccount.com",
      "https://console.cloud.google.com",
      "google.com",
      "prepare",
      "gcp",
      "project",
      "roles",
      "permissions",
      "compute",
      "engine",
      "default",
      "service",
      "account",
      "user",
      "external",
      "references",
      "role",
      "location",
      "cloud",
      "administrator",
      "console",
      "all",
      "resources",
      "belong",
      "project.",
      "before",
      "deploying",
      "suite",
      "either",
      "select",
      "existing",
      "create",
      "new",
      "one.",
      "detailed",
      "information",
      "specific",
      "including",
      "name",
      "id",
      "number",
      "navigate",
      "home",
      "dashboard",
      "info.",
      "deploy",
      "omt",
      "following",
      "accounts",
      "required",
      "permissions.",
      "change",
      "roles.",
      "email",
      "address",
      "-compute",
      "developer.gserviceaccount.com",
      "comes",
      "along",
      "automatically.",
      "access",
      "instances",
      "under",
      "editor",
      "role.",
      "grant",
      "follow",
      "steps",
      "log",
      "google",
      "platform",
      "https",
      "console.cloud.google.com",
      "iam",
      "admin",
      "iam.",
      "locate",
      "click",
      "pencil",
      "icon",
      "edit",
      "add",
      "another",
      "kubernetes",
      "account.",
      "container",
      "registry",
      "gcr",
      "accessible",
      "get",
      "ready",
      "installation",
      "administration",
      "need",
      "necessary",
      "check",
      "double-check",
      "networking",
      "missing"
    ],
    "language": "en",
    "word_count": 107,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare gcp project and roles",
    "contentLower": "role location cloud administrator gcp console all gcp resources belong to a project. before deploying the suite on gcp, you must either select an existing project or create a new one. for detailed information on a specific project including project name, project id, and project number, navigate to the gcp console home > dashboard > project info. roles and permissions on gcp to deploy omt and the suite on gcp, prepare the following accounts and resources with the required permissions. on gcp, permissions change with roles. compute engine default service account when you create a project, the compute engine default service account using email address [project_number]-compute@developer.gserviceaccount.com comes along with it automatically. by default, you can use this service account to access all instances under the project with the project editor role. to grant more required permissions to compute engine default service account, follow these steps: log in to the google cloud platform co",
    "keywordsLower": [
      "gcr.io",
      "gserviceaccount.com",
      "https://console.cloud.google.com",
      "google.com",
      "prepare",
      "gcp",
      "project",
      "roles",
      "permissions",
      "compute",
      "engine",
      "default",
      "service",
      "account",
      "user",
      "external",
      "references",
      "role",
      "location",
      "cloud",
      "administrator",
      "console",
      "all",
      "resources",
      "belong",
      "project.",
      "before",
      "deploying",
      "suite",
      "either",
      "select",
      "existing",
      "create",
      "new",
      "one.",
      "detailed",
      "information",
      "specific",
      "including",
      "name",
      "id",
      "number",
      "navigate",
      "home",
      "dashboard",
      "info.",
      "deploy",
      "omt",
      "following",
      "accounts",
      "required",
      "permissions.",
      "change",
      "roles.",
      "email",
      "address",
      "-compute",
      "developer.gserviceaccount.com",
      "comes",
      "along",
      "automatically.",
      "access",
      "instances",
      "under",
      "editor",
      "role.",
      "grant",
      "follow",
      "steps",
      "log",
      "google",
      "platform",
      "https",
      "console.cloud.google.com",
      "iam",
      "admin",
      "iam.",
      "locate",
      "click",
      "pencil",
      "icon",
      "edit",
      "add",
      "another",
      "kubernetes",
      "account.",
      "container",
      "registry",
      "gcr",
      "accessible",
      "get",
      "ready",
      "installation",
      "administration",
      "need",
      "necessary",
      "check",
      "double-check",
      "networking",
      "missing"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes and claims for OO (GKE)",
    "content": "OO Containerized installation requires below Persistent Volumes (PVs) and Persistent Volume Claims (PVCs). PVs are pieces of storage relevant to Google Cloud Filestore volumes, and PVCs are requests for a piece of storage on a volume. OO operates independently from SM at both the database and storage layers. Persistent Volume (PV) and Persistent Volume Claim (PVC) separation is mandatory. Currently, OO uses persistent storage by providing the names of the required PVCs at OO install time via Helm parameters. This means that you must create both PVs and PVCs upfront. Component Google Cloud Filestore volume name Description Example Directory path Size OO oo-config-vol Stores OO configuration files. /var/vols/itom/oo/oo_config_vol ~ 1 GB OO oo-data-vol Stores OO, OO deployed AutoPass, and OO deployed Vault-generated data. /var/vols/itom/oo/oo_data_vol ~ 1 GB OO oo-logs-vol Stores logs generated by the OO components, except RAS logs which are stored in a different volume. /var/vols/itom/oo",
    "url": "399-gcppreparepvooc",
    "filename": "399-gcppreparepvooc",
    "headings": [
      "Create persistent volumes",
      "Create persistent volume claims"
    ],
    "keywords": [
      "run.The",
      "volumes.yaml",
      "claims.yaml",
      "encrypted.yaml",
      "prepare",
      "persistent",
      "volumes",
      "claims",
      "oo",
      "gke",
      "create",
      "volume",
      "containerized",
      "installation",
      "requires",
      "below",
      "pvs",
      "pvcs",
      "pieces",
      "storage",
      "relevant",
      "google",
      "cloud",
      "filestore",
      "requests",
      "piece",
      "volume.",
      "operates",
      "independently",
      "sm",
      "both",
      "database",
      "layers.",
      "pv",
      "claim",
      "pvc",
      "separation",
      "mandatory.",
      "currently",
      "uses",
      "providing",
      "names",
      "required",
      "install",
      "time",
      "via",
      "helm",
      "parameters.",
      "means",
      "upfront.",
      "component",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "size",
      "oo-config-vol",
      "stores",
      "configuration",
      "files.",
      "var",
      "vols",
      "itom",
      "gb",
      "oo-data-vol",
      "deployed",
      "autopass",
      "vault-generated",
      "data.",
      "oo-logs-vol",
      "logs",
      "generated",
      "components",
      "except",
      "ras",
      "stored",
      "different",
      "1-5",
      "oo-data-export-vol",
      "runs",
      "kubernetes",
      "cron",
      "job",
      "purging",
      "retention",
      "period",
      "set",
      "375",
      "days",
      "default",
      "value",
      "all",
      "data",
      "marked",
      "purge",
      "gets",
      "exported",
      "first",
      "exposed"
    ],
    "language": "en",
    "word_count": 128,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes and claims for oo (gke)",
    "contentLower": "oo containerized installation requires below persistent volumes (pvs) and persistent volume claims (pvcs). pvs are pieces of storage relevant to google cloud filestore volumes, and pvcs are requests for a piece of storage on a volume. oo operates independently from sm at both the database and storage layers. persistent volume (pv) and persistent volume claim (pvc) separation is mandatory. currently, oo uses persistent storage by providing the names of the required pvcs at oo install time via helm parameters. this means that you must create both pvs and pvcs upfront. component google cloud filestore volume name description example directory path size oo oo-config-vol stores oo configuration files. /var/vols/itom/oo/oo_config_vol ~ 1 gb oo oo-data-vol stores oo, oo deployed autopass, and oo deployed vault-generated data. /var/vols/itom/oo/oo_data_vol ~ 1 gb oo oo-logs-vol stores logs generated by the oo components, except ras logs which are stored in a different volume. /var/vols/itom/oo",
    "keywordsLower": [
      "run.the",
      "volumes.yaml",
      "claims.yaml",
      "encrypted.yaml",
      "prepare",
      "persistent",
      "volumes",
      "claims",
      "oo",
      "gke",
      "create",
      "volume",
      "containerized",
      "installation",
      "requires",
      "below",
      "pvs",
      "pvcs",
      "pieces",
      "storage",
      "relevant",
      "google",
      "cloud",
      "filestore",
      "requests",
      "piece",
      "volume.",
      "operates",
      "independently",
      "sm",
      "both",
      "database",
      "layers.",
      "pv",
      "claim",
      "pvc",
      "separation",
      "mandatory.",
      "currently",
      "uses",
      "providing",
      "names",
      "required",
      "install",
      "time",
      "via",
      "helm",
      "parameters.",
      "means",
      "upfront.",
      "component",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "size",
      "oo-config-vol",
      "stores",
      "configuration",
      "files.",
      "var",
      "vols",
      "itom",
      "gb",
      "oo-data-vol",
      "deployed",
      "autopass",
      "vault-generated",
      "data.",
      "oo-logs-vol",
      "logs",
      "generated",
      "components",
      "except",
      "ras",
      "stored",
      "different",
      "1-5",
      "oo-data-export-vol",
      "runs",
      "kubernetes",
      "cron",
      "job",
      "purging",
      "retention",
      "period",
      "set",
      "375",
      "days",
      "default",
      "value",
      "all",
      "data",
      "marked",
      "purge",
      "gets",
      "exported",
      "first",
      "exposed"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for UD/UCMDB",
    "content": "UD/UCMDB installation in Helm mode requires the following Persistent Volumes (PVs). PVs are relevant to Google Cloud Filestore volumes. Component Google Cloud Filestore volumes name Description Example directory path UD/UCMDB ucmdb-data-volume Stores data that's generated by UD/UCMDB. /var/vols/itom/ucmdb/data_volume UD/UCMDB ucmdb-config-volume Stores UD/UCMDB configuration files. /var/vols/itom/ucmdb/conf_volume UD/UCMDB ucmdb-log-volume Stores logs generated by UD/UCMDB. /var/vols/itom/ucmdb/log_volume If you already have the PVs available for use, you need to ensure that the storageClassName attribute of the PVs is the same as the storage class name in the my-values.yaml file. In the later step, you will need to specify the name of the PV to be bound with one of the persistent volume claims (PVCs). If you want to create the required PVs from scratch, perform the following tasks: Log in to the bastion node and navigate to the chart package directory. For example: cd UCMDB_Helm_Chart",
    "url": "403-ucmdbgcppreppersistentvol",
    "filename": "403-ucmdbgcppreppersistentvol",
    "headings": [],
    "keywords": [
      "values.yaml",
      "pv.yaml",
      "google.com",
      "configureFilestore.sh",
      "prepare",
      "persistent",
      "volumes",
      "ud",
      "ucmdb",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "google",
      "cloud",
      "filestore",
      "volumes.",
      "component",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "ucmdb-data-volume",
      "stores",
      "data",
      "generated",
      "ucmdb.",
      "var",
      "vols",
      "itom",
      "ucmdb-config-volume",
      "configuration",
      "files.",
      "ucmdb-log-volume",
      "logs",
      "already",
      "available",
      "need",
      "ensure",
      "storageclassname",
      "attribute",
      "same",
      "storage",
      "class",
      "my-values.yaml",
      "file.",
      "later",
      "step",
      "specify",
      "pv",
      "bound",
      "one",
      "volume",
      "claims",
      "pvcs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "log",
      "bastion",
      "node",
      "navigate",
      "chart",
      "package",
      "directory.",
      "cd",
      "ucmdb-helm-charts",
      "samples",
      "open",
      "ucmdb-pv.yaml",
      "file",
      "text",
      "editor",
      "configure",
      "parameters",
      "save",
      "ip",
      "address",
      "paths",
      "provided",
      "creating",
      "uducmdb",
      "value",
      "see",
      "subscribe",
      "filestore.",
      "standard-rwo.",
      "kubectl",
      "get",
      "sc",
      "command",
      "name.",
      "add"
    ],
    "language": "en",
    "word_count": 115,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for ud/ucmdb",
    "contentLower": "ud/ucmdb installation in helm mode requires the following persistent volumes (pvs). pvs are relevant to google cloud filestore volumes. component google cloud filestore volumes name description example directory path ud/ucmdb ucmdb-data-volume stores data that's generated by ud/ucmdb. /var/vols/itom/ucmdb/data_volume ud/ucmdb ucmdb-config-volume stores ud/ucmdb configuration files. /var/vols/itom/ucmdb/conf_volume ud/ucmdb ucmdb-log-volume stores logs generated by ud/ucmdb. /var/vols/itom/ucmdb/log_volume if you already have the pvs available for use, you need to ensure that the storageclassname attribute of the pvs is the same as the storage class name in the my-values.yaml file. in the later step, you will need to specify the name of the pv to be bound with one of the persistent volume claims (pvcs). if you want to create the required pvs from scratch, perform the following tasks: log in to the bastion node and navigate to the chart package directory. for example: cd ucmdb_helm_chart",
    "keywordsLower": [
      "values.yaml",
      "pv.yaml",
      "google.com",
      "configurefilestore.sh",
      "prepare",
      "persistent",
      "volumes",
      "ud",
      "ucmdb",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "google",
      "cloud",
      "filestore",
      "volumes.",
      "component",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "ucmdb-data-volume",
      "stores",
      "data",
      "generated",
      "ucmdb.",
      "var",
      "vols",
      "itom",
      "ucmdb-config-volume",
      "configuration",
      "files.",
      "ucmdb-log-volume",
      "logs",
      "already",
      "available",
      "need",
      "ensure",
      "storageclassname",
      "attribute",
      "same",
      "storage",
      "class",
      "my-values.yaml",
      "file.",
      "later",
      "step",
      "specify",
      "pv",
      "bound",
      "one",
      "volume",
      "claims",
      "pvcs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "log",
      "bastion",
      "node",
      "navigate",
      "chart",
      "package",
      "directory.",
      "cd",
      "ucmdb-helm-charts",
      "samples",
      "open",
      "ucmdb-pv.yaml",
      "file",
      "text",
      "editor",
      "configure",
      "parameters",
      "save",
      "ip",
      "address",
      "paths",
      "provided",
      "creating",
      "uducmdb",
      "value",
      "see",
      "subscribe",
      "filestore.",
      "standard-rwo.",
      "kubectl",
      "get",
      "sc",
      "command",
      "name.",
      "add"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for Audit service (GCP)",
    "content": "Audit service installation in Helm mode requires the following Persistent Volumes (PVs). PVs are relevant to Elastic File System (EFS) volumes. Component Persistent volume name Description ITOM Audit as-vault-volume-<Audit Namespace> PV for configuration ITOM Audit as-log-volume-<Audit Namespace> PV for logs If you want to create the required PVs from scratch, perform the following tasks: Go to the audit install directory (audit-<version>) that you created when downloading the audit Helm chart. For example: cd <audit_install-dir>/audit-2x.x To find or verify the FQDN of the EFS server and the EFS directory path, run the following two commands. To find the name of the config volume, run the following command: kubectl get pv | grep 'config' To describe the PV, run this command: kubectl describe pv <pv name of config volume that you found from the previous step> Part of the output: ... Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: aws-efs.example.net Path: /var",
    "url": "131-preparepvgcp",
    "filename": "131-preparepvgcp",
    "headings": [],
    "keywords": [
      "pv.yaml",
      "example.net",
      "prepare",
      "persistent",
      "volumes",
      "audit",
      "service",
      "gcp",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "elastic",
      "file",
      "system",
      "efs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "itom",
      "as-vault-volume-",
      "pv",
      "configuration",
      "as-log-volume-",
      "logs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "go",
      "install",
      "directory",
      "audit-",
      "created",
      "downloading",
      "chart.",
      "example",
      "cd",
      "audit-2x.x",
      "find",
      "verify",
      "fqdn",
      "server",
      "path",
      "run",
      "two",
      "commands.",
      "config",
      "command",
      "kubectl",
      "get",
      "grep",
      "describe",
      "part",
      "output",
      "source",
      "type",
      "nfs",
      "mount",
      "lasts",
      "lifetime",
      "pod",
      "aws-efs.example.net",
      "var",
      "vols",
      "itsma",
      "config-volume",
      "readonly",
      "false",
      "gives",
      "information",
      "need",
      "give",
      "itom-audit-pv.yaml",
      "start",
      "same",
      "till",
      "directory.",
      "shown",
      "attribute",
      "config-volume.",
      "open",
      "text",
      "editor",
      "provided",
      "during",
      "creation",
      "save",
      "file.",
      "-f",
      "persistentvolume",
      "as-vault-volume-auditns",
      "as-log-volume-auditns"
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for audit service (gcp)",
    "contentLower": "audit service installation in helm mode requires the following persistent volumes (pvs). pvs are relevant to elastic file system (efs) volumes. component persistent volume name description itom audit as-vault-volume-<audit namespace> pv for configuration itom audit as-log-volume-<audit namespace> pv for logs if you want to create the required pvs from scratch, perform the following tasks: go to the audit install directory (audit-<version>) that you created when downloading the audit helm chart. for example: cd <audit_install-dir>/audit-2x.x to find or verify the fqdn of the efs server and the efs directory path, run the following two commands. to find the name of the config volume, run the following command: kubectl get pv | grep 'config' to describe the pv, run this command: kubectl describe pv <pv name of config volume that you found from the previous step> part of the output: ... source: type: nfs (an nfs mount that lasts the lifetime of a pod) server: aws-efs.example.net path: /var",
    "keywordsLower": [
      "pv.yaml",
      "example.net",
      "prepare",
      "persistent",
      "volumes",
      "audit",
      "service",
      "gcp",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "elastic",
      "file",
      "system",
      "efs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "itom",
      "as-vault-volume-",
      "pv",
      "configuration",
      "as-log-volume-",
      "logs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "go",
      "install",
      "directory",
      "audit-",
      "created",
      "downloading",
      "chart.",
      "example",
      "cd",
      "audit-2x.x",
      "find",
      "verify",
      "fqdn",
      "server",
      "path",
      "run",
      "two",
      "commands.",
      "config",
      "command",
      "kubectl",
      "get",
      "grep",
      "describe",
      "part",
      "output",
      "source",
      "type",
      "nfs",
      "mount",
      "lasts",
      "lifetime",
      "pod",
      "aws-efs.example.net",
      "var",
      "vols",
      "itsma",
      "config-volume",
      "readonly",
      "false",
      "gives",
      "information",
      "need",
      "give",
      "itom-audit-pv.yaml",
      "start",
      "same",
      "till",
      "directory.",
      "shown",
      "attribute",
      "config-volume.",
      "open",
      "text",
      "editor",
      "provided",
      "during",
      "creation",
      "save",
      "file.",
      "-f",
      "persistentvolume",
      "as-vault-volume-auditns",
      "as-log-volume-auditns"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan for BYOK",
    "content": "The following table provides an overview of the topics covered in the Plan section: Topic Description Support matrix Based on your deployment plan, review the system requirements for the suite installation. At the conclusion of this phase, you must have the following resources in place: Bastion node required to communicate with the Kubernetes cluster Worker node(s) to run suite services NFS server to store OMT and suite data Relational database Sizing consideration Collect and evaluate data from your current environment to decide the following: Suite deployment option that suits your environment the best Size and complexity of your suite deployment",
    "url": "installonbyokplanforbyok",
    "filename": "installonbyokplanforbyok",
    "headings": [],
    "keywords": [
      "plan",
      "byok",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "support",
      "matrix",
      "based",
      "deployment",
      "review",
      "system",
      "requirements",
      "suite",
      "installation.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "data",
      "relational",
      "database",
      "sizing",
      "consideration",
      "collect",
      "evaluate",
      "current",
      "environment",
      "decide",
      "option",
      "suits",
      "best",
      "size",
      "complexity"
    ],
    "language": "en",
    "word_count": 66,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan for byok",
    "contentLower": "the following table provides an overview of the topics covered in the plan section: topic description support matrix based on your deployment plan, review the system requirements for the suite installation. at the conclusion of this phase, you must have the following resources in place: bastion node required to communicate with the kubernetes cluster worker node(s) to run suite services nfs server to store omt and suite data relational database sizing consideration collect and evaluate data from your current environment to decide the following: suite deployment option that suits your environment the best size and complexity of your suite deployment",
    "keywordsLower": [
      "plan",
      "byok",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "support",
      "matrix",
      "based",
      "deployment",
      "review",
      "system",
      "requirements",
      "suite",
      "installation.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "data",
      "relational",
      "database",
      "sizing",
      "consideration",
      "collect",
      "evaluate",
      "current",
      "environment",
      "decide",
      "option",
      "suits",
      "best",
      "size",
      "complexity"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan for OpenShift",
    "content": "The following table provides an overview of the topics covered in the Plan section: Topic Description Support matrix Based on your deployment plan, review the system requirements for the suite installation. At the conclusion of this phase, you must have the following resources in place: Bastion node required to communicate with the Kubernetes cluster Worker node(s) to run suite services NFS server to store OMT and suite data Relational database Sizing consideration Collect and evaluate data from your current environment to decide the following: Suite deployment option that suits your environment the best Size and complexity of your suite deployment",
    "url": "planopenshiftdeployment",
    "filename": "planopenshiftdeployment",
    "headings": [],
    "keywords": [
      "plan",
      "openshift",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "support",
      "matrix",
      "based",
      "deployment",
      "review",
      "system",
      "requirements",
      "suite",
      "installation.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "data",
      "relational",
      "database",
      "sizing",
      "consideration",
      "collect",
      "evaluate",
      "current",
      "environment",
      "decide",
      "option",
      "suits",
      "best",
      "size",
      "complexity"
    ],
    "language": "en",
    "word_count": 66,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan for openshift",
    "contentLower": "the following table provides an overview of the topics covered in the plan section: topic description support matrix based on your deployment plan, review the system requirements for the suite installation. at the conclusion of this phase, you must have the following resources in place: bastion node required to communicate with the kubernetes cluster worker node(s) to run suite services nfs server to store omt and suite data relational database sizing consideration collect and evaluate data from your current environment to decide the following: suite deployment option that suits your environment the best size and complexity of your suite deployment",
    "keywordsLower": [
      "plan",
      "openshift",
      "following",
      "table",
      "provides",
      "overview",
      "topics",
      "covered",
      "section",
      "topic",
      "description",
      "support",
      "matrix",
      "based",
      "deployment",
      "review",
      "system",
      "requirements",
      "suite",
      "installation.",
      "conclusion",
      "phase",
      "resources",
      "place",
      "bastion",
      "node",
      "required",
      "communicate",
      "kubernetes",
      "cluster",
      "worker",
      "run",
      "services",
      "nfs",
      "server",
      "store",
      "omt",
      "data",
      "relational",
      "database",
      "sizing",
      "consideration",
      "collect",
      "evaluate",
      "current",
      "environment",
      "decide",
      "option",
      "suits",
      "best",
      "size",
      "complexity"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare external databases (OpenShift)",
    "content": "You must use external databases for OPTIC Management Toolkit (OMT) and the suite in a production environment. For information about supported external databases, see Support matrix for on-premises deployment. This topic doesn't include instructions on how to Configure external PostgreSQL databases for OMT. This is because your deployment doesn't require OMT or you have already done so during the OMT installation. The following procedures include steps to configure the pg_hba.conf file for PostgreSQL. For more information about how to configure the parameters in this file (connection type, database name, user name, client IP address range, and authentication method), refer to your specific PostgreSQL documentation (for example, the PostgreSQL 14 documentation). General requirements for PostgreSQL databases To set up PostgreSQL databases, you must meet the following requirements: Configurations described in this section, including parameters per profile (see the Prepare external PostgreS",
    "url": "prepareexternaldbopenshift",
    "filename": "prepareexternaldbopenshift",
    "headings": [
      "General requirements for PostgreSQL databases",
      "Install PostgreSQL",
      "Prepare external PostgreSQL for the suite",
      "Install and configure Vertica for CMP FinOps"
    ],
    "keywords": [
      "0.5",
      "0.70",
      "0.65",
      "172.0.0",
      "postgresql.conf",
      "en_US.UTF",
      "0.75",
      "pg_hba.conf",
      "sha-256",
      "172.0.0.0",
      "1.1.1",
      "SHA-256",
      "prepare",
      "external",
      "databases",
      "openshift",
      "general",
      "requirements",
      "postgresql",
      "install",
      "suite",
      "configure",
      "vertica",
      "cmp",
      "finops",
      "optic",
      "management",
      "toolkit",
      "omt",
      "production",
      "environment.",
      "information",
      "about",
      "supported",
      "see",
      "support",
      "matrix",
      "on-premises",
      "deployment.",
      "topic",
      "doesn",
      "include",
      "instructions",
      "omt.",
      "because",
      "deployment",
      "require",
      "already",
      "done",
      "during",
      "installation.",
      "following",
      "procedures",
      "steps",
      "file",
      "postgresql.",
      "parameters",
      "connection",
      "type",
      "database",
      "name",
      "user",
      "client",
      "ip",
      "address",
      "range",
      "authentication",
      "method",
      "refer",
      "specific",
      "documentation",
      "example",
      "14",
      "set",
      "meet",
      "configurations",
      "described",
      "section",
      "including",
      "per",
      "profile",
      "version",
      "encoding",
      "utf8",
      "applied.",
      "able",
      "create",
      "db",
      "role.",
      "newly",
      "created",
      "user.",
      "owner",
      "perform",
      "read",
      "update",
      "delete",
      "operations",
      "own",
      "objects"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare external databases (openshift)",
    "contentLower": "you must use external databases for optic management toolkit (omt) and the suite in a production environment. for information about supported external databases, see support matrix for on-premises deployment. this topic doesn't include instructions on how to configure external postgresql databases for omt. this is because your deployment doesn't require omt or you have already done so during the omt installation. the following procedures include steps to configure the pg_hba.conf file for postgresql. for more information about how to configure the parameters in this file (connection type, database name, user name, client ip address range, and authentication method), refer to your specific postgresql documentation (for example, the postgresql 14 documentation). general requirements for postgresql databases to set up postgresql databases, you must meet the following requirements: configurations described in this section, including parameters per profile (see the prepare external postgres",
    "keywordsLower": [
      "0.5",
      "0.70",
      "0.65",
      "172.0.0",
      "postgresql.conf",
      "en_us.utf",
      "0.75",
      "pg_hba.conf",
      "sha-256",
      "172.0.0.0",
      "1.1.1",
      "sha-256",
      "prepare",
      "external",
      "databases",
      "openshift",
      "general",
      "requirements",
      "postgresql",
      "install",
      "suite",
      "configure",
      "vertica",
      "cmp",
      "finops",
      "optic",
      "management",
      "toolkit",
      "omt",
      "production",
      "environment.",
      "information",
      "about",
      "supported",
      "see",
      "support",
      "matrix",
      "on-premises",
      "deployment.",
      "topic",
      "doesn",
      "include",
      "instructions",
      "omt.",
      "because",
      "deployment",
      "require",
      "already",
      "done",
      "during",
      "installation.",
      "following",
      "procedures",
      "steps",
      "file",
      "postgresql.",
      "parameters",
      "connection",
      "type",
      "database",
      "name",
      "user",
      "client",
      "ip",
      "address",
      "range",
      "authentication",
      "method",
      "refer",
      "specific",
      "documentation",
      "example",
      "14",
      "set",
      "meet",
      "configurations",
      "described",
      "section",
      "including",
      "per",
      "profile",
      "version",
      "encoding",
      "utf8",
      "applied.",
      "able",
      "create",
      "db",
      "role.",
      "newly",
      "created",
      "user.",
      "owner",
      "perform",
      "read",
      "update",
      "delete",
      "operations",
      "own",
      "objects"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare an OpenShift project for UD/UCMDB",
    "content": "Before installing the containerized UD/UCMDB, you need to create an OpenShift project. Create an OpenShift project Log in to your OpenShift Container Platform. Go to Home > Projects, and then click Create Project. Enter a name in the Name field. For example, ucmdb-prod. Click Create to create a deployment for UD/UCMDB. Check the UID and GID You will need to check and take notes of the user ID (UID) and group ID (GID) in the project so that later you can use UID and GID when configuring NFS volumes and defining your custom values.yaml file. Go to Home > Projects, and then select the project created for the UD/UCMDB deployment. For example, ucmdb-prod. Click Details, and then click the edit icon under Annotations. Check the UID and GID values. For example: openshift.io/sa.scc.uid-range=1000590000/10000 openshift.io/sa.scc.supplemental-groups=1000590000/10000 Take notes of the UID and GID, and then click Save. You will need to use the UID and GID later when you Configure NFS volumes for U",
    "url": "prepareopenshiftcms",
    "filename": "prepareopenshiftcms",
    "headings": [
      "Create an OpenShift project",
      "Check the UID and GID",
      "Create the registry pull secret"
    ],
    "keywords": [
      "uducmdb",
      "openshift.io",
      "values.yaml",
      "sa.scc",
      "prepare",
      "openshift",
      "project",
      "ud",
      "ucmdb",
      "create",
      "check",
      "uid",
      "gid",
      "registry",
      "pull",
      "secret",
      "before",
      "installing",
      "containerized",
      "need",
      "project.",
      "log",
      "container",
      "platform.",
      "go",
      "home",
      "projects",
      "click",
      "enter",
      "name",
      "field.",
      "example",
      "ucmdb-prod.",
      "deployment",
      "ucmdb.",
      "take",
      "notes",
      "user",
      "id",
      "group",
      "later",
      "configuring",
      "nfs",
      "volumes",
      "defining",
      "custom",
      "file.",
      "select",
      "created",
      "deployment.",
      "details",
      "edit",
      "icon",
      "under",
      "annotations.",
      "values.",
      "sa.scc.uid-range",
      "1000590000",
      "10000",
      "sa.scc.supplemental-groups",
      "save.",
      "configure",
      "my-values.yaml.",
      "run",
      "following",
      "command",
      "secret.",
      "note",
      "bastion",
      "node",
      "rather",
      "kubectl",
      "docker-registry",
      "registrypullsecret",
      "-n",
      "--docker-server",
      "--docker-username",
      "--docker-password",
      "specify",
      "creating",
      "first",
      "step.",
      "image",
      "url.",
      "username.",
      "registry-admin",
      "password."
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare an openshift project for ud/ucmdb",
    "contentLower": "before installing the containerized ud/ucmdb, you need to create an openshift project. create an openshift project log in to your openshift container platform. go to home > projects, and then click create project. enter a name in the name field. for example, ucmdb-prod. click create to create a deployment for ud/ucmdb. check the uid and gid you will need to check and take notes of the user id (uid) and group id (gid) in the project so that later you can use uid and gid when configuring nfs volumes and defining your custom values.yaml file. go to home > projects, and then select the project created for the ud/ucmdb deployment. for example, ucmdb-prod. click details, and then click the edit icon under annotations. check the uid and gid values. for example: openshift.io/sa.scc.uid-range=1000590000/10000 openshift.io/sa.scc.supplemental-groups=1000590000/10000 take notes of the uid and gid, and then click save. you will need to use the uid and gid later when you configure nfs volumes for u",
    "keywordsLower": [
      "uducmdb",
      "openshift.io",
      "values.yaml",
      "sa.scc",
      "prepare",
      "openshift",
      "project",
      "ud",
      "ucmdb",
      "create",
      "check",
      "uid",
      "gid",
      "registry",
      "pull",
      "secret",
      "before",
      "installing",
      "containerized",
      "need",
      "project.",
      "log",
      "container",
      "platform.",
      "go",
      "home",
      "projects",
      "click",
      "enter",
      "name",
      "field.",
      "example",
      "ucmdb-prod.",
      "deployment",
      "ucmdb.",
      "take",
      "notes",
      "user",
      "id",
      "group",
      "later",
      "configuring",
      "nfs",
      "volumes",
      "defining",
      "custom",
      "file.",
      "select",
      "created",
      "deployment.",
      "details",
      "edit",
      "icon",
      "under",
      "annotations.",
      "values.",
      "sa.scc.uid-range",
      "1000590000",
      "10000",
      "sa.scc.supplemental-groups",
      "save.",
      "configure",
      "my-values.yaml.",
      "run",
      "following",
      "command",
      "secret.",
      "note",
      "bastion",
      "node",
      "rather",
      "kubectl",
      "docker-registry",
      "registrypullsecret",
      "-n",
      "--docker-server",
      "--docker-username",
      "--docker-password",
      "specify",
      "creating",
      "first",
      "step.",
      "image",
      "url.",
      "username.",
      "registry-admin",
      "password."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for UD/UCMDB (OpenShift)",
    "content": "Static PV creation UD/UCMDB installation in Helm mode requires the following Persistent Volumes (PVs). PVs are relevant to NFS volumes. Component Persistent volume name Description UD/UCMDB ucmdb-config-volume PV for configuration UD/UCMDB ucmdb-data-volume PV for data UD/UCMDB ucmdb-log-volume PV for logs If you already have the PVs available for use, you need to ensure that the storageClassName attribute of the PVs is the same as the storage class name in the values.yaml file. In the later step, you will need to specify the name of PV to be bound with one of the persistent volume claims (PVCs). If you want to create the required PVs from scratch, perform the following tasks: Navigate to the chart package directory. For example: cd UCMDB_Helm_Chart-2x.x/ucmdb-helm-charts/samples/ Open the ucmdb-pv.yaml file with a text editor, provide the FQDN of the NFS server and the NFS directory path that you provided during the creation of the NFS volumes, and then save the file. By default, the ",
    "url": "cmswithsmaxpreparepvsopenshift",
    "filename": "cmswithsmaxpreparepvsopenshift",
    "headings": [
      "Static PV creation",
      "Dynamic PV creation"
    ],
    "keywords": [
      "uducmdb",
      "button.In",
      "values.yaml",
      "pv.yaml",
      "option.Size",
      "prepare",
      "persistent",
      "volumes",
      "ud",
      "ucmdb",
      "openshift",
      "static",
      "pv",
      "creation",
      "dynamic",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "nfs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "ucmdb-config-volume",
      "configuration",
      "ucmdb-data-volume",
      "data",
      "ucmdb-log-volume",
      "logs",
      "already",
      "available",
      "need",
      "ensure",
      "storageclassname",
      "attribute",
      "same",
      "storage",
      "class",
      "file.",
      "later",
      "step",
      "specify",
      "bound",
      "one",
      "claims",
      "pvcs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "navigate",
      "chart",
      "package",
      "directory.",
      "example",
      "cd",
      "ucmdb-helm-charts",
      "samples",
      "open",
      "ucmdb-pv.yaml",
      "file",
      "text",
      "editor",
      "provide",
      "fqdn",
      "server",
      "directory",
      "path",
      "provided",
      "during",
      "save",
      "default",
      "empty.",
      "change",
      "value",
      "make",
      "sure",
      "together.",
      "run",
      "command",
      "kubectl",
      "-f",
      "verify",
      "get",
      "grep",
      "dynamically",
      "follow",
      "steps",
      "console",
      "select",
      "left",
      "side",
      "panel"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for ud/ucmdb (openshift)",
    "contentLower": "static pv creation ud/ucmdb installation in helm mode requires the following persistent volumes (pvs). pvs are relevant to nfs volumes. component persistent volume name description ud/ucmdb ucmdb-config-volume pv for configuration ud/ucmdb ucmdb-data-volume pv for data ud/ucmdb ucmdb-log-volume pv for logs if you already have the pvs available for use, you need to ensure that the storageclassname attribute of the pvs is the same as the storage class name in the values.yaml file. in the later step, you will need to specify the name of pv to be bound with one of the persistent volume claims (pvcs). if you want to create the required pvs from scratch, perform the following tasks: navigate to the chart package directory. for example: cd ucmdb_helm_chart-2x.x/ucmdb-helm-charts/samples/ open the ucmdb-pv.yaml file with a text editor, provide the fqdn of the nfs server and the nfs directory path that you provided during the creation of the nfs volumes, and then save the file. by default, the ",
    "keywordsLower": [
      "uducmdb",
      "button.in",
      "values.yaml",
      "pv.yaml",
      "option.size",
      "prepare",
      "persistent",
      "volumes",
      "ud",
      "ucmdb",
      "openshift",
      "static",
      "pv",
      "creation",
      "dynamic",
      "installation",
      "helm",
      "mode",
      "requires",
      "following",
      "pvs",
      "relevant",
      "nfs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "ucmdb-config-volume",
      "configuration",
      "ucmdb-data-volume",
      "data",
      "ucmdb-log-volume",
      "logs",
      "already",
      "available",
      "need",
      "ensure",
      "storageclassname",
      "attribute",
      "same",
      "storage",
      "class",
      "file.",
      "later",
      "step",
      "specify",
      "bound",
      "one",
      "claims",
      "pvcs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "navigate",
      "chart",
      "package",
      "directory.",
      "example",
      "cd",
      "ucmdb-helm-charts",
      "samples",
      "open",
      "ucmdb-pv.yaml",
      "file",
      "text",
      "editor",
      "provide",
      "fqdn",
      "server",
      "directory",
      "path",
      "provided",
      "during",
      "save",
      "default",
      "empty.",
      "change",
      "value",
      "make",
      "sure",
      "together.",
      "run",
      "command",
      "kubectl",
      "-f",
      "verify",
      "get",
      "grep",
      "dynamically",
      "follow",
      "steps",
      "console",
      "select",
      "left",
      "side",
      "panel"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare external databases for UD/UCMDB (OpenShift)",
    "content": "For this release, you must use separate PostgreSQL databases instances for Service Management and the containerized UD/UCMDB for these reasons: Because of the different dimensions of Service Management and UD/UCMDB database sizing, if Service Management and UD/UCMDB share the same PostgreSQL database, the PostgreSQL instance size will be much larger than the size of separate ones. There are unknown issues and risks if Service Management and UD/UCMDB share the same PostgreSQL database. To prepare PostgreSQL for UD/UCMDB, perform the following tasks. Install PostgreSQL Follow the official PostgreSQL documentation for installation instructions to create a separate PostgreSQL database server for UD/UCMDB. Note It is strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it. Also, if the database server machine is a virtual machine, the resource MUST be dedicated to the database server. Database se",
    "url": "cmswithsmaxprepareexternaldbopenshift",
    "filename": "cmswithsmaxprepareexternaldbopenshift",
    "headings": [
      "Install PostgreSQL",
      "Configure database server",
      "Create database users and databases for UD/UCMDB"
    ],
    "keywords": [
      "uducmdb",
      "en_US.UTF",
      "postgresql.conf",
      "prepare",
      "external",
      "databases",
      "ud",
      "ucmdb",
      "openshift",
      "install",
      "postgresql",
      "configure",
      "database",
      "server",
      "create",
      "users",
      "release",
      "separate",
      "instances",
      "service",
      "management",
      "containerized",
      "reasons",
      "because",
      "different",
      "dimensions",
      "sizing",
      "share",
      "same",
      "instance",
      "size",
      "much",
      "larger",
      "ones.",
      "there",
      "unknown",
      "issues",
      "risks",
      "database.",
      "perform",
      "following",
      "tasks.",
      "follow",
      "official",
      "documentation",
      "installation",
      "instructions",
      "ucmdb.",
      "note",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "it.",
      "virtual",
      "resource",
      "dedicated",
      "server.",
      "servers",
      "set",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "universal",
      "cmdb",
      "servers.",
      "located",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "system",
      "performance",
      "impacted.",
      "optionally",
      "enable",
      "tls",
      "connections",
      "see",
      "postgresql.",
      "both",
      "ssl",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "details",
      "documentation.",
      "file",
      "parameters",
      "depending",
      "deployment",
      "small"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare external databases for ud/ucmdb (openshift)",
    "contentLower": "for this release, you must use separate postgresql databases instances for service management and the containerized ud/ucmdb for these reasons: because of the different dimensions of service management and ud/ucmdb database sizing, if service management and ud/ucmdb share the same postgresql database, the postgresql instance size will be much larger than the size of separate ones. there are unknown issues and risks if service management and ud/ucmdb share the same postgresql database. to prepare postgresql for ud/ucmdb, perform the following tasks. install postgresql follow the official postgresql documentation for installation instructions to create a separate postgresql database server for ud/ucmdb. note it is strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it. also, if the database server machine is a virtual machine, the resource must be dedicated to the database server. database se",
    "keywordsLower": [
      "uducmdb",
      "en_us.utf",
      "postgresql.conf",
      "prepare",
      "external",
      "databases",
      "ud",
      "ucmdb",
      "openshift",
      "install",
      "postgresql",
      "configure",
      "database",
      "server",
      "create",
      "users",
      "release",
      "separate",
      "instances",
      "service",
      "management",
      "containerized",
      "reasons",
      "because",
      "different",
      "dimensions",
      "sizing",
      "share",
      "same",
      "instance",
      "size",
      "much",
      "larger",
      "ones.",
      "there",
      "unknown",
      "issues",
      "risks",
      "database.",
      "perform",
      "following",
      "tasks.",
      "follow",
      "official",
      "documentation",
      "installation",
      "instructions",
      "ucmdb.",
      "note",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "it.",
      "virtual",
      "resource",
      "dedicated",
      "server.",
      "servers",
      "set",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "universal",
      "cmdb",
      "servers.",
      "located",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "system",
      "performance",
      "impacted.",
      "optionally",
      "enable",
      "tls",
      "connections",
      "see",
      "postgresql.",
      "both",
      "ssl",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "details",
      "documentation.",
      "file",
      "parameters",
      "depending",
      "deployment",
      "small"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare NFS and PostgreSQL servers",
    "content": "Before installing OO, perform the following tasks: Prepare NFS and PostgreSQL servers Decide the load balancer option Prepare NFS and PostgreSQL servers Use a separate NFS and PostgreSQL server for OO, instead of using the same servers as for Service Management or Cloud Management. Before you begin, prepare separate servers for OO installation: Decide on the hardware resources (worker nodes, NFS server, and PostgreSQL server) that are required for your OO deployment. For details, see the Service Management or Cloud Management sizing guide. Prepare a separate NFS server and separate PostgreSQL database server, and required worker nodes for OO, according to your OO sizing profile. Decide the load balancer option Before deploying OO Containerized in a cluster along with Service Management, decide how to access the OO system. You can either: Use the same FQDN but different ports: OO shares the same FQDN with Service Management but uses a different port. For example, xxx.example.com:443 for",
    "url": "preparenfspgsqloo",
    "filename": "preparenfspgsqloo",
    "headings": [
      "Prepare NFS and PostgreSQL servers",
      "Decide the load balancer option"
    ],
    "keywords": [
      "example.com",
      "prepare",
      "nfs",
      "postgresql",
      "servers",
      "decide",
      "load",
      "balancer",
      "option",
      "before",
      "installing",
      "oo",
      "perform",
      "following",
      "tasks",
      "separate",
      "server",
      "instead",
      "same",
      "service",
      "management",
      "cloud",
      "management.",
      "begin",
      "installation",
      "hardware",
      "resources",
      "worker",
      "nodes",
      "required",
      "deployment.",
      "details",
      "see",
      "sizing",
      "guide.",
      "database",
      "according",
      "profile.",
      "deploying",
      "containerized",
      "cluster",
      "along",
      "access",
      "system.",
      "either",
      "fqdn",
      "different",
      "ports",
      "shares",
      "uses",
      "port.",
      "example",
      "xxx.example.com",
      "443",
      "9443",
      "oo.",
      "leverage",
      "hence",
      "requires",
      "additional",
      "however",
      "expose",
      "port",
      "balancer.",
      "fqdns",
      "443.",
      "sma.example.com",
      "oo.example.com",
      "both",
      "configure",
      "sharing",
      "one."
    ],
    "language": "en",
    "word_count": 110,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare nfs and postgresql servers",
    "contentLower": "before installing oo, perform the following tasks: prepare nfs and postgresql servers decide the load balancer option prepare nfs and postgresql servers use a separate nfs and postgresql server for oo, instead of using the same servers as for service management or cloud management. before you begin, prepare separate servers for oo installation: decide on the hardware resources (worker nodes, nfs server, and postgresql server) that are required for your oo deployment. for details, see the service management or cloud management sizing guide. prepare a separate nfs server and separate postgresql database server, and required worker nodes for oo, according to your oo sizing profile. decide the load balancer option before deploying oo containerized in a cluster along with service management, decide how to access the oo system. you can either: use the same fqdn but different ports: oo shares the same fqdn with service management but uses a different port. for example, xxx.example.com:443 for",
    "keywordsLower": [
      "example.com",
      "prepare",
      "nfs",
      "postgresql",
      "servers",
      "decide",
      "load",
      "balancer",
      "option",
      "before",
      "installing",
      "oo",
      "perform",
      "following",
      "tasks",
      "separate",
      "server",
      "instead",
      "same",
      "service",
      "management",
      "cloud",
      "management.",
      "begin",
      "installation",
      "hardware",
      "resources",
      "worker",
      "nodes",
      "required",
      "deployment.",
      "details",
      "see",
      "sizing",
      "guide.",
      "database",
      "according",
      "profile.",
      "deploying",
      "containerized",
      "cluster",
      "along",
      "access",
      "system.",
      "either",
      "fqdn",
      "different",
      "ports",
      "shares",
      "uses",
      "port.",
      "example",
      "xxx.example.com",
      "443",
      "9443",
      "oo.",
      "leverage",
      "hence",
      "requires",
      "additional",
      "however",
      "expose",
      "port",
      "balancer.",
      "fqdns",
      "443.",
      "sma.example.com",
      "oo.example.com",
      "both",
      "configure",
      "sharing",
      "one."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for OO on OpenShift",
    "content": "OO Containerized installation requires below Persistent Volumes (PVs) and Persistent Volume Claims (PVCs). PVs are pieces of storage relevant to file system volumes, and PVCs are requests for a piece of storage on a volume. Currently, OO Containerized uses persistent storage by providing the names of the required PVCs at OO Containerized install time via Helm parameters. This means that you must create both PVs and PVCs upfront. Component Volume name Description Example Directory path Size OO oo-config-vol Stores OO configuration files. /var/vols/itom/oo/oo_config_vol ~ 1 GB OO oo-data-vol Stores OO, OO deployed AutoPass, and OO deployed Vault-generated data. /var/vols/itom/oo/oo_data_vol ~ 1 GB OO oo-logs-vol Stores logs generated by the OO components, except RAS logs which are stored in a different volume. /var/vols/itom/oo/oo_logs_vol 1-5 GB OO oo-data-export-vol OO runs a Kubernetes cron job for purging the OO database with a retention period set to 375 days (default value). All th",
    "url": "399-preparepvooocp",
    "filename": "399-preparepvooocp",
    "headings": [
      "Create persistent volumes",
      "Create persistent volume claims"
    ],
    "keywords": [
      "volumes.yaml",
      "claims.yaml",
      "prepare",
      "persistent",
      "volumes",
      "oo",
      "openshift",
      "create",
      "volume",
      "claims",
      "containerized",
      "installation",
      "requires",
      "below",
      "pvs",
      "pvcs",
      "pieces",
      "storage",
      "relevant",
      "file",
      "system",
      "requests",
      "piece",
      "volume.",
      "currently",
      "uses",
      "providing",
      "names",
      "required",
      "install",
      "time",
      "via",
      "helm",
      "parameters.",
      "means",
      "both",
      "upfront.",
      "component",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "size",
      "oo-config-vol",
      "stores",
      "configuration",
      "files.",
      "var",
      "vols",
      "itom",
      "gb",
      "oo-data-vol",
      "deployed",
      "autopass",
      "vault-generated",
      "data.",
      "oo-logs-vol",
      "logs",
      "generated",
      "components",
      "except",
      "ras",
      "stored",
      "different",
      "1-5",
      "oo-data-export-vol",
      "runs",
      "kubernetes",
      "cron",
      "job",
      "purging",
      "database",
      "retention",
      "period",
      "set",
      "375",
      "days",
      "default",
      "value",
      "all",
      "data",
      "marked",
      "purge",
      "gets",
      "exported",
      "first",
      "exposed",
      "before",
      "purging.",
      "50-90",
      "see",
      "notes",
      "oo-ras-logs-vol",
      "logs.",
      "note",
      "maintenance",
      "export",
      "find",
      "information"
    ],
    "language": "en",
    "word_count": 130,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for oo on openshift",
    "contentLower": "oo containerized installation requires below persistent volumes (pvs) and persistent volume claims (pvcs). pvs are pieces of storage relevant to file system volumes, and pvcs are requests for a piece of storage on a volume. currently, oo containerized uses persistent storage by providing the names of the required pvcs at oo containerized install time via helm parameters. this means that you must create both pvs and pvcs upfront. component volume name description example directory path size oo oo-config-vol stores oo configuration files. /var/vols/itom/oo/oo_config_vol ~ 1 gb oo oo-data-vol stores oo, oo deployed autopass, and oo deployed vault-generated data. /var/vols/itom/oo/oo_data_vol ~ 1 gb oo oo-logs-vol stores logs generated by the oo components, except ras logs which are stored in a different volume. /var/vols/itom/oo/oo_logs_vol 1-5 gb oo oo-data-export-vol oo runs a kubernetes cron job for purging the oo database with a retention period set to 375 days (default value). all th",
    "keywordsLower": [
      "volumes.yaml",
      "claims.yaml",
      "prepare",
      "persistent",
      "volumes",
      "oo",
      "openshift",
      "create",
      "volume",
      "claims",
      "containerized",
      "installation",
      "requires",
      "below",
      "pvs",
      "pvcs",
      "pieces",
      "storage",
      "relevant",
      "file",
      "system",
      "requests",
      "piece",
      "volume.",
      "currently",
      "uses",
      "providing",
      "names",
      "required",
      "install",
      "time",
      "via",
      "helm",
      "parameters.",
      "means",
      "both",
      "upfront.",
      "component",
      "name",
      "description",
      "example",
      "directory",
      "path",
      "size",
      "oo-config-vol",
      "stores",
      "configuration",
      "files.",
      "var",
      "vols",
      "itom",
      "gb",
      "oo-data-vol",
      "deployed",
      "autopass",
      "vault-generated",
      "data.",
      "oo-logs-vol",
      "logs",
      "generated",
      "components",
      "except",
      "ras",
      "stored",
      "different",
      "1-5",
      "oo-data-export-vol",
      "runs",
      "kubernetes",
      "cron",
      "job",
      "purging",
      "database",
      "retention",
      "period",
      "set",
      "375",
      "days",
      "default",
      "value",
      "all",
      "data",
      "marked",
      "purge",
      "gets",
      "exported",
      "first",
      "exposed",
      "before",
      "purging.",
      "50-90",
      "see",
      "notes",
      "oo-ras-logs-vol",
      "logs.",
      "note",
      "maintenance",
      "export",
      "find",
      "information"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare external databases for OO on OpenShift",
    "content": "You must use separate PostgreSQL databases instances for the suite and OO for these reasons: The PostgreSQL instance size increases more than the size of the other databases. There are unknown issues and risks if the suite and OO share the same PostgreSQL database. Note that: It's strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it. If the database server machine is a virtual machine, the resource must be a dedicated database server. Set the database servers to the same time zone, daylight savings settings, and time as the OO servers. Make sure the OO installation server is in the same LAN as the database servers (without a proxy and firewalls between them). Otherwise, this may impact your system's performance. Set the machine's system resources where you install PostgreSQL database according to the on-premises deployment sizing guide. Install the PostgreSQL database You can use a Postgr",
    "url": "prepareexternaldbooocp",
    "filename": "prepareexternaldbooocp",
    "headings": [
      "Install the PostgreSQL database",
      "Configure the database server",
      "Configure the PostgreSQL database server",
      "Create OO database users and database names"
    ],
    "keywords": [
      "en_US.UTF",
      "postgresql.conf",
      "7.5",
      "4.5",
      "prepare",
      "external",
      "databases",
      "oo",
      "openshift",
      "install",
      "postgresql",
      "database",
      "configure",
      "server",
      "create",
      "users",
      "names",
      "separate",
      "instances",
      "suite",
      "reasons",
      "instance",
      "size",
      "increases",
      "databases.",
      "there",
      "unknown",
      "issues",
      "risks",
      "share",
      "same",
      "database.",
      "note",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "it.",
      "virtual",
      "resource",
      "dedicated",
      "server.",
      "set",
      "servers",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "servers.",
      "make",
      "sure",
      "installation",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "impact",
      "system",
      "performance.",
      "resources",
      "according",
      "on-premises",
      "deployment",
      "sizing",
      "guide.",
      "containerized.",
      "please",
      "see",
      "documentation",
      "details.",
      "optional",
      "enable",
      "tls",
      "connections",
      "both",
      "ssl",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "details",
      "documentation.",
      "following",
      "parameters",
      "file.",
      "parameter",
      "value",
      "gb",
      "256",
      "mb",
      "1400",
      "300",
      "default",
      "localhost"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare external databases for oo on openshift",
    "contentLower": "you must use separate postgresql databases instances for the suite and oo for these reasons: the postgresql instance size increases more than the size of the other databases. there are unknown issues and risks if the suite and oo share the same postgresql database. note that: it's strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it. if the database server machine is a virtual machine, the resource must be a dedicated database server. set the database servers to the same time zone, daylight savings settings, and time as the oo servers. make sure the oo installation server is in the same lan as the database servers (without a proxy and firewalls between them). otherwise, this may impact your system's performance. set the machine's system resources where you install postgresql database according to the on-premises deployment sizing guide. install the postgresql database you can use a postgr",
    "keywordsLower": [
      "en_us.utf",
      "postgresql.conf",
      "7.5",
      "4.5",
      "prepare",
      "external",
      "databases",
      "oo",
      "openshift",
      "install",
      "postgresql",
      "database",
      "configure",
      "server",
      "create",
      "users",
      "names",
      "separate",
      "instances",
      "suite",
      "reasons",
      "instance",
      "size",
      "increases",
      "databases.",
      "there",
      "unknown",
      "issues",
      "risks",
      "share",
      "same",
      "database.",
      "note",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "it.",
      "virtual",
      "resource",
      "dedicated",
      "server.",
      "set",
      "servers",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "servers.",
      "make",
      "sure",
      "installation",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "impact",
      "system",
      "performance.",
      "resources",
      "according",
      "on-premises",
      "deployment",
      "sizing",
      "guide.",
      "containerized.",
      "please",
      "see",
      "documentation",
      "details.",
      "optional",
      "enable",
      "tls",
      "connections",
      "both",
      "ssl",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "details",
      "documentation.",
      "following",
      "parameters",
      "file.",
      "parameter",
      "value",
      "gb",
      "256",
      "mb",
      "1400",
      "300",
      "default",
      "localhost"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent volumes for Audit (OpenShift)",
    "content": "Audit service installation on OpenShift requires the following Persistent Volumes (PVs). PVs are relevant to NFS volumes. Component Persistent volume name Description ITOM Audit as-vault-volume-<Audit Namespace> PV for configuration ITOM Audit as-log-volume-<Audit Namespace> PV for logs If you want to create the required PVs from scratch, perform the following tasks: Navigate to the audit install directory audit-<version> that you created when downloading the audit Helm chart. For example: cd <audit_install-dir>/audit-2x.x Open the itom-audit-pv.yaml file with a text editor, enter the FQDN of the NFS server and the NFS directory path that you provided during the creation of the NFS volumes, and then save the file. Run the following command to create persistent volumes: kubectl create -f itom-audit-pv.yaml Output: persistentvolume/as-vault-volume-auditns created persistentvolume/as-log-volume-auditns created Run the following command to verify the volume creation: kubectl get pv |grep a",
    "url": "auditpreparepvopenshift",
    "filename": "auditpreparepvopenshift",
    "headings": [],
    "keywords": [
      "pv.yaml",
      "prepare",
      "persistent",
      "volumes",
      "audit",
      "openshift",
      "service",
      "installation",
      "requires",
      "following",
      "pvs",
      "relevant",
      "nfs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "itom",
      "as-vault-volume-",
      "pv",
      "configuration",
      "as-log-volume-",
      "logs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "navigate",
      "install",
      "directory",
      "audit-",
      "created",
      "downloading",
      "helm",
      "chart.",
      "example",
      "cd",
      "audit-2x.x",
      "open",
      "itom-audit-pv.yaml",
      "file",
      "text",
      "editor",
      "enter",
      "fqdn",
      "server",
      "path",
      "provided",
      "during",
      "creation",
      "save",
      "file.",
      "run",
      "command",
      "kubectl",
      "-f",
      "output",
      "persistentvolume",
      "as-vault-volume-auditns",
      "as-log-volume-auditns",
      "verify",
      "get",
      "grep",
      "5gi",
      "rwx",
      "retain",
      "available",
      "itom-audit-sc",
      "17s"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent volumes for audit (openshift)",
    "contentLower": "audit service installation on openshift requires the following persistent volumes (pvs). pvs are relevant to nfs volumes. component persistent volume name description itom audit as-vault-volume-<audit namespace> pv for configuration itom audit as-log-volume-<audit namespace> pv for logs if you want to create the required pvs from scratch, perform the following tasks: navigate to the audit install directory audit-<version> that you created when downloading the audit helm chart. for example: cd <audit_install-dir>/audit-2x.x open the itom-audit-pv.yaml file with a text editor, enter the fqdn of the nfs server and the nfs directory path that you provided during the creation of the nfs volumes, and then save the file. run the following command to create persistent volumes: kubectl create -f itom-audit-pv.yaml output: persistentvolume/as-vault-volume-auditns created persistentvolume/as-log-volume-auditns created run the following command to verify the volume creation: kubectl get pv |grep a",
    "keywordsLower": [
      "pv.yaml",
      "prepare",
      "persistent",
      "volumes",
      "audit",
      "openshift",
      "service",
      "installation",
      "requires",
      "following",
      "pvs",
      "relevant",
      "nfs",
      "volumes.",
      "component",
      "volume",
      "name",
      "description",
      "itom",
      "as-vault-volume-",
      "pv",
      "configuration",
      "as-log-volume-",
      "logs",
      "want",
      "create",
      "required",
      "scratch",
      "perform",
      "tasks",
      "navigate",
      "install",
      "directory",
      "audit-",
      "created",
      "downloading",
      "helm",
      "chart.",
      "example",
      "cd",
      "audit-2x.x",
      "open",
      "itom-audit-pv.yaml",
      "file",
      "text",
      "editor",
      "enter",
      "fqdn",
      "server",
      "path",
      "provided",
      "during",
      "creation",
      "save",
      "file.",
      "run",
      "command",
      "kubectl",
      "-f",
      "output",
      "persistentvolume",
      "as-vault-volume-auditns",
      "as-log-volume-auditns",
      "verify",
      "get",
      "grep",
      "5gi",
      "rwx",
      "retain",
      "available",
      "itom-audit-sc",
      "17s"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare an external database for Audit (OpenShift)",
    "content": "To prepare PostgreSQL, perform the following tasks. Install PostgreSQL You must create a separate PostgreSQL database for Audit service. This database can share the same PostgreSQL instance as Service Management. Make sure you enable SSL on the PostgreSQL because Audit uses SSL to communicate with PostgreSQL. Instructions on how to create a user and a database for a service for PostgreSQL are provided in the PostgreSQL section below. Note It is strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it. Also, if the database server machine is a virtual machine, you must dedicate the resource to the database server. You must set the database servers to the same time zone, daylight savings settings, and time as the Audit server. You must keep the Audit server in the same LAN as the database servers (without a proxy or firewalls between them). Otherwise, your system's performance may get impacted.",
    "url": "auditprepareexternaldbopenshift",
    "filename": "auditprepareexternaldbopenshift",
    "headings": [
      "Install PostgreSQL",
      "Configure PostgreSQL database server",
      "PostgreSQL"
    ],
    "keywords": [
      "en_US.UTF",
      "postgresql.conf",
      "prepare",
      "external",
      "database",
      "audit",
      "openshift",
      "install",
      "postgresql",
      "configure",
      "server",
      "perform",
      "following",
      "tasks.",
      "create",
      "separate",
      "service.",
      "share",
      "same",
      "instance",
      "service",
      "management.",
      "make",
      "sure",
      "enable",
      "ssl",
      "because",
      "uses",
      "communicate",
      "postgresql.",
      "instructions",
      "user",
      "provided",
      "section",
      "below.",
      "note",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "it.",
      "virtual",
      "dedicate",
      "resource",
      "server.",
      "set",
      "servers",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "keep",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "system",
      "performance",
      "get",
      "impacted.",
      "creating",
      "follow",
      "steps",
      "tls",
      "connections",
      "both",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "details",
      "see",
      "documentation",
      "file",
      "parameters",
      "depending",
      "itom",
      "deployment",
      "size",
      "small",
      "medium",
      "large",
      "default",
      "value",
      "parameter",
      "localhost",
      "means",
      "connect",
      "instance.",
      "need",
      "allow",
      "remote",
      "connections.",
      "table"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare an external database for audit (openshift)",
    "contentLower": "to prepare postgresql, perform the following tasks. install postgresql you must create a separate postgresql database for audit service. this database can share the same postgresql instance as service management. make sure you enable ssl on the postgresql because audit uses ssl to communicate with postgresql. instructions on how to create a user and a database for a service for postgresql are provided in the postgresql section below. note it is strongly recommended to host the database server on a physical machine, and it should be an independent server without other applications running on it. also, if the database server machine is a virtual machine, you must dedicate the resource to the database server. you must set the database servers to the same time zone, daylight savings settings, and time as the audit server. you must keep the audit server in the same lan as the database servers (without a proxy or firewalls between them). otherwise, your system's performance may get impacted.",
    "keywordsLower": [
      "en_us.utf",
      "postgresql.conf",
      "prepare",
      "external",
      "database",
      "audit",
      "openshift",
      "install",
      "postgresql",
      "configure",
      "server",
      "perform",
      "following",
      "tasks.",
      "create",
      "separate",
      "service.",
      "share",
      "same",
      "instance",
      "service",
      "management.",
      "make",
      "sure",
      "enable",
      "ssl",
      "because",
      "uses",
      "communicate",
      "postgresql.",
      "instructions",
      "user",
      "provided",
      "section",
      "below.",
      "note",
      "strongly",
      "recommended",
      "host",
      "physical",
      "machine",
      "independent",
      "applications",
      "running",
      "it.",
      "virtual",
      "dedicate",
      "resource",
      "server.",
      "set",
      "servers",
      "time",
      "zone",
      "daylight",
      "savings",
      "settings",
      "keep",
      "lan",
      "proxy",
      "firewalls",
      "between",
      "otherwise",
      "system",
      "performance",
      "get",
      "impacted.",
      "creating",
      "follow",
      "steps",
      "tls",
      "connections",
      "both",
      "modes",
      "verify-full",
      "verify-ca",
      "supported.",
      "details",
      "see",
      "documentation",
      "file",
      "parameters",
      "depending",
      "itom",
      "deployment",
      "size",
      "small",
      "medium",
      "large",
      "default",
      "value",
      "parameter",
      "localhost",
      "means",
      "connect",
      "instance.",
      "need",
      "allow",
      "remote",
      "connections.",
      "table"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare persistent storage for ESM",
    "content": "To install the application in the Helm mode, you must create Perisistent Volumes (PVs). You can create PVs either using static volume provisioning or dynamic volume provisioning. To learn more about the two methods, see Dynamic vs. static volume provisioning. If using Azure, see Prepare persistent volumes for ESM on Azure instead. If using AWS, Ensure you have Amazon EFS CSI driver deployed to your Amazon EKS cluster. See Installing the Amazon EFS CSI driver. Create PVs using dynamic volume provisioning To create PVs for the suite using dynamic provisioning, you must update the deployment YAML file with the storage class name defined in the PVC. In the case of CLI installtion, add the storage class name to the my-values.yaml file. For AppHub installation, use the Yaml Editor to add the storage class name. PVC uses this storage class to create PVs required for the suite. The following sections provide instructions to create PVs in various deployment environments such as embedded Kuberne",
    "url": "prepareesmpvs",
    "filename": "prepareesmpvs",
    "headings": [
      "Create PVs using dynamic volume provisioning",
      "Create PVs on embedded Kubernetes, OpenShift, or BYOK",
      "Create PVs on AWS",
      "(Optional) Mount EFS server",
      "Prepare storageclass",
      "Create PVs using static volume provisioning",
      "Create PVs using script",
      "Create PVs manually"
    ],
    "keywords": [
      "efs.csi",
      "PVC.name",
      "example.net",
      "4.1",
      "createPV.sh",
      "storageclass.yaml",
      "storage.k8s",
      "aws.com",
      "values.yaml",
      "prepare",
      "persistent",
      "storage",
      "esm",
      "create",
      "pvs",
      "dynamic",
      "volume",
      "provisioning",
      "embedded",
      "kubernetes",
      "openshift",
      "byok",
      "aws",
      "optional",
      "mount",
      "efs",
      "server",
      "storageclass",
      "static",
      "script",
      "manually",
      "install",
      "application",
      "helm",
      "mode",
      "perisistent",
      "volumes",
      "either",
      "provisioning.",
      "learn",
      "about",
      "two",
      "methods",
      "see",
      "vs.",
      "azure",
      "instead.",
      "ensure",
      "amazon",
      "csi",
      "driver",
      "deployed",
      "eks",
      "cluster.",
      "installing",
      "driver.",
      "suite",
      "update",
      "deployment",
      "yaml",
      "file",
      "class",
      "name",
      "defined",
      "pvc.",
      "case",
      "cli",
      "installtion",
      "add",
      "my-values.yaml",
      "file.",
      "apphub",
      "installation",
      "editor",
      "name.",
      "pvc",
      "uses",
      "required",
      "suite.",
      "following",
      "sections",
      "provide",
      "instructions",
      "various",
      "environments",
      "such",
      "aws.",
      "refer",
      "section",
      "best",
      "suited",
      "deployment.",
      "optic",
      "management",
      "toolkit",
      "omt",
      "provides",
      "nfs",
      "provisioner",
      "part"
    ],
    "language": "en",
    "word_count": 111,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare persistent storage for esm",
    "contentLower": "to install the application in the helm mode, you must create perisistent volumes (pvs). you can create pvs either using static volume provisioning or dynamic volume provisioning. to learn more about the two methods, see dynamic vs. static volume provisioning. if using azure, see prepare persistent volumes for esm on azure instead. if using aws, ensure you have amazon efs csi driver deployed to your amazon eks cluster. see installing the amazon efs csi driver. create pvs using dynamic volume provisioning to create pvs for the suite using dynamic provisioning, you must update the deployment yaml file with the storage class name defined in the pvc. in the case of cli installtion, add the storage class name to the my-values.yaml file. for apphub installation, use the yaml editor to add the storage class name. pvc uses this storage class to create pvs required for the suite. the following sections provide instructions to create pvs in various deployment environments such as embedded kuberne",
    "keywordsLower": [
      "efs.csi",
      "pvc.name",
      "example.net",
      "4.1",
      "createpv.sh",
      "storageclass.yaml",
      "storage.k8s",
      "aws.com",
      "values.yaml",
      "prepare",
      "persistent",
      "storage",
      "esm",
      "create",
      "pvs",
      "dynamic",
      "volume",
      "provisioning",
      "embedded",
      "kubernetes",
      "openshift",
      "byok",
      "aws",
      "optional",
      "mount",
      "efs",
      "server",
      "storageclass",
      "static",
      "script",
      "manually",
      "install",
      "application",
      "helm",
      "mode",
      "perisistent",
      "volumes",
      "either",
      "provisioning.",
      "learn",
      "about",
      "two",
      "methods",
      "see",
      "vs.",
      "azure",
      "instead.",
      "ensure",
      "amazon",
      "csi",
      "driver",
      "deployed",
      "eks",
      "cluster.",
      "installing",
      "driver.",
      "suite",
      "update",
      "deployment",
      "yaml",
      "file",
      "class",
      "name",
      "defined",
      "pvc.",
      "case",
      "cli",
      "installtion",
      "add",
      "my-values.yaml",
      "file.",
      "apphub",
      "installation",
      "editor",
      "name.",
      "pvc",
      "uses",
      "required",
      "suite.",
      "following",
      "sections",
      "provide",
      "instructions",
      "various",
      "environments",
      "such",
      "aws.",
      "refer",
      "section",
      "best",
      "suited",
      "deployment.",
      "optic",
      "management",
      "toolkit",
      "omt",
      "provides",
      "nfs",
      "provisioner",
      "part"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform pre-installation checks",
    "content": "This topic contains steps needed to check if your environment is ready for the new installation. Perform the steps listed here before starting your installation. Check the status of database Use the checkPG.sh script included in the ESM helm chart to check the status of your database server: If you want to use the bastion node or a control plane node to do this, make sure the host has a PostgreSQL client installed, and then navigate to the following directory: <ESM_Helm_Chart-2x.x>/scripts/precheck. If you want to use the PostgreSQL server to do this, you need to copy the script <ESM_Helm_Chart-2x.x>/scripts/precheck/checkPG.sh from the bastion node or a control plane node to the PostgreSQL server. Then, run the following commands on the bastion node, a control plane node, or the PostgreSQL server: chmod u+x checkPG.sh ./checkPG.sh -h <pgserver> -p <port> -u <dbUserName> Example: ./checkPG.sh -h example.test.pgserver -p 5432 -u postgres If pg_trgm extension does not exist, refer to Gen",
    "url": "preinstallcheck",
    "filename": "preinstallcheck",
    "headings": [
      "Check the status of database",
      "Check Vertica (deployment with FinOPs Only)",
      "Check NFS (embedded K8s deployment only)"
    ],
    "keywords": [
      "example.nfs",
      "checkNFS.sh",
      "example.test",
      "checkPG.sh",
      "perform",
      "pre-installation",
      "checks",
      "check",
      "status",
      "database",
      "vertica",
      "deployment",
      "finops",
      "nfs",
      "embedded",
      "k8s",
      "topic",
      "contains",
      "steps",
      "needed",
      "environment",
      "ready",
      "new",
      "installation.",
      "listed",
      "here",
      "before",
      "starting",
      "script",
      "included",
      "esm",
      "helm",
      "chart",
      "server",
      "want",
      "bastion",
      "node",
      "control",
      "plane",
      "make",
      "sure",
      "host",
      "postgresql",
      "client",
      "installed",
      "navigate",
      "following",
      "directory",
      "scripts",
      "precheck.",
      "need",
      "copy",
      "precheck",
      "server.",
      "run",
      "commands",
      "chmod",
      "-h",
      "-p",
      "-u",
      "example",
      "example.test.pgserver",
      "5432",
      "postgres",
      "extension",
      "exist",
      "refer",
      "general",
      "requirements",
      "databases",
      "install",
      "it.",
      "task",
      "deploying",
      "cmp",
      "capability.",
      "log",
      "command",
      "below",
      "connected",
      "successfully.",
      "vsql",
      "port",
      "user",
      "-w",
      "password",
      "-d",
      "opt",
      "bin",
      "example.test.verticaserver",
      "5433",
      "dbuser",
      "dbname",
      "manually",
      "-t",
      "-s",
      "-f",
      "example.nfs.server",
      "var",
      "vols"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform pre-installation checks",
    "contentLower": "this topic contains steps needed to check if your environment is ready for the new installation. perform the steps listed here before starting your installation. check the status of database use the checkpg.sh script included in the esm helm chart to check the status of your database server: if you want to use the bastion node or a control plane node to do this, make sure the host has a postgresql client installed, and then navigate to the following directory: <esm_helm_chart-2x.x>/scripts/precheck. if you want to use the postgresql server to do this, you need to copy the script <esm_helm_chart-2x.x>/scripts/precheck/checkpg.sh from the bastion node or a control plane node to the postgresql server. then, run the following commands on the bastion node, a control plane node, or the postgresql server: chmod u+x checkpg.sh ./checkpg.sh -h <pgserver> -p <port> -u <dbusername> example: ./checkpg.sh -h example.test.pgserver -p 5432 -u postgres if pg_trgm extension does not exist, refer to gen",
    "keywordsLower": [
      "example.nfs",
      "checknfs.sh",
      "example.test",
      "checkpg.sh",
      "perform",
      "pre-installation",
      "checks",
      "check",
      "status",
      "database",
      "vertica",
      "deployment",
      "finops",
      "nfs",
      "embedded",
      "k8s",
      "topic",
      "contains",
      "steps",
      "needed",
      "environment",
      "ready",
      "new",
      "installation.",
      "listed",
      "here",
      "before",
      "starting",
      "script",
      "included",
      "esm",
      "helm",
      "chart",
      "server",
      "want",
      "bastion",
      "node",
      "control",
      "plane",
      "make",
      "sure",
      "host",
      "postgresql",
      "client",
      "installed",
      "navigate",
      "following",
      "directory",
      "scripts",
      "precheck.",
      "need",
      "copy",
      "precheck",
      "server.",
      "run",
      "commands",
      "chmod",
      "-h",
      "-p",
      "-u",
      "example",
      "example.test.pgserver",
      "5432",
      "postgres",
      "extension",
      "exist",
      "refer",
      "general",
      "requirements",
      "databases",
      "install",
      "it.",
      "task",
      "deploying",
      "cmp",
      "capability.",
      "log",
      "command",
      "below",
      "connected",
      "successfully.",
      "vsql",
      "port",
      "user",
      "-w",
      "password",
      "-d",
      "opt",
      "bin",
      "example.test.verticaserver",
      "5433",
      "dbuser",
      "dbname",
      "manually",
      "-t",
      "-s",
      "-f",
      "example.nfs.server",
      "var",
      "vols"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Post-installation tasks",
    "content": "Once the suite installation is complete, you should be able to access the following interfaces: The Suite Administration interface: https://<EXTERNAL_ACCESS_HOST>/bo (user name:suite-admin; password: specified during the suite installation) (if OMT installed) The OMT Management Portal: https://<EXTERNAL_ACCESS_HOST>:5443 (user name: admin; password: specified during the OMT installation) Perform general post-installation tasks Remove the public schema from external PostgreSQL Perform post-installation tuning Create replicas and get them connected - Perform this task only if you adopt the OData integration for BI synchronization. Set up tenants Next, you can set up one or more tenants as needed. For details, see Set up a tenant. Related topics Install add-on capabilities",
    "url": "postinstalltasks",
    "filename": "postinstalltasks",
    "headings": [
      "Perform general post-installation tasks",
      "Set up tenants",
      "Related topics"
    ],
    "keywords": [
      "https://<EXTERNAL_ACCESS_HOST>:5443",
      "https://<EXTERNAL_ACCESS_HOST>/bo",
      "post-installation",
      "tasks",
      "perform",
      "general",
      "set",
      "tenants",
      "related",
      "topics",
      "once",
      "suite",
      "installation",
      "complete",
      "able",
      "access",
      "following",
      "interfaces",
      "administration",
      "interface",
      "https",
      "bo",
      "user",
      "name",
      "suite-admin",
      "password",
      "specified",
      "during",
      "omt",
      "installed",
      "management",
      "portal",
      "5443",
      "admin",
      "remove",
      "public",
      "schema",
      "external",
      "postgresql",
      "tuning",
      "create",
      "replicas",
      "get",
      "connected",
      "task",
      "adopt",
      "odata",
      "integration",
      "bi",
      "synchronization.",
      "next",
      "one",
      "needed.",
      "details",
      "see",
      "tenant.",
      "install",
      "add-on",
      "capabilities"
    ],
    "language": "en",
    "word_count": 77,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "post-installation tasks",
    "contentLower": "once the suite installation is complete, you should be able to access the following interfaces: the suite administration interface: https://<external_access_host>/bo (user name:suite-admin; password: specified during the suite installation) (if omt installed) the omt management portal: https://<external_access_host>:5443 (user name: admin; password: specified during the omt installation) perform general post-installation tasks remove the public schema from external postgresql perform post-installation tuning create replicas and get them connected - perform this task only if you adopt the odata integration for bi synchronization. set up tenants next, you can set up one or more tenants as needed. for details, see set up a tenant. related topics install add-on capabilities",
    "keywordsLower": [
      "https://<external_access_host>:5443",
      "https://<external_access_host>/bo",
      "post-installation",
      "tasks",
      "perform",
      "general",
      "set",
      "tenants",
      "related",
      "topics",
      "once",
      "suite",
      "installation",
      "complete",
      "able",
      "access",
      "following",
      "interfaces",
      "administration",
      "interface",
      "https",
      "bo",
      "user",
      "name",
      "suite-admin",
      "password",
      "specified",
      "during",
      "omt",
      "installed",
      "management",
      "portal",
      "5443",
      "admin",
      "remove",
      "public",
      "schema",
      "external",
      "postgresql",
      "tuning",
      "create",
      "replicas",
      "get",
      "connected",
      "task",
      "adopt",
      "odata",
      "integration",
      "bi",
      "synchronization.",
      "next",
      "one",
      "needed.",
      "details",
      "see",
      "tenant.",
      "install",
      "add-on",
      "capabilities"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform post-installation tuning",
    "content": "Perform the following tuning tasks after the suite installation. Trust the browser SSL certificate For optimal browser performance, your Certification Authority signed SSL certificate must be trusted in the browser. Once your certificate is trusted, a green lock appears in the browser address bar, and browser cached resources should work as expected. This is the most important thing, especially when web users are working under some network latency. Tune external IDOL server configuration (Service Manager tenants) If you are connecting SMA Service Portal to external Service Manager, and using external IDOL servers, to gain better performance, you can tune IDOL server configuration by following the steps here. Configure sm-gateway pod replicas (Service Manager tenants) By default, after the SMA installation, the replicas value of the sm-gateway service is set to 0. If you need to deploy one or more Service Manager tenants, you need to configure a proper replicas value for this pod accord",
    "url": "postinstalltuning",
    "filename": "postinstalltuning",
    "headings": [
      "Trust the browser SSL certificate",
      "Tune external IDOL server configuration (Service Manager tenants)",
      "Configure sm-gateway pod replicas (Service Manager tenants)",
      "Disable the fluentd daemonset in the core namespace",
      "Tune external Service Manager (Service Manager tenants)",
      "Create missing indexes for upper(...) conditions on entity table",
      "Related topics"
    ],
    "keywords": [
      "postgresql.conf",
      "9.6x",
      "9.70",
      "xxxx.tgz",
      "apphub.xxxx",
      "perform",
      "post-installation",
      "tuning",
      "trust",
      "browser",
      "ssl",
      "certificate",
      "tune",
      "external",
      "idol",
      "server",
      "configuration",
      "service",
      "manager",
      "tenants",
      "configure",
      "sm-gateway",
      "pod",
      "replicas",
      "disable",
      "fluentd",
      "daemonset",
      "core",
      "namespace",
      "create",
      "missing",
      "indexes",
      "upper",
      "conditions",
      "entity",
      "table",
      "related",
      "topics",
      "following",
      "tasks",
      "after",
      "suite",
      "installation.",
      "optimal",
      "performance",
      "certification",
      "authority",
      "signed",
      "trusted",
      "browser.",
      "once",
      "green",
      "lock",
      "appears",
      "address",
      "bar",
      "cached",
      "resources",
      "work",
      "expected.",
      "most",
      "important",
      "thing",
      "especially",
      "web",
      "users",
      "working",
      "under",
      "network",
      "latency.",
      "connecting",
      "sma",
      "portal",
      "servers",
      "gain",
      "better",
      "steps",
      "here.",
      "default",
      "installation",
      "value",
      "set",
      "0.",
      "need",
      "deploy",
      "one",
      "proper",
      "according",
      "sizing",
      "profile.",
      "recommendation",
      "configuring",
      "per",
      "500",
      "concurrent",
      "users.",
      "example",
      "000",
      "four",
      "pods."
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform post-installation tuning",
    "contentLower": "perform the following tuning tasks after the suite installation. trust the browser ssl certificate for optimal browser performance, your certification authority signed ssl certificate must be trusted in the browser. once your certificate is trusted, a green lock appears in the browser address bar, and browser cached resources should work as expected. this is the most important thing, especially when web users are working under some network latency. tune external idol server configuration (service manager tenants) if you are connecting sma service portal to external service manager, and using external idol servers, to gain better performance, you can tune idol server configuration by following the steps here. configure sm-gateway pod replicas (service manager tenants) by default, after the sma installation, the replicas value of the sm-gateway service is set to 0. if you need to deploy one or more service manager tenants, you need to configure a proper replicas value for this pod accord",
    "keywordsLower": [
      "postgresql.conf",
      "9.6x",
      "9.70",
      "xxxx.tgz",
      "apphub.xxxx",
      "perform",
      "post-installation",
      "tuning",
      "trust",
      "browser",
      "ssl",
      "certificate",
      "tune",
      "external",
      "idol",
      "server",
      "configuration",
      "service",
      "manager",
      "tenants",
      "configure",
      "sm-gateway",
      "pod",
      "replicas",
      "disable",
      "fluentd",
      "daemonset",
      "core",
      "namespace",
      "create",
      "missing",
      "indexes",
      "upper",
      "conditions",
      "entity",
      "table",
      "related",
      "topics",
      "following",
      "tasks",
      "after",
      "suite",
      "installation.",
      "optimal",
      "performance",
      "certification",
      "authority",
      "signed",
      "trusted",
      "browser.",
      "once",
      "green",
      "lock",
      "appears",
      "address",
      "bar",
      "cached",
      "resources",
      "work",
      "expected.",
      "most",
      "important",
      "thing",
      "especially",
      "web",
      "users",
      "working",
      "under",
      "network",
      "latency.",
      "connecting",
      "sma",
      "portal",
      "servers",
      "gain",
      "better",
      "steps",
      "here.",
      "default",
      "installation",
      "value",
      "set",
      "0.",
      "need",
      "deploy",
      "one",
      "proper",
      "according",
      "sizing",
      "profile.",
      "recommendation",
      "configuring",
      "per",
      "500",
      "concurrent",
      "users.",
      "example",
      "000",
      "four",
      "pods."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan for OO Workflow Designer",
    "content": "This section provides the information required to plan the installation of the OO Workflow Designer.",
    "url": "planoowfd",
    "filename": "planoowfd",
    "headings": [],
    "keywords": [
      "plan",
      "oo",
      "workflow",
      "designer",
      "section",
      "provides",
      "information",
      "required",
      "installation",
      "designer."
    ],
    "language": "en",
    "word_count": 13,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan for oo workflow designer",
    "contentLower": "this section provides the information required to plan the installation of the oo workflow designer.",
    "keywordsLower": [
      "plan",
      "oo",
      "workflow",
      "designer",
      "section",
      "provides",
      "information",
      "required",
      "installation",
      "designer."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare for OO Workflow Designer",
    "content": "This section provides the information required to prepare your environment for OO Workflow Designer installation. Make sure you meet all the hardware and software requirements of all the OO Workflow Designer.",
    "url": "prepareoowfd",
    "filename": "prepareoowfd",
    "headings": [],
    "keywords": [
      "prepare",
      "oo",
      "workflow",
      "designer",
      "section",
      "provides",
      "information",
      "required",
      "environment",
      "installation.",
      "make",
      "sure",
      "meet",
      "all",
      "hardware",
      "software",
      "requirements",
      "designer."
    ],
    "language": "en",
    "word_count": 25,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare for oo workflow designer",
    "contentLower": "this section provides the information required to prepare your environment for oo workflow designer installation. make sure you meet all the hardware and software requirements of all the oo workflow designer.",
    "keywordsLower": [
      "prepare",
      "oo",
      "workflow",
      "designer",
      "section",
      "provides",
      "information",
      "required",
      "environment",
      "installation.",
      "make",
      "sure",
      "meet",
      "all",
      "hardware",
      "software",
      "requirements",
      "designer."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Post installation tasks for OO Workflow Designer",
    "content": "This topic describes the tasks that you must do immediately after installing the Operations Orchestration (OO) Workflow Designer. Delete the silent properties file It's recommended to remove the silent properties files from your system or secure the files from any unauthorized access after installing OO Workflow Designer in silent mode. We encourage the customer to delete the <my-silent.properties>, which isn't provided. By not deleting the properties file you may be exposing the system to increased security risks. You understand and agree to assume all associated risks and hold OpenText harmless for the same. It remains at all times the Customer’s sole responsibility to assess its own regulatory and business requirements. OpenText doesn't represent or warrant that its products comply with any specific legal or regulatory standards applicable to Customer in conducting Customer's business.",
    "url": "oowfdpostinstallation",
    "filename": "oowfdpostinstallation",
    "headings": [
      "Delete the silent properties file"
    ],
    "keywords": [
      "post",
      "installation",
      "tasks",
      "oo",
      "workflow",
      "designer",
      "delete",
      "silent",
      "properties",
      "file",
      "topic",
      "describes",
      "immediately",
      "after",
      "installing",
      "operations",
      "orchestration",
      "designer.",
      "recommended",
      "remove",
      "files",
      "system",
      "secure",
      "any",
      "unauthorized",
      "access",
      "mode.",
      "encourage",
      "customer",
      "isn",
      "provided.",
      "deleting",
      "exposing",
      "increased",
      "security",
      "risks.",
      "understand",
      "agree",
      "assume",
      "all",
      "associated",
      "risks",
      "hold",
      "opentext",
      "harmless",
      "same.",
      "remains",
      "times",
      "sole",
      "responsibility",
      "assess",
      "own",
      "regulatory",
      "business",
      "requirements.",
      "doesn",
      "represent",
      "warrant",
      "products",
      "comply",
      "specific",
      "legal",
      "standards",
      "applicable",
      "conducting",
      "business."
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "post installation tasks for oo workflow designer",
    "contentLower": "this topic describes the tasks that you must do immediately after installing the operations orchestration (oo) workflow designer. delete the silent properties file it's recommended to remove the silent properties files from your system or secure the files from any unauthorized access after installing oo workflow designer in silent mode. we encourage the customer to delete the <my-silent.properties>, which isn't provided. by not deleting the properties file you may be exposing the system to increased security risks. you understand and agree to assume all associated risks and hold opentext harmless for the same. it remains at all times the customer’s sole responsibility to assess its own regulatory and business requirements. opentext doesn't represent or warrant that its products comply with any specific legal or regulatory standards applicable to customer in conducting customer's business.",
    "keywordsLower": [
      "post",
      "installation",
      "tasks",
      "oo",
      "workflow",
      "designer",
      "delete",
      "silent",
      "properties",
      "file",
      "topic",
      "describes",
      "immediately",
      "after",
      "installing",
      "operations",
      "orchestration",
      "designer.",
      "recommended",
      "remove",
      "files",
      "system",
      "secure",
      "any",
      "unauthorized",
      "access",
      "mode.",
      "encourage",
      "customer",
      "isn",
      "provided.",
      "deleting",
      "exposing",
      "increased",
      "security",
      "risks.",
      "understand",
      "agree",
      "assume",
      "all",
      "associated",
      "risks",
      "hold",
      "opentext",
      "harmless",
      "same.",
      "remains",
      "times",
      "sole",
      "responsibility",
      "assess",
      "own",
      "regulatory",
      "business",
      "requirements.",
      "doesn",
      "represent",
      "warrant",
      "products",
      "comply",
      "specific",
      "legal",
      "standards",
      "applicable",
      "conducting",
      "business."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare infrastructure",
    "content": "This topic gives you information on infrastructure specifications to prepare the infrastructure for external Kubernetes distributions manually. While preparing the infrastructure make sure you follow the specifications provided in the following subtopics: Database related infrastructure Storage related infrastructure Network related infrastructure Kubernetes related infrastructure Image registry",
    "url": "402-prepinfra",
    "filename": "402-prepinfra",
    "headings": [],
    "keywords": [
      "prepare",
      "infrastructure",
      "topic",
      "gives",
      "information",
      "specifications",
      "external",
      "kubernetes",
      "distributions",
      "manually.",
      "while",
      "preparing",
      "make",
      "sure",
      "follow",
      "provided",
      "following",
      "subtopics",
      "database",
      "related",
      "storage",
      "network",
      "image",
      "registry"
    ],
    "language": "en",
    "word_count": 37,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare infrastructure",
    "contentLower": "this topic gives you information on infrastructure specifications to prepare the infrastructure for external kubernetes distributions manually. while preparing the infrastructure make sure you follow the specifications provided in the following subtopics: database related infrastructure storage related infrastructure network related infrastructure kubernetes related infrastructure image registry",
    "keywordsLower": [
      "prepare",
      "infrastructure",
      "topic",
      "gives",
      "information",
      "specifications",
      "external",
      "kubernetes",
      "distributions",
      "manually.",
      "while",
      "preparing",
      "make",
      "sure",
      "follow",
      "provided",
      "following",
      "subtopics",
      "database",
      "related",
      "storage",
      "network",
      "image",
      "registry"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Network related infrastructure",
    "content": "This topic provides you with the required specifications to configure the network requirements for establishing communication between external access hosts and cloud environments such as AWS, Azure, or GCP. It also provides guidance on the required ports to ensure successful deployment in on-premises or cloud environments. Communication connection on cloud The installation uses an external access host for some services. As a prerequisite, ensure that you must allow an L3 Bi directional connection between the remote system and AWS, Azure, or GCP setup to access services. You can do this using a VPN connection or Direct Connect. If L3 communication isn't enabled between the remote system and AWS, Azure, or GCP setup, you must create an environment in the same VPC or VNet to access services. You must set up a private DNS zone. You can either create a private domain or use an existing private domain in Register domain. For steps to create a private domain, see the AWS documentation, Azure ",
    "url": "402-networkreqs",
    "filename": "402-networkreqs",
    "headings": [
      "Communication connection on cloud",
      "Required Ports"
    ],
    "keywords": [
      "network",
      "related",
      "infrastructure",
      "communication",
      "connection",
      "cloud",
      "required",
      "ports",
      "topic",
      "provides",
      "specifications",
      "configure",
      "requirements",
      "establishing",
      "between",
      "external",
      "access",
      "hosts",
      "environments",
      "such",
      "aws",
      "azure",
      "gcp.",
      "guidance",
      "ensure",
      "successful",
      "deployment",
      "on-premises",
      "environments.",
      "installation",
      "uses",
      "host",
      "services.",
      "prerequisite",
      "allow",
      "l3",
      "bi",
      "directional",
      "remote",
      "system",
      "gcp",
      "setup",
      "vpn",
      "direct",
      "connect.",
      "isn",
      "enabled",
      "create",
      "environment",
      "same",
      "vpc",
      "vnet",
      "set",
      "private",
      "dns",
      "zone.",
      "either",
      "domain",
      "existing",
      "register",
      "domain.",
      "steps",
      "see",
      "documentation",
      "google",
      "documentation.",
      "record",
      "name",
      "configuration.",
      "self-signed",
      "certificate",
      "certificate.",
      "management",
      "console",
      "search",
      "click",
      "manager.",
      "import",
      "host.",
      "once",
      "upload",
      "appears",
      "imported",
      "manager",
      "tab.",
      "provide",
      "details",
      "servercrt",
      "serverkey",
      "parameters",
      "configuring",
      "load",
      "balancer.",
      "services",
      "exposed",
      "over",
      "node",
      "port",
      "deploying",
      "service"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "network related infrastructure",
    "contentLower": "this topic provides you with the required specifications to configure the network requirements for establishing communication between external access hosts and cloud environments such as aws, azure, or gcp. it also provides guidance on the required ports to ensure successful deployment in on-premises or cloud environments. communication connection on cloud the installation uses an external access host for some services. as a prerequisite, ensure that you must allow an l3 bi directional connection between the remote system and aws, azure, or gcp setup to access services. you can do this using a vpn connection or direct connect. if l3 communication isn't enabled between the remote system and aws, azure, or gcp setup, you must create an environment in the same vpc or vnet to access services. you must set up a private dns zone. you can either create a private domain or use an existing private domain in register domain. for steps to create a private domain, see the aws documentation, azure ",
    "keywordsLower": [
      "network",
      "related",
      "infrastructure",
      "communication",
      "connection",
      "cloud",
      "required",
      "ports",
      "topic",
      "provides",
      "specifications",
      "configure",
      "requirements",
      "establishing",
      "between",
      "external",
      "access",
      "hosts",
      "environments",
      "such",
      "aws",
      "azure",
      "gcp.",
      "guidance",
      "ensure",
      "successful",
      "deployment",
      "on-premises",
      "environments.",
      "installation",
      "uses",
      "host",
      "services.",
      "prerequisite",
      "allow",
      "l3",
      "bi",
      "directional",
      "remote",
      "system",
      "gcp",
      "setup",
      "vpn",
      "direct",
      "connect.",
      "isn",
      "enabled",
      "create",
      "environment",
      "same",
      "vpc",
      "vnet",
      "set",
      "private",
      "dns",
      "zone.",
      "either",
      "domain",
      "existing",
      "register",
      "domain.",
      "steps",
      "see",
      "documentation",
      "google",
      "documentation.",
      "record",
      "name",
      "configuration.",
      "self-signed",
      "certificate",
      "certificate.",
      "management",
      "console",
      "search",
      "click",
      "manager.",
      "import",
      "host.",
      "once",
      "upload",
      "appears",
      "imported",
      "manager",
      "tab.",
      "provide",
      "details",
      "servercrt",
      "serverkey",
      "parameters",
      "configuring",
      "load",
      "balancer.",
      "services",
      "exposed",
      "over",
      "node",
      "port",
      "deploying",
      "service"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Operations Cloud audit logs",
    "content": "This topic helps you to audit the changes in the dashboard configurations and predefined queries in Operations Cloud. Auditing the changes in the dashboard configurations, changes in predefined queries, and reports is critical for reducing security risk. Audit for operations like create, update, and delete happens after you save the changes in Operations Cloud. The operations triggered from the user interface are audited through the background process. You can use the audit log to keep track of different actions performed by the users. You can view the audit logs in the bvd-explore-deployment-<pod value> pod. The log file consists of the user name details, the changes in the dashboard configurations or the predefined queries with both old and new values. View the audit logs Log on to the control plane or the bastion node as the root user. Run the following command to get the pod name: kubectl get pods -n <application namespace> | grep bvd-explore Note down the pod name along with the p",
    "url": "402-auditinglogs",
    "filename": "402-auditinglogs",
    "headings": [
      "View the audit logs"
    ],
    "keywords": [
      "operations",
      "cloud",
      "audit",
      "logs",
      "view",
      "topic",
      "helps",
      "changes",
      "dashboard",
      "configurations",
      "predefined",
      "queries",
      "cloud.",
      "auditing",
      "reports",
      "critical",
      "reducing",
      "security",
      "risk.",
      "like",
      "create",
      "update",
      "delete",
      "happens",
      "after",
      "save",
      "triggered",
      "user",
      "interface",
      "audited",
      "through",
      "background",
      "process.",
      "log",
      "keep",
      "track",
      "different",
      "actions",
      "performed",
      "users.",
      "bvd-explore-deployment-",
      "pod.",
      "file",
      "consists",
      "name",
      "details",
      "both",
      "old",
      "new",
      "values.",
      "control",
      "plane",
      "bastion",
      "node",
      "root",
      "user.",
      "run",
      "following",
      "command",
      "get",
      "pod",
      "kubectl",
      "pods",
      "-n",
      "grep",
      "bvd-explore",
      "note",
      "along",
      "value.",
      "-f",
      "bvd"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "operations cloud audit logs",
    "contentLower": "this topic helps you to audit the changes in the dashboard configurations and predefined queries in operations cloud. auditing the changes in the dashboard configurations, changes in predefined queries, and reports is critical for reducing security risk. audit for operations like create, update, and delete happens after you save the changes in operations cloud. the operations triggered from the user interface are audited through the background process. you can use the audit log to keep track of different actions performed by the users. you can view the audit logs in the bvd-explore-deployment-<pod value> pod. the log file consists of the user name details, the changes in the dashboard configurations or the predefined queries with both old and new values. view the audit logs log on to the control plane or the bastion node as the root user. run the following command to get the pod name: kubectl get pods -n <application namespace> | grep bvd-explore note down the pod name along with the p",
    "keywordsLower": [
      "operations",
      "cloud",
      "audit",
      "logs",
      "view",
      "topic",
      "helps",
      "changes",
      "dashboard",
      "configurations",
      "predefined",
      "queries",
      "cloud.",
      "auditing",
      "reports",
      "critical",
      "reducing",
      "security",
      "risk.",
      "like",
      "create",
      "update",
      "delete",
      "happens",
      "after",
      "save",
      "triggered",
      "user",
      "interface",
      "audited",
      "through",
      "background",
      "process.",
      "log",
      "keep",
      "track",
      "different",
      "actions",
      "performed",
      "users.",
      "bvd-explore-deployment-",
      "pod.",
      "file",
      "consists",
      "name",
      "details",
      "both",
      "old",
      "new",
      "values.",
      "control",
      "plane",
      "bastion",
      "node",
      "root",
      "user.",
      "run",
      "following",
      "command",
      "get",
      "pod",
      "kubectl",
      "pods",
      "-n",
      "grep",
      "bvd-explore",
      "note",
      "along",
      "value.",
      "-f",
      "bvd"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage OO Central license",
    "content": "This topic describes how to manage OO Central license as suite admin and tenant admin. Concurrent workflow licenses The concurrent workflow license plan counts the number of workflow executions (OO native and CloudSlang flows) that may include parallel lane execution in unlimited workers. For example, suppose you have purchased 10 concurrent workflow licenses and have five Central workers set up in your environment. In that case, you can only run 10 workflows (including a limited parallel lanes execution) irrespective of the five Central workers used in your environment. If you need to execute the eleventh workflow, you must buy an additional concurrent workflow license. Triggering new flow executions when AutoPass License Server (APLS) connection is unavailable is not possible by default. Manage license as a suite admin Upload license Follow these steps to upload the OO licenses: Log in to the OO AutoPass License Server https://<OO_HOST>:<OO_PORT>/autopassClick the CONFIGURATION menu,",
    "url": "manageoolicense",
    "filename": "manageoolicense",
    "headings": [
      "Concurrent workflow licenses",
      "Manage license as a suite admin",
      "Upload license",
      "Associate OO license to newly created OO tenant",
      "Manage license as a tenant admin",
      "Upload license"
    ],
    "keywords": [
      "submenu.On",
      "Close.To",
      "306306306.In",
      "https://<OO_HOST>:<OO_PORT>/autopass.Click",
      "tab.On",
      "https://<OO_HOST>:<OO_PORT>/autopassClick",
      "appears.In",
      "https://FQDN:PORT/autopass/?tenant=<id>.Click",
      "manage",
      "oo",
      "central",
      "license",
      "concurrent",
      "workflow",
      "licenses",
      "suite",
      "admin",
      "upload",
      "associate",
      "newly",
      "created",
      "tenant",
      "topic",
      "describes",
      "admin.",
      "plan",
      "counts",
      "number",
      "executions",
      "native",
      "cloudslang",
      "flows",
      "include",
      "parallel",
      "lane",
      "execution",
      "unlimited",
      "workers.",
      "example",
      "suppose",
      "purchased",
      "10",
      "five",
      "workers",
      "set",
      "environment.",
      "case",
      "run",
      "workflows",
      "including",
      "limited",
      "lanes",
      "irrespective",
      "need",
      "execute",
      "eleventh",
      "buy",
      "additional",
      "license.",
      "triggering",
      "new",
      "flow",
      "autopass",
      "server",
      "apls",
      "connection",
      "unavailable",
      "possible",
      "default.",
      "follow",
      "steps",
      "log",
      "https",
      "autopassclick",
      "configuration",
      "menu",
      "select",
      "install",
      "option",
      "under",
      "submenu.click",
      "add",
      "file",
      "browse",
      "appropriate",
      "files.click",
      "next.",
      "uploaded",
      "appears",
      "information.click",
      "licenses.",
      "view",
      "tab",
      "details",
      "applied.",
      "follows",
      "autopass.click",
      "click",
      "product",
      "drop-down"
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage oo central license",
    "contentLower": "this topic describes how to manage oo central license as suite admin and tenant admin. concurrent workflow licenses the concurrent workflow license plan counts the number of workflow executions (oo native and cloudslang flows) that may include parallel lane execution in unlimited workers. for example, suppose you have purchased 10 concurrent workflow licenses and have five central workers set up in your environment. in that case, you can only run 10 workflows (including a limited parallel lanes execution) irrespective of the five central workers used in your environment. if you need to execute the eleventh workflow, you must buy an additional concurrent workflow license. triggering new flow executions when autopass license server (apls) connection is unavailable is not possible by default. manage license as a suite admin upload license follow these steps to upload the oo licenses: log in to the oo autopass license server https://<oo_host>:<oo_port>/autopassclick the configuration menu,",
    "keywordsLower": [
      "submenu.on",
      "close.to",
      "306306306.in",
      "https://<oo_host>:<oo_port>/autopass.click",
      "tab.on",
      "https://<oo_host>:<oo_port>/autopassclick",
      "appears.in",
      "https://fqdn:port/autopass/?tenant=<id>.click",
      "manage",
      "oo",
      "central",
      "license",
      "concurrent",
      "workflow",
      "licenses",
      "suite",
      "admin",
      "upload",
      "associate",
      "newly",
      "created",
      "tenant",
      "topic",
      "describes",
      "admin.",
      "plan",
      "counts",
      "number",
      "executions",
      "native",
      "cloudslang",
      "flows",
      "include",
      "parallel",
      "lane",
      "execution",
      "unlimited",
      "workers.",
      "example",
      "suppose",
      "purchased",
      "10",
      "five",
      "workers",
      "set",
      "environment.",
      "case",
      "run",
      "workflows",
      "including",
      "limited",
      "lanes",
      "irrespective",
      "need",
      "execute",
      "eleventh",
      "buy",
      "additional",
      "license.",
      "triggering",
      "new",
      "flow",
      "autopass",
      "server",
      "apls",
      "connection",
      "unavailable",
      "possible",
      "default.",
      "follow",
      "steps",
      "log",
      "https",
      "autopassclick",
      "configuration",
      "menu",
      "select",
      "install",
      "option",
      "under",
      "submenu.click",
      "add",
      "file",
      "browse",
      "appropriate",
      "files.click",
      "next.",
      "uploaded",
      "appears",
      "information.click",
      "licenses.",
      "view",
      "tab",
      "details",
      "applied.",
      "follows",
      "autopass.click",
      "click",
      "product",
      "drop-down"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare for the Native SACM deployment",
    "content": "Please ensure the following prerequisites are met to use the Native SACM solution. Service Management In this release, to use Native SACM, you are recommended to deploy Service Management in an on-premises, AWS (EKS), Azure, or Red Hat OpenShift environment. UCMDB Server To use Native SACM, you need to install UCMDB Server and make sure Service Management and UCMDB versions comply with the support matrix: Service Management with Universal Discovery and CMDB In this release, UCMDB and UCMDB Gateway of the Native SACM solution can be external (classic mode) or containerized (multi-deployment mode). In addition, when Service Management is deployed on-premises, UCMDB Server also needs to be installed on-premises; when Service Management is deployed in AWS (EKS), UCMDB Server needs to be installed in an AWS EC2 environment. UCMDB Gateway (for classic UD/UCMDB) UCMDB Gateway is required when classic UD/UCMDB is used. UCMDB Gateway is supported on all the operating systems that UCMDB server i",
    "url": "nativesacmsysrequirements",
    "filename": "nativesacmsysrequirements",
    "headings": [
      "Service Management",
      "UCMDB Server",
      "UCMDB Gateway (for classic UD/UCMDB)",
      "Obtain the UCMDB Gateway installation package",
      "DND-UCMDB integration",
      "Related topics"
    ],
    "keywords": [
      "prepare",
      "native",
      "sacm",
      "deployment",
      "service",
      "management",
      "ucmdb",
      "server",
      "gateway",
      "classic",
      "ud",
      "obtain",
      "installation",
      "package",
      "dnd-ucmdb",
      "integration",
      "related",
      "topics",
      "please",
      "ensure",
      "following",
      "prerequisites",
      "met",
      "solution.",
      "release",
      "recommended",
      "deploy",
      "on-premises",
      "aws",
      "eks",
      "azure",
      "red",
      "hat",
      "openshift",
      "environment.",
      "need",
      "install",
      "make",
      "sure",
      "versions",
      "comply",
      "support",
      "matrix",
      "universal",
      "discovery",
      "cmdb",
      "solution",
      "external",
      "mode",
      "containerized",
      "multi-deployment",
      "addition",
      "deployed",
      "needs",
      "installed",
      "ec2",
      "required",
      "used.",
      "supported",
      "all",
      "operating",
      "systems",
      "supported.",
      "details",
      "see",
      "systems.",
      "download",
      "installer",
      "released",
      "together",
      "foundation",
      "software.",
      "cmp",
      "design",
      "capability",
      "tenant",
      "wish",
      "leverage",
      "advantages",
      "cloud",
      "services",
      "configure",
      "resource",
      "provider",
      "allows",
      "between",
      "dnd",
      "ucmdb.",
      "information",
      "configuration",
      "integration.",
      "providers",
      "providers.",
      "sizing",
      "consider"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare for the native sacm deployment",
    "contentLower": "please ensure the following prerequisites are met to use the native sacm solution. service management in this release, to use native sacm, you are recommended to deploy service management in an on-premises, aws (eks), azure, or red hat openshift environment. ucmdb server to use native sacm, you need to install ucmdb server and make sure service management and ucmdb versions comply with the support matrix: service management with universal discovery and cmdb in this release, ucmdb and ucmdb gateway of the native sacm solution can be external (classic mode) or containerized (multi-deployment mode). in addition, when service management is deployed on-premises, ucmdb server also needs to be installed on-premises; when service management is deployed in aws (eks), ucmdb server needs to be installed in an aws ec2 environment. ucmdb gateway (for classic ud/ucmdb) ucmdb gateway is required when classic ud/ucmdb is used. ucmdb gateway is supported on all the operating systems that ucmdb server i",
    "keywordsLower": [
      "prepare",
      "native",
      "sacm",
      "deployment",
      "service",
      "management",
      "ucmdb",
      "server",
      "gateway",
      "classic",
      "ud",
      "obtain",
      "installation",
      "package",
      "dnd-ucmdb",
      "integration",
      "related",
      "topics",
      "please",
      "ensure",
      "following",
      "prerequisites",
      "met",
      "solution.",
      "release",
      "recommended",
      "deploy",
      "on-premises",
      "aws",
      "eks",
      "azure",
      "red",
      "hat",
      "openshift",
      "environment.",
      "need",
      "install",
      "make",
      "sure",
      "versions",
      "comply",
      "support",
      "matrix",
      "universal",
      "discovery",
      "cmdb",
      "solution",
      "external",
      "mode",
      "containerized",
      "multi-deployment",
      "addition",
      "deployed",
      "needs",
      "installed",
      "ec2",
      "required",
      "used.",
      "supported",
      "all",
      "operating",
      "systems",
      "supported.",
      "details",
      "see",
      "systems.",
      "download",
      "installer",
      "released",
      "together",
      "foundation",
      "software.",
      "cmp",
      "design",
      "capability",
      "tenant",
      "wish",
      "leverage",
      "advantages",
      "cloud",
      "services",
      "configure",
      "resource",
      "provider",
      "allows",
      "between",
      "dnd",
      "ucmdb.",
      "information",
      "configuration",
      "integration.",
      "providers",
      "providers.",
      "sizing",
      "consider"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Native SACM hybrid mode",
    "content": "What's Native SACM hybrid mode? When implementing Native SACM, the following deployment isn't rare: Service Management is installed in a cloud environment such as Azure, AWS, or Google Cloud. UCMDB is running in an on-premises data center. Due to those different setups, some special considerations must be taken into account in order to connect the on-premises UCMDB with the cloud-based Service Management. This is what we call the “Hybrid deployment” mode. Communication methods Local data-center components Before connecting UD/UCMDB and Service Management through UCMDB Gateway, it's required to understand how these 3 components communicate together. All actions done in Service Management such as CI creations and updates are synchronized to UCMDB through HTTP requests orchestrated by UCMDB Gateway. For all background activities, a WebSocket connection is established by Service Management to the Gateway and then from the Gateway to UCMDB. This WebSocket connection handles all requests fro",
    "url": "nativesacmhybridmode",
    "filename": "nativesacmhybridmode",
    "headings": [
      "What's Native SACM hybrid mode?",
      "Communication methods",
      "Local data-center components",
      "Implementing the hybrid mode",
      "Service Management cloud deployment",
      "Prerequisites for Native SACM",
      "Implementation steps",
      "Enable Single Sign-On"
    ],
    "keywords": [
      "https://<UCMDB_Server_Host>:<UCMDB_Server_Port>/ucmdb-browser",
      "https://developer.mozilla.org/en-US/docs/Web/HTTP/Protocol_upgrade_mechanism",
      "mozilla.org",
      "native",
      "sacm",
      "hybrid",
      "mode",
      "what",
      "communication",
      "methods",
      "local",
      "data-center",
      "components",
      "implementing",
      "service",
      "management",
      "cloud",
      "deployment",
      "prerequisites",
      "implementation",
      "steps",
      "enable",
      "single",
      "sign-on",
      "following",
      "isn",
      "rare",
      "installed",
      "environment",
      "such",
      "azure",
      "aws",
      "google",
      "cloud.",
      "ucmdb",
      "running",
      "on-premises",
      "data",
      "center.",
      "due",
      "different",
      "setups",
      "special",
      "considerations",
      "taken",
      "account",
      "order",
      "connect",
      "cloud-based",
      "management.",
      "call",
      "mode.",
      "before",
      "connecting",
      "ud",
      "through",
      "gateway",
      "required",
      "understand",
      "communicate",
      "together.",
      "all",
      "actions",
      "done",
      "ci",
      "creations",
      "updates",
      "synchronized",
      "http",
      "requests",
      "orchestrated",
      "gateway.",
      "background",
      "activities",
      "websocket",
      "connection",
      "established",
      "ucmdb.",
      "handles",
      "discoveries",
      "keeps",
      "synchronized.",
      "allows",
      "long-lasting",
      "connections",
      "context.",
      "network",
      "like",
      "firewalls",
      "proxy",
      "server",
      "load-balancer",
      "need",
      "support",
      "upgrade",
      "header.",
      "details",
      "about",
      "mechanism",
      "see"
    ],
    "language": "en",
    "word_count": 107,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "native sacm hybrid mode",
    "contentLower": "what's native sacm hybrid mode? when implementing native sacm, the following deployment isn't rare: service management is installed in a cloud environment such as azure, aws, or google cloud. ucmdb is running in an on-premises data center. due to those different setups, some special considerations must be taken into account in order to connect the on-premises ucmdb with the cloud-based service management. this is what we call the “hybrid deployment” mode. communication methods local data-center components before connecting ud/ucmdb and service management through ucmdb gateway, it's required to understand how these 3 components communicate together. all actions done in service management such as ci creations and updates are synchronized to ucmdb through http requests orchestrated by ucmdb gateway. for all background activities, a websocket connection is established by service management to the gateway and then from the gateway to ucmdb. this websocket connection handles all requests fro",
    "keywordsLower": [
      "https://<ucmdb_server_host>:<ucmdb_server_port>/ucmdb-browser",
      "https://developer.mozilla.org/en-us/docs/web/http/protocol_upgrade_mechanism",
      "mozilla.org",
      "native",
      "sacm",
      "hybrid",
      "mode",
      "what",
      "communication",
      "methods",
      "local",
      "data-center",
      "components",
      "implementing",
      "service",
      "management",
      "cloud",
      "deployment",
      "prerequisites",
      "implementation",
      "steps",
      "enable",
      "single",
      "sign-on",
      "following",
      "isn",
      "rare",
      "installed",
      "environment",
      "such",
      "azure",
      "aws",
      "google",
      "cloud.",
      "ucmdb",
      "running",
      "on-premises",
      "data",
      "center.",
      "due",
      "different",
      "setups",
      "special",
      "considerations",
      "taken",
      "account",
      "order",
      "connect",
      "cloud-based",
      "management.",
      "call",
      "mode.",
      "before",
      "connecting",
      "ud",
      "through",
      "gateway",
      "required",
      "understand",
      "communicate",
      "together.",
      "all",
      "actions",
      "done",
      "ci",
      "creations",
      "updates",
      "synchronized",
      "http",
      "requests",
      "orchestrated",
      "gateway.",
      "background",
      "activities",
      "websocket",
      "connection",
      "established",
      "ucmdb.",
      "handles",
      "discoveries",
      "keeps",
      "synchronized.",
      "allows",
      "long-lasting",
      "connections",
      "context.",
      "network",
      "like",
      "firewalls",
      "proxy",
      "server",
      "load-balancer",
      "need",
      "support",
      "upgrade",
      "header.",
      "details",
      "about",
      "mechanism",
      "see"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform post-upgrade tasks as the suite admin",
    "content": "After you upgrade the suite to this version, the suite admin needs to perform the following tasks. AWS specific tasks If your suite is deployed on AWS, make sure you complete the following AWS specific tasks. Recreate worker nodes running on AL2 If your worker or bastion nodes are running on Amazon Linux 2 (AL2), we recommend upgrading the nodes to to Amazon Linux 2023. For instuctions to upgrade, refer to the official AWS documenation - Upgrade from Amazon Linux 2 to Amazon Linux 2023. To recreate the bastion or worker nodes using the lastest cloudformation script, see Configure bastion To enable the OMT Tools capability on the bastion or worker nodes, see Enable the Tools capability on a new bastion node. To save cost, you may delete the old bastion or worker nodes. Clean up unused images The instructions in this section are specific to suite deployments in an embedded Kubernetes environment. If your suite is deployed in a managed Kubernetes environment such as GCP, AWS, or Azure, re",
    "url": "performpostupgradetaskssuiteadmin",
    "filename": "performpostupgradetaskssuiteadmin",
    "headings": [
      "AWS specific tasks",
      "Recreate worker nodes running on AL2",
      "Clean up unused images",
      "Remove temporary worker node",
      "Upgrade Suite Support Assistant",
      "Upgrade the language pack",
      "Dev2Prod package export issue"
    ],
    "keywords": [
      "start.sh",
      "26.1",
      "25.2",
      "25.3",
      "25.4",
      "package.From",
      "25.4You",
      "podReScheduler.sh",
      "perform",
      "post-upgrade",
      "tasks",
      "suite",
      "admin",
      "aws",
      "specific",
      "recreate",
      "worker",
      "nodes",
      "running",
      "al2",
      "clean",
      "unused",
      "images",
      "remove",
      "temporary",
      "node",
      "upgrade",
      "support",
      "assistant",
      "language",
      "pack",
      "dev2prod",
      "package",
      "export",
      "issue",
      "after",
      "version",
      "needs",
      "following",
      "tasks.",
      "deployed",
      "make",
      "sure",
      "complete",
      "bastion",
      "amazon",
      "linux",
      "recommend",
      "upgrading",
      "2023.",
      "instuctions",
      "refer",
      "official",
      "documenation",
      "lastest",
      "cloudformation",
      "script",
      "see",
      "configure",
      "enable",
      "omt",
      "tools",
      "capability",
      "new",
      "node.",
      "save",
      "cost",
      "delete",
      "old",
      "nodes.",
      "instructions",
      "section",
      "deployments",
      "embedded",
      "kubernetes",
      "environment.",
      "managed",
      "environment",
      "such",
      "gcp",
      "azure",
      "documentation",
      "instructions.",
      "release",
      "pulls",
      "private",
      "registry",
      "nfs",
      "control",
      "plane",
      "start",
      "containers.",
      "however",
      "doesn",
      "older",
      "releases.",
      "cause",
      "docker",
      "storage",
      "become"
    ],
    "language": "en",
    "word_count": 112,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform post-upgrade tasks as the suite admin",
    "contentLower": "after you upgrade the suite to this version, the suite admin needs to perform the following tasks. aws specific tasks if your suite is deployed on aws, make sure you complete the following aws specific tasks. recreate worker nodes running on al2 if your worker or bastion nodes are running on amazon linux 2 (al2), we recommend upgrading the nodes to to amazon linux 2023. for instuctions to upgrade, refer to the official aws documenation - upgrade from amazon linux 2 to amazon linux 2023. to recreate the bastion or worker nodes using the lastest cloudformation script, see configure bastion to enable the omt tools capability on the bastion or worker nodes, see enable the tools capability on a new bastion node. to save cost, you may delete the old bastion or worker nodes. clean up unused images the instructions in this section are specific to suite deployments in an embedded kubernetes environment. if your suite is deployed in a managed kubernetes environment such as gcp, aws, or azure, re",
    "keywordsLower": [
      "start.sh",
      "26.1",
      "25.2",
      "25.3",
      "25.4",
      "package.from",
      "25.4you",
      "podrescheduler.sh",
      "perform",
      "post-upgrade",
      "tasks",
      "suite",
      "admin",
      "aws",
      "specific",
      "recreate",
      "worker",
      "nodes",
      "running",
      "al2",
      "clean",
      "unused",
      "images",
      "remove",
      "temporary",
      "node",
      "upgrade",
      "support",
      "assistant",
      "language",
      "pack",
      "dev2prod",
      "package",
      "export",
      "issue",
      "after",
      "version",
      "needs",
      "following",
      "tasks.",
      "deployed",
      "make",
      "sure",
      "complete",
      "bastion",
      "amazon",
      "linux",
      "recommend",
      "upgrading",
      "2023.",
      "instuctions",
      "refer",
      "official",
      "documenation",
      "lastest",
      "cloudformation",
      "script",
      "see",
      "configure",
      "enable",
      "omt",
      "tools",
      "capability",
      "new",
      "node.",
      "save",
      "cost",
      "delete",
      "old",
      "nodes.",
      "instructions",
      "section",
      "deployments",
      "embedded",
      "kubernetes",
      "environment.",
      "managed",
      "environment",
      "such",
      "gcp",
      "azure",
      "documentation",
      "instructions.",
      "release",
      "pulls",
      "private",
      "registry",
      "nfs",
      "control",
      "plane",
      "start",
      "containers.",
      "however",
      "doesn",
      "older",
      "releases.",
      "cause",
      "docker",
      "storage",
      "become"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform post-upgrade tasks as the tenant admin",
    "content": "To accommodate new and updated capabilities that are part of the current release, a tenant admin needs to review the following tasks, which are grouped into the following categories: Critical tasks: mandatory tasks that you must perform as soon as possible to ensure the uninterrupted functionality of the solution. Neglecting these tasks may have adverse effects on its proper operation.Essential tasks: mandatory tasks that you need to perform soon after this version update. Enhancement tasks: tasks that you need to perform before the next version update, if applicable and relevant. If not applicable or relevant, you can skip them. To use certain new features or enhancements in an upgraded environment, the tenant admin may need to edit forms and apply business rule changes from Studio in the agent interface (Administration > Configuration > Studio). For detailed instructions, see Edit a form and Configuration comparison. Critical tasks The following is a list of critical tasks. See the b",
    "url": "performpostupgradetaskstenantadmin",
    "filename": "performpostupgradetaskstenantadmin",
    "headings": [
      "Critical tasks",
      "OPB Agent upgrade",
      "External OO RAS and OO Workflow Designer upgrade",
      "Essential tasks",
      "Regenerate non-expiring PATs",
      "Enhancement tasks"
    ],
    "keywords": [
      "25.4",
      "Listener.To",
      "perform",
      "post-upgrade",
      "tasks",
      "tenant",
      "admin",
      "critical",
      "opb",
      "agent",
      "upgrade",
      "external",
      "oo",
      "ras",
      "workflow",
      "designer",
      "essential",
      "regenerate",
      "non-expiring",
      "pats",
      "enhancement",
      "accommodate",
      "new",
      "updated",
      "capabilities",
      "part",
      "current",
      "release",
      "needs",
      "review",
      "following",
      "grouped",
      "categories",
      "mandatory",
      "soon",
      "possible",
      "ensure",
      "uninterrupted",
      "functionality",
      "solution.",
      "neglecting",
      "adverse",
      "effects",
      "proper",
      "operation.essential",
      "need",
      "after",
      "version",
      "update.",
      "before",
      "next",
      "update",
      "applicable",
      "relevant.",
      "relevant",
      "skip",
      "them.",
      "certain",
      "features",
      "enhancements",
      "upgraded",
      "environment",
      "edit",
      "forms",
      "apply",
      "business",
      "rule",
      "changes",
      "studio",
      "interface",
      "administration",
      "configuration",
      "detailed",
      "instructions",
      "see",
      "form",
      "comparison.",
      "list",
      "tasks.",
      "beginning",
      "section",
      "topic",
      "definition",
      "task.",
      "automatically",
      "during",
      "make",
      "sure",
      "existing",
      "connect",
      "system",
      "successfully",
      "opb-based",
      "integrations",
      "continue",
      "work.",
      "doesn",
      "work",
      "troubleshooting",
      "instructions."
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform post-upgrade tasks as the tenant admin",
    "contentLower": "to accommodate new and updated capabilities that are part of the current release, a tenant admin needs to review the following tasks, which are grouped into the following categories: critical tasks: mandatory tasks that you must perform as soon as possible to ensure the uninterrupted functionality of the solution. neglecting these tasks may have adverse effects on its proper operation.essential tasks: mandatory tasks that you need to perform soon after this version update. enhancement tasks: tasks that you need to perform before the next version update, if applicable and relevant. if not applicable or relevant, you can skip them. to use certain new features or enhancements in an upgraded environment, the tenant admin may need to edit forms and apply business rule changes from studio in the agent interface (administration > configuration > studio). for detailed instructions, see edit a form and configuration comparison. critical tasks the following is a list of critical tasks. see the b",
    "keywordsLower": [
      "25.4",
      "listener.to",
      "perform",
      "post-upgrade",
      "tasks",
      "tenant",
      "admin",
      "critical",
      "opb",
      "agent",
      "upgrade",
      "external",
      "oo",
      "ras",
      "workflow",
      "designer",
      "essential",
      "regenerate",
      "non-expiring",
      "pats",
      "enhancement",
      "accommodate",
      "new",
      "updated",
      "capabilities",
      "part",
      "current",
      "release",
      "needs",
      "review",
      "following",
      "grouped",
      "categories",
      "mandatory",
      "soon",
      "possible",
      "ensure",
      "uninterrupted",
      "functionality",
      "solution.",
      "neglecting",
      "adverse",
      "effects",
      "proper",
      "operation.essential",
      "need",
      "after",
      "version",
      "update.",
      "before",
      "next",
      "update",
      "applicable",
      "relevant.",
      "relevant",
      "skip",
      "them.",
      "certain",
      "features",
      "enhancements",
      "upgraded",
      "environment",
      "edit",
      "forms",
      "apply",
      "business",
      "rule",
      "changes",
      "studio",
      "interface",
      "administration",
      "configuration",
      "detailed",
      "instructions",
      "see",
      "form",
      "comparison.",
      "list",
      "tasks.",
      "beginning",
      "section",
      "topic",
      "definition",
      "task.",
      "automatically",
      "during",
      "make",
      "sure",
      "existing",
      "connect",
      "system",
      "successfully",
      "opb-based",
      "integrations",
      "continue",
      "work.",
      "doesn",
      "work",
      "troubleshooting",
      "instructions."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform post-upgrade tasks as the tenant admin",
    "content": "For details, see Perform post-upgrade tasks as the tenant admin.",
    "url": "postupgradetenantadminmanagedk8ssuite",
    "filename": "postupgradetenantadminmanagedk8ssuite",
    "headings": [],
    "keywords": [
      "perform",
      "post-upgrade",
      "tasks",
      "tenant",
      "admin",
      "details",
      "see",
      "admin."
    ],
    "language": "en",
    "word_count": 12,
    "importance_score": 1.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform post-upgrade tasks as the tenant admin",
    "contentLower": "for details, see perform post-upgrade tasks as the tenant admin.",
    "keywordsLower": [
      "perform",
      "post-upgrade",
      "tasks",
      "tenant",
      "admin",
      "details",
      "see",
      "admin."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "On-Premises Bridge Agents and Endpoints",
    "content": "The On-Premises Bridge (OPB) is a platform that enables communication between on-premises applications and Service Management. Because on-premises applications are located behind firewalls, initiating a connection from Service Management to an on-premises application isn't possible. Therefore you will require a \"bridge\" component. On-Premises Bridge comprises of: On-Premises Bridge Agent - The agent is a software component that's installed on the customer site and mediates between Service Management and an on-premises application. The agent initiates communications with Service Management to receive tasks from it. Service Management On-Premises Bridge Service - The service is the entry point for submitting tasks by Service Management applications and for On-Premises Bridge Agents to request tasks. In addition to allowing execution of tasks on the agents, this layer is also responsible for the registration and management of agents, monitoring, secure communication with the agents. Note ",
    "url": "onpremisebridge",
    "filename": "onpremisebridge",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "on-premises",
      "bridge",
      "agents",
      "endpoints",
      "related",
      "topics",
      "opb",
      "platform",
      "enables",
      "communication",
      "between",
      "applications",
      "service",
      "management.",
      "because",
      "located",
      "behind",
      "firewalls",
      "initiating",
      "connection",
      "management",
      "application",
      "isn",
      "possible.",
      "therefore",
      "require",
      "component.",
      "comprises",
      "agent",
      "software",
      "component",
      "installed",
      "customer",
      "site",
      "mediates",
      "application.",
      "initiates",
      "communications",
      "receive",
      "tasks",
      "it.",
      "entry",
      "point",
      "submitting",
      "request",
      "tasks.",
      "addition",
      "allowing",
      "execution",
      "layer",
      "responsible",
      "registration",
      "monitoring",
      "secure",
      "agents.",
      "note",
      "recommended",
      "replace",
      "out",
      "box",
      "certificate",
      "automation",
      "before",
      "refer",
      "sma.",
      "suite",
      "version",
      "update",
      "automatically",
      "upgrades",
      "newer",
      "available.",
      "continue",
      "work",
      "after",
      "upgrade",
      "new",
      "version.",
      "however",
      "doesn",
      "see",
      "upgrade.",
      "auto-upgrade",
      "block",
      "util",
      "tools",
      "usage",
      "resulting",
      "issues",
      "during",
      "post",
      "process.",
      "issue",
      "occurs",
      "auto-upgrade.",
      "windows",
      "linux",
      "security",
      "additional",
      "information"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "on-premises bridge agents and endpoints",
    "contentLower": "the on-premises bridge (opb) is a platform that enables communication between on-premises applications and service management. because on-premises applications are located behind firewalls, initiating a connection from service management to an on-premises application isn't possible. therefore you will require a \"bridge\" component. on-premises bridge comprises of: on-premises bridge agent - the agent is a software component that's installed on the customer site and mediates between service management and an on-premises application. the agent initiates communications with service management to receive tasks from it. service management on-premises bridge service - the service is the entry point for submitting tasks by service management applications and for on-premises bridge agents to request tasks. in addition to allowing execution of tasks on the agents, this layer is also responsible for the registration and management of agents, monitoring, secure communication with the agents. note ",
    "keywordsLower": [
      "on-premises",
      "bridge",
      "agents",
      "endpoints",
      "related",
      "topics",
      "opb",
      "platform",
      "enables",
      "communication",
      "between",
      "applications",
      "service",
      "management.",
      "because",
      "located",
      "behind",
      "firewalls",
      "initiating",
      "connection",
      "management",
      "application",
      "isn",
      "possible.",
      "therefore",
      "require",
      "component.",
      "comprises",
      "agent",
      "software",
      "component",
      "installed",
      "customer",
      "site",
      "mediates",
      "application.",
      "initiates",
      "communications",
      "receive",
      "tasks",
      "it.",
      "entry",
      "point",
      "submitting",
      "request",
      "tasks.",
      "addition",
      "allowing",
      "execution",
      "layer",
      "responsible",
      "registration",
      "monitoring",
      "secure",
      "agents.",
      "note",
      "recommended",
      "replace",
      "out",
      "box",
      "certificate",
      "automation",
      "before",
      "refer",
      "sma.",
      "suite",
      "version",
      "update",
      "automatically",
      "upgrades",
      "newer",
      "available.",
      "continue",
      "work",
      "after",
      "upgrade",
      "new",
      "version.",
      "however",
      "doesn",
      "see",
      "upgrade.",
      "auto-upgrade",
      "block",
      "util",
      "tools",
      "usage",
      "resulting",
      "issues",
      "during",
      "post",
      "process.",
      "issue",
      "occurs",
      "auto-upgrade.",
      "windows",
      "linux",
      "security",
      "additional",
      "information"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "On-Premises Bridge download center",
    "content": "The On-Premises Bridge download center is located on the right side of the Endpoints tab. It contains links to download remote products and services, which can be used in connection with the On-Premises Bridge. The following links are available. IDOL You can download IDOL connectors and other required files for the IDOL knowledge indexing solution, which enables you to index knowledge articles to Service Management. For more information, see Index external knowledge using IDOL connectors. UCMDB UCMDB/Universal Discovery By default, it's a link that forwards you to the software and license download site, where you can download UCMDB/UD. If your suite administrator has replaced the software and license download site link with direct download links to the UD Probe and UCMDB Local Client embedded in the containerized UD/UCMDB (see Enable direct download of UD Probe and Local Client from the Service Management UI), you can download the following UCMDB installation files: Universal Discovery",
    "url": "opbdownloadcenter",
    "filename": "opbdownloadcenter",
    "headings": [
      "IDOL",
      "UCMDB",
      "UCMDB/Universal Discovery",
      "SMAX Push Adapter",
      "Operations Orchestration",
      "Operations Manager i",
      "Project and Portfolio Management"
    ],
    "keywords": [
      "10.20",
      "on-premises",
      "bridge",
      "download",
      "center",
      "idol",
      "ucmdb",
      "universal",
      "discovery",
      "smax",
      "push",
      "adapter",
      "operations",
      "orchestration",
      "manager",
      "project",
      "portfolio",
      "management",
      "located",
      "right",
      "side",
      "endpoints",
      "tab.",
      "contains",
      "links",
      "remote",
      "products",
      "services",
      "connection",
      "bridge.",
      "following",
      "available.",
      "connectors",
      "required",
      "files",
      "knowledge",
      "indexing",
      "solution",
      "enables",
      "index",
      "articles",
      "service",
      "management.",
      "information",
      "see",
      "external",
      "connectors.",
      "default",
      "link",
      "forwards",
      "software",
      "license",
      "site",
      "ud.",
      "suite",
      "administrator",
      "replaced",
      "direct",
      "ud",
      "probe",
      "local",
      "client",
      "embedded",
      "containerized",
      "enable",
      "ui",
      "installation",
      "windows",
      "linux",
      "macos",
      "later.",
      "oo",
      "section",
      "search",
      "files.",
      "alternatively",
      "ras",
      "workflow",
      "designer",
      "upgrade",
      "option",
      "replace",
      "product",
      "website",
      "clicking",
      "trigger",
      "automatic",
      "corresponding",
      "file.",
      "ui.",
      "action",
      "enabled",
      "links.",
      "omi",
      "integrate",
      "i.",
      "ppm",
      "synchronize",
      "ppm.",
      "integration"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "on-premises bridge download center",
    "contentLower": "the on-premises bridge download center is located on the right side of the endpoints tab. it contains links to download remote products and services, which can be used in connection with the on-premises bridge. the following links are available. idol you can download idol connectors and other required files for the idol knowledge indexing solution, which enables you to index knowledge articles to service management. for more information, see index external knowledge using idol connectors. ucmdb ucmdb/universal discovery by default, it's a link that forwards you to the software and license download site, where you can download ucmdb/ud. if your suite administrator has replaced the software and license download site link with direct download links to the ud probe and ucmdb local client embedded in the containerized ud/ucmdb (see enable direct download of ud probe and local client from the service management ui), you can download the following ucmdb installation files: universal discovery",
    "keywordsLower": [
      "10.20",
      "on-premises",
      "bridge",
      "download",
      "center",
      "idol",
      "ucmdb",
      "universal",
      "discovery",
      "smax",
      "push",
      "adapter",
      "operations",
      "orchestration",
      "manager",
      "project",
      "portfolio",
      "management",
      "located",
      "right",
      "side",
      "endpoints",
      "tab.",
      "contains",
      "links",
      "remote",
      "products",
      "services",
      "connection",
      "bridge.",
      "following",
      "available.",
      "connectors",
      "required",
      "files",
      "knowledge",
      "indexing",
      "solution",
      "enables",
      "index",
      "articles",
      "service",
      "management.",
      "information",
      "see",
      "external",
      "connectors.",
      "default",
      "link",
      "forwards",
      "software",
      "license",
      "site",
      "ud.",
      "suite",
      "administrator",
      "replaced",
      "direct",
      "ud",
      "probe",
      "local",
      "client",
      "embedded",
      "containerized",
      "enable",
      "ui",
      "installation",
      "windows",
      "linux",
      "macos",
      "later.",
      "oo",
      "section",
      "search",
      "files.",
      "alternatively",
      "ras",
      "workflow",
      "designer",
      "upgrade",
      "option",
      "replace",
      "product",
      "website",
      "clicking",
      "trigger",
      "automatic",
      "corresponding",
      "file.",
      "ui.",
      "action",
      "enabled",
      "links.",
      "omi",
      "integrate",
      "i.",
      "ppm",
      "synchronize",
      "ppm.",
      "integration"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "On-Premises Bridge security additional information",
    "content": "Security aspects addressed by the agent Communication between an On-Premises Bridge Agent (OPB) and Service Management uses SSL to secure the connection. In addition, the OPB Agent connects to Service Management using the user and password provided during installation. This user is created by the customer for the dedicated OPB Remote Agent role. Passwords for the endpoint credentials are saved encrypted on the customer's machine, which prevents the credentials from being transferred to another machine. The encryption method uses keys that are randomly generated during installation. The agent uses AES 128 as the main encryption method. The agent doesn't expose any internal information. Network configuration Deploy the agent in an isolated network with a firewall between the agent and the target on-premises applications. The outbound OPB communication with Service Management requires port 443 to be opened, and no inbound OPB connectivity is required on any ports. The RMI port is port 109",
    "url": "opbsecurity",
    "filename": "opbsecurity",
    "headings": [
      "Security aspects addressed by the agent",
      "Network configuration",
      "Security recommendations",
      "OPB certificates",
      "Finding the location of the OPB controller log",
      "Finding the location of the OPB trusted keystore",
      "Obtaining the certificate of the remote server",
      "Adding the certificate to the trusted keystore",
      "Verify if the certificate is present"
    ],
    "keywords": [
      "additional.108",
      "myCert.crt",
      "server.port",
      "custom.conf",
      "controller.log",
      "1.3",
      "javax.net",
      "contents.txt",
      "wrapper.java",
      "on-premises",
      "bridge",
      "security",
      "additional",
      "information",
      "aspects",
      "addressed",
      "agent",
      "network",
      "configuration",
      "recommendations",
      "opb",
      "certificates",
      "finding",
      "location",
      "controller",
      "log",
      "trusted",
      "keystore",
      "obtaining",
      "certificate",
      "remote",
      "server",
      "adding",
      "verify",
      "present",
      "communication",
      "between",
      "service",
      "management",
      "uses",
      "ssl",
      "secure",
      "connection.",
      "addition",
      "connects",
      "user",
      "password",
      "provided",
      "during",
      "installation.",
      "created",
      "customer",
      "dedicated",
      "role.",
      "passwords",
      "endpoint",
      "credentials",
      "saved",
      "encrypted",
      "machine",
      "prevents",
      "transferred",
      "another",
      "machine.",
      "encryption",
      "method",
      "keys",
      "randomly",
      "generated",
      "aes",
      "128",
      "main",
      "method.",
      "doesn",
      "expose",
      "any",
      "internal",
      "information.",
      "deploy",
      "isolated",
      "firewall",
      "target",
      "applications.",
      "outbound",
      "requires",
      "port",
      "443",
      "opened",
      "inbound",
      "connectivity",
      "required",
      "ports.",
      "rmi",
      "1099",
      "default",
      "configured",
      "wrapper.java.additional.108",
      "-drmi.server.port",
      "wrapper-custom.conf",
      "file"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "on-premises bridge security additional information",
    "contentLower": "security aspects addressed by the agent communication between an on-premises bridge agent (opb) and service management uses ssl to secure the connection. in addition, the opb agent connects to service management using the user and password provided during installation. this user is created by the customer for the dedicated opb remote agent role. passwords for the endpoint credentials are saved encrypted on the customer's machine, which prevents the credentials from being transferred to another machine. the encryption method uses keys that are randomly generated during installation. the agent uses aes 128 as the main encryption method. the agent doesn't expose any internal information. network configuration deploy the agent in an isolated network with a firewall between the agent and the target on-premises applications. the outbound opb communication with service management requires port 443 to be opened, and no inbound opb connectivity is required on any ports. the rmi port is port 109",
    "keywordsLower": [
      "additional.108",
      "mycert.crt",
      "server.port",
      "custom.conf",
      "controller.log",
      "1.3",
      "javax.net",
      "contents.txt",
      "wrapper.java",
      "on-premises",
      "bridge",
      "security",
      "additional",
      "information",
      "aspects",
      "addressed",
      "agent",
      "network",
      "configuration",
      "recommendations",
      "opb",
      "certificates",
      "finding",
      "location",
      "controller",
      "log",
      "trusted",
      "keystore",
      "obtaining",
      "certificate",
      "remote",
      "server",
      "adding",
      "verify",
      "present",
      "communication",
      "between",
      "service",
      "management",
      "uses",
      "ssl",
      "secure",
      "connection.",
      "addition",
      "connects",
      "user",
      "password",
      "provided",
      "during",
      "installation.",
      "created",
      "customer",
      "dedicated",
      "role.",
      "passwords",
      "endpoint",
      "credentials",
      "saved",
      "encrypted",
      "machine",
      "prevents",
      "transferred",
      "another",
      "machine.",
      "encryption",
      "method",
      "keys",
      "randomly",
      "generated",
      "aes",
      "128",
      "main",
      "method.",
      "doesn",
      "expose",
      "any",
      "internal",
      "information.",
      "deploy",
      "isolated",
      "firewall",
      "target",
      "applications.",
      "outbound",
      "requires",
      "port",
      "443",
      "opened",
      "inbound",
      "connectivity",
      "required",
      "ports.",
      "rmi",
      "1099",
      "default",
      "configured",
      "wrapper.java.additional.108",
      "-drmi.server.port",
      "wrapper-custom.conf",
      "file"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Objects",
    "content": "Objects are data structures that contain a collection of properties and objects. The Integration Studio supports two types of objects: user-defined objects and built-in objects. The difference between the two lies in how they are used: you can directly reference the properties of built-in objects, but you need to manually create a user-defined object before you can reference its properties. Note that you can use built-in object properties in user-defined objects. The object name must be unique across the entire scenario. Otherwise, the object defined at the beginning of the scenario will be overridden by the same object defined in a later rule. This applies to built-in objects as well, so don't use the built-in object names for user-defined objects (unless you do intend to override the built-in objects). Object and property names are case-sensitive. User-defined objects User-defined objects enable you to achieve the following: Facilitate reuse of properties in scenario rules. For examp",
    "url": "xieobjects",
    "filename": "xieobjects",
    "headings": [
      "User-defined objects",
      "Choose where to define objects",
      "Define objects",
      "Use object properties",
      "How to reference object properties",
      "Use optional chaining operator to prevent errors",
      "Change object property values",
      "For objects defined in Prepare Data rules and the Output section of normal rules",
      "For objects defined in Store Data rules",
      "Examples",
      "Prepare Data example",
      "Store Data example",
      "Built-in objects",
      "payload object",
      "response object",
      "currentSystem object",
      "currentIntegration object",
      "currentScenario object",
      "Related topics"
    ],
    "keywords": [
      "payload.data",
      "headers.eTag",
      "response.Code",
      "https://<SMAX_external_access_host",
      "response.data",
      "objects",
      "user-defined",
      "choose",
      "define",
      "object",
      "properties",
      "reference",
      "optional",
      "chaining",
      "operator",
      "prevent",
      "errors",
      "change",
      "property",
      "values",
      "defined",
      "prepare",
      "data",
      "rules",
      "output",
      "section",
      "normal",
      "store",
      "examples",
      "example",
      "built-in",
      "payload",
      "response",
      "currentsystem",
      "currentintegration",
      "currentscenario",
      "related",
      "topics",
      "structures",
      "contain",
      "collection",
      "objects.",
      "integration",
      "studio",
      "supports",
      "two",
      "types",
      "difference",
      "between",
      "lies",
      "directly",
      "need",
      "manually",
      "create",
      "before",
      "properties.",
      "note",
      "name",
      "unique",
      "across",
      "entire",
      "scenario.",
      "otherwise",
      "beginning",
      "scenario",
      "overridden",
      "same",
      "later",
      "rule.",
      "applies",
      "well",
      "don",
      "names",
      "unless",
      "intend",
      "override",
      "case-sensitive.",
      "enable",
      "achieve",
      "following",
      "facilitate",
      "reuse",
      "rules.",
      "many",
      "places",
      "throughout",
      "once",
      "multiple",
      "value",
      "one",
      "place.",
      "pass",
      "rule",
      "another.",
      "system",
      "request",
      "another",
      "action",
      "even",
      "different"
    ],
    "language": "en",
    "word_count": 91,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "objects",
    "contentLower": "objects are data structures that contain a collection of properties and objects. the integration studio supports two types of objects: user-defined objects and built-in objects. the difference between the two lies in how they are used: you can directly reference the properties of built-in objects, but you need to manually create a user-defined object before you can reference its properties. note that you can use built-in object properties in user-defined objects. the object name must be unique across the entire scenario. otherwise, the object defined at the beginning of the scenario will be overridden by the same object defined in a later rule. this applies to built-in objects as well, so don't use the built-in object names for user-defined objects (unless you do intend to override the built-in objects). object and property names are case-sensitive. user-defined objects user-defined objects enable you to achieve the following: facilitate reuse of properties in scenario rules. for examp",
    "keywordsLower": [
      "payload.data",
      "headers.etag",
      "response.code",
      "https://<smax_external_access_host",
      "response.data",
      "objects",
      "user-defined",
      "choose",
      "define",
      "object",
      "properties",
      "reference",
      "optional",
      "chaining",
      "operator",
      "prevent",
      "errors",
      "change",
      "property",
      "values",
      "defined",
      "prepare",
      "data",
      "rules",
      "output",
      "section",
      "normal",
      "store",
      "examples",
      "example",
      "built-in",
      "payload",
      "response",
      "currentsystem",
      "currentintegration",
      "currentscenario",
      "related",
      "topics",
      "structures",
      "contain",
      "collection",
      "objects.",
      "integration",
      "studio",
      "supports",
      "two",
      "types",
      "difference",
      "between",
      "lies",
      "directly",
      "need",
      "manually",
      "create",
      "before",
      "properties.",
      "note",
      "name",
      "unique",
      "across",
      "entire",
      "scenario.",
      "otherwise",
      "beginning",
      "scenario",
      "overridden",
      "same",
      "later",
      "rule.",
      "applies",
      "well",
      "don",
      "names",
      "unless",
      "intend",
      "override",
      "case-sensitive.",
      "enable",
      "achieve",
      "following",
      "facilitate",
      "reuse",
      "rules.",
      "many",
      "places",
      "throughout",
      "once",
      "multiple",
      "value",
      "one",
      "place.",
      "pass",
      "rule",
      "another.",
      "system",
      "request",
      "another",
      "action",
      "even",
      "different"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "On-Premises Bridge task performance tuning",
    "content": "To improve performance and keep your On-Premises Bridge (OPB) tasks running at peak potential, consider the following methods: Enable OPB task cleanup job. Database tuning of indexing the OpbTaskDataJSON table. Enable OPB task cleanup job Enable a scheduled job to clean up historical data of completed OPB tasks when the number of task records exceeds a predefined threshold. See Clean up historical OPB task data from database. Database tuning The following section contains information for tuning the Postgres database. Important: Perform these steps for each existing and all newly added tenant as they are added. Indexing the OpbTaskDataJSON table Check your query to see if you find similar queries like: SELECT body FROM \"OpbTaskDataJSON_850280433\" WHERE (body->'Task'@>'{\"TaskId\":\"837d7c42-d5c6-4461-a134-dd2a00d8192f\"}'::jsonb) ORDER BY id LIMIT 2000 If a similar query exists, use the steps below to index the OpbTaskDataJSON table: Create a new index for the OpbTaskDataJSON with the follo",
    "url": "opbperformancetuning",
    "filename": "opbperformancetuning",
    "headings": [
      "Enable OPB task cleanup job",
      "Database tuning",
      "Indexing the OpbTaskDataJSON table"
    ],
    "keywords": [
      "on-premises",
      "bridge",
      "task",
      "performance",
      "tuning",
      "enable",
      "opb",
      "cleanup",
      "job",
      "database",
      "indexing",
      "opbtaskdatajson",
      "table",
      "improve",
      "keep",
      "tasks",
      "running",
      "peak",
      "potential",
      "consider",
      "following",
      "methods",
      "job.",
      "table.",
      "scheduled",
      "clean",
      "historical",
      "data",
      "completed",
      "number",
      "records",
      "exceeds",
      "predefined",
      "threshold.",
      "see",
      "database.",
      "section",
      "contains",
      "information",
      "postgres",
      "important",
      "perform",
      "steps",
      "existing",
      "all",
      "newly",
      "added",
      "tenant",
      "added.",
      "check",
      "query",
      "find",
      "similar",
      "queries",
      "like",
      "select",
      "body",
      "body-",
      "taskid",
      "837d7c42-d5c6-4461-a134-dd2a00d8192f",
      "jsonb",
      "order",
      "id",
      "limit",
      "2000",
      "exists",
      "below",
      "index",
      "create",
      "new",
      "command",
      "gin",
      "text",
      "test",
      "explain",
      "analyze",
      "return",
      "scan"
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "on-premises bridge task performance tuning",
    "contentLower": "to improve performance and keep your on-premises bridge (opb) tasks running at peak potential, consider the following methods: enable opb task cleanup job. database tuning of indexing the opbtaskdatajson table. enable opb task cleanup job enable a scheduled job to clean up historical data of completed opb tasks when the number of task records exceeds a predefined threshold. see clean up historical opb task data from database. database tuning the following section contains information for tuning the postgres database. important: perform these steps for each existing and all newly added tenant as they are added. indexing the opbtaskdatajson table check your query to see if you find similar queries like: select body from \"opbtaskdatajson_850280433\" where (body->'task'@>'{\"taskid\":\"837d7c42-d5c6-4461-a134-dd2a00d8192f\"}'::jsonb) order by id limit 2000 if a similar query exists, use the steps below to index the opbtaskdatajson table: create a new index for the opbtaskdatajson with the follo",
    "keywordsLower": [
      "on-premises",
      "bridge",
      "task",
      "performance",
      "tuning",
      "enable",
      "opb",
      "cleanup",
      "job",
      "database",
      "indexing",
      "opbtaskdatajson",
      "table",
      "improve",
      "keep",
      "tasks",
      "running",
      "peak",
      "potential",
      "consider",
      "following",
      "methods",
      "job.",
      "table.",
      "scheduled",
      "clean",
      "historical",
      "data",
      "completed",
      "number",
      "records",
      "exceeds",
      "predefined",
      "threshold.",
      "see",
      "database.",
      "section",
      "contains",
      "information",
      "postgres",
      "important",
      "perform",
      "steps",
      "existing",
      "all",
      "newly",
      "added",
      "tenant",
      "added.",
      "check",
      "query",
      "find",
      "similar",
      "queries",
      "like",
      "select",
      "body",
      "body-",
      "taskid",
      "837d7c42-d5c6-4461-a134-dd2a00d8192f",
      "jsonb",
      "order",
      "id",
      "limit",
      "2000",
      "exists",
      "below",
      "index",
      "create",
      "new",
      "command",
      "gin",
      "text",
      "test",
      "explain",
      "analyze",
      "return",
      "scan"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Move knowledge articles",
    "content": "Use case This integration enables you to import knowledge articles from a source Service Management tenant into the target tenant (local system). The integration imports the majority of data from the knowledge articles, such as the title, article content, date fields, services, categories, and attachments. However, it does not import audiences, models, related documents, task plans, and related asset models. Set up the integration As an example, the procedure below will set up the integration to implement the use case. This procedure consists of steps that you must perform on the target tenant (local system), the tenant where the documents will be imported. Prepare integration users We recommend creating a dedicated integration user for each system. Prepare an integration user in the source tenant: Create an integration user. See How to create an integration user. Log in to the agent interface of the source tenant as the tenant admin. Navigate to Administration > Master Data > People, ",
    "url": "integratesmax4knowledgeexchg",
    "filename": "integratesmax4knowledgeexchg",
    "headings": [
      "Use case",
      "Set up the integration",
      "Prepare integration users",
      "Download and install the OPB Agent (optional)",
      "Configure the target tenant (local system)",
      "Create an endpoint",
      "Create an integration",
      "Configure a scenario",
      "Use the scenario"
    ],
    "keywords": [
      "2.0",
      "https://<FQDN",
      "X.509",
      "move",
      "knowledge",
      "articles",
      "case",
      "set",
      "integration",
      "prepare",
      "users",
      "download",
      "install",
      "opb",
      "agent",
      "optional",
      "configure",
      "target",
      "tenant",
      "local",
      "system",
      "create",
      "endpoint",
      "scenario",
      "enables",
      "import",
      "source",
      "service",
      "management",
      "imports",
      "majority",
      "data",
      "such",
      "title",
      "article",
      "content",
      "date",
      "fields",
      "services",
      "categories",
      "attachments.",
      "however",
      "audiences",
      "models",
      "related",
      "documents",
      "task",
      "plans",
      "asset",
      "models.",
      "example",
      "procedure",
      "below",
      "implement",
      "case.",
      "consists",
      "steps",
      "perform",
      "imported.",
      "recommend",
      "creating",
      "dedicated",
      "user",
      "system.",
      "user.",
      "see",
      "log",
      "interface",
      "admin.",
      "navigate",
      "administration",
      "master",
      "people",
      "locate",
      "assign",
      "appropriate",
      "role",
      "read",
      "articles.",
      "alternatively",
      "admin",
      "role.",
      "update",
      "remote",
      "directly",
      "connect",
      "don",
      "need",
      "on-premises",
      "bridge",
      "agent.",
      "there",
      "web",
      "application",
      "firewall",
      "deployed",
      "front",
      "needs",
      "instance",
      "running"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "move knowledge articles",
    "contentLower": "use case this integration enables you to import knowledge articles from a source service management tenant into the target tenant (local system). the integration imports the majority of data from the knowledge articles, such as the title, article content, date fields, services, categories, and attachments. however, it does not import audiences, models, related documents, task plans, and related asset models. set up the integration as an example, the procedure below will set up the integration to implement the use case. this procedure consists of steps that you must perform on the target tenant (local system), the tenant where the documents will be imported. prepare integration users we recommend creating a dedicated integration user for each system. prepare an integration user in the source tenant: create an integration user. see how to create an integration user. log in to the agent interface of the source tenant as the tenant admin. navigate to administration > master data > people, ",
    "keywordsLower": [
      "2.0",
      "https://<fqdn",
      "x.509",
      "move",
      "knowledge",
      "articles",
      "case",
      "set",
      "integration",
      "prepare",
      "users",
      "download",
      "install",
      "opb",
      "agent",
      "optional",
      "configure",
      "target",
      "tenant",
      "local",
      "system",
      "create",
      "endpoint",
      "scenario",
      "enables",
      "import",
      "source",
      "service",
      "management",
      "imports",
      "majority",
      "data",
      "such",
      "title",
      "article",
      "content",
      "date",
      "fields",
      "services",
      "categories",
      "attachments.",
      "however",
      "audiences",
      "models",
      "related",
      "documents",
      "task",
      "plans",
      "asset",
      "models.",
      "example",
      "procedure",
      "below",
      "implement",
      "case.",
      "consists",
      "steps",
      "perform",
      "imported.",
      "recommend",
      "creating",
      "dedicated",
      "user",
      "system.",
      "user.",
      "see",
      "log",
      "interface",
      "admin.",
      "navigate",
      "administration",
      "master",
      "people",
      "locate",
      "assign",
      "appropriate",
      "role",
      "read",
      "articles.",
      "alternatively",
      "admin",
      "role.",
      "update",
      "remote",
      "directly",
      "connect",
      "don",
      "need",
      "on-premises",
      "bridge",
      "agent.",
      "there",
      "web",
      "application",
      "firewall",
      "deployed",
      "front",
      "needs",
      "instance",
      "running"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Matching CIs",
    "content": "There is a set of matching rules for each supported configuration item (CI) type. The service tries each rule, in order, stopping on the first successful match. CI type Matching rules and order Device GlobalId BarcodeOrRFIDTag AssetTag Model + Serial Number BiosAssetTag HostName + Domain Name Note: for a successful match with the last rule, both Model + Serial Number and BiosAssetTag must be empty. ActualService GlobalId DisplayLabel SystemElement GlobalId ServiceComponent GlobalId DisplayLabel Related topics Identification How to consume identification Output",
    "url": "matchingci",
    "filename": "matchingci",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "matching",
      "cis",
      "related",
      "topics",
      "there",
      "set",
      "rules",
      "supported",
      "configuration",
      "item",
      "ci",
      "type.",
      "service",
      "tries",
      "rule",
      "order",
      "stopping",
      "first",
      "successful",
      "match.",
      "type",
      "device",
      "globalid",
      "barcodeorrfidtag",
      "assettag",
      "model",
      "serial",
      "number",
      "biosassettag",
      "hostname",
      "domain",
      "name",
      "note",
      "match",
      "last",
      "both",
      "empty.",
      "actualservice",
      "displaylabel",
      "systemelement",
      "servicecomponent",
      "identification",
      "consume",
      "output"
    ],
    "language": "en",
    "word_count": 60,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "matching cis",
    "contentLower": "there is a set of matching rules for each supported configuration item (ci) type. the service tries each rule, in order, stopping on the first successful match. ci type matching rules and order device globalid barcodeorrfidtag assettag model + serial number biosassettag hostname + domain name note: for a successful match with the last rule, both model + serial number and biosassettag must be empty. actualservice globalid displaylabel systemelement globalid servicecomponent globalid displaylabel related topics identification how to consume identification output",
    "keywordsLower": [
      "matching",
      "cis",
      "related",
      "topics",
      "there",
      "set",
      "rules",
      "supported",
      "configuration",
      "item",
      "ci",
      "type.",
      "service",
      "tries",
      "rule",
      "order",
      "stopping",
      "first",
      "successful",
      "match.",
      "type",
      "device",
      "globalid",
      "barcodeorrfidtag",
      "assettag",
      "model",
      "serial",
      "number",
      "biosassettag",
      "hostname",
      "domain",
      "name",
      "note",
      "match",
      "last",
      "both",
      "empty.",
      "actualservice",
      "displaylabel",
      "systemelement",
      "servicecomponent",
      "identification",
      "consume",
      "output"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Output",
    "content": "The Identification output is a JSON string with the following collections of CIs. The format is as shown in Example of configuration item (CI) type input: CIs Description ambiguousEntities CIs for which the service found more than one matching CI in Service Management. Example: an incoming device whose name + domainName combination exists twice in Service Management. Note: in this collection the JSON format differs slightly, as it also includes the ID for all the matching CIs found in Service Management. Example: the following JSON format shows one incoming device that matches four devices in Service Management: \"ambiguousEntities\":[{ \"entity\": {\"properties\": {\"Name\":\"na\", \"DomainName\":\"dna\", \"DisplayName\":\"dispb\"}, \"entityType\":\"Device\", \"matchingEntityIds\":[\"13\",\"7\",\"14\",\"10\"] } ]} entitiesToCreate CIs the service has identified for which no matching CI was found in Service Management. entitiesToUpdate CIs the service has identified for which exactly one matching CI was found in Serv",
    "url": "idoutput",
    "filename": "idoutput",
    "headings": [
      "Merged CIs",
      "Component CIs",
      "Related topics"
    ],
    "keywords": [
      "output",
      "merged",
      "cis",
      "component",
      "related",
      "topics",
      "identification",
      "json",
      "string",
      "following",
      "collections",
      "cis.",
      "format",
      "shown",
      "example",
      "configuration",
      "item",
      "ci",
      "type",
      "input",
      "description",
      "ambiguousentities",
      "service",
      "found",
      "one",
      "matching",
      "management.",
      "incoming",
      "device",
      "whose",
      "name",
      "domainname",
      "combination",
      "exists",
      "twice",
      "note",
      "collection",
      "differs",
      "slightly",
      "includes",
      "id",
      "all",
      "shows",
      "matches",
      "four",
      "devices",
      "management",
      "entity",
      "properties",
      "na",
      "dna",
      "displayname",
      "dispb",
      "entitytype",
      "matchingentityids",
      "13",
      "14",
      "10",
      "entitiestocreate",
      "identified",
      "entitiestoupdate",
      "exactly",
      "least",
      "property",
      "value",
      "different.",
      "returns",
      "form.",
      "see",
      "section.",
      "unchangedentities",
      "identical",
      "values.",
      "unidentifiableentities",
      "unable",
      "identify",
      "existing",
      "due",
      "insufficient",
      "information.",
      "creates",
      "collection.",
      "merges",
      "follows",
      "matched",
      "merge",
      "action",
      "takes",
      "ci.",
      "keep",
      "set",
      "true",
      "maintains",
      "regardless",
      "allows",
      "augmentation",
      "additional",
      "none",
      "uses",
      "augment"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "output",
    "contentLower": "the identification output is a json string with the following collections of cis. the format is as shown in example of configuration item (ci) type input: cis description ambiguousentities cis for which the service found more than one matching ci in service management. example: an incoming device whose name + domainname combination exists twice in service management. note: in this collection the json format differs slightly, as it also includes the id for all the matching cis found in service management. example: the following json format shows one incoming device that matches four devices in service management: \"ambiguousentities\":[{ \"entity\": {\"properties\": {\"name\":\"na\", \"domainname\":\"dna\", \"displayname\":\"dispb\"}, \"entitytype\":\"device\", \"matchingentityids\":[\"13\",\"7\",\"14\",\"10\"] } ]} entitiestocreate cis the service has identified for which no matching ci was found in service management. entitiestoupdate cis the service has identified for which exactly one matching ci was found in serv",
    "keywordsLower": [
      "output",
      "merged",
      "cis",
      "component",
      "related",
      "topics",
      "identification",
      "json",
      "string",
      "following",
      "collections",
      "cis.",
      "format",
      "shown",
      "example",
      "configuration",
      "item",
      "ci",
      "type",
      "input",
      "description",
      "ambiguousentities",
      "service",
      "found",
      "one",
      "matching",
      "management.",
      "incoming",
      "device",
      "whose",
      "name",
      "domainname",
      "combination",
      "exists",
      "twice",
      "note",
      "collection",
      "differs",
      "slightly",
      "includes",
      "id",
      "all",
      "shows",
      "matches",
      "four",
      "devices",
      "management",
      "entity",
      "properties",
      "na",
      "dna",
      "displayname",
      "dispb",
      "entitytype",
      "matchingentityids",
      "13",
      "14",
      "10",
      "entitiestocreate",
      "identified",
      "entitiestoupdate",
      "exactly",
      "least",
      "property",
      "value",
      "different.",
      "returns",
      "form.",
      "see",
      "section.",
      "unchangedentities",
      "identical",
      "values.",
      "unidentifiableentities",
      "unable",
      "identify",
      "existing",
      "due",
      "insufficient",
      "information.",
      "creates",
      "collection.",
      "merges",
      "follows",
      "matched",
      "merge",
      "action",
      "takes",
      "ci.",
      "keep",
      "set",
      "true",
      "maintains",
      "regardless",
      "allows",
      "augmentation",
      "additional",
      "none",
      "uses",
      "augment"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform an integration with Operations Orchestration Containerized",
    "content": "Prerequisites Before you proceed, make sure that you have met the following prerequisites: Your suite admin has installed OO Containerized on the same OMT as Service Management. For details, see the relevant sections in Install. Your suite admin has deployed and enabled the Operations Orchestration capability in Suite Administration. For detailed steps, see Deploy OO Containerized in Suite Administration. You have installed the OPB Agent and added an agent. For details, see How to use On-Premises Bridge agents on Windows and How to use On-Premises Bridge agents on Linux. If the Operations Orchestration flow you are importing uses encryption, you have set up encryption between Service Management and the OPB agent before you perform the integration. For more information, see How to set up encryption for an Operations Orchestration integration. To set up the integration, the suite admin and tenant admin need to perform the following tasks. Tasks for the suite admin The suite admin needs t",
    "url": "integrateoocontainerizedsteps",
    "filename": "integrateoocontainerizedsteps",
    "headings": [
      "Prerequisites",
      "Tasks for the suite admin",
      "Create two integration users for the tenant in Suite Administration",
      "Configure OO permissions for the OO integration user in the IdM Admin Portal",
      "Assign OO permissions to users",
      "Tasks for the tenant admin",
      "Assign the required role to the Service Management integration user",
      "Configure tenant settings",
      "Import the OO CA certificate into the OPB agent",
      "Create an endpoint credentials record",
      "Configure an OO endpoint",
      "Reuse existing OO endpoint",
      "Verify the integration setup"
    ],
    "keywords": [
      "X.509",
      "https://myexamplehost:9443/oo/?tenantId=822946929",
      "Base-64",
      "https://<host>:<port>/oo/?tenantId=<tenant_ID",
      "oo.cer",
      "https://[host]:[port]/oo/?tenantId=xxxxxxxxxx",
      "perform",
      "integration",
      "operations",
      "orchestration",
      "containerized",
      "prerequisites",
      "tasks",
      "suite",
      "admin",
      "create",
      "two",
      "users",
      "tenant",
      "administration",
      "configure",
      "oo",
      "permissions",
      "user",
      "idm",
      "portal",
      "assign",
      "required",
      "role",
      "service",
      "management",
      "settings",
      "import",
      "ca",
      "certificate",
      "opb",
      "agent",
      "endpoint",
      "credentials",
      "record",
      "reuse",
      "existing",
      "verify",
      "setup",
      "before",
      "proceed",
      "make",
      "sure",
      "met",
      "following",
      "installed",
      "same",
      "omt",
      "management.",
      "details",
      "see",
      "relevant",
      "sections",
      "install.",
      "deployed",
      "enabled",
      "capability",
      "administration.",
      "detailed",
      "steps",
      "deploy",
      "added",
      "agent.",
      "on-premises",
      "bridge",
      "agents",
      "windows",
      "linux.",
      "flow",
      "importing",
      "uses",
      "encryption",
      "set",
      "between",
      "integration.",
      "information",
      "need",
      "tasks.",
      "needs",
      "integrate",
      "through",
      "one",
      "access",
      "oo.",
      "there",
      "already",
      "available",
      "log",
      "suite-admin",
      "https",
      "bo.",
      "click",
      "new.",
      "select",
      "account"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform an integration with operations orchestration containerized",
    "contentLower": "prerequisites before you proceed, make sure that you have met the following prerequisites: your suite admin has installed oo containerized on the same omt as service management. for details, see the relevant sections in install. your suite admin has deployed and enabled the operations orchestration capability in suite administration. for detailed steps, see deploy oo containerized in suite administration. you have installed the opb agent and added an agent. for details, see how to use on-premises bridge agents on windows and how to use on-premises bridge agents on linux. if the operations orchestration flow you are importing uses encryption, you have set up encryption between service management and the opb agent before you perform the integration. for more information, see how to set up encryption for an operations orchestration integration. to set up the integration, the suite admin and tenant admin need to perform the following tasks. tasks for the suite admin the suite admin needs t",
    "keywordsLower": [
      "x.509",
      "https://myexamplehost:9443/oo/?tenantid=822946929",
      "base-64",
      "https://<host>:<port>/oo/?tenantid=<tenant_id",
      "oo.cer",
      "https://[host]:[port]/oo/?tenantid=xxxxxxxxxx",
      "perform",
      "integration",
      "operations",
      "orchestration",
      "containerized",
      "prerequisites",
      "tasks",
      "suite",
      "admin",
      "create",
      "two",
      "users",
      "tenant",
      "administration",
      "configure",
      "oo",
      "permissions",
      "user",
      "idm",
      "portal",
      "assign",
      "required",
      "role",
      "service",
      "management",
      "settings",
      "import",
      "ca",
      "certificate",
      "opb",
      "agent",
      "endpoint",
      "credentials",
      "record",
      "reuse",
      "existing",
      "verify",
      "setup",
      "before",
      "proceed",
      "make",
      "sure",
      "met",
      "following",
      "installed",
      "same",
      "omt",
      "management.",
      "details",
      "see",
      "relevant",
      "sections",
      "install.",
      "deployed",
      "enabled",
      "capability",
      "administration.",
      "detailed",
      "steps",
      "deploy",
      "added",
      "agent.",
      "on-premises",
      "bridge",
      "agents",
      "windows",
      "linux.",
      "flow",
      "importing",
      "uses",
      "encryption",
      "set",
      "between",
      "integration.",
      "information",
      "need",
      "tasks.",
      "needs",
      "integrate",
      "through",
      "one",
      "access",
      "oo.",
      "there",
      "already",
      "available",
      "log",
      "suite-admin",
      "https",
      "bo.",
      "click",
      "new.",
      "select",
      "account"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform an integration with classic Operations Orchestration",
    "content": "To import Operations Orchestration (OO) content and run OO flows, you need to set up an integration with OO, and then run OO flows by running a business rule either at the record type/record type workflow level for a group of records that meet a specified condition or at the record level as a planned task. Prerequisites Before you proceed, complete the following prerequisite tasks: Install the On-Premises Bridge (OPB) agent and add an OPB agent. For details, see How to use On-Premises Bridge agents on Windows and How to use On-Premises Bridge agents on Linux. If the Operations Orchestration flow you are importing uses encryption, you must set up encryption between Service Management and the OPB agent before you perform the integration. For more information, see How to set up encryption for an Operations Orchestration integration. Import the OO certificate into the OPB agent When working with a security hardened Operations Orchestration (OO) server, perform the following steps to enable",
    "url": "integrateoosteps",
    "filename": "integrateoosteps",
    "headings": [
      "Prerequisites",
      "Import the OO certificate into the OPB agent",
      "Create an integration user in OO",
      "Create an endpoint credentials record",
      "Configure an OO endpoint",
      "Reuse existing OO endpoint",
      "Related topics"
    ],
    "keywords": [
      "mycompany.net",
      "server.cert",
      "https://myhost.mycompany.net:8444",
      "perform",
      "integration",
      "classic",
      "operations",
      "orchestration",
      "prerequisites",
      "import",
      "oo",
      "certificate",
      "opb",
      "agent",
      "create",
      "user",
      "endpoint",
      "credentials",
      "record",
      "configure",
      "reuse",
      "existing",
      "related",
      "topics",
      "content",
      "run",
      "flows",
      "need",
      "set",
      "running",
      "business",
      "rule",
      "either",
      "type",
      "workflow",
      "level",
      "group",
      "records",
      "meet",
      "specified",
      "condition",
      "planned",
      "task.",
      "before",
      "proceed",
      "complete",
      "following",
      "prerequisite",
      "tasks",
      "install",
      "on-premises",
      "bridge",
      "add",
      "agent.",
      "details",
      "see",
      "agents",
      "windows",
      "linux.",
      "flow",
      "importing",
      "uses",
      "encryption",
      "between",
      "service",
      "management",
      "integration.",
      "information",
      "working",
      "security",
      "hardened",
      "server",
      "steps",
      "enable",
      "execution",
      "locate",
      "installation",
      "folder",
      "default",
      "location",
      "program",
      "files",
      "micro",
      "focus",
      "hewlett-packard",
      "hp",
      "depending",
      "version.",
      "export",
      "central",
      "var",
      "key.store.",
      "get",
      "alias",
      "script",
      "java",
      "bin",
      "keytool",
      "-list",
      "-keystore"
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform an integration with classic operations orchestration",
    "contentLower": "to import operations orchestration (oo) content and run oo flows, you need to set up an integration with oo, and then run oo flows by running a business rule either at the record type/record type workflow level for a group of records that meet a specified condition or at the record level as a planned task. prerequisites before you proceed, complete the following prerequisite tasks: install the on-premises bridge (opb) agent and add an opb agent. for details, see how to use on-premises bridge agents on windows and how to use on-premises bridge agents on linux. if the operations orchestration flow you are importing uses encryption, you must set up encryption between service management and the opb agent before you perform the integration. for more information, see how to set up encryption for an operations orchestration integration. import the oo certificate into the opb agent when working with a security hardened operations orchestration (oo) server, perform the following steps to enable",
    "keywordsLower": [
      "mycompany.net",
      "server.cert",
      "https://myhost.mycompany.net:8444",
      "perform",
      "integration",
      "classic",
      "operations",
      "orchestration",
      "prerequisites",
      "import",
      "oo",
      "certificate",
      "opb",
      "agent",
      "create",
      "user",
      "endpoint",
      "credentials",
      "record",
      "configure",
      "reuse",
      "existing",
      "related",
      "topics",
      "content",
      "run",
      "flows",
      "need",
      "set",
      "running",
      "business",
      "rule",
      "either",
      "type",
      "workflow",
      "level",
      "group",
      "records",
      "meet",
      "specified",
      "condition",
      "planned",
      "task.",
      "before",
      "proceed",
      "complete",
      "following",
      "prerequisite",
      "tasks",
      "install",
      "on-premises",
      "bridge",
      "add",
      "agent.",
      "details",
      "see",
      "agents",
      "windows",
      "linux.",
      "flow",
      "importing",
      "uses",
      "encryption",
      "between",
      "service",
      "management",
      "integration.",
      "information",
      "working",
      "security",
      "hardened",
      "server",
      "steps",
      "enable",
      "execution",
      "locate",
      "installation",
      "folder",
      "default",
      "location",
      "program",
      "files",
      "micro",
      "focus",
      "hewlett-packard",
      "hp",
      "depending",
      "version.",
      "export",
      "central",
      "var",
      "key.store.",
      "get",
      "alias",
      "script",
      "java",
      "bin",
      "keytool",
      "-list",
      "-keystore"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Operations Orchestration integration use case",
    "content": "You can implement an Operations Orchestration flow for resetting a password. At a user's request for a reset, a new temporary random password is generated. The user is then sent a link so they can log in and change to a permanent password. To implement an Operations Orchestration password reset flow, follow these steps. Prerequisites You have set up an integration with Operations Orchestration. Create an offering Create a new support offering. From the main menu, select Plan > Service Catalog > Offerings. Click New. In the New Offering dialog box: Set Offering type to Support offering. Set Status to Active. Complete the remaining fields in the dialog box as appropriate for your business logic. Click Save & edit. For more information, see How to create an offering. Create the user options for the offering. In the offering record, click the User options tab. Click New field and type the following values: Field Value Name Account name Note The system automatically adds “_c” to the end of ",
    "url": "integrateoousecase",
    "filename": "integrateoousecase",
    "headings": [
      "Prerequisites",
      "Create an offering",
      "Apply the offering to a request",
      "Related topics"
    ],
    "keywords": [
      "mircofocus.com",
      "server.com",
      "55.55",
      "55.55.55",
      "55.255",
      "55.55.55.255",
      "operations",
      "orchestration",
      "integration",
      "case",
      "prerequisites",
      "create",
      "offering",
      "apply",
      "request",
      "related",
      "topics",
      "implement",
      "flow",
      "resetting",
      "password.",
      "user",
      "reset",
      "new",
      "temporary",
      "random",
      "password",
      "generated.",
      "sent",
      "link",
      "log",
      "change",
      "permanent",
      "follow",
      "steps.",
      "set",
      "orchestration.",
      "support",
      "offering.",
      "main",
      "menu",
      "select",
      "plan",
      "service",
      "catalog",
      "offerings.",
      "click",
      "new.",
      "dialog",
      "box",
      "type",
      "status",
      "active.",
      "complete",
      "remaining",
      "fields",
      "appropriate",
      "business",
      "logic.",
      "save",
      "edit.",
      "information",
      "see",
      "options",
      "record",
      "tab.",
      "field",
      "following",
      "values",
      "value",
      "name",
      "account",
      "note",
      "system",
      "automatically",
      "adds",
      "end",
      "name.",
      "display",
      "string",
      "visibility",
      "save.",
      "task",
      "first",
      "line",
      "tab",
      "form.",
      "add",
      "automated",
      "based",
      "execute",
      "oo",
      "rule",
      "operation",
      "ok.",
      "endpoint",
      "parameter",
      "endpoint.",
      "defined",
      "beginning"
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "operations orchestration integration use case",
    "contentLower": "you can implement an operations orchestration flow for resetting a password. at a user's request for a reset, a new temporary random password is generated. the user is then sent a link so they can log in and change to a permanent password. to implement an operations orchestration password reset flow, follow these steps. prerequisites you have set up an integration with operations orchestration. create an offering create a new support offering. from the main menu, select plan > service catalog > offerings. click new. in the new offering dialog box: set offering type to support offering. set status to active. complete the remaining fields in the dialog box as appropriate for your business logic. click save & edit. for more information, see how to create an offering. create the user options for the offering. in the offering record, click the user options tab. click new field and type the following values: field value name account name note the system automatically adds “_c” to the end of ",
    "keywordsLower": [
      "mircofocus.com",
      "server.com",
      "55.55",
      "55.55.55",
      "55.255",
      "55.55.55.255",
      "operations",
      "orchestration",
      "integration",
      "case",
      "prerequisites",
      "create",
      "offering",
      "apply",
      "request",
      "related",
      "topics",
      "implement",
      "flow",
      "resetting",
      "password.",
      "user",
      "reset",
      "new",
      "temporary",
      "random",
      "password",
      "generated.",
      "sent",
      "link",
      "log",
      "change",
      "permanent",
      "follow",
      "steps.",
      "set",
      "orchestration.",
      "support",
      "offering.",
      "main",
      "menu",
      "select",
      "plan",
      "service",
      "catalog",
      "offerings.",
      "click",
      "new.",
      "dialog",
      "box",
      "type",
      "status",
      "active.",
      "complete",
      "remaining",
      "fields",
      "appropriate",
      "business",
      "logic.",
      "save",
      "edit.",
      "information",
      "see",
      "options",
      "record",
      "tab.",
      "field",
      "following",
      "values",
      "value",
      "name",
      "account",
      "note",
      "system",
      "automatically",
      "adds",
      "end",
      "name.",
      "display",
      "string",
      "visibility",
      "save.",
      "task",
      "first",
      "line",
      "tab",
      "form.",
      "add",
      "automated",
      "based",
      "execute",
      "oo",
      "rule",
      "operation",
      "ok.",
      "endpoint",
      "parameter",
      "endpoint.",
      "defined",
      "beginning"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Microsoft Teams integration network requirements",
    "content": "The Microsoft Teams (\"MS Teams\") integration requires bidirectional communications between Service Management and MS Teams. Depending on your environment, you can achieve this connectivity either without modifications or with certain enhancements to your network architecture. The following describes two typical Service Management networking scenarios and their requirements for MS Teams integration. In both scenarios, the integration uses the following URLs built on one of the FQDNs. URL Built on Note URL registered on Azure for authentication callback The custom domain configured for the tenant in Suite Administration (first priority) The external access host of Service Management (second priority) You configure this URL on the Azure portal. See Register the Service Management app on the Azure portal. Azure consent URL The custom domain configured for the tenant in Suite Administration (first priority) The external access host of Service Management (second priority) The system generate",
    "url": "msteamsintegrationnetworkrequirements",
    "filename": "msteamsintegrationnetworkrequirements",
    "headings": [
      "Scenario 1: Public Service Management",
      "Scenario 2: Private Service Management"
    ],
    "keywords": [
      "https://<external-access-host>|public-fqdn=https://<integration-external-FQDN",
      "https://<integration-external-FQDN>/xie/public/v1",
      "1.0",
      "microsoft.com",
      "microsoftonline.com",
      "https://<integration-external-FQDN>/xie",
      "microsoft",
      "teams",
      "integration",
      "network",
      "requirements",
      "scenario",
      "public",
      "service",
      "management",
      "private",
      "ms",
      "requires",
      "bidirectional",
      "communications",
      "between",
      "teams.",
      "depending",
      "environment",
      "achieve",
      "connectivity",
      "either",
      "modifications",
      "certain",
      "enhancements",
      "architecture.",
      "following",
      "describes",
      "two",
      "typical",
      "networking",
      "scenarios",
      "integration.",
      "both",
      "uses",
      "urls",
      "built",
      "one",
      "fqdns.",
      "url",
      "note",
      "registered",
      "azure",
      "authentication",
      "callback",
      "custom",
      "domain",
      "configured",
      "tenant",
      "suite",
      "administration",
      "first",
      "priority",
      "external",
      "access",
      "host",
      "second",
      "configure",
      "portal.",
      "see",
      "register",
      "app",
      "consent",
      "system",
      "generates",
      "displays",
      "section",
      "configuration",
      "ui.",
      "connect",
      "subscription",
      "fqdn",
      "defined",
      "itom-sma-xie-configmap",
      "via",
      "public-fqdn",
      "parameter.",
      "default",
      "value",
      "parameter",
      "management.",
      "value.",
      "need",
      "prepare",
      "described",
      "below",
      "reachable",
      "internet.",
      "here",
      "environments",
      "deployed",
      "opentext",
      "saas",
      "cloud",
      "open"
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "microsoft teams integration network requirements",
    "contentLower": "the microsoft teams (\"ms teams\") integration requires bidirectional communications between service management and ms teams. depending on your environment, you can achieve this connectivity either without modifications or with certain enhancements to your network architecture. the following describes two typical service management networking scenarios and their requirements for ms teams integration. in both scenarios, the integration uses the following urls built on one of the fqdns. url built on note url registered on azure for authentication callback the custom domain configured for the tenant in suite administration (first priority) the external access host of service management (second priority) you configure this url on the azure portal. see register the service management app on the azure portal. azure consent url the custom domain configured for the tenant in suite administration (first priority) the external access host of service management (second priority) the system generate",
    "keywordsLower": [
      "https://<external-access-host>|public-fqdn=https://<integration-external-fqdn",
      "https://<integration-external-fqdn>/xie/public/v1",
      "1.0",
      "microsoft.com",
      "microsoftonline.com",
      "https://<integration-external-fqdn>/xie",
      "microsoft",
      "teams",
      "integration",
      "network",
      "requirements",
      "scenario",
      "public",
      "service",
      "management",
      "private",
      "ms",
      "requires",
      "bidirectional",
      "communications",
      "between",
      "teams.",
      "depending",
      "environment",
      "achieve",
      "connectivity",
      "either",
      "modifications",
      "certain",
      "enhancements",
      "architecture.",
      "following",
      "describes",
      "two",
      "typical",
      "networking",
      "scenarios",
      "integration.",
      "both",
      "uses",
      "urls",
      "built",
      "one",
      "fqdns.",
      "url",
      "note",
      "registered",
      "azure",
      "authentication",
      "callback",
      "custom",
      "domain",
      "configured",
      "tenant",
      "suite",
      "administration",
      "first",
      "priority",
      "external",
      "access",
      "host",
      "second",
      "configure",
      "portal.",
      "see",
      "register",
      "app",
      "consent",
      "system",
      "generates",
      "displays",
      "section",
      "configuration",
      "ui.",
      "connect",
      "subscription",
      "fqdn",
      "defined",
      "itom-sma-xie-configmap",
      "via",
      "public-fqdn",
      "parameter.",
      "default",
      "value",
      "parameter",
      "management.",
      "value.",
      "need",
      "prepare",
      "described",
      "below",
      "reachable",
      "internet.",
      "here",
      "environments",
      "deployed",
      "opentext",
      "saas",
      "cloud",
      "open"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage IDOL knowledge indexing",
    "content": "This page describes the general setup, administration, and best practices for IDOL knowledge indexing. For how to set up IDOL indexing for a particular type of knowledge source, see the page for the corresponding IDOL connector listed in the Related topics section. Recommended hardware specifications for IDOL connectors OpenText recommends the following minimum hardware specifications for servers running IDOL components. a dedicated SCSI disk 4 GB RAM 100 GB Disk a minimum of 2 dedicated CPU - Intel Xeon or AMD Opteron or above For details, see IDOL connector system requirements. Setting up IDOL knowledge indexing The following describes the high-level setup procedure for the IDOL indexing solution: Set up CFS. Set up IDOL connectors. Set up the required components in Service Management (an OPB Agent and a Knowledge Indexing endpoint). Setting up the shared components The sections below describe the knowledge indexing setup procedure for the shared components (CFS, IDOL, the OPB Agent,",
    "url": "idolindexingsetup",
    "filename": "idolindexingsetup",
    "headings": [
      "Recommended hardware specifications for IDOL connectors",
      "Setting up IDOL knowledge indexing",
      "Setting up the shared components",
      "Set up CFS",
      "Install the OPB Agent and configure an agent",
      "Create and configure an endpoint",
      "Administering IDOL knowledge indexing",
      "Trigger IDOL knowledge indexing",
      "Validate your connector configuration",
      "Verify IDOL knowledge indexing",
      "Make configuration changes take effect",
      "Re-index knowledge articles",
      "Control access to external knowledge for portal users",
      "Upgrade IDOL connectors",
      "IDOL knowledge indexing best practices",
      "Related topics"
    ],
    "keywords": [
      "deleteDoc.lua",
      "webconnector.cfg",
      "_datastore.db",
      "versionkey.dat",
      "util.lua",
      "synchorize.log",
      "synchronize.log",
      "addDoc.lua",
      "config.ini",
      "licensekey.dat",
      "https://<External",
      "manage",
      "idol",
      "knowledge",
      "indexing",
      "recommended",
      "hardware",
      "specifications",
      "connectors",
      "setting",
      "shared",
      "components",
      "set",
      "cfs",
      "install",
      "opb",
      "agent",
      "configure",
      "create",
      "endpoint",
      "administering",
      "trigger",
      "validate",
      "connector",
      "configuration",
      "verify",
      "make",
      "changes",
      "take",
      "effect",
      "re-index",
      "articles",
      "control",
      "access",
      "external",
      "portal",
      "users",
      "upgrade",
      "best",
      "practices",
      "related",
      "topics",
      "page",
      "describes",
      "general",
      "setup",
      "administration",
      "indexing.",
      "particular",
      "type",
      "source",
      "see",
      "corresponding",
      "listed",
      "section.",
      "opentext",
      "recommends",
      "following",
      "minimum",
      "servers",
      "running",
      "components.",
      "dedicated",
      "scsi",
      "disk",
      "gb",
      "ram",
      "100",
      "cpu",
      "intel",
      "xeon",
      "amd",
      "opteron",
      "above",
      "details",
      "system",
      "requirements.",
      "high-level",
      "procedure",
      "solution",
      "cfs.",
      "connectors.",
      "required",
      "service",
      "management",
      "sections",
      "below",
      "describe",
      "complete",
      "first"
    ],
    "language": "en",
    "word_count": 112,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage idol knowledge indexing",
    "contentLower": "this page describes the general setup, administration, and best practices for idol knowledge indexing. for how to set up idol indexing for a particular type of knowledge source, see the page for the corresponding idol connector listed in the related topics section. recommended hardware specifications for idol connectors opentext recommends the following minimum hardware specifications for servers running idol components. a dedicated scsi disk 4 gb ram 100 gb disk a minimum of 2 dedicated cpu - intel xeon or amd opteron or above for details, see idol connector system requirements. setting up idol knowledge indexing the following describes the high-level setup procedure for the idol indexing solution: set up cfs. set up idol connectors. set up the required components in service management (an opb agent and a knowledge indexing endpoint). setting up the shared components the sections below describe the knowledge indexing setup procedure for the shared components (cfs, idol, the opb agent,",
    "keywordsLower": [
      "deletedoc.lua",
      "webconnector.cfg",
      "_datastore.db",
      "versionkey.dat",
      "util.lua",
      "synchorize.log",
      "synchronize.log",
      "adddoc.lua",
      "config.ini",
      "licensekey.dat",
      "https://<external",
      "manage",
      "idol",
      "knowledge",
      "indexing",
      "recommended",
      "hardware",
      "specifications",
      "connectors",
      "setting",
      "shared",
      "components",
      "set",
      "cfs",
      "install",
      "opb",
      "agent",
      "configure",
      "create",
      "endpoint",
      "administering",
      "trigger",
      "validate",
      "connector",
      "configuration",
      "verify",
      "make",
      "changes",
      "take",
      "effect",
      "re-index",
      "articles",
      "control",
      "access",
      "external",
      "portal",
      "users",
      "upgrade",
      "best",
      "practices",
      "related",
      "topics",
      "page",
      "describes",
      "general",
      "setup",
      "administration",
      "indexing.",
      "particular",
      "type",
      "source",
      "see",
      "corresponding",
      "listed",
      "section.",
      "opentext",
      "recommends",
      "following",
      "minimum",
      "servers",
      "running",
      "components.",
      "dedicated",
      "scsi",
      "disk",
      "gb",
      "ram",
      "100",
      "cpu",
      "intel",
      "xeon",
      "amd",
      "opteron",
      "above",
      "details",
      "system",
      "requirements.",
      "high-level",
      "procedure",
      "solution",
      "cfs.",
      "connectors.",
      "required",
      "service",
      "management",
      "sections",
      "below",
      "describe",
      "complete",
      "first"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OPB based email integration",
    "content": "The email integration enables users to request appropriate knowledge articles, create requests, add comments, and accept solutions to existing requests without logging in, using specially prepared email messages to do so. This integration uses the On-Premises Bridge (OPB) agent to connect to an external mailbox to process inbound emails. To enable the email integration, you must install an OPB agent and configure an email integration endpoint for the agent to use. Additionally, you can also set up notification templates used when the system sends automatic replies to inbound emails. These emails contain an encrypted authentication string that's appended to the body of the email, and contains all the internal data that's required for Service Management to uniquely identify the user and required action. These authentication strings are valid only for a limited period and can be used only once. The end user mustn't change the authentication string. Changing the authentication string might",
    "url": "emailintegration",
    "filename": "emailintegration",
    "headings": [
      "Use cases",
      "Use case 1: Send knowledge and offering suggestions to inbound email",
      "Use case 2: Automatically create requests based on inbound email",
      "Set up the integration",
      "General security guidelines",
      "Prerequisites",
      "Configure the external mailbox account",
      "Generate an App Password in your Google account",
      "Prepare an integration user",
      "Install the OPB agent",
      "Import the suite CA certificate to the OPB trusted keystore",
      "Add the OPB agent",
      "Add external mailbox credentials to the OPB agent",
      "Set up an integration endpoint",
      "Enable request creation from email in the default offering",
      "Configure email integration settings",
      "Add eml extension to the tenant attachment allowlist",
      "(Optional) Set up notification templates for email integration",
      "Generate a reply email",
      "Link to knowledge articles"
    ],
    "keywords": [
      "integration.On",
      "browser.Copy",
      "New.From",
      "email.body",
      "Management.This",
      "connection.conf",
      "OFFERINGS.Open",
      "RequestedForPerson.Id",
      "tab.In",
      "gmail.com",
      "outlook.com",
      "button.Save",
      "folder.From",
      "encryption.Set",
      "configuration.Send",
      "option.Save",
      "admin.Go",
      "entity.Id",
      "entity.Type",
      "record.If",
      "https://<EXTERNAL_ACCESS_HOST>/bo.Click",
      "account.You",
      "opb",
      "based",
      "email",
      "integration",
      "cases",
      "case",
      "send",
      "knowledge",
      "offering",
      "suggestions",
      "inbound",
      "automatically",
      "create",
      "requests",
      "set",
      "general",
      "security",
      "guidelines",
      "prerequisites",
      "configure",
      "external",
      "mailbox",
      "account",
      "generate",
      "app",
      "password",
      "google",
      "prepare",
      "user",
      "install",
      "agent",
      "import",
      "suite",
      "ca",
      "certificate",
      "trusted",
      "keystore",
      "add",
      "credentials",
      "endpoint",
      "enable",
      "request",
      "creation",
      "default",
      "settings",
      "eml",
      "extension",
      "tenant",
      "attachment",
      "allowlist",
      "optional",
      "notification",
      "templates",
      "reply",
      "link",
      "articles",
      "offerings",
      "verify",
      "setup",
      "limitations",
      "troubleshooting",
      "enables",
      "users",
      "appropriate",
      "comments",
      "accept",
      "solutions",
      "existing",
      "logging",
      "specially",
      "prepared",
      "messages",
      "so.",
      "uses",
      "on-premises",
      "bridge",
      "connect",
      "process"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "opb based email integration",
    "contentLower": "the email integration enables users to request appropriate knowledge articles, create requests, add comments, and accept solutions to existing requests without logging in, using specially prepared email messages to do so. this integration uses the on-premises bridge (opb) agent to connect to an external mailbox to process inbound emails. to enable the email integration, you must install an opb agent and configure an email integration endpoint for the agent to use. additionally, you can also set up notification templates used when the system sends automatic replies to inbound emails. these emails contain an encrypted authentication string that's appended to the body of the email, and contains all the internal data that's required for service management to uniquely identify the user and required action. these authentication strings are valid only for a limited period and can be used only once. the end user mustn't change the authentication string. changing the authentication string might",
    "keywordsLower": [
      "integration.on",
      "browser.copy",
      "new.from",
      "email.body",
      "management.this",
      "connection.conf",
      "offerings.open",
      "requestedforperson.id",
      "tab.in",
      "gmail.com",
      "outlook.com",
      "button.save",
      "folder.from",
      "encryption.set",
      "configuration.send",
      "option.save",
      "admin.go",
      "entity.id",
      "entity.type",
      "record.if",
      "https://<external_access_host>/bo.click",
      "account.you",
      "opb",
      "based",
      "email",
      "integration",
      "cases",
      "case",
      "send",
      "knowledge",
      "offering",
      "suggestions",
      "inbound",
      "automatically",
      "create",
      "requests",
      "set",
      "general",
      "security",
      "guidelines",
      "prerequisites",
      "configure",
      "external",
      "mailbox",
      "account",
      "generate",
      "app",
      "password",
      "google",
      "prepare",
      "user",
      "install",
      "agent",
      "import",
      "suite",
      "ca",
      "certificate",
      "trusted",
      "keystore",
      "add",
      "credentials",
      "endpoint",
      "enable",
      "request",
      "creation",
      "default",
      "settings",
      "eml",
      "extension",
      "tenant",
      "attachment",
      "allowlist",
      "optional",
      "notification",
      "templates",
      "reply",
      "link",
      "articles",
      "offerings",
      "verify",
      "setup",
      "limitations",
      "troubleshooting",
      "enables",
      "users",
      "appropriate",
      "comments",
      "accept",
      "solutions",
      "existing",
      "logging",
      "specially",
      "prepared",
      "messages",
      "so.",
      "uses",
      "on-premises",
      "bridge",
      "connect",
      "process"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OPB based Gmail Integration",
    "content": "Introduction Service Management provides an email integration that may be used to request appropriate knowledge articles, create requests, add comments, and accept solutions to existing requests. The email integration uses the On-Premises Bridge to connect to an external inbox to process incoming emails. This document describes how to configure Gmail as the external inbox to use with the Service Management email integration. This chapter will include: Configuring the Gmail accountDetermining where to install the On-Premises Bridge (OPB)Downloading the OPB configuration fileDownloading the OPB installation binaryInstalling the OPBAdding the Gmail addresses email integration credentials to the OPBCreating and configuring the email integration endpoint on Service ManagementTroubleshooting issues Configuring the Gmail account The Service Management email integration uses IMAP to communicate with the remote email server. By default, Gmail disables the ability to connect using IMAP, so IMAP ",
    "url": "wow_gmail",
    "filename": "wow_gmail",
    "headings": [
      "Introduction",
      "Configuring the Gmail account",
      "Determining where to install the OPB",
      "Downloading the OPB configuration file",
      "Downloading the OPB installation binary",
      "Installing the OPB",
      "Adding the email integration credentials to the OPB",
      "Creating and configuring the email integration endpoint",
      "Troubleshooting"
    ],
    "keywords": [
      "domain.log",
      "installation.exe",
      "account.The",
      "connection.conf",
      "controller.log",
      "gmail.com",
      "enabled.The",
      "opb",
      "based",
      "gmail",
      "integration",
      "introduction",
      "configuring",
      "account",
      "determining",
      "install",
      "downloading",
      "configuration",
      "file",
      "installation",
      "binary",
      "installing",
      "adding",
      "email",
      "credentials",
      "creating",
      "endpoint",
      "troubleshooting",
      "service",
      "management",
      "provides",
      "request",
      "appropriate",
      "knowledge",
      "articles",
      "create",
      "requests",
      "add",
      "comments",
      "accept",
      "solutions",
      "existing",
      "requests.",
      "uses",
      "on-premises",
      "bridge",
      "connect",
      "external",
      "inbox",
      "process",
      "incoming",
      "emails.",
      "document",
      "describes",
      "configure",
      "integration.",
      "chapter",
      "include",
      "accountdetermining",
      "filedownloading",
      "binaryinstalling",
      "opbadding",
      "addresses",
      "opbcreating",
      "managementtroubleshooting",
      "issues",
      "imap",
      "communicate",
      "remote",
      "server.",
      "default",
      "disables",
      "ability",
      "enabled",
      "settings.",
      "enable",
      "access",
      "settings",
      "select",
      "forwarding",
      "pop",
      "menu.",
      "button",
      "save",
      "protocol",
      "allows",
      "move",
      "emails",
      "folders",
      "once",
      "processed.",
      "successfully",
      "processed",
      "moved",
      "folder",
      "named",
      "while",
      "issue",
      "errors.",
      "created"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "opb based gmail integration",
    "contentLower": "introduction service management provides an email integration that may be used to request appropriate knowledge articles, create requests, add comments, and accept solutions to existing requests. the email integration uses the on-premises bridge to connect to an external inbox to process incoming emails. this document describes how to configure gmail as the external inbox to use with the service management email integration. this chapter will include: configuring the gmail accountdetermining where to install the on-premises bridge (opb)downloading the opb configuration filedownloading the opb installation binaryinstalling the opbadding the gmail addresses email integration credentials to the opbcreating and configuring the email integration endpoint on service managementtroubleshooting issues configuring the gmail account the service management email integration uses imap to communicate with the remote email server. by default, gmail disables the ability to connect using imap, so imap ",
    "keywordsLower": [
      "domain.log",
      "installation.exe",
      "account.the",
      "connection.conf",
      "controller.log",
      "gmail.com",
      "enabled.the",
      "opb",
      "based",
      "gmail",
      "integration",
      "introduction",
      "configuring",
      "account",
      "determining",
      "install",
      "downloading",
      "configuration",
      "file",
      "installation",
      "binary",
      "installing",
      "adding",
      "email",
      "credentials",
      "creating",
      "endpoint",
      "troubleshooting",
      "service",
      "management",
      "provides",
      "request",
      "appropriate",
      "knowledge",
      "articles",
      "create",
      "requests",
      "add",
      "comments",
      "accept",
      "solutions",
      "existing",
      "requests.",
      "uses",
      "on-premises",
      "bridge",
      "connect",
      "external",
      "inbox",
      "process",
      "incoming",
      "emails.",
      "document",
      "describes",
      "configure",
      "integration.",
      "chapter",
      "include",
      "accountdetermining",
      "filedownloading",
      "binaryinstalling",
      "opbadding",
      "addresses",
      "opbcreating",
      "managementtroubleshooting",
      "issues",
      "imap",
      "communicate",
      "remote",
      "server.",
      "default",
      "disables",
      "ability",
      "enabled",
      "settings.",
      "enable",
      "access",
      "settings",
      "select",
      "forwarding",
      "pop",
      "menu.",
      "button",
      "save",
      "protocol",
      "allows",
      "move",
      "emails",
      "folders",
      "once",
      "processed.",
      "successfully",
      "processed",
      "moved",
      "folder",
      "named",
      "while",
      "issue",
      "errors.",
      "created"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OPB based email integration using EWS",
    "content": "This topic describes how to set up the email integration with the Exchange Online based on Exchange Web Services (EWS). This topic documents only Exchange Online-specific configurations. For common content for all email integrations , such as the use case, and common Service Management configurations, see Email Integration. The EWS integration supports both OAuth 2.0 ROPC and OAuth 2.0 client credentials authentication methods, but the OAuth 2.0 ROPC authentication is preferred because the EWS integration with the OAuth 2.0 client credentials authentication currently has some performance issues. Set up the Email Integration app in Azure For the integration to work, you must register an app in Azure, and configure a client secret and permissions for the app. During the setup process, take note of the following information, which you will use when configuring Service Management: Application (client) IDDirectory (tenant) IDClient secret Prerequisites Before you proceed, make sure that you",
    "url": "emailintegrationews",
    "filename": "emailintegrationews",
    "headings": [
      "Set up the Email Integration app in Azure",
      "Prerequisites",
      "Step 1. Register an application for the integration on the Azure portal",
      "Step 2. Configure a client secret for the application",
      "Step 3. Configure API permissions for the application",
      "Create an integration user",
      "Install the OPB agent",
      "Import the suite CA certificate to the OPB trusted keystore",
      "Add the OPB agent",
      "Add email integration credentials to the OPB agent",
      "Windows",
      "Linux",
      "Set up an integration endpoint",
      "Complete other Service Management configurations"
    ],
    "keywords": [
      "ReadWrite.All",
      "admin.Go",
      "New.From",
      "Exchange.asmx",
      "Mail.Send",
      "credentials_mng_console.sh",
      "emailIntegration.ews",
      "Mail.Read",
      "2.0",
      "AccessAsUser.All",
      "Management.This",
      "opb",
      "based",
      "email",
      "integration",
      "ews",
      "set",
      "app",
      "azure",
      "prerequisites",
      "step",
      "1.",
      "register",
      "application",
      "portal",
      "2.",
      "configure",
      "client",
      "secret",
      "3.",
      "api",
      "permissions",
      "create",
      "user",
      "install",
      "agent",
      "import",
      "suite",
      "ca",
      "certificate",
      "trusted",
      "keystore",
      "add",
      "credentials",
      "windows",
      "linux",
      "endpoint",
      "complete",
      "service",
      "management",
      "configurations",
      "topic",
      "describes",
      "exchange",
      "online",
      "web",
      "services",
      "documents",
      "online-specific",
      "configurations.",
      "common",
      "content",
      "all",
      "integrations",
      "such",
      "case",
      "see",
      "integration.",
      "supports",
      "both",
      "oauth",
      "ropc",
      "authentication",
      "methods",
      "preferred",
      "because",
      "currently",
      "performance",
      "issues.",
      "work",
      "app.",
      "during",
      "setup",
      "process",
      "take",
      "note",
      "following",
      "information",
      "configuring",
      "iddirectory",
      "tenant",
      "idclient",
      "before",
      "proceed",
      "make",
      "sure",
      "met",
      "admin",
      "rightsan",
      "existing"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "opb based email integration using ews",
    "contentLower": "this topic describes how to set up the email integration with the exchange online based on exchange web services (ews). this topic documents only exchange online-specific configurations. for common content for all email integrations , such as the use case, and common service management configurations, see email integration. the ews integration supports both oauth 2.0 ropc and oauth 2.0 client credentials authentication methods, but the oauth 2.0 ropc authentication is preferred because the ews integration with the oauth 2.0 client credentials authentication currently has some performance issues. set up the email integration app in azure for the integration to work, you must register an app in azure, and configure a client secret and permissions for the app. during the setup process, take note of the following information, which you will use when configuring service management: application (client) iddirectory (tenant) idclient secret prerequisites before you proceed, make sure that you",
    "keywordsLower": [
      "readwrite.all",
      "admin.go",
      "new.from",
      "exchange.asmx",
      "mail.send",
      "credentials_mng_console.sh",
      "emailintegration.ews",
      "mail.read",
      "2.0",
      "accessasuser.all",
      "management.this",
      "opb",
      "based",
      "email",
      "integration",
      "ews",
      "set",
      "app",
      "azure",
      "prerequisites",
      "step",
      "1.",
      "register",
      "application",
      "portal",
      "2.",
      "configure",
      "client",
      "secret",
      "3.",
      "api",
      "permissions",
      "create",
      "user",
      "install",
      "agent",
      "import",
      "suite",
      "ca",
      "certificate",
      "trusted",
      "keystore",
      "add",
      "credentials",
      "windows",
      "linux",
      "endpoint",
      "complete",
      "service",
      "management",
      "configurations",
      "topic",
      "describes",
      "exchange",
      "online",
      "web",
      "services",
      "documents",
      "online-specific",
      "configurations.",
      "common",
      "content",
      "all",
      "integrations",
      "such",
      "case",
      "see",
      "integration.",
      "supports",
      "both",
      "oauth",
      "ropc",
      "authentication",
      "methods",
      "preferred",
      "because",
      "currently",
      "performance",
      "issues.",
      "work",
      "app.",
      "during",
      "setup",
      "process",
      "take",
      "note",
      "following",
      "information",
      "configuring",
      "iddirectory",
      "tenant",
      "idclient",
      "before",
      "proceed",
      "make",
      "sure",
      "met",
      "admin",
      "rightsan",
      "existing"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform ongoing system maintenance",
    "content": "This section describes system level ongoing maintenance tasks that you can perform in the SMA suite system. These tasks include but aren't limited to the following: Manage persistent storage Enable a firewall Balance the cluster resource usage Change FQDN Replace the suite certificate Restart the system Back up and restore Manage passwords",
    "url": "administersmax",
    "filename": "administersmax",
    "headings": [],
    "keywords": [
      "perform",
      "ongoing",
      "system",
      "maintenance",
      "section",
      "describes",
      "level",
      "tasks",
      "sma",
      "suite",
      "system.",
      "include",
      "aren",
      "limited",
      "following",
      "manage",
      "persistent",
      "storage",
      "enable",
      "firewall",
      "balance",
      "cluster",
      "resource",
      "usage",
      "change",
      "fqdn",
      "replace",
      "certificate",
      "restart",
      "back",
      "restore",
      "passwords"
    ],
    "language": "en",
    "word_count": 41,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform ongoing system maintenance",
    "contentLower": "this section describes system level ongoing maintenance tasks that you can perform in the sma suite system. these tasks include but aren't limited to the following: manage persistent storage enable a firewall balance the cluster resource usage change fqdn replace the suite certificate restart the system back up and restore manage passwords",
    "keywordsLower": [
      "perform",
      "ongoing",
      "system",
      "maintenance",
      "section",
      "describes",
      "level",
      "tasks",
      "sma",
      "suite",
      "system.",
      "include",
      "aren",
      "limited",
      "following",
      "manage",
      "persistent",
      "storage",
      "enable",
      "firewall",
      "balance",
      "cluster",
      "resource",
      "usage",
      "change",
      "fqdn",
      "replace",
      "certificate",
      "restart",
      "back",
      "restore",
      "passwords"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage certificates",
    "content": "This document gives an overview of the certificates required for secure communications between the various components of the suite. It also describes how to check the validity and how to renew an expired certificate. Certificate configuration architectures You can choose any of the following architectures for certificate configuration. Direct access without an external load balancer This configuration enables direct access to the suite without an external load balancer configured. The certificate configured between the user and the suite can be one of the following types. CA certificate Self-signed certificate Access through an external load balancer This configuration enables access through external load balance using the following certificate configuration: The certificate configured between the user and load balancer is a CA certificate. The certificate configured between the load balancer and suite components can be one of the following types. CA certificate - Recommended Self-sign",
    "url": "suitecertificates",
    "filename": "suitecertificates",
    "headings": [
      "Certificate configuration architectures",
      "Direct access without an external load balancer",
      "Access through an external load balancer",
      "How to check the validity and renew certificates",
      "Troubleshooting certificate issues",
      "Documentation references"
    ],
    "keywords": [
      "boostport.com",
      "data.tls",
      "server.crt",
      "status.sh",
      "values.yaml",
      "pg_ca.crt",
      "manage",
      "certificates",
      "certificate",
      "configuration",
      "architectures",
      "direct",
      "access",
      "external",
      "load",
      "balancer",
      "through",
      "check",
      "validity",
      "renew",
      "troubleshooting",
      "issues",
      "documentation",
      "references",
      "document",
      "gives",
      "overview",
      "required",
      "secure",
      "communications",
      "between",
      "various",
      "components",
      "suite.",
      "describes",
      "expired",
      "certificate.",
      "choose",
      "any",
      "following",
      "configuration.",
      "enables",
      "suite",
      "configured.",
      "configured",
      "user",
      "one",
      "types.",
      "ca",
      "self-signed",
      "balance",
      "recommended",
      "diagram",
      "illustrates",
      "setup",
      "table",
      "below",
      "instructions",
      "shown",
      "setup.",
      "service",
      "management",
      "refers",
      "opentext",
      "application.",
      "number",
      "interface",
      "command",
      "link",
      "renewal",
      "user-",
      "integrations-",
      "balacer",
      "line",
      "openssl",
      "-showcerts",
      "-connect",
      "443",
      "x509",
      "-noout",
      "-dates",
      "grep",
      "notafter",
      "refer",
      "instructions.",
      "ingress",
      "signed",
      "custom",
      "mf",
      "re",
      "port",
      "kubectl",
      "get",
      "secret",
      "nginx-default-secret",
      "-n",
      "ns",
      "itsma",
      "cut",
      "-f1"
    ],
    "language": "en",
    "word_count": 91,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage certificates",
    "contentLower": "this document gives an overview of the certificates required for secure communications between the various components of the suite. it also describes how to check the validity and how to renew an expired certificate. certificate configuration architectures you can choose any of the following architectures for certificate configuration. direct access without an external load balancer this configuration enables direct access to the suite without an external load balancer configured. the certificate configured between the user and the suite can be one of the following types. ca certificate self-signed certificate access through an external load balancer this configuration enables access through external load balance using the following certificate configuration: the certificate configured between the user and load balancer is a ca certificate. the certificate configured between the load balancer and suite components can be one of the following types. ca certificate - recommended self-sign",
    "keywordsLower": [
      "boostport.com",
      "data.tls",
      "server.crt",
      "status.sh",
      "values.yaml",
      "pg_ca.crt",
      "manage",
      "certificates",
      "certificate",
      "configuration",
      "architectures",
      "direct",
      "access",
      "external",
      "load",
      "balancer",
      "through",
      "check",
      "validity",
      "renew",
      "troubleshooting",
      "issues",
      "documentation",
      "references",
      "document",
      "gives",
      "overview",
      "required",
      "secure",
      "communications",
      "between",
      "various",
      "components",
      "suite.",
      "describes",
      "expired",
      "certificate.",
      "choose",
      "any",
      "following",
      "configuration.",
      "enables",
      "suite",
      "configured.",
      "configured",
      "user",
      "one",
      "types.",
      "ca",
      "self-signed",
      "balance",
      "recommended",
      "diagram",
      "illustrates",
      "setup",
      "table",
      "below",
      "instructions",
      "shown",
      "setup.",
      "service",
      "management",
      "refers",
      "opentext",
      "application.",
      "number",
      "interface",
      "command",
      "link",
      "renewal",
      "user-",
      "integrations-",
      "balacer",
      "line",
      "openssl",
      "-showcerts",
      "-connect",
      "443",
      "x509",
      "-noout",
      "-dates",
      "grep",
      "notafter",
      "refer",
      "instructions.",
      "ingress",
      "signed",
      "custom",
      "mf",
      "re",
      "port",
      "kubectl",
      "get",
      "secret",
      "nginx-default-secret",
      "-n",
      "ns",
      "itsma",
      "cut",
      "-f1"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage persistent storage for the suite",
    "content": "This topic describes the storage sizing requirements for the suite, and also gives performance recommendations for the storage. Storage sizing requirements The suite uses many persistent volumes. In a production environment, the volume sizes increase over time. For a new installation, you must meet the initial volume sizing requirements; after the system goes live, you need to monitor the storage usage on a regular basis and take actions as needed. The tables in this section provides a list of the persistent volumes that the suite uses (not including those used for OMT), their initial sizing requirements for each deployment size (Small, Medium, or Large), how often you need to monitor their storage usage, and what actions you may need to take. The storage requirements are split into two performance groups: standard and high performance, based on the performance requirements. Service Management supports using StorageClass (for dynamic volume provisioning). It supports two storage classe",
    "url": "managestoragesuite",
    "filename": "managestoragesuite",
    "headings": [
      "Storage sizing requirements",
      "Standard performance storage",
      "High performance storage",
      "Storage performance recommendations",
      "Use the fio command (recommended)",
      "Use the dd command"
    ],
    "keywords": [
      "0.5",
      "a.dat",
      "manage",
      "persistent",
      "storage",
      "suite",
      "sizing",
      "requirements",
      "standard",
      "performance",
      "high",
      "recommendations",
      "fio",
      "command",
      "recommended",
      "dd",
      "topic",
      "describes",
      "gives",
      "storage.",
      "uses",
      "many",
      "volumes.",
      "production",
      "environment",
      "volume",
      "sizes",
      "increase",
      "over",
      "time.",
      "new",
      "installation",
      "meet",
      "initial",
      "after",
      "system",
      "goes",
      "live",
      "need",
      "monitor",
      "usage",
      "regular",
      "basis",
      "take",
      "actions",
      "needed.",
      "tables",
      "section",
      "provides",
      "list",
      "volumes",
      "including",
      "omt",
      "deployment",
      "size",
      "small",
      "medium",
      "large",
      "often",
      "what",
      "take.",
      "split",
      "two",
      "groups",
      "based",
      "requirements.",
      "service",
      "management",
      "supports",
      "storageclass",
      "dynamic",
      "provisioning",
      "classes",
      "itom-standard",
      "itom-fast",
      "respectively.",
      "map",
      "own",
      "class",
      "names.",
      "azure",
      "both",
      "files",
      "disks.",
      "below",
      "shared",
      "space",
      "disks",
      "separate",
      "lists",
      "helm",
      "deployment.",
      "non-production",
      "gb",
      "action",
      "threshold",
      "monitoring",
      "interval",
      "comments",
      "data-volume"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage persistent storage for the suite",
    "contentLower": "this topic describes the storage sizing requirements for the suite, and also gives performance recommendations for the storage. storage sizing requirements the suite uses many persistent volumes. in a production environment, the volume sizes increase over time. for a new installation, you must meet the initial volume sizing requirements; after the system goes live, you need to monitor the storage usage on a regular basis and take actions as needed. the tables in this section provides a list of the persistent volumes that the suite uses (not including those used for omt), their initial sizing requirements for each deployment size (small, medium, or large), how often you need to monitor their storage usage, and what actions you may need to take. the storage requirements are split into two performance groups: standard and high performance, based on the performance requirements. service management supports using storageclass (for dynamic volume provisioning). it supports two storage classe",
    "keywordsLower": [
      "0.5",
      "a.dat",
      "manage",
      "persistent",
      "storage",
      "suite",
      "sizing",
      "requirements",
      "standard",
      "performance",
      "high",
      "recommendations",
      "fio",
      "command",
      "recommended",
      "dd",
      "topic",
      "describes",
      "gives",
      "storage.",
      "uses",
      "many",
      "volumes.",
      "production",
      "environment",
      "volume",
      "sizes",
      "increase",
      "over",
      "time.",
      "new",
      "installation",
      "meet",
      "initial",
      "after",
      "system",
      "goes",
      "live",
      "need",
      "monitor",
      "usage",
      "regular",
      "basis",
      "take",
      "actions",
      "needed.",
      "tables",
      "section",
      "provides",
      "list",
      "volumes",
      "including",
      "omt",
      "deployment",
      "size",
      "small",
      "medium",
      "large",
      "often",
      "what",
      "take.",
      "split",
      "two",
      "groups",
      "based",
      "requirements.",
      "service",
      "management",
      "supports",
      "storageclass",
      "dynamic",
      "provisioning",
      "classes",
      "itom-standard",
      "itom-fast",
      "respectively.",
      "map",
      "own",
      "class",
      "names.",
      "azure",
      "both",
      "files",
      "disks.",
      "below",
      "shared",
      "space",
      "disks",
      "separate",
      "lists",
      "helm",
      "deployment.",
      "non-production",
      "gb",
      "action",
      "threshold",
      "monitoring",
      "interval",
      "comments",
      "data-volume"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Monitor suite log volumes",
    "content": "The Service Management log size increases fast and you need to keep monitoring the Persistent Volume size. If it runs out of disk space, the Service Management cluster goes down and leads to a service outage. Use the following methods to monitor the disk usage for Service Management. If the usage is more than 80%, take action immediately; otherwise, the system might be down shortly. Persistent Volumes managed by IT If the Persistent Volumes (disks) are managed by your IT team, they're responsible for monitoring and maintaining the disk volumes to make sure each Persistent Volume has enough free disk. Check Persistent Volumes manually As an administrator of Service Management, you can monitor the disk usage by mounting the disk to the storage folder. For ReadWriteMany(RWX) storage, you can use the mount command to mount to the remote server folder and run df -h to check the disk usage. For ReadWriteOnce (RWO) storage, you can't mount to the remote server and have to log in to the consol",
    "url": "monitorsuitelogvolume",
    "filename": "monitorsuitelogvolume",
    "headings": [
      "Persistent Volumes managed by IT",
      "Check Persistent Volumes manually",
      "Extend the disk size",
      "References"
    ],
    "keywords": [
      "monitor",
      "suite",
      "log",
      "volumes",
      "persistent",
      "managed",
      "check",
      "manually",
      "extend",
      "disk",
      "size",
      "references",
      "service",
      "management",
      "increases",
      "fast",
      "need",
      "keep",
      "monitoring",
      "volume",
      "size.",
      "runs",
      "out",
      "space",
      "cluster",
      "goes",
      "leads",
      "outage.",
      "following",
      "methods",
      "usage",
      "management.",
      "80",
      "take",
      "action",
      "immediately",
      "otherwise",
      "system",
      "shortly.",
      "disks",
      "team",
      "re",
      "responsible",
      "maintaining",
      "make",
      "sure",
      "enough",
      "free",
      "disk.",
      "administrator",
      "mounting",
      "storage",
      "folder.",
      "readwritemany",
      "rwx",
      "mount",
      "command",
      "remote",
      "server",
      "folder",
      "run",
      "df",
      "-h",
      "usage.",
      "readwriteonce",
      "rwo",
      "console",
      "received",
      "warning",
      "message",
      "over",
      "expand",
      "embedded",
      "kubernetes",
      "deployment",
      "expanding",
      "nfs",
      "cause",
      "downtime.",
      "storageclass",
      "go",
      "azure",
      "portal",
      "find",
      "account",
      "capacity",
      "directly.",
      "there",
      "downtime",
      "introduced.",
      "openshift",
      "follow",
      "links",
      "section",
      "below",
      "file",
      "shares"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "monitor suite log volumes",
    "contentLower": "the service management log size increases fast and you need to keep monitoring the persistent volume size. if it runs out of disk space, the service management cluster goes down and leads to a service outage. use the following methods to monitor the disk usage for service management. if the usage is more than 80%, take action immediately; otherwise, the system might be down shortly. persistent volumes managed by it if the persistent volumes (disks) are managed by your it team, they're responsible for monitoring and maintaining the disk volumes to make sure each persistent volume has enough free disk. check persistent volumes manually as an administrator of service management, you can monitor the disk usage by mounting the disk to the storage folder. for readwritemany(rwx) storage, you can use the mount command to mount to the remote server folder and run df -h to check the disk usage. for readwriteonce (rwo) storage, you can't mount to the remote server and have to log in to the consol",
    "keywordsLower": [
      "monitor",
      "suite",
      "log",
      "volumes",
      "persistent",
      "managed",
      "check",
      "manually",
      "extend",
      "disk",
      "size",
      "references",
      "service",
      "management",
      "increases",
      "fast",
      "need",
      "keep",
      "monitoring",
      "volume",
      "size.",
      "runs",
      "out",
      "space",
      "cluster",
      "goes",
      "leads",
      "outage.",
      "following",
      "methods",
      "usage",
      "management.",
      "80",
      "take",
      "action",
      "immediately",
      "otherwise",
      "system",
      "shortly.",
      "disks",
      "team",
      "re",
      "responsible",
      "maintaining",
      "make",
      "sure",
      "enough",
      "free",
      "disk.",
      "administrator",
      "mounting",
      "storage",
      "folder.",
      "readwritemany",
      "rwx",
      "mount",
      "command",
      "remote",
      "server",
      "folder",
      "run",
      "df",
      "-h",
      "usage.",
      "readwriteonce",
      "rwo",
      "console",
      "received",
      "warning",
      "message",
      "over",
      "expand",
      "embedded",
      "kubernetes",
      "deployment",
      "expanding",
      "nfs",
      "cause",
      "downtime.",
      "storageclass",
      "go",
      "azure",
      "portal",
      "find",
      "account",
      "capacity",
      "directly.",
      "there",
      "downtime",
      "introduced.",
      "openshift",
      "follow",
      "links",
      "section",
      "below",
      "file",
      "shares"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Migrate from self-hosted PostgreSQL to AlloyDB Omni",
    "content": "You can use this document to migrate the suite from a standard PostgreSQL database to AlloyDB Omni. There are two main steps in the migration process: Data migration. Migrate the data including the database structure, business data, and database authentication data from the self-hosted PostgreSQL to AlloyDB Omni by using the script db_migrate.sh, which will be introduced later. This script uses the pg_dump and pg_restore commands to perform the data dump and restore. Connect the suite to the AlloyDB Omni database. In this step, the production database connection configurations will be modified and applied. This database migration solution requires service downtime, which may vary depending on your data volume. You may want to run this migration in your test environment and plan a proper migration time window for production accordingly. In this release, support for AlloyDB Omni is under controlled availability. Please contact our product management team before you begin the implementati",
    "url": "migrateselfhostpg2alloydb",
    "filename": "migrateselfhostpg2alloydb",
    "headings": [
      "Prerequisites",
      "Create an AlloyDB Omni database",
      "Prepare a PostgreSQL client system",
      "Use the same password encryption algorithm",
      "Obtain the DBA credentials for the source and target database servers",
      "Prepare for migration",
      "Download the target database CA certificate",
      "Download the migration scripts and then install the scripts and the PostgreSQL CA certificate chain file",
      "Grant the \"maas_admin\" database role to the DBA user",
      "Check the database.json file",
      "Shut down the system to prevent any database write operation during the migration",
      "Run database migration",
      "Switch OMT to the new database server with SSL",
      "Switch to the new database server with SSL",
      "For a classic deployment",
      "For a helm deployment"
    ],
    "keywords": [
      "postgresql.conf",
      "my_values.yaml",
      "postgresql://<target",
      "pg_hba.conf",
      "chain.crt",
      "1.0.0",
      "cdfctl.sh",
      "database.json",
      "db_migrate.sh",
      "xxx.tgz",
      "updateSMAExternalDBInfo.sh",
      "component.json",
      "pg_ca.crt",
      "migrate",
      "self-hosted",
      "postgresql",
      "alloydb",
      "omni",
      "prerequisites",
      "create",
      "database",
      "prepare",
      "client",
      "system",
      "same",
      "password",
      "encryption",
      "algorithm",
      "obtain",
      "dba",
      "credentials",
      "source",
      "target",
      "servers",
      "migration",
      "download",
      "ca",
      "certificate",
      "scripts",
      "install",
      "chain",
      "file",
      "grant",
      "role",
      "user",
      "check",
      "shut",
      "prevent",
      "any",
      "write",
      "operation",
      "during",
      "run",
      "switch",
      "omt",
      "new",
      "server",
      "ssl",
      "classic",
      "deployment",
      "helm",
      "document",
      "suite",
      "standard",
      "omni.",
      "there",
      "two",
      "main",
      "steps",
      "process",
      "data",
      "migration.",
      "including",
      "structure",
      "business",
      "authentication",
      "script",
      "introduced",
      "later.",
      "uses",
      "commands",
      "perform",
      "dump",
      "restore.",
      "connect",
      "database.",
      "step",
      "production",
      "connection",
      "configurations",
      "modified",
      "applied.",
      "solution",
      "requires",
      "service",
      "downtime",
      "vary",
      "depending",
      "volume.",
      "want"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "migrate from self-hosted postgresql to alloydb omni",
    "contentLower": "you can use this document to migrate the suite from a standard postgresql database to alloydb omni. there are two main steps in the migration process: data migration. migrate the data including the database structure, business data, and database authentication data from the self-hosted postgresql to alloydb omni by using the script db_migrate.sh, which will be introduced later. this script uses the pg_dump and pg_restore commands to perform the data dump and restore. connect the suite to the alloydb omni database. in this step, the production database connection configurations will be modified and applied. this database migration solution requires service downtime, which may vary depending on your data volume. you may want to run this migration in your test environment and plan a proper migration time window for production accordingly. in this release, support for alloydb omni is under controlled availability. please contact our product management team before you begin the implementati",
    "keywordsLower": [
      "postgresql.conf",
      "my_values.yaml",
      "postgresql://<target",
      "pg_hba.conf",
      "chain.crt",
      "1.0.0",
      "cdfctl.sh",
      "database.json",
      "db_migrate.sh",
      "xxx.tgz",
      "updatesmaexternaldbinfo.sh",
      "component.json",
      "pg_ca.crt",
      "migrate",
      "self-hosted",
      "postgresql",
      "alloydb",
      "omni",
      "prerequisites",
      "create",
      "database",
      "prepare",
      "client",
      "system",
      "same",
      "password",
      "encryption",
      "algorithm",
      "obtain",
      "dba",
      "credentials",
      "source",
      "target",
      "servers",
      "migration",
      "download",
      "ca",
      "certificate",
      "scripts",
      "install",
      "chain",
      "file",
      "grant",
      "role",
      "user",
      "check",
      "shut",
      "prevent",
      "any",
      "write",
      "operation",
      "during",
      "run",
      "switch",
      "omt",
      "new",
      "server",
      "ssl",
      "classic",
      "deployment",
      "helm",
      "document",
      "suite",
      "standard",
      "omni.",
      "there",
      "two",
      "main",
      "steps",
      "process",
      "data",
      "migration.",
      "including",
      "structure",
      "business",
      "authentication",
      "script",
      "introduced",
      "later.",
      "uses",
      "commands",
      "perform",
      "dump",
      "restore.",
      "connect",
      "database.",
      "step",
      "production",
      "connection",
      "configurations",
      "modified",
      "applied.",
      "solution",
      "requires",
      "service",
      "downtime",
      "vary",
      "depending",
      "volume.",
      "want"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Migrate from self-hosted PostgreSQL to Azure Flexible Server",
    "content": "If you currently have a classic deployment with self-hosted PostgreSQL on Azure, you need to follow the steps below to migrate to Azure Flexible Server before transforming to a Helm deployment. The steps below don't work if you have already transformed the suite to a Helm deployment. There are two major steps in the entire migration process: Data migration. Migrate the data including the database structure, business data, and database authentication data from the self-hosted PostgreSQL to Azure Flexible Server by using the script db_migrate.sh, which will be introduced later. In this script, the pg_dump and pg_restore commands are used to perform the data dump and restore. Apply the suite to connect to Azure Flexible Server. In this step, the production database connection configurations will be modified and applied. This database migration solution requires system downtime, which may vary depending on your data volume. You may want to run this migration in your test environment and pl",
    "url": "migrateselfhostedpg2azureserver",
    "filename": "migrateselfhostedpg2azureserver",
    "headings": [
      "Prerequisites",
      "Create an Azure Flexible Server that meets the requirements for the database server",
      "Prepare a PostgreSQL client system",
      "Use the same password encryption algorithm",
      "Obtain the DBA credentials for the source and target database servers",
      "Prepare for migration",
      "Download the Azure Flexible Server CA certificate",
      "Download the migration scripts and then install the scripts and the PostgreSQL CA certificate chain file",
      "Grant the database role \"maas_admin\" to the DBA user",
      "Check the database.json file",
      "Shut down the system to prevent any DB write operation during the database migration",
      "Run database migration",
      "Switch OMT to the new Azure flexible server with SSL",
      "Switch the suite to the new Azure flexible server with SSL"
    ],
    "keywords": [
      "postgresql://<azure",
      "chain.crt",
      "cdfctl.sh",
      "database.json",
      "db_migrate.sh",
      "updateSMAExternalDBInfo.sh",
      "component.json",
      "migrate",
      "self-hosted",
      "postgresql",
      "azure",
      "flexible",
      "server",
      "prerequisites",
      "create",
      "meets",
      "requirements",
      "database",
      "prepare",
      "client",
      "system",
      "same",
      "password",
      "encryption",
      "algorithm",
      "obtain",
      "dba",
      "credentials",
      "source",
      "target",
      "servers",
      "migration",
      "download",
      "ca",
      "certificate",
      "scripts",
      "install",
      "chain",
      "file",
      "grant",
      "role",
      "user",
      "check",
      "shut",
      "prevent",
      "any",
      "db",
      "write",
      "operation",
      "during",
      "run",
      "switch",
      "omt",
      "new",
      "ssl",
      "suite",
      "currently",
      "classic",
      "deployment",
      "need",
      "follow",
      "steps",
      "below",
      "before",
      "transforming",
      "helm",
      "deployment.",
      "don",
      "work",
      "already",
      "transformed",
      "there",
      "two",
      "major",
      "entire",
      "process",
      "data",
      "migration.",
      "including",
      "structure",
      "business",
      "authentication",
      "script",
      "introduced",
      "later.",
      "commands",
      "perform",
      "dump",
      "restore.",
      "apply",
      "connect",
      "server.",
      "step",
      "production",
      "connection",
      "configurations",
      "modified",
      "applied.",
      "solution",
      "requires"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "migrate from self-hosted postgresql to azure flexible server",
    "contentLower": "if you currently have a classic deployment with self-hosted postgresql on azure, you need to follow the steps below to migrate to azure flexible server before transforming to a helm deployment. the steps below don't work if you have already transformed the suite to a helm deployment. there are two major steps in the entire migration process: data migration. migrate the data including the database structure, business data, and database authentication data from the self-hosted postgresql to azure flexible server by using the script db_migrate.sh, which will be introduced later. in this script, the pg_dump and pg_restore commands are used to perform the data dump and restore. apply the suite to connect to azure flexible server. in this step, the production database connection configurations will be modified and applied. this database migration solution requires system downtime, which may vary depending on your data volume. you may want to run this migration in your test environment and pl",
    "keywordsLower": [
      "postgresql://<azure",
      "chain.crt",
      "cdfctl.sh",
      "database.json",
      "db_migrate.sh",
      "updatesmaexternaldbinfo.sh",
      "component.json",
      "migrate",
      "self-hosted",
      "postgresql",
      "azure",
      "flexible",
      "server",
      "prerequisites",
      "create",
      "meets",
      "requirements",
      "database",
      "prepare",
      "client",
      "system",
      "same",
      "password",
      "encryption",
      "algorithm",
      "obtain",
      "dba",
      "credentials",
      "source",
      "target",
      "servers",
      "migration",
      "download",
      "ca",
      "certificate",
      "scripts",
      "install",
      "chain",
      "file",
      "grant",
      "role",
      "user",
      "check",
      "shut",
      "prevent",
      "any",
      "db",
      "write",
      "operation",
      "during",
      "run",
      "switch",
      "omt",
      "new",
      "ssl",
      "suite",
      "currently",
      "classic",
      "deployment",
      "need",
      "follow",
      "steps",
      "below",
      "before",
      "transforming",
      "helm",
      "deployment.",
      "don",
      "work",
      "already",
      "transformed",
      "there",
      "two",
      "major",
      "entire",
      "process",
      "data",
      "migration.",
      "including",
      "structure",
      "business",
      "authentication",
      "script",
      "introduced",
      "later.",
      "commands",
      "perform",
      "dump",
      "restore.",
      "apply",
      "connect",
      "server.",
      "step",
      "production",
      "connection",
      "configurations",
      "modified",
      "applied.",
      "solution",
      "requires"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Migrate the suite to a new deployment platform",
    "content": "This topic describes a general solution to migrating the SMA suite from one deployment platform to another. SMA can be deployed on-premises and on public cloud platforms such as AWS, Azure, and GCP. No matter which platform the suite was initially deployed on, you can migrate it to a new supported platform. For example, you are currently running an on-premises setup of SMA, and you can choose to move it to a public cloud to save the costs of hardware and software maintenance. Another typical scenario is where your suite is running on a public cloud platform, and you plan to migrate it to another public cloud with a better cost-saving offer or for aligning the company strategy. Limitations This solution has the following limitations: Version limited: The SMA version in the source system must be the same as in the target. The Kubernetes version in the source system must be the same as in the target. Migration from on-premises to cloud platforms must be performed on the SMA 2020.08 or new",
    "url": "migrateplatform",
    "filename": "migrateplatform",
    "headings": [
      "Limitations",
      "SMA platform migration solution",
      "1. Set up the target system",
      "2. Stop the suite in the target system",
      "3. Migrate the storage data from the source system to the target system",
      "4. Migrate the PostgreSQL databases from the source system to the target system",
      "5. Start the suite in the target system",
      "6. Switch the traffic to the target system"
    ],
    "keywords": [
      "2020.08",
      "cdfctl.sh",
      "2019.11",
      "2020.02",
      "samlKeystore.jks",
      "migrate",
      "suite",
      "new",
      "deployment",
      "platform",
      "limitations",
      "sma",
      "migration",
      "solution",
      "1.",
      "set",
      "target",
      "system",
      "2.",
      "stop",
      "3.",
      "storage",
      "data",
      "source",
      "4.",
      "postgresql",
      "databases",
      "5.",
      "start",
      "6.",
      "switch",
      "traffic",
      "topic",
      "describes",
      "general",
      "migrating",
      "one",
      "another.",
      "deployed",
      "on-premises",
      "public",
      "cloud",
      "platforms",
      "such",
      "aws",
      "azure",
      "gcp.",
      "matter",
      "initially",
      "supported",
      "platform.",
      "example",
      "currently",
      "running",
      "setup",
      "choose",
      "move",
      "save",
      "costs",
      "hardware",
      "software",
      "maintenance.",
      "another",
      "typical",
      "scenario",
      "plan",
      "better",
      "cost-saving",
      "offer",
      "aligning",
      "company",
      "strategy.",
      "following",
      "version",
      "limited",
      "same",
      "target.",
      "kubernetes",
      "performed",
      "newer",
      "versions.",
      "classic",
      "obsoleted",
      "since",
      "therefore",
      "eks",
      "see",
      "included",
      "customized",
      "configurations",
      "including",
      "replicas",
      "liveness",
      "readiness",
      "migrated.",
      "entire",
      "process",
      "follows",
      "fresh",
      "environment"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "migrate the suite to a new deployment platform",
    "contentLower": "this topic describes a general solution to migrating the sma suite from one deployment platform to another. sma can be deployed on-premises and on public cloud platforms such as aws, azure, and gcp. no matter which platform the suite was initially deployed on, you can migrate it to a new supported platform. for example, you are currently running an on-premises setup of sma, and you can choose to move it to a public cloud to save the costs of hardware and software maintenance. another typical scenario is where your suite is running on a public cloud platform, and you plan to migrate it to another public cloud with a better cost-saving offer or for aligning the company strategy. limitations this solution has the following limitations: version limited: the sma version in the source system must be the same as in the target. the kubernetes version in the source system must be the same as in the target. migration from on-premises to cloud platforms must be performed on the sma 2020.08 or new",
    "keywordsLower": [
      "2020.08",
      "cdfctl.sh",
      "2019.11",
      "2020.02",
      "samlkeystore.jks",
      "migrate",
      "suite",
      "new",
      "deployment",
      "platform",
      "limitations",
      "sma",
      "migration",
      "solution",
      "1.",
      "set",
      "target",
      "system",
      "2.",
      "stop",
      "3.",
      "storage",
      "data",
      "source",
      "4.",
      "postgresql",
      "databases",
      "5.",
      "start",
      "6.",
      "switch",
      "traffic",
      "topic",
      "describes",
      "general",
      "migrating",
      "one",
      "another.",
      "deployed",
      "on-premises",
      "public",
      "cloud",
      "platforms",
      "such",
      "aws",
      "azure",
      "gcp.",
      "matter",
      "initially",
      "supported",
      "platform.",
      "example",
      "currently",
      "running",
      "setup",
      "choose",
      "move",
      "save",
      "costs",
      "hardware",
      "software",
      "maintenance.",
      "another",
      "typical",
      "scenario",
      "plan",
      "better",
      "cost-saving",
      "offer",
      "aligning",
      "company",
      "strategy.",
      "following",
      "version",
      "limited",
      "same",
      "target.",
      "kubernetes",
      "performed",
      "newer",
      "versions.",
      "classic",
      "obsoleted",
      "since",
      "therefore",
      "eks",
      "see",
      "included",
      "customized",
      "configurations",
      "including",
      "replicas",
      "liveness",
      "readiness",
      "migrated.",
      "entire",
      "process",
      "follows",
      "fresh",
      "environment"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Open required ports in a managed Kubernetes environment",
    "content": "If the suite is deployed in a managed Kubernetes environment, you need to open certain ports according to your cloud platform. EKS You need to open the following ports if your corporate firewall blocks them. For more information, see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules-reference.html and https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html. Source Source port range Destination Destination port range Protocol Description External client * Bastion node 22 TCP Required for SSH access to the bastion node by external clients. Internet * LoadBalancer 3000, 443, 5443 TCP Required for access to the installation portal, the suite, and the OMT management portal. Bastion node * Worker nodes 8200 TCP Required for running the Smart Analytics reindexing tool (action_reindex.py) Bastion node * Internet 443, 80 TCP Required for the bastion node to communicate with Internet (for example, when running theaws ecr get-login command. Bastion node * Datab",
    "url": "openportsinmanagedk8s",
    "filename": "openportsinmanagedk8s",
    "headings": [
      "EKS",
      "AKS",
      "GCP",
      "Related topics"
    ],
    "keywords": [
      "reqs.html",
      "amazon.com",
      "https://docs.microsoft.com/en-us/azure/aks/limit-egress-traffic#required-ports-and-addresses-for-aks-clusters",
      "microsoft.com",
      "action_reindex.py",
      "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/security-group-rules-reference.html",
      "docs.aws",
      "reference.html",
      "https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html",
      "open",
      "required",
      "ports",
      "managed",
      "kubernetes",
      "environment",
      "eks",
      "aks",
      "gcp",
      "related",
      "topics",
      "suite",
      "deployed",
      "need",
      "certain",
      "according",
      "cloud",
      "platform.",
      "following",
      "corporate",
      "firewall",
      "blocks",
      "them.",
      "information",
      "see",
      "https",
      "docs.aws.amazon.com",
      "awsec2",
      "latest",
      "userguide",
      "security-group-rules-reference.html",
      "sec-group-reqs.html.",
      "source",
      "port",
      "range",
      "destination",
      "protocol",
      "description",
      "external",
      "client",
      "bastion",
      "node",
      "22",
      "tcp",
      "ssh",
      "access",
      "clients.",
      "internet",
      "loadbalancer",
      "3000",
      "443",
      "5443",
      "installation",
      "portal",
      "omt",
      "management",
      "portal.",
      "worker",
      "nodes",
      "8200",
      "running",
      "smart",
      "analytics",
      "reindexing",
      "tool",
      "80",
      "communicate",
      "example",
      "theaws",
      "ecr",
      "get-login",
      "command.",
      "database",
      "server",
      "5432",
      "server.",
      "efs",
      "111",
      "udp",
      "2049",
      "20048",
      "smtp",
      "25",
      "465",
      "587",
      "sma",
      "ldap",
      "389",
      "636",
      "refer",
      "table"
    ],
    "language": "en",
    "word_count": 111,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "open required ports in a managed kubernetes environment",
    "contentLower": "if the suite is deployed in a managed kubernetes environment, you need to open certain ports according to your cloud platform. eks you need to open the following ports if your corporate firewall blocks them. for more information, see https://docs.aws.amazon.com/awsec2/latest/userguide/security-group-rules-reference.html and https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html. source source port range destination destination port range protocol description external client * bastion node 22 tcp required for ssh access to the bastion node by external clients. internet * loadbalancer 3000, 443, 5443 tcp required for access to the installation portal, the suite, and the omt management portal. bastion node * worker nodes 8200 tcp required for running the smart analytics reindexing tool (action_reindex.py) bastion node * internet 443, 80 tcp required for the bastion node to communicate with internet (for example, when running theaws ecr get-login command. bastion node * datab",
    "keywordsLower": [
      "reqs.html",
      "amazon.com",
      "https://docs.microsoft.com/en-us/azure/aks/limit-egress-traffic#required-ports-and-addresses-for-aks-clusters",
      "microsoft.com",
      "action_reindex.py",
      "https://docs.aws.amazon.com/awsec2/latest/userguide/security-group-rules-reference.html",
      "docs.aws",
      "reference.html",
      "https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html",
      "open",
      "required",
      "ports",
      "managed",
      "kubernetes",
      "environment",
      "eks",
      "aks",
      "gcp",
      "related",
      "topics",
      "suite",
      "deployed",
      "need",
      "certain",
      "according",
      "cloud",
      "platform.",
      "following",
      "corporate",
      "firewall",
      "blocks",
      "them.",
      "information",
      "see",
      "https",
      "docs.aws.amazon.com",
      "awsec2",
      "latest",
      "userguide",
      "security-group-rules-reference.html",
      "sec-group-reqs.html.",
      "source",
      "port",
      "range",
      "destination",
      "protocol",
      "description",
      "external",
      "client",
      "bastion",
      "node",
      "22",
      "tcp",
      "ssh",
      "access",
      "clients.",
      "internet",
      "loadbalancer",
      "3000",
      "443",
      "5443",
      "installation",
      "portal",
      "omt",
      "management",
      "portal.",
      "worker",
      "nodes",
      "8200",
      "running",
      "smart",
      "analytics",
      "reindexing",
      "tool",
      "80",
      "communicate",
      "example",
      "theaws",
      "ecr",
      "get-login",
      "command.",
      "database",
      "server",
      "5432",
      "server.",
      "efs",
      "111",
      "udp",
      "2049",
      "20048",
      "smtp",
      "25",
      "465",
      "587",
      "sma",
      "ldap",
      "389",
      "636",
      "refer",
      "table"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Overview of backup and restore",
    "content": "To maintain business continuity, it's necessary to back up the suite data periodically so you can restore the data when the cluster crashes or data loss happens. For all deployments, use a backup and restore solution that leverages Velero and cloud-native tools and services. This page gives an overview of the solution: what data you need to back up and restore, how to plan your backup, and what scenarios this backup and restore solution can apply to. Prerequisites Before implementing the backup and restore solution, note the following items: The source and target environments must be hosted on the same platform. The Kubernetes versions in both environments must be the same. To back up and restore managed Kubernetes configurations and clusters, set up Velero according to your cloud deployment platforms. There's no need to do this for a deployment using the Kubernetes embedded in OMT, which comes with preinstalled Velero. What can I back up and restore The suite backup and restore soluti",
    "url": "backuprestorehandbook",
    "filename": "backuprestorehandbook",
    "headings": [
      "Prerequisites",
      "What can I back up and restore",
      "Disaster scenarios",
      "Scenario 1: Kubernetes configurations corruption",
      "Scenario 2: Persistent storage data corruption",
      "Scenario 3: Databases data corruption",
      "Scenario 4: Kubernetes cluster crash",
      "How to back up and restore",
      "Phase 1: Plan for disaster recovery",
      "Recovery objectives",
      "Stakeholders",
      "Tests and validation",
      "Phase 2: Apply your Disaster Recovery Plan"
    ],
    "keywords": [
      "https://velero.io",
      "velero.io",
      "overview",
      "backup",
      "restore",
      "prerequisites",
      "what",
      "back",
      "disaster",
      "scenarios",
      "scenario",
      "kubernetes",
      "configurations",
      "corruption",
      "persistent",
      "storage",
      "data",
      "databases",
      "cluster",
      "crash",
      "phase",
      "plan",
      "recovery",
      "objectives",
      "stakeholders",
      "tests",
      "validation",
      "apply",
      "maintain",
      "business",
      "continuity",
      "necessary",
      "suite",
      "periodically",
      "crashes",
      "loss",
      "happens.",
      "all",
      "deployments",
      "solution",
      "leverages",
      "velero",
      "cloud-native",
      "tools",
      "services.",
      "page",
      "gives",
      "need",
      "to.",
      "before",
      "implementing",
      "note",
      "following",
      "items",
      "source",
      "target",
      "environments",
      "hosted",
      "same",
      "platform.",
      "versions",
      "both",
      "same.",
      "managed",
      "clusters",
      "set",
      "according",
      "cloud",
      "deployment",
      "platforms.",
      "there",
      "embedded",
      "omt",
      "comes",
      "preinstalled",
      "velero.",
      "performs",
      "full",
      "restoration",
      "known",
      "api",
      "objects",
      "including",
      "daemonsets",
      "statefulsets",
      "namespaces",
      "volumes",
      "secrets",
      "more.",
      "postgresql",
      "database",
      "stateful",
      "components.",
      "external",
      "configuration",
      "files",
      "logs",
      "attachments",
      "on.",
      "three"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "overview of backup and restore",
    "contentLower": "to maintain business continuity, it's necessary to back up the suite data periodically so you can restore the data when the cluster crashes or data loss happens. for all deployments, use a backup and restore solution that leverages velero and cloud-native tools and services. this page gives an overview of the solution: what data you need to back up and restore, how to plan your backup, and what scenarios this backup and restore solution can apply to. prerequisites before implementing the backup and restore solution, note the following items: the source and target environments must be hosted on the same platform. the kubernetes versions in both environments must be the same. to back up and restore managed kubernetes configurations and clusters, set up velero according to your cloud deployment platforms. there's no need to do this for a deployment using the kubernetes embedded in omt, which comes with preinstalled velero. what can i back up and restore the suite backup and restore soluti",
    "keywordsLower": [
      "https://velero.io",
      "velero.io",
      "overview",
      "backup",
      "restore",
      "prerequisites",
      "what",
      "back",
      "disaster",
      "scenarios",
      "scenario",
      "kubernetes",
      "configurations",
      "corruption",
      "persistent",
      "storage",
      "data",
      "databases",
      "cluster",
      "crash",
      "phase",
      "plan",
      "recovery",
      "objectives",
      "stakeholders",
      "tests",
      "validation",
      "apply",
      "maintain",
      "business",
      "continuity",
      "necessary",
      "suite",
      "periodically",
      "crashes",
      "loss",
      "happens.",
      "all",
      "deployments",
      "solution",
      "leverages",
      "velero",
      "cloud-native",
      "tools",
      "services.",
      "page",
      "gives",
      "need",
      "to.",
      "before",
      "implementing",
      "note",
      "following",
      "items",
      "source",
      "target",
      "environments",
      "hosted",
      "same",
      "platform.",
      "versions",
      "both",
      "same.",
      "managed",
      "clusters",
      "set",
      "according",
      "cloud",
      "deployment",
      "platforms.",
      "there",
      "embedded",
      "omt",
      "comes",
      "preinstalled",
      "velero.",
      "performs",
      "full",
      "restoration",
      "known",
      "api",
      "objects",
      "including",
      "daemonsets",
      "statefulsets",
      "namespaces",
      "volumes",
      "secrets",
      "more.",
      "postgresql",
      "database",
      "stateful",
      "components.",
      "external",
      "configuration",
      "files",
      "logs",
      "attachments",
      "on.",
      "three"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Performance optimization by adjusting indexes on the system level",
    "content": "This topic gives general recommendations for adjusting indexes in order to improve the SQL query performance on the system level. We recommend that you create indexes for the following fields. Record type Field Index type Action Use the upper function (for string fields only) Incident Active B-tree Create index No Incident PhaseId B-tree Create index Yes Request PhaseId B-tree Create index Yes Request Active B-tree Create index No Request RequestedByPerson B-tree Create index No Request RequestedForPerson B-tree Create index No Request AssignedToGroup B-tree Create index No Request AssignedToPerson B-tree Create index No Change PhaseId B-tree Create index Yes Task ParentEntityId B-tree Create index Yes In addition, we recommend that you create indexes with the B-tree engine in the following table. Table Field Index type Action transaction_context_<TenantId> transaction_timestamp B-tree Create index ReportRuntime_<TenantId> ReportRuntimeId, Key gin Create index ReportConfig_<TenantId> M",
    "url": "recommendationsindexcreation",
    "filename": "recommendationsindexcreation",
    "headings": [
      "Create B-tree indexes for fields of a record type",
      "Create B-tree indexes for the transaction_timestamp field",
      "Create gin indexes for the ReportRuntimeId field in the ReportRuntime_<TenantId> table",
      "Create gin indexes for the MetaData field in the ReportConfig_<TenantId> table"
    ],
    "keywords": [
      "performance",
      "optimization",
      "adjusting",
      "indexes",
      "system",
      "level",
      "create",
      "b-tree",
      "fields",
      "record",
      "type",
      "field",
      "gin",
      "reportruntimeid",
      "table",
      "metadata",
      "topic",
      "gives",
      "general",
      "recommendations",
      "order",
      "improve",
      "sql",
      "query",
      "level.",
      "recommend",
      "following",
      "fields.",
      "index",
      "action",
      "upper",
      "function",
      "string",
      "incident",
      "active",
      "phaseid",
      "request",
      "requestedbyperson",
      "requestedforperson",
      "assignedtogroup",
      "assignedtoperson",
      "change",
      "task",
      "parententityid",
      "addition",
      "engine",
      "table.",
      "key",
      "admin",
      "user",
      "needs",
      "test",
      "scripts",
      "first",
      "before",
      "actually",
      "applying",
      "changes",
      "production",
      "environment.",
      "try",
      "avoid",
      "business",
      "hours",
      "otherwise",
      "database",
      "tables",
      "locked",
      "time",
      "resulting",
      "impact",
      "business.",
      "details",
      "about",
      "specific",
      "see",
      "here.",
      "run",
      "command",
      "concurrently",
      "exists",
      "btree",
      "example",
      "body-"
    ],
    "language": "en",
    "word_count": 110,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "performance optimization by adjusting indexes on the system level",
    "contentLower": "this topic gives general recommendations for adjusting indexes in order to improve the sql query performance on the system level. we recommend that you create indexes for the following fields. record type field index type action use the upper function (for string fields only) incident active b-tree create index no incident phaseid b-tree create index yes request phaseid b-tree create index yes request active b-tree create index no request requestedbyperson b-tree create index no request requestedforperson b-tree create index no request assignedtogroup b-tree create index no request assignedtoperson b-tree create index no change phaseid b-tree create index yes task parententityid b-tree create index yes in addition, we recommend that you create indexes with the b-tree engine in the following table. table field index type action transaction_context_<tenantid> transaction_timestamp b-tree create index reportruntime_<tenantid> reportruntimeid, key gin create index reportconfig_<tenantid> m",
    "keywordsLower": [
      "performance",
      "optimization",
      "adjusting",
      "indexes",
      "system",
      "level",
      "create",
      "b-tree",
      "fields",
      "record",
      "type",
      "field",
      "gin",
      "reportruntimeid",
      "table",
      "metadata",
      "topic",
      "gives",
      "general",
      "recommendations",
      "order",
      "improve",
      "sql",
      "query",
      "level.",
      "recommend",
      "following",
      "fields.",
      "index",
      "action",
      "upper",
      "function",
      "string",
      "incident",
      "active",
      "phaseid",
      "request",
      "requestedbyperson",
      "requestedforperson",
      "assignedtogroup",
      "assignedtoperson",
      "change",
      "task",
      "parententityid",
      "addition",
      "engine",
      "table.",
      "key",
      "admin",
      "user",
      "needs",
      "test",
      "scripts",
      "first",
      "before",
      "actually",
      "applying",
      "changes",
      "production",
      "environment.",
      "try",
      "avoid",
      "business",
      "hours",
      "otherwise",
      "database",
      "tables",
      "locked",
      "time",
      "resulting",
      "impact",
      "business.",
      "details",
      "about",
      "specific",
      "see",
      "here.",
      "run",
      "command",
      "concurrently",
      "exists",
      "btree",
      "example",
      "body-"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform tasks for a PostgreSQL upgrade",
    "content": "Follow the instructions below to upgrade your PostgreSQL database server. Upgrade self-hosted PostgreSQL Perform the following steps. Install the new version of the PostgreSQL server Install the new version of the PostgreSQL server with the postgresql-contrib module. Upgrade database data Upgrade database data to the new version. We recommend that you use the pg_upgrade command to perform the upgrade. If you haven't installed the postgresql-contrib module on the new PostgreSQL server, the pg_upgrade command will fail. Upgrade the pg_trgm extension To upgrade the pg_trgm extension, run the following SQL commands with the database superuser. \\c xservices_ems; ALTER EXTENSION \"pg_trgm\" UPDATE; To check that you have upgraded the extension successfully, run the following command. select * from pg_available_extensions where default_version=installed_version and name='pg_trgm'; If the default_version is the same as the installed_version, the extension upgrade was a success. Optimizer statist",
    "url": "postpgupgradetasks",
    "filename": "postpgupgradetasks",
    "headings": [
      "Upgrade self-hosted PostgreSQL",
      "Install the new version of the PostgreSQL server",
      "Upgrade database data",
      "Upgrade the pg_trgm extension",
      "Optimizer statistics",
      "Upgrade managed PostgreSQL",
      "Upgrade PostgreSQL",
      "Upgrade the pg_trgm extension",
      "Check the SSL parameter after upgrading RDS to PostgreSQL 15 or later"
    ],
    "keywords": [
      "perform",
      "tasks",
      "postgresql",
      "upgrade",
      "self-hosted",
      "install",
      "new",
      "version",
      "server",
      "database",
      "data",
      "extension",
      "optimizer",
      "statistics",
      "managed",
      "check",
      "ssl",
      "parameter",
      "after",
      "upgrading",
      "rds",
      "15",
      "later",
      "follow",
      "instructions",
      "below",
      "server.",
      "following",
      "steps.",
      "postgresql-contrib",
      "module.",
      "version.",
      "recommend",
      "command",
      "upgrade.",
      "haven",
      "installed",
      "module",
      "fail.",
      "run",
      "sql",
      "commands",
      "superuser.",
      "alter",
      "update",
      "upgraded",
      "successfully",
      "command.",
      "select",
      "name",
      "same",
      "success.",
      "doesn",
      "transfer",
      "statistics.",
      "consider",
      "running",
      "example",
      "os",
      "postgres",
      "user",
      "rerun",
      "usr",
      "pgsql-15",
      "bin",
      "vacuumdb",
      "--all",
      "--analyze-in-stages",
      "instance",
      "such",
      "azure",
      "flexible",
      "note",
      "default",
      "set",
      "means",
      "connection.",
      "application",
      "uses",
      "non-tls",
      "need",
      "group."
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform tasks for a postgresql upgrade",
    "contentLower": "follow the instructions below to upgrade your postgresql database server. upgrade self-hosted postgresql perform the following steps. install the new version of the postgresql server install the new version of the postgresql server with the postgresql-contrib module. upgrade database data upgrade database data to the new version. we recommend that you use the pg_upgrade command to perform the upgrade. if you haven't installed the postgresql-contrib module on the new postgresql server, the pg_upgrade command will fail. upgrade the pg_trgm extension to upgrade the pg_trgm extension, run the following sql commands with the database superuser. \\c xservices_ems; alter extension \"pg_trgm\" update; to check that you have upgraded the extension successfully, run the following command. select * from pg_available_extensions where default_version=installed_version and name='pg_trgm'; if the default_version is the same as the installed_version, the extension upgrade was a success. optimizer statist",
    "keywordsLower": [
      "perform",
      "tasks",
      "postgresql",
      "upgrade",
      "self-hosted",
      "install",
      "new",
      "version",
      "server",
      "database",
      "data",
      "extension",
      "optimizer",
      "statistics",
      "managed",
      "check",
      "ssl",
      "parameter",
      "after",
      "upgrading",
      "rds",
      "15",
      "later",
      "follow",
      "instructions",
      "below",
      "server.",
      "following",
      "steps.",
      "postgresql-contrib",
      "module.",
      "version.",
      "recommend",
      "command",
      "upgrade.",
      "haven",
      "installed",
      "module",
      "fail.",
      "run",
      "sql",
      "commands",
      "superuser.",
      "alter",
      "update",
      "upgraded",
      "successfully",
      "command.",
      "select",
      "name",
      "same",
      "success.",
      "doesn",
      "transfer",
      "statistics.",
      "consider",
      "running",
      "example",
      "os",
      "postgres",
      "user",
      "rerun",
      "usr",
      "pgsql-15",
      "bin",
      "vacuumdb",
      "--all",
      "--analyze-in-stages",
      "instance",
      "such",
      "azure",
      "flexible",
      "note",
      "default",
      "set",
      "means",
      "connection.",
      "application",
      "uses",
      "non-tls",
      "need",
      "group."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Modify the configuration information of external databases",
    "content": "This topic describes how to modify the configuration information of external databases for Service Management. To learn how to modify the configuration information of OMT's external IdM database, refer to the OMT documentation: Modify the configuration information of an external IdM database. Use the following steps to modify the configuration information of external databases. These steps allows you to update the database host, port and SSL configuration. In the following procedure, step 5 requires a cluster restart, which will cause a system downtime. If you want to avoid this, you can: Download the SMA Operation Toolkit package (SMA-operation-toolkit-xxxx.xx.tar.gz and SMA-operation-toolkit-xxxx.xx.tar.gz.sig) and upload it to a control plane node (in an embedded Kubernetes environment) or to the bastion node (in a managed Kubernetes environment). Extract the toolkit package by running command \"tar xzvf  SMA-operation-toolkit-xxxx.xx.tar.gz\". Skip step 5. Instead, go to step 6. Run ",
    "url": "modifyexternaldbconfig",
    "filename": "modifyexternaldbconfig",
    "headings": [
      "Monitor the database update result"
    ],
    "keywords": [
      "my_values.yaml",
      "pg_server1_ca.crt",
      "pods.sh",
      "1.0.0",
      "xxx.tgz",
      "xxxx.xx",
      "tar.gz",
      "modify",
      "configuration",
      "information",
      "external",
      "databases",
      "monitor",
      "database",
      "update",
      "result",
      "topic",
      "describes",
      "service",
      "management.",
      "learn",
      "omt",
      "idm",
      "refer",
      "documentation",
      "database.",
      "following",
      "steps",
      "databases.",
      "allows",
      "host",
      "port",
      "ssl",
      "configuration.",
      "procedure",
      "step",
      "requires",
      "cluster",
      "restart",
      "cause",
      "system",
      "downtime.",
      "want",
      "avoid",
      "download",
      "sma",
      "operation",
      "toolkit",
      "package",
      "sma-operation-toolkit-xxxx.xx.tar.gz",
      "sma-operation-toolkit-xxxx.xx.tar.gz.sig",
      "upload",
      "control",
      "plane",
      "node",
      "embedded",
      "kubernetes",
      "environment",
      "bastion",
      "managed",
      "extract",
      "running",
      "command",
      "tar",
      "xzvf",
      "skip",
      "5.",
      "instead",
      "go",
      "6.",
      "run",
      "get",
      "current",
      "helm",
      "values.",
      "values",
      "-n",
      "release",
      "name",
      "obtain",
      "list",
      "awk",
      "print",
      "unique",
      "namespace",
      "esm",
      "chart",
      "deployment.",
      "yaml",
      "file",
      "customized",
      "settings.",
      "example",
      "itsma-xxx",
      "new",
      "configurations",
      "wish",
      "update.",
      "global",
      "admin"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "modify the configuration information of external databases",
    "contentLower": "this topic describes how to modify the configuration information of external databases for service management. to learn how to modify the configuration information of omt's external idm database, refer to the omt documentation: modify the configuration information of an external idm database. use the following steps to modify the configuration information of external databases. these steps allows you to update the database host, port and ssl configuration. in the following procedure, step 5 requires a cluster restart, which will cause a system downtime. if you want to avoid this, you can: download the sma operation toolkit package (sma-operation-toolkit-xxxx.xx.tar.gz and sma-operation-toolkit-xxxx.xx.tar.gz.sig) and upload it to a control plane node (in an embedded kubernetes environment) or to the bastion node (in a managed kubernetes environment). extract the toolkit package by running command \"tar xzvf  sma-operation-toolkit-xxxx.xx.tar.gz\". skip step 5. instead, go to step 6. run ",
    "keywordsLower": [
      "my_values.yaml",
      "pg_server1_ca.crt",
      "pods.sh",
      "1.0.0",
      "xxx.tgz",
      "xxxx.xx",
      "tar.gz",
      "modify",
      "configuration",
      "information",
      "external",
      "databases",
      "monitor",
      "database",
      "update",
      "result",
      "topic",
      "describes",
      "service",
      "management.",
      "learn",
      "omt",
      "idm",
      "refer",
      "documentation",
      "database.",
      "following",
      "steps",
      "databases.",
      "allows",
      "host",
      "port",
      "ssl",
      "configuration.",
      "procedure",
      "step",
      "requires",
      "cluster",
      "restart",
      "cause",
      "system",
      "downtime.",
      "want",
      "avoid",
      "download",
      "sma",
      "operation",
      "toolkit",
      "package",
      "sma-operation-toolkit-xxxx.xx.tar.gz",
      "sma-operation-toolkit-xxxx.xx.tar.gz.sig",
      "upload",
      "control",
      "plane",
      "node",
      "embedded",
      "kubernetes",
      "environment",
      "bastion",
      "managed",
      "extract",
      "running",
      "command",
      "tar",
      "xzvf",
      "skip",
      "5.",
      "instead",
      "go",
      "6.",
      "run",
      "get",
      "current",
      "helm",
      "values.",
      "values",
      "-n",
      "release",
      "name",
      "obtain",
      "list",
      "awk",
      "print",
      "unique",
      "namespace",
      "esm",
      "chart",
      "deployment.",
      "yaml",
      "file",
      "customized",
      "settings.",
      "example",
      "itsma-xxx",
      "new",
      "configurations",
      "wish",
      "update.",
      "global",
      "admin"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Observe SMA",
    "content": "Observability helps you understand how a system works from the external. It allows you to easily troubleshoot and find the root cause of a problem by using three key signals: traces, metrics, and logs. Traces Traces give you a big picture of what happens when a user or an application makes a request across different services. You can use OpenTelemetry to collect the traces of the suite system. See Observe traces with OpenTelemetry. Metrics A metric is a measurement of a service captured at runtime. Application and request metrics are important indicators of availability and performance. You can use Prometheus to collect statistical information about the CPU, memory, network I/O, and disk IO of the worker nodes, pods, and services in the suite system. Then, use Grafana to view data via charts and graphs that are unified into one dashboard (or multiple dashboards) for easier interpretation and understanding. See Observe metrics with Prometheus and Grafana. Logs A log is a timestamped tex",
    "url": "observesma",
    "filename": "observesma",
    "headings": [
      "Traces",
      "Metrics",
      "Logs"
    ],
    "keywords": [
      "observe",
      "sma",
      "traces",
      "metrics",
      "logs",
      "observability",
      "helps",
      "understand",
      "system",
      "works",
      "external.",
      "allows",
      "easily",
      "troubleshoot",
      "find",
      "root",
      "cause",
      "problem",
      "three",
      "key",
      "signals",
      "logs.",
      "give",
      "big",
      "picture",
      "what",
      "happens",
      "user",
      "application",
      "makes",
      "request",
      "across",
      "different",
      "services.",
      "opentelemetry",
      "collect",
      "suite",
      "system.",
      "see",
      "opentelemetry.",
      "metric",
      "measurement",
      "service",
      "captured",
      "runtime.",
      "important",
      "indicators",
      "availability",
      "performance.",
      "prometheus",
      "statistical",
      "information",
      "about",
      "cpu",
      "memory",
      "network",
      "disk",
      "io",
      "worker",
      "nodes",
      "pods",
      "services",
      "grafana",
      "view",
      "data",
      "via",
      "charts",
      "graphs",
      "unified",
      "one",
      "dashboard",
      "multiple",
      "dashboards",
      "easier",
      "interpretation",
      "understanding.",
      "grafana.",
      "log",
      "timestamped",
      "text",
      "record",
      "either",
      "structured",
      "unstructured",
      "metadata."
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "observe sma",
    "contentLower": "observability helps you understand how a system works from the external. it allows you to easily troubleshoot and find the root cause of a problem by using three key signals: traces, metrics, and logs. traces traces give you a big picture of what happens when a user or an application makes a request across different services. you can use opentelemetry to collect the traces of the suite system. see observe traces with opentelemetry. metrics a metric is a measurement of a service captured at runtime. application and request metrics are important indicators of availability and performance. you can use prometheus to collect statistical information about the cpu, memory, network i/o, and disk io of the worker nodes, pods, and services in the suite system. then, use grafana to view data via charts and graphs that are unified into one dashboard (or multiple dashboards) for easier interpretation and understanding. see observe metrics with prometheus and grafana. logs a log is a timestamped tex",
    "keywordsLower": [
      "observe",
      "sma",
      "traces",
      "metrics",
      "logs",
      "observability",
      "helps",
      "understand",
      "system",
      "works",
      "external.",
      "allows",
      "easily",
      "troubleshoot",
      "find",
      "root",
      "cause",
      "problem",
      "three",
      "key",
      "signals",
      "logs.",
      "give",
      "big",
      "picture",
      "what",
      "happens",
      "user",
      "application",
      "makes",
      "request",
      "across",
      "different",
      "services.",
      "opentelemetry",
      "collect",
      "suite",
      "system.",
      "see",
      "opentelemetry.",
      "metric",
      "measurement",
      "service",
      "captured",
      "runtime.",
      "important",
      "indicators",
      "availability",
      "performance.",
      "prometheus",
      "statistical",
      "information",
      "about",
      "cpu",
      "memory",
      "network",
      "disk",
      "io",
      "worker",
      "nodes",
      "pods",
      "services",
      "grafana",
      "view",
      "data",
      "via",
      "charts",
      "graphs",
      "unified",
      "one",
      "dashboard",
      "multiple",
      "dashboards",
      "easier",
      "interpretation",
      "understanding.",
      "grafana.",
      "log",
      "timestamped",
      "text",
      "record",
      "either",
      "structured",
      "unstructured",
      "metadata."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Observe traces with OpenTelemetry",
    "content": "OpenTelemetry is a collection of APIs, SDKs, and tools. You can use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) to help analyze your software’s performance and behavior. This topic describes how to use OpenTelemetry to observe and analyze the performance of Service Management. Overview This diagram illustrates the architecture of the OpenTelemetry solution. Before using the OpenTelemetry solution, be aware of the following: You'll need to install some OpenTelemetry services including Operation, Collector, and certificate management. These services will consume resources in your environment, such as one CPU and 2 GB of memory. It will create a new namespace cert-manager in your Kubernetes cluster. The installation of OpenTelemetry will result in rolling restarts of some Service Management services. The default services for observation include: Service Pod Platform itom-xruntime-gateway, itom-xruntime-serviceportal, itom-xruntime-platform No",
    "url": "otelforsma",
    "filename": "otelforsma",
    "headings": [
      "Overview",
      "Prerequisite",
      "Install OpenTelemetry",
      "Configure instrumentation",
      "Export to the back end",
      "Export to AWS OpenSearch",
      "Export to Jaeger",
      "Configure filters"
    ],
    "keywords": [
      "default.svc",
      "0.57.0",
      "install_otel.sh",
      "0.57",
      "http://jaeger-collector.default.svc.cluster.local:14250",
      "https://pipeline-endpoint.us-east-1.osis.amazonaws.com/v1/traces",
      "endpoint.us",
      "amazonaws.com",
      "https://<pipeline-endpoint>.us-east-1.osis.amazonaws.com",
      "0.29",
      "1.osis",
      "0.29.1",
      "tar.gz",
      "observe",
      "traces",
      "opentelemetry",
      "overview",
      "prerequisite",
      "install",
      "configure",
      "instrumentation",
      "export",
      "back",
      "end",
      "aws",
      "opensearch",
      "jaeger",
      "filters",
      "collection",
      "apis",
      "sdks",
      "tools.",
      "instrument",
      "generate",
      "collect",
      "telemetry",
      "data",
      "metrics",
      "logs",
      "help",
      "analyze",
      "software",
      "performance",
      "behavior.",
      "topic",
      "describes",
      "service",
      "management.",
      "diagram",
      "illustrates",
      "architecture",
      "solution.",
      "before",
      "solution",
      "aware",
      "following",
      "ll",
      "need",
      "services",
      "including",
      "operation",
      "collector",
      "certificate",
      "consume",
      "resources",
      "environment",
      "such",
      "one",
      "cpu",
      "gb",
      "memory.",
      "create",
      "new",
      "namespace",
      "cert-manager",
      "kubernetes",
      "cluster.",
      "installation",
      "result",
      "rolling",
      "restarts",
      "management",
      "services.",
      "default",
      "observation",
      "include",
      "pod",
      "platform",
      "itom-xruntime-gateway",
      "itom-xruntime-serviceportal",
      "itom-xruntime-platform",
      "note",
      "asynchronous",
      "ems",
      "operations",
      "update",
      "aren",
      "supported.",
      "smart",
      "analytics"
    ],
    "language": "en",
    "word_count": 86,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "observe traces with opentelemetry",
    "contentLower": "opentelemetry is a collection of apis, sdks, and tools. you can use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) to help analyze your software’s performance and behavior. this topic describes how to use opentelemetry to observe and analyze the performance of service management. overview this diagram illustrates the architecture of the opentelemetry solution. before using the opentelemetry solution, be aware of the following: you'll need to install some opentelemetry services including operation, collector, and certificate management. these services will consume resources in your environment, such as one cpu and 2 gb of memory. it will create a new namespace cert-manager in your kubernetes cluster. the installation of opentelemetry will result in rolling restarts of some service management services. the default services for observation include: service pod platform itom-xruntime-gateway, itom-xruntime-serviceportal, itom-xruntime-platform no",
    "keywordsLower": [
      "default.svc",
      "0.57.0",
      "install_otel.sh",
      "0.57",
      "http://jaeger-collector.default.svc.cluster.local:14250",
      "https://pipeline-endpoint.us-east-1.osis.amazonaws.com/v1/traces",
      "endpoint.us",
      "amazonaws.com",
      "https://<pipeline-endpoint>.us-east-1.osis.amazonaws.com",
      "0.29",
      "1.osis",
      "0.29.1",
      "tar.gz",
      "observe",
      "traces",
      "opentelemetry",
      "overview",
      "prerequisite",
      "install",
      "configure",
      "instrumentation",
      "export",
      "back",
      "end",
      "aws",
      "opensearch",
      "jaeger",
      "filters",
      "collection",
      "apis",
      "sdks",
      "tools.",
      "instrument",
      "generate",
      "collect",
      "telemetry",
      "data",
      "metrics",
      "logs",
      "help",
      "analyze",
      "software",
      "performance",
      "behavior.",
      "topic",
      "describes",
      "service",
      "management.",
      "diagram",
      "illustrates",
      "architecture",
      "solution.",
      "before",
      "solution",
      "aware",
      "following",
      "ll",
      "need",
      "services",
      "including",
      "operation",
      "collector",
      "certificate",
      "consume",
      "resources",
      "environment",
      "such",
      "one",
      "cpu",
      "gb",
      "memory.",
      "create",
      "new",
      "namespace",
      "cert-manager",
      "kubernetes",
      "cluster.",
      "installation",
      "result",
      "rolling",
      "restarts",
      "management",
      "services.",
      "default",
      "observation",
      "include",
      "pod",
      "platform",
      "itom-xruntime-gateway",
      "itom-xruntime-serviceportal",
      "itom-xruntime-platform",
      "note",
      "asynchronous",
      "ems",
      "operations",
      "update",
      "aren",
      "supported.",
      "smart",
      "analytics"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Monitoring with open source tools",
    "content": "Once you have successfully deployed the suite, it is important to set up a monitoring system to track potential issues with your worker nodes, pods, and services. This section explains how to utilize Prometheus, an open-source tool, to gather statistical data on CPU usage, memory consumption, network I/O, and disk I/O for the worker nodes, pods, and services within the suite system. Any mention of SMAX in this document refers to OpenText Service Management, which was formerly known as SMAX. Overview The following diagram illustrates the architecture of this monitoring solution. Grafana Grafana is a popular dashboarding solution that allows users to create and manage separate access to dashboards for different individuals and teams. Additionally, Grafana uses the concept of organizations to support multi-tenancy, enabling you to group resources and assign access privileges to specific users. Grafana offers numerous plug-ins for various data sources, including InfluxDB, Graphite, Elastic",
    "url": "smamonitoring",
    "filename": "smamonitoring",
    "headings": [
      "Overview",
      "Grafana",
      "Prometheus",
      "Confirm the monitoring tools status",
      "Configure Prometheus",
      "Configure alert rules",
      "Customize JMX metrics configurations",
      "Out-of-the-box dashboards",
      "Covered component",
      "Use cases",
      "Dashboard-1: General-os-metrics",
      "User case 1",
      "User case 2",
      "Dashboard-2: Kubernetes-cluster-metrics",
      "User case 3",
      "User case 4",
      "User case 5",
      "Enable PostgreSQL metrics"
    ],
    "keywords": [
      "days.Root",
      "coreos.com",
      "2021.05",
      "1.2",
      "https://smarta-saw-dih-svc:1444/DRECOMPACT?backup=false&noarchive=true",
      "java.lang",
      "cdfctl.sh",
      "items.SMAX",
      "information.SMAX",
      "tenant.CI",
      "values.yaml",
      "http://<ip>:31371/DRECOMPACT?backup=false&noarchive=true",
      "config.yml",
      "Portal.IdM",
      "usage.SMAX",
      "nio-8443",
      "0.0f",
      "3.7",
      "github.com",
      "https://github.com/prometheus/jmx_exporter/examples",
      "https://<external_access_host>:5443/grafana",
      "HP.MAAS",
      "alert.yaml",
      "0.2f",
      "monitoring",
      "open",
      "source",
      "tools",
      "overview",
      "grafana",
      "prometheus",
      "confirm",
      "status",
      "configure",
      "alert",
      "rules",
      "customize",
      "jmx",
      "metrics",
      "configurations",
      "out-of-the-box",
      "dashboards",
      "covered",
      "component",
      "cases",
      "dashboard-1",
      "general-os-metrics",
      "user",
      "case",
      "dashboard-2",
      "kubernetes-cluster-metrics",
      "enable",
      "postgresql",
      "once",
      "successfully",
      "deployed",
      "suite",
      "important",
      "set",
      "system",
      "track",
      "potential",
      "issues",
      "worker",
      "nodes",
      "pods",
      "services.",
      "section",
      "explains",
      "utilize",
      "open-source",
      "tool",
      "gather",
      "statistical",
      "data",
      "cpu",
      "usage",
      "memory",
      "consumption",
      "network",
      "disk",
      "services",
      "system.",
      "any",
      "mention",
      "smax",
      "document",
      "refers",
      "opentext",
      "service",
      "management",
      "formerly",
      "known",
      "smax.",
      "following",
      "diagram",
      "illustrates",
      "architecture",
      "solution.",
      "popular"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "monitoring with open source tools",
    "contentLower": "once you have successfully deployed the suite, it is important to set up a monitoring system to track potential issues with your worker nodes, pods, and services. this section explains how to utilize prometheus, an open-source tool, to gather statistical data on cpu usage, memory consumption, network i/o, and disk i/o for the worker nodes, pods, and services within the suite system. any mention of smax in this document refers to opentext service management, which was formerly known as smax. overview the following diagram illustrates the architecture of this monitoring solution. grafana grafana is a popular dashboarding solution that allows users to create and manage separate access to dashboards for different individuals and teams. additionally, grafana uses the concept of organizations to support multi-tenancy, enabling you to group resources and assign access privileges to specific users. grafana offers numerous plug-ins for various data sources, including influxdb, graphite, elastic",
    "keywordsLower": [
      "days.root",
      "coreos.com",
      "2021.05",
      "1.2",
      "https://smarta-saw-dih-svc:1444/drecompact?backup=false&noarchive=true",
      "java.lang",
      "cdfctl.sh",
      "items.smax",
      "information.smax",
      "tenant.ci",
      "values.yaml",
      "http://<ip>:31371/drecompact?backup=false&noarchive=true",
      "config.yml",
      "portal.idm",
      "usage.smax",
      "nio-8443",
      "0.0f",
      "3.7",
      "github.com",
      "https://github.com/prometheus/jmx_exporter/examples",
      "https://<external_access_host>:5443/grafana",
      "hp.maas",
      "alert.yaml",
      "0.2f",
      "monitoring",
      "open",
      "source",
      "tools",
      "overview",
      "grafana",
      "prometheus",
      "confirm",
      "status",
      "configure",
      "alert",
      "rules",
      "customize",
      "jmx",
      "metrics",
      "configurations",
      "out-of-the-box",
      "dashboards",
      "covered",
      "component",
      "cases",
      "dashboard-1",
      "general-os-metrics",
      "user",
      "case",
      "dashboard-2",
      "kubernetes-cluster-metrics",
      "enable",
      "postgresql",
      "once",
      "successfully",
      "deployed",
      "suite",
      "important",
      "set",
      "system",
      "track",
      "potential",
      "issues",
      "worker",
      "nodes",
      "pods",
      "services.",
      "section",
      "explains",
      "utilize",
      "open-source",
      "tool",
      "gather",
      "statistical",
      "data",
      "cpu",
      "usage",
      "memory",
      "consumption",
      "network",
      "disk",
      "services",
      "system.",
      "any",
      "mention",
      "smax",
      "document",
      "refers",
      "opentext",
      "service",
      "management",
      "formerly",
      "known",
      "smax.",
      "following",
      "diagram",
      "illustrates",
      "architecture",
      "solution.",
      "popular"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage UD/UCMDB users",
    "content": "This section includes the following topics: UD/UCMDB user management overview Create UCMDB Consumer customer Create UD/UCMDB custom roles, users, and groups Use the original UD/UCMDB admin or sysadmin instead of suite admin",
    "url": "sharedidm",
    "filename": "sharedidm",
    "headings": [],
    "keywords": [
      "uducmdb",
      "manage",
      "ud",
      "ucmdb",
      "users",
      "section",
      "includes",
      "following",
      "topics",
      "user",
      "management",
      "overview",
      "create",
      "consumer",
      "customer",
      "custom",
      "roles",
      "groups",
      "original",
      "admin",
      "sysadmin",
      "instead",
      "suite"
    ],
    "language": "en",
    "word_count": 32,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage ud/ucmdb users",
    "contentLower": "this section includes the following topics: ud/ucmdb user management overview create ucmdb consumer customer create ud/ucmdb custom roles, users, and groups use the original ud/ucmdb admin or sysadmin instead of suite admin",
    "keywordsLower": [
      "uducmdb",
      "manage",
      "ud",
      "ucmdb",
      "users",
      "section",
      "includes",
      "following",
      "topics",
      "user",
      "management",
      "overview",
      "create",
      "consumer",
      "customer",
      "custom",
      "roles",
      "groups",
      "original",
      "admin",
      "sysadmin",
      "instead",
      "suite"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Push data from the source UCMDB customer to the target UCMDB customer",
    "content": "This topic describes how to migrate CIs from the source UCMDB customer to the target UCMDB customer for Native SACM. Prerequisite Make sure that the existing UD/UCMDB tenants in the source UCMDB customer are also created as preparation in the target UCMDB customer. There are no CIs existing in the target UCMDB customer. To guarantee CIs' global IDs remain unchanged after CIs are pushed from the UCMDB source customer to the target customer, you can use the UCMDB JMX console command for both customers, as shown below, and make sure that \"All\" is returned in each response. Migrate customizations Follow the instructions in this section to migrate customizations. Assumption You haven't enabled multi-tenant on the source UCMDB. Migrating customizations is only supported on the tenantless mode of UCMDB. All actions performed in the source UCMDB  require administrator privileges of UCMDB. Scenario A - When you have a limited amount of customized resources Persona Location Customer admin Source",
    "url": "pushdatabetweencmscustomers",
    "filename": "pushdatabetweencmscustomers",
    "headings": [
      "Prerequisite",
      "Migrate customizations",
      "Assumption",
      "Scenario A - When you have a limited amount of customized resources",
      "Scenario B - When you have lots of customized resources",
      "Task 1. Upgrade UD Content Pack (CP)",
      "Task 2. Maintain all customizations for out-of-the-box (OOTB) resources",
      "Task 3. Create a new package for new resources",
      "Task 4. Export all packages, including OOTB resources and new resources",
      "Migrate Native SACM federated CIs",
      "Assumptions",
      "Prepare the system before migration",
      "Create an integration point",
      "Run push job: 01 Node",
      "Run push job: 02 NodeElement",
      "Run push job: 03 Discovery",
      "Run push job: 04 Business",
      "Migrate remaining CIs"
    ],
    "keywords": [
      "2022.05",
      "xxx.zip",
      "tql.max",
      "CmdbToSaaSAdapter.zip",
      "_installation.zip",
      "push",
      "data",
      "source",
      "ucmdb",
      "customer",
      "target",
      "prerequisite",
      "migrate",
      "customizations",
      "assumption",
      "scenario",
      "limited",
      "amount",
      "customized",
      "resources",
      "lots",
      "task",
      "1.",
      "upgrade",
      "ud",
      "content",
      "pack",
      "cp",
      "2.",
      "maintain",
      "all",
      "out-of-the-box",
      "ootb",
      "3.",
      "create",
      "new",
      "package",
      "4.",
      "export",
      "packages",
      "including",
      "native",
      "sacm",
      "federated",
      "cis",
      "assumptions",
      "prepare",
      "system",
      "before",
      "migration",
      "integration",
      "point",
      "run",
      "job",
      "01",
      "node",
      "02",
      "nodeelement",
      "03",
      "discovery",
      "04",
      "business",
      "remaining",
      "topic",
      "describes",
      "sacm.",
      "make",
      "sure",
      "existing",
      "tenants",
      "created",
      "preparation",
      "customer.",
      "there",
      "guarantee",
      "global",
      "ids",
      "remain",
      "unchanged",
      "after",
      "pushed",
      "jmx",
      "console",
      "command",
      "both",
      "customers",
      "shown",
      "below",
      "returned",
      "response.",
      "follow",
      "instructions",
      "section",
      "customizations.",
      "haven",
      "enabled",
      "multi-tenant",
      "ucmdb.",
      "migrating",
      "supported"
    ],
    "language": "en",
    "word_count": 107,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "push data from the source ucmdb customer to the target ucmdb customer",
    "contentLower": "this topic describes how to migrate cis from the source ucmdb customer to the target ucmdb customer for native sacm. prerequisite make sure that the existing ud/ucmdb tenants in the source ucmdb customer are also created as preparation in the target ucmdb customer. there are no cis existing in the target ucmdb customer. to guarantee cis' global ids remain unchanged after cis are pushed from the ucmdb source customer to the target customer, you can use the ucmdb jmx console command for both customers, as shown below, and make sure that \"all\" is returned in each response. migrate customizations follow the instructions in this section to migrate customizations. assumption you haven't enabled multi-tenant on the source ucmdb. migrating customizations is only supported on the tenantless mode of ucmdb. all actions performed in the source ucmdb  require administrator privileges of ucmdb. scenario a - when you have a limited amount of customized resources persona location customer admin source",
    "keywordsLower": [
      "2022.05",
      "xxx.zip",
      "tql.max",
      "cmdbtosaasadapter.zip",
      "_installation.zip",
      "push",
      "data",
      "source",
      "ucmdb",
      "customer",
      "target",
      "prerequisite",
      "migrate",
      "customizations",
      "assumption",
      "scenario",
      "limited",
      "amount",
      "customized",
      "resources",
      "lots",
      "task",
      "1.",
      "upgrade",
      "ud",
      "content",
      "pack",
      "cp",
      "2.",
      "maintain",
      "all",
      "out-of-the-box",
      "ootb",
      "3.",
      "create",
      "new",
      "package",
      "4.",
      "export",
      "packages",
      "including",
      "native",
      "sacm",
      "federated",
      "cis",
      "assumptions",
      "prepare",
      "system",
      "before",
      "migration",
      "integration",
      "point",
      "run",
      "job",
      "01",
      "node",
      "02",
      "nodeelement",
      "03",
      "discovery",
      "04",
      "business",
      "remaining",
      "topic",
      "describes",
      "sacm.",
      "make",
      "sure",
      "existing",
      "tenants",
      "created",
      "preparation",
      "customer.",
      "there",
      "guarantee",
      "global",
      "ids",
      "remain",
      "unchanged",
      "after",
      "pushed",
      "jmx",
      "console",
      "command",
      "both",
      "customers",
      "shown",
      "below",
      "returned",
      "response.",
      "follow",
      "instructions",
      "section",
      "customizations.",
      "haven",
      "enabled",
      "multi-tenant",
      "ucmdb.",
      "migrating",
      "supported"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Maintain customizations for UD/UCMDB out-of-the-box resources",
    "content": "Persona Location Customer admin Source UCMDB Out-of-the-box (OOTB) resource types of class model - auto merged The following OOTB resource types of class model are auto merged during the Content Pack upgrade: Configuration Item Types Identification Relationships Calculated Relationships validlinks No manual maintenance work is required at all. Maintain customizations for OOTB resource types included in the Content Pack upgrade report For OOTB discovery resource types covered by the Content Pack upgrade report, you can leverage the upgrade report to compare and maintain the customization data. Go to <UCMDBServer>\\runtime\\log\\package_reports\\customer_1\\cpUpgrade\\defaultFolder, you should see folders named by the timestamps when you deploy the Content Packs, for example, 2021-12-12_15-16-33-864. Each of the <Timestamp> folders contains the following: Item Description backup.zip This package contains a backup copy of the following 4 types of resources only: discoveryConfigFiles EnricherSer",
    "url": "comparemergemaintaincustomizations",
    "filename": "comparemergemaintaincustomizations",
    "headings": [
      "Out-of-the-box (OOTB) resource types of class model - auto merged",
      "Maintain customizations for OOTB resource types included in the Content Pack upgrade report"
    ],
    "keywords": [
      "uducmdb",
      "backup.zip",
      "upgrade_report.csv",
      "EnricherServiceSettings.ini",
      "globalSettings.xml",
      "DDMInfra.zip",
      "globalFiltering.xml",
      "maintain",
      "customizations",
      "ud",
      "ucmdb",
      "out-of-the-box",
      "resources",
      "ootb",
      "resource",
      "types",
      "class",
      "model",
      "auto",
      "merged",
      "included",
      "content",
      "pack",
      "upgrade",
      "report",
      "persona",
      "location",
      "customer",
      "admin",
      "source",
      "following",
      "during",
      "configuration",
      "item",
      "identification",
      "relationships",
      "calculated",
      "validlinks",
      "manual",
      "maintenance",
      "work",
      "required",
      "all.",
      "discovery",
      "covered",
      "leverage",
      "compare",
      "customization",
      "data.",
      "go",
      "runtime",
      "log",
      "cpupgrade",
      "defaultfolder",
      "see",
      "folders",
      "named",
      "timestamps",
      "deploy",
      "packs",
      "example",
      "contains",
      "description",
      "package",
      "backup",
      "copy",
      "discoveryconfigfiles",
      "discoveryjobs",
      "discoverypatterns",
      "discoveryscripts.",
      "sub-folders",
      "installed",
      "currently",
      "in-use",
      "pack.",
      "customized",
      "resources.",
      "new",
      "deployed",
      "details.",
      "details",
      "about",
      "below.",
      "data",
      "find",
      "out",
      "packages",
      "require",
      "updates",
      "include",
      "open",
      "file",
      "apply",
      "filter.",
      "filter",
      "two",
      "columns",
      "equality",
      "status",
      "column"
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "maintain customizations for ud/ucmdb out-of-the-box resources",
    "contentLower": "persona location customer admin source ucmdb out-of-the-box (ootb) resource types of class model - auto merged the following ootb resource types of class model are auto merged during the content pack upgrade: configuration item types identification relationships calculated relationships validlinks no manual maintenance work is required at all. maintain customizations for ootb resource types included in the content pack upgrade report for ootb discovery resource types covered by the content pack upgrade report, you can leverage the upgrade report to compare and maintain the customization data. go to <ucmdbserver>\\runtime\\log\\package_reports\\customer_1\\cpupgrade\\defaultfolder, you should see folders named by the timestamps when you deploy the content packs, for example, 2021-12-12_15-16-33-864. each of the <timestamp> folders contains the following: item description backup.zip this package contains a backup copy of the following 4 types of resources only: discoveryconfigfiles enricherser",
    "keywordsLower": [
      "uducmdb",
      "backup.zip",
      "upgrade_report.csv",
      "enricherservicesettings.ini",
      "globalsettings.xml",
      "ddminfra.zip",
      "globalfiltering.xml",
      "maintain",
      "customizations",
      "ud",
      "ucmdb",
      "out-of-the-box",
      "resources",
      "ootb",
      "resource",
      "types",
      "class",
      "model",
      "auto",
      "merged",
      "included",
      "content",
      "pack",
      "upgrade",
      "report",
      "persona",
      "location",
      "customer",
      "admin",
      "source",
      "following",
      "during",
      "configuration",
      "item",
      "identification",
      "relationships",
      "calculated",
      "validlinks",
      "manual",
      "maintenance",
      "work",
      "required",
      "all.",
      "discovery",
      "covered",
      "leverage",
      "compare",
      "customization",
      "data.",
      "go",
      "runtime",
      "log",
      "cpupgrade",
      "defaultfolder",
      "see",
      "folders",
      "named",
      "timestamps",
      "deploy",
      "packs",
      "example",
      "contains",
      "description",
      "package",
      "backup",
      "copy",
      "discoveryconfigfiles",
      "discoveryjobs",
      "discoverypatterns",
      "discoveryscripts.",
      "sub-folders",
      "installed",
      "currently",
      "in-use",
      "pack.",
      "customized",
      "resources.",
      "new",
      "deployed",
      "details.",
      "details",
      "about",
      "below.",
      "data",
      "find",
      "out",
      "packages",
      "require",
      "updates",
      "include",
      "open",
      "file",
      "apply",
      "filter.",
      "filter",
      "two",
      "columns",
      "equality",
      "status",
      "column"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage groups",
    "content": "Adding groups helps to manage what roles and permissions can be assigned to its users. To manage groups, click an organization name, then click on the Groups tab. This page displays the group name and related roles. You can: Add: Click on the top-right menu to add a group. Enter the related values for Name, Display name, Description, Application, Associate role, and Associate users. The table below lists the detailed descriptions of the related group settings. Field Required Description Name true The name of this role. Display name true The display name of this group. Description no The description of this group Associate role no The associated role of this group. Associated Users no The associated users of this group Then click SAVE. Edit: Choose a group, then click on the top-right menu to edit an existing group. You can change the Display name, Description, Application, and Associate role. You can also change or update the associate group rules. You can also change the associated us",
    "url": "managegroups",
    "filename": "managegroups",
    "headings": [],
    "keywords": [
      "manage",
      "groups",
      "adding",
      "helps",
      "what",
      "roles",
      "permissions",
      "assigned",
      "users.",
      "click",
      "organization",
      "name",
      "tab.",
      "page",
      "displays",
      "group",
      "related",
      "roles.",
      "add",
      "top-right",
      "menu",
      "group.",
      "enter",
      "values",
      "display",
      "description",
      "application",
      "associate",
      "role",
      "table",
      "below",
      "lists",
      "detailed",
      "descriptions",
      "settings.",
      "field",
      "required",
      "true",
      "role.",
      "associated",
      "users",
      "save.",
      "edit",
      "choose",
      "existing",
      "change",
      "update",
      "rules.",
      "save",
      "edits.",
      "remove",
      "action",
      "button",
      "one",
      "rules",
      "new",
      "rule.",
      "following",
      "ldap",
      "ou",
      "calculated",
      "rule",
      "type",
      "ouor",
      "dn",
      "configuration.",
      "criteria",
      "key",
      "value",
      "match",
      "method.",
      "ok.",
      "combination",
      "strategy."
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage groups",
    "contentLower": "adding groups helps to manage what roles and permissions can be assigned to its users. to manage groups, click an organization name, then click on the groups tab. this page displays the group name and related roles. you can: add: click on the top-right menu to add a group. enter the related values for name, display name, description, application, associate role, and associate users. the table below lists the detailed descriptions of the related group settings. field required description name true the name of this role. display name true the display name of this group. description no the description of this group associate role no the associated role of this group. associated users no the associated users of this group then click save. edit: choose a group, then click on the top-right menu to edit an existing group. you can change the display name, description, application, and associate role. you can also change or update the associate group rules. you can also change the associated us",
    "keywordsLower": [
      "manage",
      "groups",
      "adding",
      "helps",
      "what",
      "roles",
      "permissions",
      "assigned",
      "users.",
      "click",
      "organization",
      "name",
      "tab.",
      "page",
      "displays",
      "group",
      "related",
      "roles.",
      "add",
      "top-right",
      "menu",
      "group.",
      "enter",
      "values",
      "display",
      "description",
      "application",
      "associate",
      "role",
      "table",
      "below",
      "lists",
      "detailed",
      "descriptions",
      "settings.",
      "field",
      "required",
      "true",
      "role.",
      "associated",
      "users",
      "save.",
      "edit",
      "choose",
      "existing",
      "change",
      "update",
      "rules.",
      "save",
      "edits.",
      "remove",
      "action",
      "button",
      "one",
      "rules",
      "new",
      "rule.",
      "following",
      "ldap",
      "ou",
      "calculated",
      "rule",
      "type",
      "ouor",
      "dn",
      "configuration.",
      "criteria",
      "key",
      "value",
      "match",
      "method.",
      "ok.",
      "combination",
      "strategy."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage roles in IdM Admin Portal",
    "content": "Roles added or updated in IdM won't be automatically replicated to Service Management. Therefore, it's better to manage roles in Service Management. However, you can use IdM Admin Portal to manage roles when: You must set up roles and use them at the farm level in the IdM sysbo organization for your capabilities. You don't use the 1 tier tenancy model. You don't need converged authorization. To open the IdM Admin Portal, go to the tenant's details page from Suite Administration and then click the IdM settings tab. The following roles are read-only. The only field that you can update is the Associated Permissions field. Roles added in Service Management, which are automatically replicated to IdM. Roles added in IdM, which meet these conditions: The roles' Application field is one of these values: SMAX, CMDB, DND, CGRO, CONTENTSTORE, OO, AUDIT. Note: The system doesn't prevent you from updating the Application field for these roles. Don't do this because it will result in data consistenc",
    "url": "manageroles",
    "filename": "manageroles",
    "headings": [],
    "keywords": [
      "manage",
      "roles",
      "idm",
      "admin",
      "portal",
      "added",
      "updated",
      "won",
      "automatically",
      "replicated",
      "service",
      "management.",
      "therefore",
      "better",
      "however",
      "set",
      "farm",
      "level",
      "sysbo",
      "organization",
      "capabilities.",
      "don",
      "tier",
      "tenancy",
      "model.",
      "need",
      "converged",
      "authorization.",
      "open",
      "go",
      "tenant",
      "details",
      "page",
      "suite",
      "administration",
      "click",
      "settings",
      "tab.",
      "following",
      "read-only.",
      "field",
      "update",
      "associated",
      "permissions",
      "field.",
      "management",
      "idm.",
      "meet",
      "conditions",
      "application",
      "one",
      "values",
      "smax",
      "cmdb",
      "dnd",
      "cgro",
      "contentstore",
      "oo",
      "audit.",
      "note",
      "system",
      "doesn",
      "prevent",
      "updating",
      "roles.",
      "because",
      "result",
      "data",
      "consistency",
      "between",
      "unexpected",
      "behaviors.",
      "gone",
      "through",
      "authorization",
      "migration",
      "process",
      "administration.",
      "align",
      "out-of-the-box",
      "audit",
      "applications",
      "currently",
      "locked",
      "try",
      "delete",
      "any",
      "global",
      "error",
      "message",
      "appear",
      "indicating",
      "action",
      "isn",
      "allowed.",
      "custom",
      "caution",
      "doing",
      "remove",
      "corresponding"
    ],
    "language": "en",
    "word_count": 109,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage roles in idm admin portal",
    "contentLower": "roles added or updated in idm won't be automatically replicated to service management. therefore, it's better to manage roles in service management. however, you can use idm admin portal to manage roles when: you must set up roles and use them at the farm level in the idm sysbo organization for your capabilities. you don't use the 1 tier tenancy model. you don't need converged authorization. to open the idm admin portal, go to the tenant's details page from suite administration and then click the idm settings tab. the following roles are read-only. the only field that you can update is the associated permissions field. roles added in service management, which are automatically replicated to idm. roles added in idm, which meet these conditions: the roles' application field is one of these values: smax, cmdb, dnd, cgro, contentstore, oo, audit. note: the system doesn't prevent you from updating the application field for these roles. don't do this because it will result in data consistenc",
    "keywordsLower": [
      "manage",
      "roles",
      "idm",
      "admin",
      "portal",
      "added",
      "updated",
      "won",
      "automatically",
      "replicated",
      "service",
      "management.",
      "therefore",
      "better",
      "however",
      "set",
      "farm",
      "level",
      "sysbo",
      "organization",
      "capabilities.",
      "don",
      "tier",
      "tenancy",
      "model.",
      "need",
      "converged",
      "authorization.",
      "open",
      "go",
      "tenant",
      "details",
      "page",
      "suite",
      "administration",
      "click",
      "settings",
      "tab.",
      "following",
      "read-only.",
      "field",
      "update",
      "associated",
      "permissions",
      "field.",
      "management",
      "idm.",
      "meet",
      "conditions",
      "application",
      "one",
      "values",
      "smax",
      "cmdb",
      "dnd",
      "cgro",
      "contentstore",
      "oo",
      "audit.",
      "note",
      "system",
      "doesn",
      "prevent",
      "updating",
      "roles.",
      "because",
      "result",
      "data",
      "consistency",
      "between",
      "unexpected",
      "behaviors.",
      "gone",
      "through",
      "authorization",
      "migration",
      "process",
      "administration.",
      "align",
      "out-of-the-box",
      "audit",
      "applications",
      "currently",
      "locked",
      "try",
      "delete",
      "any",
      "global",
      "error",
      "message",
      "appear",
      "indicating",
      "action",
      "isn",
      "allowed.",
      "custom",
      "caution",
      "doing",
      "remove",
      "corresponding"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Operation history",
    "content": "Operation history enables you to view the history of the following operations. Create tenant Deploy tenant Delete tenant Import tenant Export tenant Update tenant Deploy capability The failed jobs to sync updated managed tenants or managed accounts from Suite Administration to Service Management. You can filter the records by job ID or operation type. Note The system will clean the operation history after 7 days.",
    "url": "operationhistory",
    "filename": "operationhistory",
    "headings": [],
    "keywords": [
      "operation",
      "history",
      "enables",
      "view",
      "following",
      "operations.",
      "create",
      "tenant",
      "deploy",
      "delete",
      "import",
      "export",
      "update",
      "capability",
      "failed",
      "jobs",
      "sync",
      "updated",
      "managed",
      "tenants",
      "accounts",
      "suite",
      "administration",
      "service",
      "management.",
      "filter",
      "records",
      "job",
      "id",
      "type.",
      "note",
      "system",
      "clean",
      "after",
      "days."
    ],
    "language": "en",
    "word_count": 49,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "operation history",
    "contentLower": "operation history enables you to view the history of the following operations. create tenant deploy tenant delete tenant import tenant export tenant update tenant deploy capability the failed jobs to sync updated managed tenants or managed accounts from suite administration to service management. you can filter the records by job id or operation type. note the system will clean the operation history after 7 days.",
    "keywordsLower": [
      "operation",
      "history",
      "enables",
      "view",
      "following",
      "operations.",
      "create",
      "tenant",
      "deploy",
      "delete",
      "import",
      "export",
      "update",
      "capability",
      "failed",
      "jobs",
      "sync",
      "updated",
      "managed",
      "tenants",
      "accounts",
      "suite",
      "administration",
      "service",
      "management.",
      "filter",
      "records",
      "job",
      "id",
      "type.",
      "note",
      "system",
      "clean",
      "after",
      "days."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "PDF export",
    "content": "You can export PDF documents from Change, Incident, Request, and BYOA (customized application) records. Export to a PDF document Open a Change, an Incident, a Request, or a BYOA (customized application) record, and then click More > Export to PDF. The system opens the Export to PDF dialog. Select tabs for PDF export. If you select no tabs, the default is the General tab. See the following table for the candidate tabs. Module Tabs available for export Change General Schedule Plan and execute Targets Involved CIs Discussions User options Incident General Task plan Targets Involved CIs Discussions User options Request General Task plan Targets Involved CIs Discussions User options BYOA General Discussions Note: In the table above, user options isn't the name of a tab. Instead, it refers to a section in an offering or a model. A PDF document will include user options when it exists in a record. Select a layout type: Portrait or Landscape. Click OK to trigger the PDF generation. The system ",
    "url": "pdfexport",
    "filename": "pdfexport",
    "headings": [
      "Export to a PDF document",
      "Limitation",
      "Character output",
      "Right-to-left support",
      "Tabs",
      "Task plan",
      "Offering",
      "Involved CIs",
      "Discussions",
      "Format",
      "Data format",
      "Image",
      "Rich text",
      "Enum set",
      "Table"
    ],
    "keywords": [
      "pdf",
      "export",
      "document",
      "limitation",
      "character",
      "output",
      "right-to-left",
      "support",
      "tabs",
      "task",
      "plan",
      "offering",
      "involved",
      "cis",
      "discussions",
      "format",
      "data",
      "image",
      "rich",
      "text",
      "enum",
      "set",
      "table",
      "documents",
      "change",
      "incident",
      "request",
      "byoa",
      "customized",
      "application",
      "records.",
      "open",
      "record",
      "click",
      "pdf.",
      "system",
      "opens",
      "dialog.",
      "select",
      "export.",
      "default",
      "general",
      "tab.",
      "see",
      "following",
      "candidate",
      "tabs.",
      "module",
      "available",
      "schedule",
      "execute",
      "targets",
      "user",
      "options",
      "note",
      "above",
      "isn",
      "name",
      "instead",
      "refers",
      "section",
      "model.",
      "include",
      "exists",
      "record.",
      "layout",
      "type",
      "portrait",
      "landscape.",
      "ok",
      "trigger",
      "generation.",
      "starts",
      "downloading",
      "after",
      "generating",
      "successfully.",
      "displays",
      "same",
      "sequence",
      "left",
      "side.",
      "backend",
      "reject",
      "queue",
      "full",
      "prevent",
      "overload.",
      "case",
      "receive",
      "rejected",
      "error",
      "message",
      "wait",
      "time",
      "try",
      "again.",
      "release",
      "function",
      "limitations."
    ],
    "language": "en",
    "word_count": 115,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "pdf export",
    "contentLower": "you can export pdf documents from change, incident, request, and byoa (customized application) records. export to a pdf document open a change, an incident, a request, or a byoa (customized application) record, and then click more > export to pdf. the system opens the export to pdf dialog. select tabs for pdf export. if you select no tabs, the default is the general tab. see the following table for the candidate tabs. module tabs available for export change general schedule plan and execute targets involved cis discussions user options incident general task plan targets involved cis discussions user options request general task plan targets involved cis discussions user options byoa general discussions note: in the table above, user options isn't the name of a tab. instead, it refers to a section in an offering or a model. a pdf document will include user options when it exists in a record. select a layout type: portrait or landscape. click ok to trigger the pdf generation. the system ",
    "keywordsLower": [
      "pdf",
      "export",
      "document",
      "limitation",
      "character",
      "output",
      "right-to-left",
      "support",
      "tabs",
      "task",
      "plan",
      "offering",
      "involved",
      "cis",
      "discussions",
      "format",
      "data",
      "image",
      "rich",
      "text",
      "enum",
      "set",
      "table",
      "documents",
      "change",
      "incident",
      "request",
      "byoa",
      "customized",
      "application",
      "records.",
      "open",
      "record",
      "click",
      "pdf.",
      "system",
      "opens",
      "dialog.",
      "select",
      "export.",
      "default",
      "general",
      "tab.",
      "see",
      "following",
      "candidate",
      "tabs.",
      "module",
      "available",
      "schedule",
      "execute",
      "targets",
      "user",
      "options",
      "note",
      "above",
      "isn",
      "name",
      "instead",
      "refers",
      "section",
      "model.",
      "include",
      "exists",
      "record.",
      "layout",
      "type",
      "portrait",
      "landscape.",
      "ok",
      "trigger",
      "generation.",
      "starts",
      "downloading",
      "after",
      "generating",
      "successfully.",
      "displays",
      "same",
      "sequence",
      "left",
      "side.",
      "backend",
      "reject",
      "queue",
      "full",
      "prevent",
      "overload.",
      "case",
      "receive",
      "rejected",
      "error",
      "message",
      "wait",
      "time",
      "try",
      "again.",
      "release",
      "function",
      "limitations."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform a full reindex for Smart Analytics",
    "content": "You need to perform a full reindex after you update the Smart Analytics stop words, synonyms, or IDOL configure file. The whole process may take hours depending on your data size and the hardware capacity (CPU, memory, and storage I/O). The search functionality is temporarily unavailable during the reindex process, users are informed with the reindex progress if they perform a search in the following locations when the reindex isn't completed: Search in Service Portal Global search in Service Management Search in live support Search when adding related records Prerequisite To run the reindex script, ensure that the curl command is available in the current Linux environment. Additionally, the OpenSSL library version used by curl must be 1.1.1 or higher to support TLS 1.3, which is required for API communication. You can use the curl -V command to check both the curl version and its OpenSSL library version. Perform a full reindex To perform a full reindex for your suite, follow these ste",
    "url": "fullreindex",
    "filename": "fullreindex",
    "headings": [
      "Prerequisite",
      "Perform a full reindex",
      "Other reindex scenarios"
    ],
    "keywords": [
      "dummy.com",
      "maas_reindex.log",
      "reindex.sh",
      "1.3",
      "1.1.1",
      "ng.com",
      "perform",
      "full",
      "reindex",
      "smart",
      "analytics",
      "prerequisite",
      "scenarios",
      "need",
      "after",
      "update",
      "stop",
      "words",
      "synonyms",
      "idol",
      "configure",
      "file.",
      "whole",
      "process",
      "take",
      "hours",
      "depending",
      "data",
      "size",
      "hardware",
      "capacity",
      "cpu",
      "memory",
      "storage",
      "search",
      "functionality",
      "temporarily",
      "unavailable",
      "during",
      "users",
      "informed",
      "progress",
      "following",
      "locations",
      "isn",
      "completed",
      "service",
      "portal",
      "global",
      "management",
      "live",
      "support",
      "adding",
      "related",
      "records",
      "run",
      "script",
      "ensure",
      "curl",
      "command",
      "available",
      "current",
      "linux",
      "environment.",
      "additionally",
      "openssl",
      "library",
      "version",
      "higher",
      "tls",
      "required",
      "api",
      "communication.",
      "-v",
      "check",
      "both",
      "version.",
      "suite",
      "follow",
      "steps",
      "download",
      "unzip",
      "included",
      "sma",
      "operation",
      "toolkit",
      "folder",
      "environment",
      "properly",
      "configured",
      "resolve",
      "fqdn.",
      "navigate",
      "sh",
      "-t",
      "id",
      "tenant",
      "want",
      "reindex.",
      "example"
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform a full reindex for smart analytics",
    "contentLower": "you need to perform a full reindex after you update the smart analytics stop words, synonyms, or idol configure file. the whole process may take hours depending on your data size and the hardware capacity (cpu, memory, and storage i/o). the search functionality is temporarily unavailable during the reindex process, users are informed with the reindex progress if they perform a search in the following locations when the reindex isn't completed: search in service portal global search in service management search in live support search when adding related records prerequisite to run the reindex script, ensure that the curl command is available in the current linux environment. additionally, the openssl library version used by curl must be 1.1.1 or higher to support tls 1.3, which is required for api communication. you can use the curl -v command to check both the curl version and its openssl library version. perform a full reindex to perform a full reindex for your suite, follow these ste",
    "keywordsLower": [
      "dummy.com",
      "maas_reindex.log",
      "reindex.sh",
      "1.3",
      "1.1.1",
      "ng.com",
      "perform",
      "full",
      "reindex",
      "smart",
      "analytics",
      "prerequisite",
      "scenarios",
      "need",
      "after",
      "update",
      "stop",
      "words",
      "synonyms",
      "idol",
      "configure",
      "file.",
      "whole",
      "process",
      "take",
      "hours",
      "depending",
      "data",
      "size",
      "hardware",
      "capacity",
      "cpu",
      "memory",
      "storage",
      "search",
      "functionality",
      "temporarily",
      "unavailable",
      "during",
      "users",
      "informed",
      "progress",
      "following",
      "locations",
      "isn",
      "completed",
      "service",
      "portal",
      "global",
      "management",
      "live",
      "support",
      "adding",
      "related",
      "records",
      "run",
      "script",
      "ensure",
      "curl",
      "command",
      "available",
      "current",
      "linux",
      "environment.",
      "additionally",
      "openssl",
      "library",
      "version",
      "higher",
      "tls",
      "required",
      "api",
      "communication.",
      "-v",
      "check",
      "both",
      "version.",
      "suite",
      "follow",
      "steps",
      "download",
      "unzip",
      "included",
      "sma",
      "operation",
      "toolkit",
      "folder",
      "environment",
      "properly",
      "configured",
      "resolve",
      "fqdn.",
      "navigate",
      "sh",
      "-t",
      "id",
      "tenant",
      "want",
      "reindex.",
      "example"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Mapping records created from a change record",
    "content": "The following tables list those fields and associations whose contents are, by default, copied to a record created from a change record. Change record to new change model Change record field Remediation plan Build and test required Category Change type Description Emergency Impact Implementation plan Owning group Owner Reason for change Risk Service Change record to new change template Change record field Remediation plan Build and test required Category Change type Description Emergency Impact Implementation plan Owning group Owner Reason for change Risk Service Change record to new change record Change record field Category Data domains Device affected by change Impact Owning group Service System element affected by change Urgency Change record cloned to new change record Category Change record field or section Regular fields Description Title (append “ - Copy”) Service Category Reason for Change Emergency Justification Urgency Change model Scheduled duration Scheduled DT duration Us",
    "url": "fmchange",
    "filename": "fmchange",
    "headings": [
      "Change record to new change model",
      "Change record to new change template",
      "Change record to new change record",
      "Change record cloned to new change record",
      "Change record to new incident record",
      "Change record to new knowledge article record",
      "Change record to new news article record",
      "Change record to new release record",
      "Related topics"
    ],
    "keywords": [
      "mapping",
      "records",
      "created",
      "change",
      "record",
      "new",
      "model",
      "template",
      "cloned",
      "incident",
      "knowledge",
      "article",
      "news",
      "release",
      "related",
      "topics",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "field",
      "remediation",
      "plan",
      "build",
      "test",
      "required",
      "category",
      "type",
      "description",
      "emergency",
      "impact",
      "implementation",
      "owning",
      "group",
      "owner",
      "reason",
      "risk",
      "service",
      "data",
      "domains",
      "device",
      "affected",
      "system",
      "element",
      "urgency",
      "section",
      "regular",
      "title",
      "append",
      "copy",
      "justification",
      "scheduled",
      "duration",
      "dt",
      "user",
      "options",
      "name",
      "cis",
      "many2many",
      "relations",
      "iinfrastructure",
      "peripheral",
      "license",
      "task",
      "plans",
      "tasks",
      "execution",
      "approve",
      "deployment",
      "remediate",
      "cmdb",
      "update",
      "review",
      "ecab",
      "execute",
      "content",
      "actual",
      "service.containment",
      "start",
      "event",
      "end",
      "until",
      "problem",
      "request",
      "definition",
      "idea"
    ],
    "language": "en",
    "word_count": 128,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "mapping records created from a change record",
    "contentLower": "the following tables list those fields and associations whose contents are, by default, copied to a record created from a change record. change record to new change model change record field remediation plan build and test required category change type description emergency impact implementation plan owning group owner reason for change risk service change record to new change template change record field remediation plan build and test required category change type description emergency impact implementation plan owning group owner reason for change risk service change record to new change record change record field category data domains device affected by change impact owning group service system element affected by change urgency change record cloned to new change record category change record field or section regular fields description title (append “ - copy”) service category reason for change emergency justification urgency change model scheduled duration scheduled dt duration us",
    "keywordsLower": [
      "mapping",
      "records",
      "created",
      "change",
      "record",
      "new",
      "model",
      "template",
      "cloned",
      "incident",
      "knowledge",
      "article",
      "news",
      "release",
      "related",
      "topics",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "field",
      "remediation",
      "plan",
      "build",
      "test",
      "required",
      "category",
      "type",
      "description",
      "emergency",
      "impact",
      "implementation",
      "owning",
      "group",
      "owner",
      "reason",
      "risk",
      "service",
      "data",
      "domains",
      "device",
      "affected",
      "system",
      "element",
      "urgency",
      "section",
      "regular",
      "title",
      "append",
      "copy",
      "justification",
      "scheduled",
      "duration",
      "dt",
      "user",
      "options",
      "name",
      "cis",
      "many2many",
      "relations",
      "iinfrastructure",
      "peripheral",
      "license",
      "task",
      "plans",
      "tasks",
      "execution",
      "approve",
      "deployment",
      "remediate",
      "cmdb",
      "update",
      "review",
      "ecab",
      "execute",
      "content",
      "actual",
      "service.containment",
      "start",
      "event",
      "end",
      "until",
      "problem",
      "request",
      "definition",
      "idea"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Mapping records created from an incident record",
    "content": "The following tables list those fields and associations whose contents are, by default, copied to a record created from an incident record. Incident record to new incident model Incident record field Assignee Assignment group Case exchange Case exchange external operation Category Completion code Description First touch Impact Knowledge candidate Location Major incident team Owner Problem candidate Service Service desk group Solution Status Title Urgency Incident record to new incident template Incident record field Assignee Assignment group Case exchange Case exchange external operation Category Completion code Description First touch Impact Knowledge candidate Location Major incident team Owner Problem candidate Service Service desk group Solution Status Title Urgency Incident record to new incident record Incident record field Category Data domains Description Device affected by incident Enterprise asset affected by incident Impact Service Solution System element affected by inciden",
    "url": "fmincident",
    "filename": "fmincident",
    "headings": [
      "Incident record to new incident model",
      "Incident record to new incident template",
      "Incident record to new incident record",
      "Incident record to new change record",
      "Incident record to new problem record",
      "Incident record to new request record",
      "Incident record to new knowledge article record",
      "Incident record to new news article record",
      "Related topics"
    ],
    "keywords": [
      "mapping",
      "records",
      "created",
      "incident",
      "record",
      "new",
      "model",
      "template",
      "change",
      "problem",
      "request",
      "knowledge",
      "article",
      "news",
      "related",
      "topics",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "field",
      "assignee",
      "assignment",
      "group",
      "case",
      "exchange",
      "external",
      "operation",
      "category",
      "completion",
      "code",
      "description",
      "first",
      "touch",
      "impact",
      "candidate",
      "location",
      "major",
      "team",
      "owner",
      "service",
      "desk",
      "solution",
      "status",
      "title",
      "urgency",
      "data",
      "domains",
      "device",
      "affected",
      "enterprise",
      "asset",
      "system",
      "element",
      "workaround",
      "resolution",
      "content",
      "actual",
      "service.containment",
      "create",
      "time",
      "event",
      "expected",
      "until",
      "definition",
      "idea"
    ],
    "language": "en",
    "word_count": 126,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "mapping records created from an incident record",
    "contentLower": "the following tables list those fields and associations whose contents are, by default, copied to a record created from an incident record. incident record to new incident model incident record field assignee assignment group case exchange case exchange external operation category completion code description first touch impact knowledge candidate location major incident team owner problem candidate service service desk group solution status title urgency incident record to new incident template incident record field assignee assignment group case exchange case exchange external operation category completion code description first touch impact knowledge candidate location major incident team owner problem candidate service service desk group solution status title urgency incident record to new incident record incident record field category data domains description device affected by incident enterprise asset affected by incident impact service solution system element affected by inciden",
    "keywordsLower": [
      "mapping",
      "records",
      "created",
      "incident",
      "record",
      "new",
      "model",
      "template",
      "change",
      "problem",
      "request",
      "knowledge",
      "article",
      "news",
      "related",
      "topics",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "field",
      "assignee",
      "assignment",
      "group",
      "case",
      "exchange",
      "external",
      "operation",
      "category",
      "completion",
      "code",
      "description",
      "first",
      "touch",
      "impact",
      "candidate",
      "location",
      "major",
      "team",
      "owner",
      "service",
      "desk",
      "solution",
      "status",
      "title",
      "urgency",
      "data",
      "domains",
      "device",
      "affected",
      "enterprise",
      "asset",
      "system",
      "element",
      "workaround",
      "resolution",
      "content",
      "actual",
      "service.containment",
      "create",
      "time",
      "event",
      "expected",
      "until",
      "definition",
      "idea"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Mapping records created from a problem record",
    "content": "The following tables list those fields and associations whose contents are, by default, copied to a record created from a problem record. Problem record to new problem template Problem record field Category Deferral code Estimated cost Estimated person days Impact Known error Owner Owning group Priority Process ID recorded by Root cause Service Solution Status Symptoms Workaround Problem record to new change record Problem record field Change record field Category Category Data domains Data domains Device affected by problem Device affected by change Service Service Solution Description Symptoms Justification System element affected by problem System element affected by change Urgency Urgency Problem record to new knowledge article record Problem record field Article record field Title Title Workaround + Root cause + Solution Article Content Actual service.Containment Service Problem record to new news article record Problem record field Article record field Title Title Description Art",
    "url": "fmproblem",
    "filename": "fmproblem",
    "headings": [
      "Problem record to new problem template",
      "Problem record to new change record",
      "Problem record to new knowledge article record",
      "Problem record to new news article record",
      "Related topics"
    ],
    "keywords": [
      "mapping",
      "records",
      "created",
      "problem",
      "record",
      "new",
      "template",
      "change",
      "knowledge",
      "article",
      "news",
      "related",
      "topics",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "field",
      "category",
      "deferral",
      "code",
      "estimated",
      "cost",
      "person",
      "days",
      "impact",
      "known",
      "error",
      "owner",
      "owning",
      "group",
      "priority",
      "process",
      "id",
      "recorded",
      "root",
      "cause",
      "service",
      "solution",
      "status",
      "symptoms",
      "workaround",
      "data",
      "domains",
      "device",
      "affected",
      "description",
      "justification",
      "system",
      "element",
      "urgency",
      "title",
      "content",
      "actual",
      "service.containment",
      "create",
      "time",
      "event",
      "incident",
      "request",
      "definition",
      "idea"
    ],
    "language": "en",
    "word_count": 129,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "mapping records created from a problem record",
    "contentLower": "the following tables list those fields and associations whose contents are, by default, copied to a record created from a problem record. problem record to new problem template problem record field category deferral code estimated cost estimated person days impact known error owner owning group priority process id recorded by root cause service solution status symptoms workaround problem record to new change record problem record field change record field category category data domains data domains device affected by problem device affected by change service service solution description symptoms justification system element affected by problem system element affected by change urgency urgency problem record to new knowledge article record problem record field article record field title title workaround + root cause + solution article content actual service.containment service problem record to new news article record problem record field article record field title title description art",
    "keywordsLower": [
      "mapping",
      "records",
      "created",
      "problem",
      "record",
      "new",
      "template",
      "change",
      "knowledge",
      "article",
      "news",
      "related",
      "topics",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "field",
      "category",
      "deferral",
      "code",
      "estimated",
      "cost",
      "person",
      "days",
      "impact",
      "known",
      "error",
      "owner",
      "owning",
      "group",
      "priority",
      "process",
      "id",
      "recorded",
      "root",
      "cause",
      "service",
      "solution",
      "status",
      "symptoms",
      "workaround",
      "data",
      "domains",
      "device",
      "affected",
      "description",
      "justification",
      "system",
      "element",
      "urgency",
      "title",
      "content",
      "actual",
      "service.containment",
      "create",
      "time",
      "event",
      "incident",
      "request",
      "definition",
      "idea"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Mapping records created from a request record",
    "content": "The following tables list those fields and associations whose contents are, by default, copied to a record created from a request record. Request record to new incident record Request record field Incident record field Category Category Data domains Data domains Description Description Device affected by request Device affected by incident Enterprise asset affected by request Enterprise asset affected by incident Expected resolution time Expected resolution time Help desk group Service desk group Impact Impact Priority Priority Report location Location Requested by Reported by Service Service System element affected by request System element affected by incident Title Title Urgency Urgency Request record to new article record Request record field Article record field Title Title Solution Article Content Actual service.Containment Service Request record to new idea record Idea record field Change record field Title Title Reported by Created by Description Description Request record to n",
    "url": "fmrequest",
    "filename": "fmrequest",
    "headings": [
      "Request record to new incident record",
      "Request record to new article record",
      "Request record to new idea record",
      "Request record to new change record",
      "Request record to new request record",
      "Related topics"
    ],
    "keywords": [
      "mapping",
      "records",
      "created",
      "request",
      "record",
      "new",
      "incident",
      "article",
      "idea",
      "change",
      "related",
      "topics",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "field",
      "category",
      "data",
      "domains",
      "description",
      "device",
      "affected",
      "enterprise",
      "asset",
      "expected",
      "resolution",
      "time",
      "help",
      "desk",
      "group",
      "service",
      "impact",
      "priority",
      "report",
      "location",
      "requested",
      "reported",
      "system",
      "element",
      "title",
      "urgency",
      "solution",
      "content",
      "actual",
      "service.containment",
      "preferred",
      "contact",
      "method",
      "infrastructure",
      "peripheral",
      "subscription",
      "problem",
      "definition"
    ],
    "language": "en",
    "word_count": 125,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "mapping records created from a request record",
    "contentLower": "the following tables list those fields and associations whose contents are, by default, copied to a record created from a request record. request record to new incident record request record field incident record field category category data domains data domains description description device affected by request device affected by incident enterprise asset affected by request enterprise asset affected by incident expected resolution time expected resolution time help desk group service desk group impact impact priority priority report location location requested by reported by service service system element affected by request system element affected by incident title title urgency urgency request record to new article record request record field article record field title title solution article content actual service.containment service request record to new idea record idea record field change record field title title reported by created by description description request record to n",
    "keywordsLower": [
      "mapping",
      "records",
      "created",
      "request",
      "record",
      "new",
      "incident",
      "article",
      "idea",
      "change",
      "related",
      "topics",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "field",
      "category",
      "data",
      "domains",
      "description",
      "device",
      "affected",
      "enterprise",
      "asset",
      "expected",
      "resolution",
      "time",
      "help",
      "desk",
      "group",
      "service",
      "impact",
      "priority",
      "report",
      "location",
      "requested",
      "reported",
      "system",
      "element",
      "title",
      "urgency",
      "solution",
      "content",
      "actual",
      "service.containment",
      "preferred",
      "contact",
      "method",
      "infrastructure",
      "peripheral",
      "subscription",
      "problem",
      "definition"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Mapping records created from a service definition record",
    "content": "The following tables list those fields and associations whose contents are, by default, copied to a record created from a service definition record. Service definition record to new article or news record Change record field Article record field Description Article Content Title Title ID Service Related topics Mapping records created from a change record Mapping records created from an incident record Mapping records created from a problem record Mapping records created from a request record Mapping records created from an idea record",
    "url": "fmsrvcdfntn",
    "filename": "fmsrvcdfntn",
    "headings": [
      "Service definition record to new article or news record",
      "Related topics"
    ],
    "keywords": [
      "mapping",
      "records",
      "created",
      "service",
      "definition",
      "record",
      "new",
      "article",
      "news",
      "related",
      "topics",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "change",
      "field",
      "description",
      "content",
      "title",
      "id",
      "incident",
      "problem",
      "request",
      "idea"
    ],
    "language": "en",
    "word_count": 67,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "mapping records created from a service definition record",
    "contentLower": "the following tables list those fields and associations whose contents are, by default, copied to a record created from a service definition record. service definition record to new article or news record change record field article record field description article content title title id service related topics mapping records created from a change record mapping records created from an incident record mapping records created from a problem record mapping records created from a request record mapping records created from an idea record",
    "keywordsLower": [
      "mapping",
      "records",
      "created",
      "service",
      "definition",
      "record",
      "new",
      "article",
      "news",
      "related",
      "topics",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "change",
      "field",
      "description",
      "content",
      "title",
      "id",
      "incident",
      "problem",
      "request",
      "idea"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Mapping records created from an idea record",
    "content": "The following tables list those fields and associations whose contents are, by default, copied to a record created from an idea record. Idea record to new proposal record Idea record field Proposal record field Title Title Description Description Idea record to new change record Idea record field Change record field Title Title Created by Reported by Description Description",
    "url": "fmidea",
    "filename": "fmidea",
    "headings": [
      "Idea record to new proposal record",
      "Idea record to new change record"
    ],
    "keywords": [
      "mapping",
      "records",
      "created",
      "idea",
      "record",
      "new",
      "proposal",
      "change",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "field",
      "title",
      "description",
      "reported"
    ],
    "language": "en",
    "word_count": 50,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "mapping records created from an idea record",
    "contentLower": "the following tables list those fields and associations whose contents are, by default, copied to a record created from an idea record. idea record to new proposal record idea record field proposal record field title title description description idea record to new change record idea record field change record field title title created by reported by description description",
    "keywordsLower": [
      "mapping",
      "records",
      "created",
      "idea",
      "record",
      "new",
      "proposal",
      "change",
      "following",
      "tables",
      "list",
      "fields",
      "associations",
      "whose",
      "contents",
      "default",
      "copied",
      "record.",
      "field",
      "title",
      "description",
      "reported"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Processes and Rules",
    "content": "The Processes and Rules tab enables you to edit processes, and to add and delete business rules for record types, processes, metaphases, and phases. To access the Processes and Rules tab, from the main menu, select Administration > Configuration > Studio > Processes and Rules. The tab displays the processes and rules for the record type selected. Each record type has at least one out-of-the-box process. You can edit a process by adding or deleting phases and transitions. However, the number and names of the metaphases are fixed for each record. The tree on the left side displays the record type, its process(es), metaphases, and phases. When you select the record type at the top of the tree, the main pane displays the Rules tab for the record type. When you select processes, metaphases, and phases, the main pane displays a workflow map and the right pane displays the Rules and Properties tabs for the selected item. You can click the arrow button to toggle between hiding and displaying t",
    "url": "processes",
    "filename": "processes",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "processes",
      "rules",
      "related",
      "topics",
      "tab",
      "enables",
      "edit",
      "add",
      "delete",
      "business",
      "record",
      "types",
      "metaphases",
      "phases.",
      "access",
      "main",
      "menu",
      "select",
      "administration",
      "configuration",
      "studio",
      "rules.",
      "displays",
      "type",
      "selected.",
      "least",
      "one",
      "out-of-the-box",
      "process.",
      "process",
      "adding",
      "deleting",
      "phases",
      "transitions.",
      "however",
      "number",
      "names",
      "fixed",
      "record.",
      "tree",
      "left",
      "side",
      "es",
      "top",
      "pane",
      "type.",
      "workflow",
      "map",
      "right",
      "properties",
      "tabs",
      "selected",
      "item.",
      "click",
      "arrow",
      "button",
      "toggle",
      "between",
      "hiding",
      "displaying",
      "map.",
      "comparison",
      "feature",
      "compare",
      "latest",
      "existing",
      "system.",
      "information",
      "see",
      "comparison.",
      "following",
      "aren",
      "customizable",
      "agreement",
      "cart",
      "item",
      "location",
      "optimization",
      "target",
      "set",
      "task",
      "delegation",
      "time",
      "period",
      "working",
      "phase",
      "transition",
      "move",
      "rule",
      "remove",
      "disable",
      "descriptions",
      "tags"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "processes and rules",
    "contentLower": "the processes and rules tab enables you to edit processes, and to add and delete business rules for record types, processes, metaphases, and phases. to access the processes and rules tab, from the main menu, select administration > configuration > studio > processes and rules. the tab displays the processes and rules for the record type selected. each record type has at least one out-of-the-box process. you can edit a process by adding or deleting phases and transitions. however, the number and names of the metaphases are fixed for each record. the tree on the left side displays the record type, its process(es), metaphases, and phases. when you select the record type at the top of the tree, the main pane displays the rules tab for the record type. when you select processes, metaphases, and phases, the main pane displays a workflow map and the right pane displays the rules and properties tabs for the selected item. you can click the arrow button to toggle between hiding and displaying t",
    "keywordsLower": [
      "processes",
      "rules",
      "related",
      "topics",
      "tab",
      "enables",
      "edit",
      "add",
      "delete",
      "business",
      "record",
      "types",
      "metaphases",
      "phases.",
      "access",
      "main",
      "menu",
      "select",
      "administration",
      "configuration",
      "studio",
      "rules.",
      "displays",
      "type",
      "selected.",
      "least",
      "one",
      "out-of-the-box",
      "process.",
      "process",
      "adding",
      "deleting",
      "phases",
      "transitions.",
      "however",
      "number",
      "names",
      "fixed",
      "record.",
      "tree",
      "left",
      "side",
      "es",
      "top",
      "pane",
      "type.",
      "workflow",
      "map",
      "right",
      "properties",
      "tabs",
      "selected",
      "item.",
      "click",
      "arrow",
      "button",
      "toggle",
      "between",
      "hiding",
      "displaying",
      "map.",
      "comparison",
      "feature",
      "compare",
      "latest",
      "existing",
      "system.",
      "information",
      "see",
      "comparison.",
      "following",
      "aren",
      "customizable",
      "agreement",
      "cart",
      "item",
      "location",
      "optimization",
      "target",
      "set",
      "task",
      "delegation",
      "time",
      "period",
      "working",
      "phase",
      "transition",
      "move",
      "rule",
      "remove",
      "disable",
      "descriptions",
      "tags"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Move a phase or transition in a process",
    "content": "The following tasks describe how to move a phase or transition within a process in Studio. Note If you have development and production tenants, all configuration changes must be made on the development tenant. For more information about synchronizing the tenants, see Dev2Prod - How to synchronize your development and production tenants. Caution When moving a transition or a phase, make sure that the move will not result in an unsupported transition scenario. The following diagram depicts the unsupported scenarios where phases A and B are in the same metaphase. Move a phase In the main menu, select Administration > Configuration > Studio. Select the record type from the drop-down box at the top of the screen and select the Processes and Rules tab. Select a process. Select a phase in the process and click the Move icon . Alternatively, select a phase in the tree on the left, click Edit and click the Move icon . In the Move phase dialog box, select the required metaphase from the drop-dow",
    "url": "movephase",
    "filename": "movephase",
    "headings": [
      "Move a phase",
      "Move a transition",
      "Related topics"
    ],
    "keywords": [
      "move",
      "phase",
      "transition",
      "process",
      "related",
      "topics",
      "following",
      "tasks",
      "describe",
      "studio.",
      "note",
      "development",
      "production",
      "tenants",
      "all",
      "configuration",
      "changes",
      "made",
      "tenant.",
      "information",
      "about",
      "synchronizing",
      "see",
      "dev2prod",
      "synchronize",
      "tenants.",
      "caution",
      "moving",
      "make",
      "sure",
      "result",
      "unsupported",
      "scenario.",
      "diagram",
      "depicts",
      "scenarios",
      "phases",
      "same",
      "metaphase.",
      "main",
      "menu",
      "select",
      "administration",
      "record",
      "type",
      "drop-down",
      "box",
      "top",
      "screen",
      "processes",
      "rules",
      "tab.",
      "process.",
      "click",
      "icon",
      "alternatively",
      "tree",
      "left",
      "edit",
      "dialog",
      "required",
      "metaphase",
      "list",
      "ok.",
      "properties",
      "tab",
      "right",
      "list.",
      "save",
      "changes.",
      "lists.",
      "want",
      "change",
      "direction",
      "button",
      "transitions",
      "while",
      "comparison",
      "mode.",
      "switch",
      "standard",
      "mode",
      "transitions.",
      "add"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "move a phase or transition in a process",
    "contentLower": "the following tasks describe how to move a phase or transition within a process in studio. note if you have development and production tenants, all configuration changes must be made on the development tenant. for more information about synchronizing the tenants, see dev2prod - how to synchronize your development and production tenants. caution when moving a transition or a phase, make sure that the move will not result in an unsupported transition scenario. the following diagram depicts the unsupported scenarios where phases a and b are in the same metaphase. move a phase in the main menu, select administration > configuration > studio. select the record type from the drop-down box at the top of the screen and select the processes and rules tab. select a process. select a phase in the process and click the move icon . alternatively, select a phase in the tree on the left, click edit and click the move icon . in the move phase dialog box, select the required metaphase from the drop-dow",
    "keywordsLower": [
      "move",
      "phase",
      "transition",
      "process",
      "related",
      "topics",
      "following",
      "tasks",
      "describe",
      "studio.",
      "note",
      "development",
      "production",
      "tenants",
      "all",
      "configuration",
      "changes",
      "made",
      "tenant.",
      "information",
      "about",
      "synchronizing",
      "see",
      "dev2prod",
      "synchronize",
      "tenants.",
      "caution",
      "moving",
      "make",
      "sure",
      "result",
      "unsupported",
      "scenario.",
      "diagram",
      "depicts",
      "scenarios",
      "phases",
      "same",
      "metaphase.",
      "main",
      "menu",
      "select",
      "administration",
      "record",
      "type",
      "drop-down",
      "box",
      "top",
      "screen",
      "processes",
      "rules",
      "tab.",
      "process.",
      "click",
      "icon",
      "alternatively",
      "tree",
      "left",
      "edit",
      "dialog",
      "required",
      "metaphase",
      "list",
      "ok.",
      "properties",
      "tab",
      "right",
      "list.",
      "save",
      "changes.",
      "lists.",
      "want",
      "change",
      "direction",
      "button",
      "transitions",
      "while",
      "comparison",
      "mode.",
      "switch",
      "standard",
      "mode",
      "transitions.",
      "add"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Make an entity read only after it has been closed",
    "content": "There are multiple ways to make an entity read-only, below are a few examples of different ways it can be done. Option 1 Add the validation rule ${current_update.PhaseId.IsChanged} to the ‘Done’ meta phase of the request entity workflow. This option won’t allow the request to be updated under any circumstances after it reaches the ‘Done’ phase. Option 2 Add the validation rule ${current_update.PhaseId.IsChangedcurrent_user.IsPermitted('EMS-Admin-Template(entityType=Request)')} to the ‘Done’ phase of the request entity workflow. This rule won’t allow the entity to be updated if it is in the ‘Done’ meta phase unless the currently logged in user has admin rights defined in their role. Option 3 Add the validation rule ${current_update.PhaseId.IsChangedcurrent_user.IsPermitted('EMS-Admin-Template(entityType=Request)')(now() - entity.CloseTime)<3600000)} to the ‘Done’ phase of the request entity workflow. This rule won’t allow the entity to be updated if it is in the ‘Done’ step unless the c",
    "url": "xcbreexample3",
    "filename": "xcbreexample3",
    "headings": [],
    "keywords": [
      "make",
      "entity",
      "read",
      "after",
      "closed",
      "there",
      "multiple",
      "ways",
      "read-only",
      "below",
      "few",
      "examples",
      "different",
      "done.",
      "option",
      "add",
      "validation",
      "rule",
      "done",
      "meta",
      "phase",
      "request",
      "workflow.",
      "won",
      "allow",
      "updated",
      "under",
      "any",
      "circumstances",
      "reaches",
      "phase.",
      "ems-admin-template",
      "entitytype",
      "unless",
      "currently",
      "logged",
      "user",
      "admin",
      "rights",
      "defined",
      "role.",
      "now",
      "entity.closetime",
      "3600000",
      "step",
      "role",
      "last",
      "hour."
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "make an entity read only after it has been closed",
    "contentLower": "there are multiple ways to make an entity read-only, below are a few examples of different ways it can be done. option 1 add the validation rule ${current_update.phaseid.ischanged} to the ‘done’ meta phase of the request entity workflow. this option won’t allow the request to be updated under any circumstances after it reaches the ‘done’ phase. option 2 add the validation rule ${current_update.phaseid.ischangedcurrent_user.ispermitted('ems-admin-template(entitytype=request)')} to the ‘done’ phase of the request entity workflow. this rule won’t allow the entity to be updated if it is in the ‘done’ meta phase unless the currently logged in user has admin rights defined in their role. option 3 add the validation rule ${current_update.phaseid.ischangedcurrent_user.ispermitted('ems-admin-template(entitytype=request)')(now() - entity.closetime)<3600000)} to the ‘done’ phase of the request entity workflow. this rule won’t allow the entity to be updated if it is in the ‘done’ step unless the c",
    "keywordsLower": [
      "make",
      "entity",
      "read",
      "after",
      "closed",
      "there",
      "multiple",
      "ways",
      "read-only",
      "below",
      "few",
      "examples",
      "different",
      "done.",
      "option",
      "add",
      "validation",
      "rule",
      "done",
      "meta",
      "phase",
      "request",
      "workflow.",
      "won",
      "allow",
      "updated",
      "under",
      "any",
      "circumstances",
      "reaches",
      "phase.",
      "ems-admin-template",
      "entitytype",
      "unless",
      "currently",
      "logged",
      "user",
      "admin",
      "rights",
      "defined",
      "role.",
      "now",
      "entity.closetime",
      "3600000",
      "step",
      "role",
      "last",
      "hour."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Only display 3rd level Record Categories on an Incident/Request/Change",
    "content": "In the category field on the request or incident form, it’s possible to only display the level 3 record categories. The following rule will search the level2Parent field and confirms that the value is populated, the level2parent field contains the name of the parent of the 3rd level value so it will only be populated if the record is a 3rd level category. A dummy value is used for the comparison, in this case, ‘Empty’. The rule will only compare it to valid values; categories with a null in the field will not be displayed in the drop-down list.",
    "url": "xcbreexample5",
    "filename": "xcbreexample5",
    "headings": [],
    "keywords": [
      "incidentrequestchange",
      "display",
      "3rd",
      "level",
      "record",
      "categories",
      "incident",
      "request",
      "change",
      "category",
      "field",
      "form",
      "possible",
      "categories.",
      "following",
      "rule",
      "search",
      "level2parent",
      "confirms",
      "value",
      "populated",
      "contains",
      "name",
      "parent",
      "category.",
      "dummy",
      "comparison",
      "case",
      "empty",
      "compare",
      "valid",
      "values",
      "null",
      "displayed",
      "drop-down",
      "list."
    ],
    "language": "en",
    "word_count": 56,
    "importance_score": 1.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "only display 3rd level record categories on an incident/request/change",
    "contentLower": "in the category field on the request or incident form, it’s possible to only display the level 3 record categories. the following rule will search the level2parent field and confirms that the value is populated, the level2parent field contains the name of the parent of the 3rd level value so it will only be populated if the record is a 3rd level category. a dummy value is used for the comparison, in this case, ‘empty’. the rule will only compare it to valid values; categories with a null in the field will not be displayed in the drop-down list.",
    "keywordsLower": [
      "incidentrequestchange",
      "display",
      "3rd",
      "level",
      "record",
      "categories",
      "incident",
      "request",
      "change",
      "category",
      "field",
      "form",
      "possible",
      "categories.",
      "following",
      "rule",
      "search",
      "level2parent",
      "confirms",
      "value",
      "populated",
      "contains",
      "name",
      "parent",
      "category.",
      "dummy",
      "comparison",
      "case",
      "empty",
      "compare",
      "valid",
      "values",
      "null",
      "displayed",
      "drop-down",
      "list."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Passing parameters to notification templates",
    "content": "There may be situations where the information needed in an email notification is not supported by the standard DSL functionality. An example of this would be the name of the requester’s manager or the name of the owner of an assignment group. This rule example will add the following to the create request email notification: The requester’s manager’s email address. The name of the owner of the service desk group. The name of the 1st level support group of the actual service assigned to the request. In order to add the parameters to the email notification, the following steps are needed: Creating the parameter name in the email notification template. Determining the DSL syntax for the fields that will populate the parameter in the template. Adding the DSL that captures the value into the rule that sends the notification. Creating the parameter name In the ‘Records’ editor, select the ‘Request’ entity and then locate the ‘CreateRequest’ template under the ‘Notifications’ tab: In the conte",
    "url": "xcbreexample11",
    "filename": "xcbreexample11",
    "headings": [],
    "keywords": [
      "Owner.Name",
      "SupportLevel1Group.Name",
      "passing",
      "parameters",
      "notification",
      "templates",
      "there",
      "situations",
      "information",
      "needed",
      "email",
      "supported",
      "standard",
      "dsl",
      "functionality.",
      "example",
      "name",
      "requester",
      "manager",
      "owner",
      "assignment",
      "group.",
      "rule",
      "add",
      "following",
      "create",
      "request",
      "address.",
      "service",
      "desk",
      "1st",
      "level",
      "support",
      "group",
      "actual",
      "assigned",
      "request.",
      "order",
      "steps",
      "creating",
      "parameter",
      "template.",
      "determining",
      "syntax",
      "fields",
      "populate",
      "adding",
      "captures",
      "value",
      "sends",
      "notification.",
      "records",
      "editor",
      "select",
      "entity",
      "locate",
      "createrequest",
      "template",
      "under",
      "notifications",
      "tab",
      "content",
      "reference",
      "names",
      "store",
      "desired",
      "field",
      "values.",
      "going",
      "requestersmanagername",
      "ownerofservicedeskgroupsname",
      "servicefirstlevelsupportgroupname",
      "any",
      "appear",
      "itself.",
      "find",
      "workflow",
      "send",
      "edited",
      "parameters.",
      "click",
      "rule.",
      "added",
      "automatically",
      "form",
      "appears.",
      "function",
      "fx",
      "button",
      "expression",
      "look",
      "values",
      "entity.servicedeskgroup.owner.name",
      "entity.registeredforactualservice.supportlevel1group.name",
      "entity.requestedbyperson.manager.email",
      "below",
      "after",
      "getting",
      "form.",
      "user"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "passing parameters to notification templates",
    "contentLower": "there may be situations where the information needed in an email notification is not supported by the standard dsl functionality. an example of this would be the name of the requester’s manager or the name of the owner of an assignment group. this rule example will add the following to the create request email notification: the requester’s manager’s email address. the name of the owner of the service desk group. the name of the 1st level support group of the actual service assigned to the request. in order to add the parameters to the email notification, the following steps are needed: creating the parameter name in the email notification template. determining the dsl syntax for the fields that will populate the parameter in the template. adding the dsl that captures the value into the rule that sends the notification. creating the parameter name in the ‘records’ editor, select the ‘request’ entity and then locate the ‘createrequest’ template under the ‘notifications’ tab: in the conte",
    "keywordsLower": [
      "owner.name",
      "supportlevel1group.name",
      "passing",
      "parameters",
      "notification",
      "templates",
      "there",
      "situations",
      "information",
      "needed",
      "email",
      "supported",
      "standard",
      "dsl",
      "functionality.",
      "example",
      "name",
      "requester",
      "manager",
      "owner",
      "assignment",
      "group.",
      "rule",
      "add",
      "following",
      "create",
      "request",
      "address.",
      "service",
      "desk",
      "1st",
      "level",
      "support",
      "group",
      "actual",
      "assigned",
      "request.",
      "order",
      "steps",
      "creating",
      "parameter",
      "template.",
      "determining",
      "syntax",
      "fields",
      "populate",
      "adding",
      "captures",
      "value",
      "sends",
      "notification.",
      "records",
      "editor",
      "select",
      "entity",
      "locate",
      "createrequest",
      "template",
      "under",
      "notifications",
      "tab",
      "content",
      "reference",
      "names",
      "store",
      "desired",
      "field",
      "values.",
      "going",
      "requestersmanagername",
      "ownerofservicedeskgroupsname",
      "servicefirstlevelsupportgroupname",
      "any",
      "appear",
      "itself.",
      "find",
      "workflow",
      "send",
      "edited",
      "parameters.",
      "click",
      "rule.",
      "added",
      "automatically",
      "form",
      "appears.",
      "function",
      "fx",
      "button",
      "expression",
      "look",
      "values",
      "entity.servicedeskgroup.owner.name",
      "entity.registeredforactualservice.supportlevel1group.name",
      "entity.requestedbyperson.manager.email",
      "below",
      "after",
      "getting",
      "form.",
      "user"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform HTML sanitization for parameters in notification templates",
    "content": "This page explains how to perform HTML sanitization for parameters in notification templates. HTML sanitization removes unwanted HTML content. This prevents security risks and ensures clean, readable text in notifications. Generally, parameters whose values are user-generated or dynamic content require HTML sanitization. Perform HTML sanitization Navigate to the notification template. Example: Template for sending solution to the followers. Identify the parameters that require HTML sanitization. Look for the parameters whose values could be user-generated or dynamic. For example, the DisplayLabel parameter in Template for sending solution to the followers . The value of this parameter is the title of the request that the follower is following. Pass the parameter to the to_plain_text() function. This function takes a string as input, removes the HTML tags from the string, and returns the plain text. For example, to_plain_text(entity.DisplayLabel). Save the notification template. Example",
    "url": "htmlsanitizationforparameters",
    "filename": "htmlsanitizationforparameters",
    "headings": [
      "Perform HTML sanitization",
      "Example"
    ],
    "keywords": [
      "bing.com",
      "http://bing.com\">test</a",
      "perform",
      "html",
      "sanitization",
      "parameters",
      "notification",
      "templates",
      "example",
      "page",
      "explains",
      "templates.",
      "removes",
      "unwanted",
      "content.",
      "prevents",
      "security",
      "risks",
      "ensures",
      "clean",
      "readable",
      "text",
      "notifications.",
      "generally",
      "whose",
      "values",
      "user-generated",
      "dynamic",
      "content",
      "require",
      "sanitization.",
      "navigate",
      "template.",
      "template",
      "sending",
      "solution",
      "followers.",
      "identify",
      "look",
      "dynamic.",
      "displaylabel",
      "parameter",
      "followers",
      "value",
      "title",
      "request",
      "follower",
      "following.",
      "pass",
      "function.",
      "function",
      "takes",
      "string",
      "input",
      "tags",
      "returns",
      "plain",
      "text.",
      "entity.displaylabel",
      "save",
      "pc",
      "test",
      "123",
      "before",
      "email",
      "received",
      "show",
      "links",
      "part",
      "see",
      "following",
      "image.",
      "after",
      "removed"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform html sanitization for parameters in notification templates",
    "contentLower": "this page explains how to perform html sanitization for parameters in notification templates. html sanitization removes unwanted html content. this prevents security risks and ensures clean, readable text in notifications. generally, parameters whose values are user-generated or dynamic content require html sanitization. perform html sanitization navigate to the notification template. example: template for sending solution to the followers. identify the parameters that require html sanitization. look for the parameters whose values could be user-generated or dynamic. for example, the displaylabel parameter in template for sending solution to the followers . the value of this parameter is the title of the request that the follower is following. pass the parameter to the to_plain_text() function. this function takes a string as input, removes the html tags from the string, and returns the plain text. for example, to_plain_text(entity.displaylabel). save the notification template. example",
    "keywordsLower": [
      "bing.com",
      "http://bing.com\">test</a",
      "perform",
      "html",
      "sanitization",
      "parameters",
      "notification",
      "templates",
      "example",
      "page",
      "explains",
      "templates.",
      "removes",
      "unwanted",
      "content.",
      "prevents",
      "security",
      "risks",
      "ensures",
      "clean",
      "readable",
      "text",
      "notifications.",
      "generally",
      "whose",
      "values",
      "user-generated",
      "dynamic",
      "content",
      "require",
      "sanitization.",
      "navigate",
      "template.",
      "template",
      "sending",
      "solution",
      "followers.",
      "identify",
      "look",
      "dynamic.",
      "displaylabel",
      "parameter",
      "followers",
      "value",
      "title",
      "request",
      "follower",
      "following.",
      "pass",
      "function.",
      "function",
      "takes",
      "string",
      "input",
      "tags",
      "returns",
      "plain",
      "text.",
      "entity.displaylabel",
      "save",
      "pc",
      "test",
      "123",
      "before",
      "email",
      "received",
      "show",
      "links",
      "part",
      "see",
      "following",
      "image.",
      "after",
      "removed"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Notifications",
    "content": "Notifications are pre-formatted email messages that the system sends when certain workflow events or business rule conditions occur. The administrators can define which notifications are sent for such specific events. Work with notification templates the system sends notifications based on predefined notification templates. Notification templates contain the structure for each type of message. You can add, update, or delete any template for an email notification if you have administration permissions. Each template contains the information that the recipient needs to know when the workflow triggers the email to be sent. For example, a template for a new incident notification includes all the details of a new incident. Service Management sends this email when the recipient is the owner of the new incident, or belongs to the assignment group that owns the new incident. A template for an impending SLT breach contains information about the service target and the amount of time that remains",
    "url": "notifications",
    "filename": "notifications",
    "headings": [
      "Work with notification templates",
      "Customize notification events",
      "Enable user notification preferences in application settings",
      "Set user notification preferences",
      "Settings in a notification template",
      "Notification frequency",
      "Limits that trigger email notifications",
      "Message rate limit",
      "Recipient rate limit",
      "Sender-recipient pair limit",
      "Related topics"
    ],
    "keywords": [
      "domain.com",
      "notifications",
      "work",
      "notification",
      "templates",
      "customize",
      "events",
      "enable",
      "user",
      "preferences",
      "application",
      "settings",
      "set",
      "template",
      "frequency",
      "limits",
      "trigger",
      "email",
      "message",
      "rate",
      "limit",
      "recipient",
      "sender-recipient",
      "pair",
      "related",
      "topics",
      "pre-formatted",
      "messages",
      "system",
      "sends",
      "certain",
      "workflow",
      "business",
      "rule",
      "conditions",
      "occur.",
      "administrators",
      "define",
      "sent",
      "such",
      "specific",
      "events.",
      "based",
      "predefined",
      "templates.",
      "contain",
      "structure",
      "type",
      "message.",
      "add",
      "update",
      "delete",
      "any",
      "administration",
      "permissions.",
      "contains",
      "information",
      "needs",
      "know",
      "triggers",
      "sent.",
      "example",
      "new",
      "incident",
      "includes",
      "all",
      "details",
      "incident.",
      "service",
      "management",
      "owner",
      "belongs",
      "assignment",
      "group",
      "owns",
      "impending",
      "slt",
      "breach",
      "about",
      "target",
      "amount",
      "time",
      "remains",
      "until",
      "occurs.",
      "note",
      "before",
      "sending",
      "validates",
      "submitter",
      "permission",
      "view",
      "record",
      "instructions",
      "see",
      "list",
      "named",
      "event",
      "names",
      "lists"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "notifications",
    "contentLower": "notifications are pre-formatted email messages that the system sends when certain workflow events or business rule conditions occur. the administrators can define which notifications are sent for such specific events. work with notification templates the system sends notifications based on predefined notification templates. notification templates contain the structure for each type of message. you can add, update, or delete any template for an email notification if you have administration permissions. each template contains the information that the recipient needs to know when the workflow triggers the email to be sent. for example, a template for a new incident notification includes all the details of a new incident. service management sends this email when the recipient is the owner of the new incident, or belongs to the assignment group that owns the new incident. a template for an impending slt breach contains information about the service target and the amount of time that remains",
    "keywordsLower": [
      "domain.com",
      "notifications",
      "work",
      "notification",
      "templates",
      "customize",
      "events",
      "enable",
      "user",
      "preferences",
      "application",
      "settings",
      "set",
      "template",
      "frequency",
      "limits",
      "trigger",
      "email",
      "message",
      "rate",
      "limit",
      "recipient",
      "sender-recipient",
      "pair",
      "related",
      "topics",
      "pre-formatted",
      "messages",
      "system",
      "sends",
      "certain",
      "workflow",
      "business",
      "rule",
      "conditions",
      "occur.",
      "administrators",
      "define",
      "sent",
      "such",
      "specific",
      "events.",
      "based",
      "predefined",
      "templates.",
      "contain",
      "structure",
      "type",
      "message.",
      "add",
      "update",
      "delete",
      "any",
      "administration",
      "permissions.",
      "contains",
      "information",
      "needs",
      "know",
      "triggers",
      "sent.",
      "example",
      "new",
      "incident",
      "includes",
      "all",
      "details",
      "incident.",
      "service",
      "management",
      "owner",
      "belongs",
      "assignment",
      "group",
      "owns",
      "impending",
      "slt",
      "breach",
      "about",
      "target",
      "amount",
      "time",
      "remains",
      "until",
      "occurs.",
      "note",
      "before",
      "sending",
      "validates",
      "submitter",
      "permission",
      "view",
      "record",
      "instructions",
      "see",
      "list",
      "named",
      "event",
      "names",
      "lists"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Processing rules in notifications",
    "content": "The message body of a notification template can contain processing rules that embed data in the email sent to a recipient when the notification event occurs. For example, you might want to do one of the following: Insert the avatar image of the email sender and the email recipient into the message, for personalization. Embed in the email the URL to a record. Hide the record name from users who don't have permission to view that record type. Make a statement conditional on a Boolean expression. Add a mailto option for the recipients to start an email conversation with Service Management. Avatar rule When the avatar rule runs, Service Management uses a person identifier (personId) to locate the linked avatar for that user. For example, you would insert ${:current_user.Id} to identify the email sender. The avatar becomes a parameter in the rule. Service Management can locate and attach the avatar image file to the message. The HTML image reference in the notification causes the actual ava",
    "url": "processingrulesinnotifs",
    "filename": "processingrulesinnotifs",
    "headings": [
      "Avatar rule",
      "Create URL rule",
      "Approval queue URL rule",
      "Hide record name rule",
      "Conditional statement rule",
      "Show related knowledge rule",
      "Show related offering rule",
      "Show button rule",
      "Create record link rule",
      "Show delegated user rule",
      "Get the display name of an SLT target type",
      "Adjust the format of date and time based on the recipient's locale setting",
      "Include a mailto option",
      "Show error message",
      "Related topics"
    ],
    "keywords": [
      "current_recipient.Id",
      "https://<hostname>:<port>/home/approval?TENANTID=<tenantId",
      "current_user.Name",
      "https://<hostname>:<port>/saw/ess/approval?TENANTID=<tenantId",
      "RequestedForPerson.Id",
      "entity.Id",
      "entity.Type",
      "email.body",
      "current_user.Id",
      "Assignee.Id",
      "processing",
      "rules",
      "notifications",
      "avatar",
      "rule",
      "create",
      "url",
      "approval",
      "queue",
      "hide",
      "record",
      "name",
      "conditional",
      "statement",
      "show",
      "related",
      "knowledge",
      "offering",
      "button",
      "link",
      "delegated",
      "user",
      "get",
      "display",
      "slt",
      "target",
      "type",
      "adjust",
      "format",
      "date",
      "time",
      "based",
      "recipient",
      "locale",
      "setting",
      "include",
      "mailto",
      "option",
      "error",
      "message",
      "topics",
      "body",
      "notification",
      "template",
      "contain",
      "embed",
      "data",
      "email",
      "sent",
      "event",
      "occurs.",
      "example",
      "want",
      "one",
      "following",
      "insert",
      "image",
      "sender",
      "personalization.",
      "record.",
      "users",
      "don",
      "permission",
      "view",
      "type.",
      "make",
      "boolean",
      "expression.",
      "add",
      "recipients",
      "start",
      "conversation",
      "service",
      "management.",
      "runs",
      "management",
      "uses",
      "person",
      "identifier",
      "personid",
      "locate",
      "linked",
      "user.",
      "identify",
      "sender.",
      "becomes",
      "parameter",
      "rule.",
      "attach",
      "file"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "processing rules in notifications",
    "contentLower": "the message body of a notification template can contain processing rules that embed data in the email sent to a recipient when the notification event occurs. for example, you might want to do one of the following: insert the avatar image of the email sender and the email recipient into the message, for personalization. embed in the email the url to a record. hide the record name from users who don't have permission to view that record type. make a statement conditional on a boolean expression. add a mailto option for the recipients to start an email conversation with service management. avatar rule when the avatar rule runs, service management uses a person identifier (personid) to locate the linked avatar for that user. for example, you would insert ${:current_user.id} to identify the email sender. the avatar becomes a parameter in the rule. service management can locate and attach the avatar image file to the message. the html image reference in the notification causes the actual ava",
    "keywordsLower": [
      "current_recipient.id",
      "https://<hostname>:<port>/home/approval?tenantid=<tenantid",
      "current_user.name",
      "https://<hostname>:<port>/saw/ess/approval?tenantid=<tenantid",
      "requestedforperson.id",
      "entity.id",
      "entity.type",
      "email.body",
      "current_user.id",
      "assignee.id",
      "processing",
      "rules",
      "notifications",
      "avatar",
      "rule",
      "create",
      "url",
      "approval",
      "queue",
      "hide",
      "record",
      "name",
      "conditional",
      "statement",
      "show",
      "related",
      "knowledge",
      "offering",
      "button",
      "link",
      "delegated",
      "user",
      "get",
      "display",
      "slt",
      "target",
      "type",
      "adjust",
      "format",
      "date",
      "time",
      "based",
      "recipient",
      "locale",
      "setting",
      "include",
      "mailto",
      "option",
      "error",
      "message",
      "topics",
      "body",
      "notification",
      "template",
      "contain",
      "embed",
      "data",
      "email",
      "sent",
      "event",
      "occurs.",
      "example",
      "want",
      "one",
      "following",
      "insert",
      "image",
      "sender",
      "personalization.",
      "record.",
      "users",
      "don",
      "permission",
      "view",
      "type.",
      "make",
      "boolean",
      "expression.",
      "add",
      "recipients",
      "start",
      "conversation",
      "service",
      "management.",
      "runs",
      "management",
      "uses",
      "person",
      "identifier",
      "personid",
      "locate",
      "linked",
      "user.",
      "identify",
      "sender.",
      "becomes",
      "parameter",
      "rule.",
      "attach",
      "file"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "People",
    "content": "System administrators manage the access that people in the organization have to different parts of different applications. The People domain includes individual users, groups, contacts, and user roles. When you add new employees to the organization, the first task is to enable them to log in to the areas where they have assigned work to do. Next, you might want to include them in one or more user groups, which makes it easy to send email to everyone with common interests or assignments. Finally, you want to assign a role to each user. Service Management uses a role-based permission system that makes it easy to assign a block of permissions to a role, and then assign that role to one or more end users. The default roles match industry-standard ITIL roles and best practice recommendations. Contacts are an enterprise-wide address book of internal and external individuals, stakeholders, vendor liaisons, or others who might have a business relationship with the organization. Encryption doma",
    "url": "people",
    "filename": "people",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "people",
      "related",
      "topics",
      "system",
      "administrators",
      "manage",
      "access",
      "organization",
      "different",
      "parts",
      "applications.",
      "domain",
      "includes",
      "individual",
      "users",
      "groups",
      "contacts",
      "user",
      "roles.",
      "add",
      "new",
      "employees",
      "first",
      "task",
      "enable",
      "log",
      "areas",
      "assigned",
      "work",
      "do.",
      "next",
      "want",
      "include",
      "one",
      "makes",
      "easy",
      "send",
      "email",
      "everyone",
      "common",
      "interests",
      "assignments.",
      "finally",
      "assign",
      "role",
      "user.",
      "service",
      "management",
      "uses",
      "role-based",
      "permission",
      "block",
      "permissions",
      "end",
      "users.",
      "default",
      "roles",
      "match",
      "industry-standard",
      "itil",
      "best",
      "practice",
      "recommendations.",
      "enterprise-wide",
      "address",
      "book",
      "internal",
      "external",
      "individuals",
      "stakeholders",
      "vendor",
      "liaisons",
      "others",
      "business",
      "relationship",
      "organization.",
      "encryption",
      "domains",
      "encrypt",
      "specific",
      "record",
      "type",
      "fields",
      "restrict",
      "sensitive",
      "information",
      "selected",
      "note",
      "data",
      "segmentation",
      "provides",
      "ability",
      "limit",
      "records",
      "permitted",
      "view.",
      "general",
      "view",
      "automatically",
      "primary"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "people",
    "contentLower": "system administrators manage the access that people in the organization have to different parts of different applications. the people domain includes individual users, groups, contacts, and user roles. when you add new employees to the organization, the first task is to enable them to log in to the areas where they have assigned work to do. next, you might want to include them in one or more user groups, which makes it easy to send email to everyone with common interests or assignments. finally, you want to assign a role to each user. service management uses a role-based permission system that makes it easy to assign a block of permissions to a role, and then assign that role to one or more end users. the default roles match industry-standard itil roles and best practice recommendations. contacts are an enterprise-wide address book of internal and external individuals, stakeholders, vendor liaisons, or others who might have a business relationship with the organization. encryption doma",
    "keywordsLower": [
      "people",
      "related",
      "topics",
      "system",
      "administrators",
      "manage",
      "access",
      "organization",
      "different",
      "parts",
      "applications.",
      "domain",
      "includes",
      "individual",
      "users",
      "groups",
      "contacts",
      "user",
      "roles.",
      "add",
      "new",
      "employees",
      "first",
      "task",
      "enable",
      "log",
      "areas",
      "assigned",
      "work",
      "do.",
      "next",
      "want",
      "include",
      "one",
      "makes",
      "easy",
      "send",
      "email",
      "everyone",
      "common",
      "interests",
      "assignments.",
      "finally",
      "assign",
      "role",
      "user.",
      "service",
      "management",
      "uses",
      "role-based",
      "permission",
      "block",
      "permissions",
      "end",
      "users.",
      "default",
      "roles",
      "match",
      "industry-standard",
      "itil",
      "best",
      "practice",
      "recommendations.",
      "enterprise-wide",
      "address",
      "book",
      "internal",
      "external",
      "individuals",
      "stakeholders",
      "vendor",
      "liaisons",
      "others",
      "business",
      "relationship",
      "organization.",
      "encryption",
      "domains",
      "encrypt",
      "specific",
      "record",
      "type",
      "fields",
      "restrict",
      "sensitive",
      "information",
      "selected",
      "note",
      "data",
      "segmentation",
      "provides",
      "ability",
      "limit",
      "records",
      "permitted",
      "view.",
      "general",
      "view",
      "automatically",
      "primary"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Out-of-the-box business rule examples",
    "content": "This section details certain out-of-the-box business rules in each of the record types' processes. Note that this section doesn't cover all the out-of-the-box business rules in the system.",
    "url": "oobbizrule",
    "filename": "oobbizrule",
    "headings": [],
    "keywords": [
      "out-of-the-box",
      "business",
      "rule",
      "examples",
      "section",
      "details",
      "certain",
      "rules",
      "record",
      "types",
      "processes.",
      "note",
      "doesn",
      "cover",
      "all",
      "system."
    ],
    "language": "en",
    "word_count": 23,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "out-of-the-box business rule examples",
    "contentLower": "this section details certain out-of-the-box business rules in each of the record types' processes. note that this section doesn't cover all the out-of-the-box business rules in the system.",
    "keywordsLower": [
      "out-of-the-box",
      "business",
      "rule",
      "examples",
      "section",
      "details",
      "certain",
      "rules",
      "record",
      "types",
      "processes.",
      "note",
      "doesn",
      "cover",
      "all",
      "system."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Problem transition rules",
    "content": "Built-in business rules define conditions for automatic transitions from one phase to another. If the condition evaluates as true, the automatic transition occurs. Some conditions are simple and some are complex. If the condition isn't true, the record remains in its original phase. The following table identifies each supported transition in the Problem Management workflow. Notice that it's possible for a record to return to an earlier phase in the workflow if the status changes to a certain value. You can return to an earlier phase when any of the following circumstances occur: The resolution doesn't resolve the problem. The problem was categorized incorrectly. The problem needs further investigation. The Problem Manager reassigns the incident to a different support analyst. The Condition column in the following tables show fields that contain data either entered by the end user or automatically provided by Service Management. At the start of each phase, Service Management verifies th",
    "url": "pblmtransitionrules",
    "filename": "pblmtransitionrules",
    "headings": [
      "Log phase",
      "Classify phase",
      "Investigate phase",
      "Resolve phase",
      "Review phase",
      "Close phase",
      "Abandon phase",
      "Related topics"
    ],
    "keywords": [
      "problem",
      "transition",
      "rules",
      "log",
      "phase",
      "classify",
      "investigate",
      "resolve",
      "review",
      "close",
      "abandon",
      "related",
      "topics",
      "built-in",
      "business",
      "define",
      "conditions",
      "automatic",
      "transitions",
      "one",
      "another.",
      "condition",
      "evaluates",
      "true",
      "occurs.",
      "simple",
      "complex.",
      "isn",
      "record",
      "remains",
      "original",
      "phase.",
      "following",
      "table",
      "identifies",
      "supported",
      "management",
      "workflow.",
      "notice",
      "possible",
      "return",
      "earlier",
      "workflow",
      "status",
      "changes",
      "certain",
      "value.",
      "any",
      "circumstances",
      "occur",
      "resolution",
      "doesn",
      "problem.",
      "categorized",
      "incorrectly.",
      "needs",
      "further",
      "investigation.",
      "manager",
      "reassigns",
      "incident",
      "different",
      "support",
      "analyst.",
      "column",
      "tables",
      "show",
      "fields",
      "contain",
      "data",
      "either",
      "entered",
      "end",
      "user",
      "automatically",
      "provided",
      "service",
      "management.",
      "start",
      "verifies",
      "user-defined",
      "values",
      "set",
      "last",
      "unchanged.",
      "there",
      "change",
      "prior",
      "repeat",
      "action.",
      "all",
      "true.",
      "manual",
      "advance",
      "next",
      "snapshot.",
      "click",
      "button",
      "problems",
      "begin"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "problem transition rules",
    "contentLower": "built-in business rules define conditions for automatic transitions from one phase to another. if the condition evaluates as true, the automatic transition occurs. some conditions are simple and some are complex. if the condition isn't true, the record remains in its original phase. the following table identifies each supported transition in the problem management workflow. notice that it's possible for a record to return to an earlier phase in the workflow if the status changes to a certain value. you can return to an earlier phase when any of the following circumstances occur: the resolution doesn't resolve the problem. the problem was categorized incorrectly. the problem needs further investigation. the problem manager reassigns the incident to a different support analyst. the condition column in the following tables show fields that contain data either entered by the end user or automatically provided by service management. at the start of each phase, service management verifies th",
    "keywordsLower": [
      "problem",
      "transition",
      "rules",
      "log",
      "phase",
      "classify",
      "investigate",
      "resolve",
      "review",
      "close",
      "abandon",
      "related",
      "topics",
      "built-in",
      "business",
      "define",
      "conditions",
      "automatic",
      "transitions",
      "one",
      "another.",
      "condition",
      "evaluates",
      "true",
      "occurs.",
      "simple",
      "complex.",
      "isn",
      "record",
      "remains",
      "original",
      "phase.",
      "following",
      "table",
      "identifies",
      "supported",
      "management",
      "workflow.",
      "notice",
      "possible",
      "return",
      "earlier",
      "workflow",
      "status",
      "changes",
      "certain",
      "value.",
      "any",
      "circumstances",
      "occur",
      "resolution",
      "doesn",
      "problem.",
      "categorized",
      "incorrectly.",
      "needs",
      "further",
      "investigation.",
      "manager",
      "reassigns",
      "incident",
      "different",
      "support",
      "analyst.",
      "column",
      "tables",
      "show",
      "fields",
      "contain",
      "data",
      "either",
      "entered",
      "end",
      "user",
      "automatically",
      "provided",
      "service",
      "management.",
      "start",
      "verifies",
      "user-defined",
      "values",
      "set",
      "last",
      "unchanged.",
      "there",
      "change",
      "prior",
      "repeat",
      "action.",
      "all",
      "true.",
      "manual",
      "advance",
      "next",
      "snapshot.",
      "click",
      "button",
      "problems",
      "begin"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Purchase order transition rules",
    "content": "The following information identifies each supported transition in the purchase order workflow. The Action or Condition column in the tables for each phase, show fields that contain data either entered by the end user or automatically provided by Service Management. At the start of each phase, Service Management verifies that the user-defined values set in the last phase are unchanged. If there is a change, the record can return to a prior phase or repeat an action. All purchase orders begin with the New phase. New phase Event Action or Condition Entering Service Management action: Set the default value to manual for reference type when the purchase order is created. Set the current time to value of Creation date field when the purchase order is created. After change Update the purchase order line of this purchase order from Suspended to New. Purchase orders transit from New to the Issued, Suspended or Canceled phase. Issued phase Event Action or Condition Entering Service Management ac",
    "url": "porules",
    "filename": "porules",
    "headings": [
      "New phase",
      "Issued phase",
      "Suspended phase",
      "Canceled phase",
      "Related topics"
    ],
    "keywords": [
      "purchase",
      "order",
      "transition",
      "rules",
      "new",
      "phase",
      "issued",
      "suspended",
      "canceled",
      "related",
      "topics",
      "following",
      "information",
      "identifies",
      "supported",
      "workflow.",
      "action",
      "condition",
      "column",
      "tables",
      "show",
      "fields",
      "contain",
      "data",
      "either",
      "entered",
      "end",
      "user",
      "automatically",
      "provided",
      "service",
      "management.",
      "start",
      "management",
      "verifies",
      "user-defined",
      "values",
      "set",
      "last",
      "unchanged.",
      "there",
      "change",
      "record",
      "return",
      "prior",
      "repeat",
      "action.",
      "all",
      "orders",
      "begin",
      "phase.",
      "event",
      "entering",
      "default",
      "value",
      "manual",
      "reference",
      "type",
      "created.",
      "current",
      "time",
      "creation",
      "date",
      "field",
      "after",
      "update",
      "line",
      "new.",
      "transit",
      "issued.",
      "validate",
      "quantity",
      "line.",
      "before",
      "changed",
      "least",
      "one",
      "suspended.",
      "canceled.",
      "workflow",
      "create",
      "manage"
    ],
    "language": "en",
    "word_count": 109,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "purchase order transition rules",
    "contentLower": "the following information identifies each supported transition in the purchase order workflow. the action or condition column in the tables for each phase, show fields that contain data either entered by the end user or automatically provided by service management. at the start of each phase, service management verifies that the user-defined values set in the last phase are unchanged. if there is a change, the record can return to a prior phase or repeat an action. all purchase orders begin with the new phase. new phase event action or condition entering service management action: set the default value to manual for reference type when the purchase order is created. set the current time to value of creation date field when the purchase order is created. after change update the purchase order line of this purchase order from suspended to new. purchase orders transit from new to the issued, suspended or canceled phase. issued phase event action or condition entering service management ac",
    "keywordsLower": [
      "purchase",
      "order",
      "transition",
      "rules",
      "new",
      "phase",
      "issued",
      "suspended",
      "canceled",
      "related",
      "topics",
      "following",
      "information",
      "identifies",
      "supported",
      "workflow.",
      "action",
      "condition",
      "column",
      "tables",
      "show",
      "fields",
      "contain",
      "data",
      "either",
      "entered",
      "end",
      "user",
      "automatically",
      "provided",
      "service",
      "management.",
      "start",
      "management",
      "verifies",
      "user-defined",
      "values",
      "set",
      "last",
      "unchanged.",
      "there",
      "change",
      "record",
      "return",
      "prior",
      "repeat",
      "action.",
      "all",
      "orders",
      "begin",
      "phase.",
      "event",
      "entering",
      "default",
      "value",
      "manual",
      "reference",
      "type",
      "created.",
      "current",
      "time",
      "creation",
      "date",
      "field",
      "after",
      "update",
      "line",
      "new.",
      "transit",
      "issued.",
      "validate",
      "quantity",
      "line.",
      "before",
      "changed",
      "least",
      "one",
      "suspended.",
      "canceled.",
      "workflow",
      "create",
      "manage"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Purchase order line transition rules",
    "content": "The following information identifies each supported transition in the purchase order line workflow. The Action or Condition column in the tables for each phase, show fields that contain data either entered by the end user or automatically provided by Service Management. At the start of each phase, Service Management verifies that the user-defined values set in the last phase are unchanged. If there is a change, the record can return to a prior phase or repeat an action. All purchase order lines begin with the New phase. New phase Event Action or Condition Before change Service Management action: Set the asset model value as the asset model of the product catalog item in the New phase when the asset model is empty. Purchase orders transit from New to the Issued, Suspended or Canceled phase. Issued phase Event Action or Condition After change Service Management action: Subtract the purchase order line order quantity from the purchase order total quantity when the purchase order line is c",
    "url": "poltransitionrules",
    "filename": "poltransitionrules",
    "headings": [
      "New phase",
      "Issued phase",
      "Related topics"
    ],
    "keywords": [
      "purchase",
      "order",
      "line",
      "transition",
      "rules",
      "new",
      "phase",
      "issued",
      "related",
      "topics",
      "following",
      "information",
      "identifies",
      "supported",
      "workflow.",
      "action",
      "condition",
      "column",
      "tables",
      "show",
      "fields",
      "contain",
      "data",
      "either",
      "entered",
      "end",
      "user",
      "automatically",
      "provided",
      "service",
      "management.",
      "start",
      "management",
      "verifies",
      "user-defined",
      "values",
      "set",
      "last",
      "unchanged.",
      "there",
      "change",
      "record",
      "return",
      "prior",
      "repeat",
      "action.",
      "all",
      "lines",
      "begin",
      "phase.",
      "event",
      "before",
      "asset",
      "model",
      "value",
      "product",
      "catalog",
      "item",
      "empty.",
      "orders",
      "transit",
      "suspended",
      "canceled",
      "after",
      "subtract",
      "quantity",
      "total",
      "canceled.",
      "cost",
      "workflow",
      "create",
      "manage"
    ],
    "language": "en",
    "word_count": 113,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "purchase order line transition rules",
    "contentLower": "the following information identifies each supported transition in the purchase order line workflow. the action or condition column in the tables for each phase, show fields that contain data either entered by the end user or automatically provided by service management. at the start of each phase, service management verifies that the user-defined values set in the last phase are unchanged. if there is a change, the record can return to a prior phase or repeat an action. all purchase order lines begin with the new phase. new phase event action or condition before change service management action: set the asset model value as the asset model of the product catalog item in the new phase when the asset model is empty. purchase orders transit from new to the issued, suspended or canceled phase. issued phase event action or condition after change service management action: subtract the purchase order line order quantity from the purchase order total quantity when the purchase order line is c",
    "keywordsLower": [
      "purchase",
      "order",
      "line",
      "transition",
      "rules",
      "new",
      "phase",
      "issued",
      "related",
      "topics",
      "following",
      "information",
      "identifies",
      "supported",
      "workflow.",
      "action",
      "condition",
      "column",
      "tables",
      "show",
      "fields",
      "contain",
      "data",
      "either",
      "entered",
      "end",
      "user",
      "automatically",
      "provided",
      "service",
      "management.",
      "start",
      "management",
      "verifies",
      "user-defined",
      "values",
      "set",
      "last",
      "unchanged.",
      "there",
      "change",
      "record",
      "return",
      "prior",
      "repeat",
      "action.",
      "all",
      "lines",
      "begin",
      "phase.",
      "event",
      "before",
      "asset",
      "model",
      "value",
      "product",
      "catalog",
      "item",
      "empty.",
      "orders",
      "transit",
      "suspended",
      "canceled",
      "after",
      "subtract",
      "quantity",
      "total",
      "canceled.",
      "cost",
      "workflow",
      "create",
      "manage"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Optimization process - Business rules",
    "content": "In the out-of-the-box optimization workflow, the following business rules apply to the indicated metaphase or phase. Optimization process - all phases Condition Service Management action Rendering forms The phase ID isn't Create Disable the following fields: Target portfolio Optimization type The phase ID isn't Create The Internal state field isn't empty Disable the following fields: Email template Due date If the phase isn't Assess or Complete Hide the Report tab If the phase isn't Complete Hide the Related proposals tab After applying changes If a comment is added and isn't empty Send a comment notification to the recipients Send survey phase Event Condition Action Before change None Validate: Email template is changed and the new value isn't empty, or Email template isn't changed and the old value isn't empty Display the following message in case of a failure: The Email template field should not be empty None Validate: Due date is changed, the new value isn't empty, and the new valu",
    "url": "processoptims",
    "filename": "processoptims",
    "headings": [
      "Send survey phase",
      "Resend survey phase",
      "Assess phase",
      "Related topics"
    ],
    "keywords": [
      "optimization",
      "process",
      "business",
      "rules",
      "send",
      "survey",
      "phase",
      "resend",
      "assess",
      "related",
      "topics",
      "out-of-the-box",
      "workflow",
      "following",
      "apply",
      "indicated",
      "metaphase",
      "phase.",
      "all",
      "phases",
      "condition",
      "service",
      "management",
      "action",
      "rendering",
      "forms",
      "id",
      "isn",
      "create",
      "disable",
      "fields",
      "target",
      "portfolio",
      "type",
      "internal",
      "state",
      "field",
      "empty",
      "email",
      "template",
      "due",
      "date",
      "complete",
      "hide",
      "report",
      "tab",
      "proposals",
      "after",
      "applying",
      "changes",
      "comment",
      "added",
      "notification",
      "recipients",
      "event",
      "before",
      "change",
      "none",
      "validate",
      "changed",
      "new",
      "value",
      "old",
      "display",
      "message",
      "case",
      "failure",
      "future",
      "check",
      "contains",
      "any",
      "applications",
      "application",
      "owner",
      "owners",
      "listed",
      "collect",
      "information",
      "transitioning",
      "set",
      "empty.",
      "start",
      "analyze",
      "data",
      "collected",
      "optimize",
      "corresponding",
      "messages",
      "calculation",
      "started",
      "failed",
      "wait",
      "response",
      "sent",
      "working",
      "processes"
    ],
    "language": "en",
    "word_count": 131,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "optimization process - business rules",
    "contentLower": "in the out-of-the-box optimization workflow, the following business rules apply to the indicated metaphase or phase. optimization process - all phases condition service management action rendering forms the phase id isn't create disable the following fields: target portfolio optimization type the phase id isn't create the internal state field isn't empty disable the following fields: email template due date if the phase isn't assess or complete hide the report tab if the phase isn't complete hide the related proposals tab after applying changes if a comment is added and isn't empty send a comment notification to the recipients send survey phase event condition action before change none validate: email template is changed and the new value isn't empty, or email template isn't changed and the old value isn't empty display the following message in case of a failure: the email template field should not be empty none validate: due date is changed, the new value isn't empty, and the new valu",
    "keywordsLower": [
      "optimization",
      "process",
      "business",
      "rules",
      "send",
      "survey",
      "phase",
      "resend",
      "assess",
      "related",
      "topics",
      "out-of-the-box",
      "workflow",
      "following",
      "apply",
      "indicated",
      "metaphase",
      "phase.",
      "all",
      "phases",
      "condition",
      "service",
      "management",
      "action",
      "rendering",
      "forms",
      "id",
      "isn",
      "create",
      "disable",
      "fields",
      "target",
      "portfolio",
      "type",
      "internal",
      "state",
      "field",
      "empty",
      "email",
      "template",
      "due",
      "date",
      "complete",
      "hide",
      "report",
      "tab",
      "proposals",
      "after",
      "applying",
      "changes",
      "comment",
      "added",
      "notification",
      "recipients",
      "event",
      "before",
      "change",
      "none",
      "validate",
      "changed",
      "new",
      "value",
      "old",
      "display",
      "message",
      "case",
      "failure",
      "future",
      "check",
      "contains",
      "any",
      "applications",
      "application",
      "owner",
      "owners",
      "listed",
      "collect",
      "information",
      "transitioning",
      "set",
      "empty.",
      "start",
      "analyze",
      "data",
      "collected",
      "optimize",
      "corresponding",
      "messages",
      "calculation",
      "started",
      "failed",
      "wait",
      "response",
      "sent",
      "working",
      "processes"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Proposal process - Business rules",
    "content": "In the out-of-the-box proposal workflow, the following business rules apply to the indicated metaphase or phase. Proposal process - all phases Event Condition Service Management action After change Created by is changed and isn't empty Owned by is empty Owned By = Created by None Validate: Review decision is empty, or Review decision isn't empty and Review feedback isn't empty Display the proposal workflow error message in case of a failure Phase Id is Review Validate: Cost, Business value, and Risk rating aren't empty Display the proposal workflow error message in case of a failure The metaphase is Assessment Task plan for review is rejected Phase Id = Close Review decision = Reject Review feedback =Rejected None Validate: Risk rating is empty, or Risk rating > 0 and <= 100 Display the proposal workflow error message in case of a failure Rendering Metaphase isn't Assessment Disable Currency None Disable Review decision Disable Review feedback None Suggested values for Business unit sh",
    "url": "processproposals",
    "filename": "processproposals",
    "headings": [
      "Proposal process - all phases",
      "Compose phase",
      "Review phase",
      "Related topics"
    ],
    "keywords": [
      "proposal",
      "process",
      "business",
      "rules",
      "all",
      "phases",
      "compose",
      "phase",
      "review",
      "related",
      "topics",
      "out-of-the-box",
      "workflow",
      "following",
      "apply",
      "indicated",
      "metaphase",
      "phase.",
      "event",
      "condition",
      "service",
      "management",
      "action",
      "after",
      "change",
      "created",
      "changed",
      "isn",
      "empty",
      "owned",
      "none",
      "validate",
      "decision",
      "feedback",
      "display",
      "error",
      "message",
      "case",
      "failure",
      "id",
      "cost",
      "value",
      "risk",
      "rating",
      "aren",
      "assessment",
      "task",
      "plan",
      "rejected",
      "close",
      "reject",
      "100",
      "rendering",
      "disable",
      "currency",
      "suggested",
      "values",
      "unit",
      "organizational",
      "group",
      "type",
      "target",
      "operate",
      "refinement",
      "priority",
      "entering",
      "current",
      "user",
      "set",
      "usd",
      "medium",
      "transitioning",
      "there",
      "processing",
      "abandon",
      "running",
      "idea",
      "leaving",
      "messages",
      "working",
      "processes"
    ],
    "language": "en",
    "word_count": 126,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "proposal process - business rules",
    "contentLower": "in the out-of-the-box proposal workflow, the following business rules apply to the indicated metaphase or phase. proposal process - all phases event condition service management action after change created by is changed and isn't empty owned by is empty owned by = created by none validate: review decision is empty, or review decision isn't empty and review feedback isn't empty display the proposal workflow error message in case of a failure phase id is review validate: cost, business value, and risk rating aren't empty display the proposal workflow error message in case of a failure the metaphase is assessment task plan for review is rejected phase id = close review decision = reject review feedback =rejected none validate: risk rating is empty, or risk rating > 0 and <= 100 display the proposal workflow error message in case of a failure rendering metaphase isn't assessment disable currency none disable review decision disable review feedback none suggested values for business unit sh",
    "keywordsLower": [
      "proposal",
      "process",
      "business",
      "rules",
      "all",
      "phases",
      "compose",
      "phase",
      "review",
      "related",
      "topics",
      "out-of-the-box",
      "workflow",
      "following",
      "apply",
      "indicated",
      "metaphase",
      "phase.",
      "event",
      "condition",
      "service",
      "management",
      "action",
      "after",
      "change",
      "created",
      "changed",
      "isn",
      "empty",
      "owned",
      "none",
      "validate",
      "decision",
      "feedback",
      "display",
      "error",
      "message",
      "case",
      "failure",
      "id",
      "cost",
      "value",
      "risk",
      "rating",
      "aren",
      "assessment",
      "task",
      "plan",
      "rejected",
      "close",
      "reject",
      "100",
      "rendering",
      "disable",
      "currency",
      "suggested",
      "values",
      "unit",
      "organizational",
      "group",
      "type",
      "target",
      "operate",
      "refinement",
      "priority",
      "entering",
      "current",
      "user",
      "set",
      "usd",
      "medium",
      "transitioning",
      "there",
      "processing",
      "abandon",
      "running",
      "idea",
      "leaving",
      "messages",
      "working",
      "processes"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Mobile app settings",
    "content": "Service Management provides default settings for the mobile app. To customize the settings, select Administration > Configuration > Application Settings > Mobile App Settings from the main menu. You can edit the following settings. Enable the mobile app By default, the ability to log into SMA using the Enterprise Service Management mobile app is enabled. In the Enable mobile app field, you can disable this by selecting Off. If you disable this feature, the tenant will be unable to use the mobile app, and the suite won't connect to the Google Firebase server, which your system requires for mobile push notifications. Mobile QR code URL In the Mobile QR code URL field, you can customize the website URL placed into the mobile app QR code; if this field is left blank, the mobile app uses the default website URL. Don't append the tenant ID when you manually add the customized website URL in this field. For more information, see Configure the Enterprise Service Management mobile app. Enable M",
    "url": "mobileappsettings",
    "filename": "mobileappsettings",
    "headings": [
      "Enable the mobile app",
      "Mobile QR code URL",
      "Enable Microsoft Intune for the mobile app",
      "Configure the mobile app in the Intune admin center",
      "Create an app protection policy for Android",
      "Create an app protection policy for iOS",
      "Pre-define the tenant URL for the Intune-managed mobile app",
      "Allowed Microsoft Intune account domains",
      "Enable Agent Mobile",
      "Enable Debug Settings on mobile",
      "Custom applications",
      "Allow screen capture on Android"
    ],
    "keywords": [
      "link.Type",
      "firstDomain.com",
      "policies.Open",
      "mobile.app",
      "25.3",
      "secondDomain.com",
      "https://login.microsoftonline.com/{tenant-id}/adminconsent?client_id=e1e0da1a-ed99-4d5b-a89d-5a962161d271&redirect_uri=msauth://com.microfocus.sma.mobile.app/gYfucgrOlZ3FLWgYctqk1bCxZbo%3D",
      "https://mydomain.net:443/saw/ess?TENANTID=123456789",
      "microsoftonline.com",
      "Select.Add",
      "com.mf",
      "mydomain.net",
      "microfocus.sma",
      "page.Go",
      "mobile",
      "app",
      "settings",
      "enable",
      "qr",
      "code",
      "url",
      "microsoft",
      "intune",
      "configure",
      "admin",
      "center",
      "create",
      "protection",
      "policy",
      "android",
      "ios",
      "pre-define",
      "tenant",
      "intune-managed",
      "allowed",
      "account",
      "domains",
      "agent",
      "debug",
      "custom",
      "applications",
      "allow",
      "screen",
      "capture",
      "service",
      "management",
      "provides",
      "default",
      "app.",
      "customize",
      "select",
      "administration",
      "configuration",
      "application",
      "main",
      "menu.",
      "edit",
      "following",
      "settings.",
      "ability",
      "log",
      "sma",
      "enterprise",
      "enabled.",
      "field",
      "disable",
      "selecting",
      "off.",
      "feature",
      "unable",
      "suite",
      "won",
      "connect",
      "google",
      "firebase",
      "server",
      "system",
      "requires",
      "push",
      "notifications.",
      "website",
      "placed",
      "left",
      "blank",
      "uses",
      "url.",
      "don",
      "append",
      "id",
      "manually",
      "add",
      "customized",
      "field.",
      "information",
      "see",
      "provide",
      "seamless",
      "sign-in",
      "experience",
      "webview-based"
    ],
    "language": "en",
    "word_count": 110,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "mobile app settings",
    "contentLower": "service management provides default settings for the mobile app. to customize the settings, select administration > configuration > application settings > mobile app settings from the main menu. you can edit the following settings. enable the mobile app by default, the ability to log into sma using the enterprise service management mobile app is enabled. in the enable mobile app field, you can disable this by selecting off. if you disable this feature, the tenant will be unable to use the mobile app, and the suite won't connect to the google firebase server, which your system requires for mobile push notifications. mobile qr code url in the mobile qr code url field, you can customize the website url placed into the mobile app qr code; if this field is left blank, the mobile app uses the default website url. don't append the tenant id when you manually add the customized website url in this field. for more information, see configure the enterprise service management mobile app. enable m",
    "keywordsLower": [
      "link.type",
      "firstdomain.com",
      "policies.open",
      "mobile.app",
      "25.3",
      "seconddomain.com",
      "https://login.microsoftonline.com/{tenant-id}/adminconsent?client_id=e1e0da1a-ed99-4d5b-a89d-5a962161d271&redirect_uri=msauth://com.microfocus.sma.mobile.app/gyfucgrolz3flwgyctqk1bcxzbo%3d",
      "https://mydomain.net:443/saw/ess?tenantid=123456789",
      "microsoftonline.com",
      "select.add",
      "com.mf",
      "mydomain.net",
      "microfocus.sma",
      "page.go",
      "mobile",
      "app",
      "settings",
      "enable",
      "qr",
      "code",
      "url",
      "microsoft",
      "intune",
      "configure",
      "admin",
      "center",
      "create",
      "protection",
      "policy",
      "android",
      "ios",
      "pre-define",
      "tenant",
      "intune-managed",
      "allowed",
      "account",
      "domains",
      "agent",
      "debug",
      "custom",
      "applications",
      "allow",
      "screen",
      "capture",
      "service",
      "management",
      "provides",
      "default",
      "app.",
      "customize",
      "select",
      "administration",
      "configuration",
      "application",
      "main",
      "menu.",
      "edit",
      "following",
      "settings.",
      "ability",
      "log",
      "sma",
      "enterprise",
      "enabled.",
      "field",
      "disable",
      "selecting",
      "off.",
      "feature",
      "unable",
      "suite",
      "won",
      "connect",
      "google",
      "firebase",
      "server",
      "system",
      "requires",
      "push",
      "notifications.",
      "website",
      "placed",
      "left",
      "blank",
      "uses",
      "url.",
      "don",
      "append",
      "id",
      "manually",
      "add",
      "customized",
      "field.",
      "information",
      "see",
      "provide",
      "seamless",
      "sign-in",
      "experience",
      "webview-based"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage the data domains list",
    "content": "The system stores all data domains in the Data domains list. As the tenant admin, you can directly update the out-of-the-box (OOB) list in the Lists Management module. However, if you have many data domains, use the Export and Import features to manage your data domains in an easier way. The Export and Import features process data domain values for English (US) and other supported languages. Notes: By default, all Service Management tenants come with a single data domain named Public. Each tenant can have up to 1,000 data domains. When you reach this limit, the system disables the Add button in the List items area. Pagination occurs when there are more than 250 data domains. There is no option to define the order of the items in the data domains list. The system always adds new list items to the end of the existing list. Update the OOB data domains list If you have a limited number of data domains, you can directly update the OOB data domains list with your custom values. To update the",
    "url": "managedatadomains",
    "filename": "managedatadomains",
    "headings": [
      "Update the OOB data domains list",
      "Import custom data domains"
    ],
    "keywords": [
      "DataDomains_YYYYMMDD_XXXXXXX.zip",
      "DataDomains_en.xls",
      "DataDomains_fr.xls",
      "GB.xls",
      "https://<External",
      "manage",
      "data",
      "domains",
      "list",
      "update",
      "oob",
      "import",
      "custom",
      "system",
      "stores",
      "all",
      "list.",
      "tenant",
      "admin",
      "directly",
      "out-of-the-box",
      "lists",
      "management",
      "module.",
      "however",
      "many",
      "export",
      "features",
      "easier",
      "way.",
      "process",
      "domain",
      "values",
      "english",
      "supported",
      "languages.",
      "notes",
      "default",
      "service",
      "tenants",
      "come",
      "single",
      "named",
      "public.",
      "000",
      "domains.",
      "reach",
      "limit",
      "disables",
      "add",
      "button",
      "items",
      "area.",
      "pagination",
      "occurs",
      "there",
      "250",
      "option",
      "define",
      "order",
      "always",
      "adds",
      "new",
      "end",
      "existing",
      "limited",
      "number",
      "values.",
      "log",
      "agent",
      "interface",
      "admin.",
      "go",
      "administration",
      "configuration",
      "open",
      "click",
      "enter",
      "name",
      "display",
      "label",
      "ok",
      "another",
      "one",
      "want",
      "disable",
      "any",
      "clear",
      "active",
      "check",
      "box",
      "them.",
      "note",
      "public",
      "domain.",
      "save",
      "needs",
      "instead",
      "adding",
      "module"
    ],
    "language": "en",
    "word_count": 111,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage the data domains list",
    "contentLower": "the system stores all data domains in the data domains list. as the tenant admin, you can directly update the out-of-the-box (oob) list in the lists management module. however, if you have many data domains, use the export and import features to manage your data domains in an easier way. the export and import features process data domain values for english (us) and other supported languages. notes: by default, all service management tenants come with a single data domain named public. each tenant can have up to 1,000 data domains. when you reach this limit, the system disables the add button in the list items area. pagination occurs when there are more than 250 data domains. there is no option to define the order of the items in the data domains list. the system always adds new list items to the end of the existing list. update the oob data domains list if you have a limited number of data domains, you can directly update the oob data domains list with your custom values. to update the",
    "keywordsLower": [
      "datadomains_yyyymmdd_xxxxxxx.zip",
      "datadomains_en.xls",
      "datadomains_fr.xls",
      "gb.xls",
      "https://<external",
      "manage",
      "data",
      "domains",
      "list",
      "update",
      "oob",
      "import",
      "custom",
      "system",
      "stores",
      "all",
      "list.",
      "tenant",
      "admin",
      "directly",
      "out-of-the-box",
      "lists",
      "management",
      "module.",
      "however",
      "many",
      "export",
      "features",
      "easier",
      "way.",
      "process",
      "domain",
      "values",
      "english",
      "supported",
      "languages.",
      "notes",
      "default",
      "service",
      "tenants",
      "come",
      "single",
      "named",
      "public.",
      "000",
      "domains.",
      "reach",
      "limit",
      "disables",
      "add",
      "button",
      "items",
      "area.",
      "pagination",
      "occurs",
      "there",
      "250",
      "option",
      "define",
      "order",
      "always",
      "adds",
      "new",
      "end",
      "existing",
      "limited",
      "number",
      "values.",
      "log",
      "agent",
      "interface",
      "admin.",
      "go",
      "administration",
      "configuration",
      "open",
      "click",
      "enter",
      "name",
      "display",
      "label",
      "ok",
      "another",
      "one",
      "want",
      "disable",
      "any",
      "clear",
      "active",
      "check",
      "box",
      "them.",
      "note",
      "public",
      "domain.",
      "save",
      "needs",
      "instead",
      "adding",
      "module"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "MT console for shared service providers",
    "content": "Service Management provides the ability for shared services providers to manage their business units' and clients' request and incident records in one user interface via a multi-tenant (MT) Console in the provider tenant. Each business unit and client can use its own Service Management tenant while the MT Console displays all data together in one consolidated view. The data provided empowers agents to prioritize their work and be more efficient. The provider tenants and managed tenants are created and managed in Suite Administration. When a managed tenant is added to the provider tenant, a vendor record is created in the provider tenant automatically. The vendor's name is the same as the managed account name and the code is the same as the managed account's shared service customer code. This code appears in the MT Console as the prefix to all incidents and requests belonging to this managed tenant. When a shared service user is granted access to a managed tenant, this user will also be",
    "url": "mtconsole",
    "filename": "mtconsole",
    "headings": [
      "Use the MT Console",
      "Viewing and Filtering in the MT Console Dashboard",
      "Viewing and Filtering in the MT Console Requests or Incidents Grid",
      "Related topics"
    ],
    "keywords": [
      "mt",
      "console",
      "shared",
      "service",
      "providers",
      "viewing",
      "filtering",
      "dashboard",
      "requests",
      "incidents",
      "grid",
      "related",
      "topics",
      "management",
      "provides",
      "ability",
      "services",
      "manage",
      "business",
      "units",
      "clients",
      "request",
      "incident",
      "records",
      "one",
      "user",
      "interface",
      "via",
      "multi-tenant",
      "provider",
      "tenant.",
      "unit",
      "client",
      "own",
      "tenant",
      "while",
      "displays",
      "all",
      "data",
      "together",
      "consolidated",
      "view.",
      "provided",
      "empowers",
      "agents",
      "prioritize",
      "work",
      "efficient.",
      "tenants",
      "managed",
      "created",
      "suite",
      "administration.",
      "added",
      "vendor",
      "record",
      "automatically.",
      "name",
      "same",
      "account",
      "code",
      "customer",
      "code.",
      "appears",
      "prefix",
      "belonging",
      "granted",
      "access",
      "synced",
      "following",
      "occurs",
      "logs",
      "admin",
      "click",
      "sync",
      "auto",
      "synchronization",
      "job",
      "completes",
      "users",
      "console.",
      "main",
      "menu.",
      "available",
      "includes",
      "tabs",
      "dashboard.",
      "customers",
      "including",
      "total",
      "open",
      "top",
      "icon",
      "per",
      "displaying",
      "client.",
      "hovering",
      "tooltip",
      "detailing",
      "priority"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "mt console for shared service providers",
    "contentLower": "service management provides the ability for shared services providers to manage their business units' and clients' request and incident records in one user interface via a multi-tenant (mt) console in the provider tenant. each business unit and client can use its own service management tenant while the mt console displays all data together in one consolidated view. the data provided empowers agents to prioritize their work and be more efficient. the provider tenants and managed tenants are created and managed in suite administration. when a managed tenant is added to the provider tenant, a vendor record is created in the provider tenant automatically. the vendor's name is the same as the managed account name and the code is the same as the managed account's shared service customer code. this code appears in the mt console as the prefix to all incidents and requests belonging to this managed tenant. when a shared service user is granted access to a managed tenant, this user will also be",
    "keywordsLower": [
      "mt",
      "console",
      "shared",
      "service",
      "providers",
      "viewing",
      "filtering",
      "dashboard",
      "requests",
      "incidents",
      "grid",
      "related",
      "topics",
      "management",
      "provides",
      "ability",
      "services",
      "manage",
      "business",
      "units",
      "clients",
      "request",
      "incident",
      "records",
      "one",
      "user",
      "interface",
      "via",
      "multi-tenant",
      "provider",
      "tenant.",
      "unit",
      "client",
      "own",
      "tenant",
      "while",
      "displays",
      "all",
      "data",
      "together",
      "consolidated",
      "view.",
      "provided",
      "empowers",
      "agents",
      "prioritize",
      "work",
      "efficient.",
      "tenants",
      "managed",
      "created",
      "suite",
      "administration.",
      "added",
      "vendor",
      "record",
      "automatically.",
      "name",
      "same",
      "account",
      "code",
      "customer",
      "code.",
      "appears",
      "prefix",
      "belonging",
      "granted",
      "access",
      "synced",
      "following",
      "occurs",
      "logs",
      "admin",
      "click",
      "sync",
      "auto",
      "synchronization",
      "job",
      "completes",
      "users",
      "console.",
      "main",
      "menu.",
      "available",
      "includes",
      "tabs",
      "dashboard.",
      "customers",
      "including",
      "total",
      "open",
      "top",
      "icon",
      "per",
      "displaying",
      "client.",
      "hovering",
      "tooltip",
      "detailing",
      "priority"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Providers",
    "content": "In the Provider module, you can manage: Aggregation providers Deployment resources providers Cloud cost data providers Image aggregation providers Related topics Aggregation providers Deployment Resource Providers Cloud cost data providers Image aggregation providers",
    "url": "provider",
    "filename": "provider",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "providers",
      "related",
      "topics",
      "provider",
      "module",
      "manage",
      "aggregation",
      "deployment",
      "resources",
      "cloud",
      "cost",
      "data",
      "image",
      "resource"
    ],
    "language": "en",
    "word_count": 30,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "providers",
    "contentLower": "in the provider module, you can manage: aggregation providers deployment resources providers cloud cost data providers image aggregation providers related topics aggregation providers deployment resource providers cloud cost data providers image aggregation providers",
    "keywordsLower": [
      "providers",
      "related",
      "topics",
      "provider",
      "module",
      "manage",
      "aggregation",
      "deployment",
      "resources",
      "cloud",
      "cost",
      "data",
      "image",
      "resource"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Provider properties",
    "content": "The default properties are available during the time of creation of provider type. To use a particular provider type, you have to configure its properties. To know more about the properties of Amazon AWS, Google Cloud Platform, Kubernetes, Microsoft Azure, Microsoft SCVMM, Openstack, Terraform, and VMware vCenter, see Use > Use capsule in Cloud Management in the respective capsule documentation. To know more about the properties of UCMDB, see Configure UCMDB provider. Custom properties of a provider capture the additional configuration information about a particular provider. For example, you can use custom properties to model provider resources, such as data centers, hypervisors, and datastores for a specific VMware vCenter provider. When a sequenced design is provisioned, an Operations Orchestration flow can read and write provider property values during service provisioning. For the Google Cloud Platform provider, you must configure the following properties: Client ID Client Secret ",
    "url": "providerproperties",
    "filename": "providerproperties",
    "headings": [
      "Tasks"
    ],
    "keywords": [
      "provider",
      "properties",
      "tasks",
      "default",
      "available",
      "during",
      "time",
      "creation",
      "type.",
      "particular",
      "type",
      "configure",
      "properties.",
      "know",
      "about",
      "amazon",
      "aws",
      "google",
      "cloud",
      "platform",
      "kubernetes",
      "microsoft",
      "azure",
      "scvmm",
      "openstack",
      "terraform",
      "vmware",
      "vcenter",
      "see",
      "capsule",
      "management",
      "respective",
      "documentation.",
      "ucmdb",
      "provider.",
      "custom",
      "capture",
      "additional",
      "configuration",
      "information",
      "example",
      "model",
      "resources",
      "such",
      "data",
      "centers",
      "hypervisors",
      "datastores",
      "specific",
      "sequenced",
      "design",
      "provisioned",
      "operations",
      "orchestration",
      "flow",
      "read",
      "write",
      "property",
      "values",
      "service",
      "provisioning.",
      "following",
      "client",
      "id",
      "secret",
      "refresh",
      "token",
      "gcp",
      "cloudslang",
      "optional",
      "needed",
      "provisioning",
      "process",
      "requires",
      "them.",
      "flows",
      "designs.",
      "create",
      "click",
      "create.",
      "listed",
      "table.",
      "edit",
      "gear",
      "icon",
      "select",
      "edit.",
      "table",
      "items",
      "delete",
      "delete.",
      "refresh.",
      "item",
      "description",
      "one",
      "boolean",
      "whose",
      "value",
      "true",
      "false."
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "provider properties",
    "contentLower": "the default properties are available during the time of creation of provider type. to use a particular provider type, you have to configure its properties. to know more about the properties of amazon aws, google cloud platform, kubernetes, microsoft azure, microsoft scvmm, openstack, terraform, and vmware vcenter, see use > use capsule in cloud management in the respective capsule documentation. to know more about the properties of ucmdb, see configure ucmdb provider. custom properties of a provider capture the additional configuration information about a particular provider. for example, you can use custom properties to model provider resources, such as data centers, hypervisors, and datastores for a specific vmware vcenter provider. when a sequenced design is provisioned, an operations orchestration flow can read and write provider property values during service provisioning. for the google cloud platform provider, you must configure the following properties: client id client secret ",
    "keywordsLower": [
      "provider",
      "properties",
      "tasks",
      "default",
      "available",
      "during",
      "time",
      "creation",
      "type.",
      "particular",
      "type",
      "configure",
      "properties.",
      "know",
      "about",
      "amazon",
      "aws",
      "google",
      "cloud",
      "platform",
      "kubernetes",
      "microsoft",
      "azure",
      "scvmm",
      "openstack",
      "terraform",
      "vmware",
      "vcenter",
      "see",
      "capsule",
      "management",
      "respective",
      "documentation.",
      "ucmdb",
      "provider.",
      "custom",
      "capture",
      "additional",
      "configuration",
      "information",
      "example",
      "model",
      "resources",
      "such",
      "data",
      "centers",
      "hypervisors",
      "datastores",
      "specific",
      "sequenced",
      "design",
      "provisioned",
      "operations",
      "orchestration",
      "flow",
      "read",
      "write",
      "property",
      "values",
      "service",
      "provisioning.",
      "following",
      "client",
      "id",
      "secret",
      "refresh",
      "token",
      "gcp",
      "cloudslang",
      "optional",
      "needed",
      "provisioning",
      "process",
      "requires",
      "them.",
      "flows",
      "designs.",
      "create",
      "click",
      "create.",
      "listed",
      "table.",
      "edit",
      "gear",
      "icon",
      "select",
      "edit.",
      "table",
      "items",
      "delete",
      "delete.",
      "refresh.",
      "item",
      "description",
      "one",
      "boolean",
      "whose",
      "value",
      "true",
      "false."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Provider resource pools",
    "content": "Resource pools create an association between resources that can be provisioned by a provider and subscriptions. You can create a resource pool on a provider to represent a pool of resources associated with that provider. For example, you can create a resource pool on a VMware vCenter resource provider that corresponds to a VMware cluster. You can also model an infrastructure orchestration resource pool (a pool of CPU, memory, storage, and networking) as a resource pool. You can decide which provider concepts, if any, you wish to model as resource pools; the resource pool concept may not be applicable to all provider types. When you model resources on a provider in resource pools, you should model them in one of the two fashions described below: A single resource pool on a resource provider that models all resources that can be allocated on this provider Multiple resource pools on a resource provider, each of which models its own portion of the total available resources on the provider.",
    "url": "resourcepool",
    "filename": "resourcepool",
    "headings": [
      "Tasks",
      "Best practices"
    ],
    "keywords": [
      "provider",
      "resource",
      "pools",
      "tasks",
      "best",
      "practices",
      "create",
      "association",
      "between",
      "resources",
      "provisioned",
      "subscriptions.",
      "pool",
      "represent",
      "associated",
      "provider.",
      "example",
      "vmware",
      "vcenter",
      "corresponds",
      "cluster.",
      "model",
      "infrastructure",
      "orchestration",
      "cpu",
      "memory",
      "storage",
      "networking",
      "pool.",
      "decide",
      "concepts",
      "any",
      "wish",
      "concept",
      "applicable",
      "all",
      "types.",
      "one",
      "two",
      "fashions",
      "described",
      "below",
      "single",
      "models",
      "allocated",
      "multiple",
      "own",
      "portion",
      "total",
      "available",
      "view",
      "selected",
      "see",
      "list",
      "descriptions.",
      "disabled",
      "indicated",
      "label",
      "participate",
      "allocation",
      "processing",
      "new",
      "click",
      "create.",
      "provide",
      "information",
      "listed",
      "following",
      "table.",
      "about",
      "whose",
      "want",
      "view.",
      "item",
      "description",
      "display",
      "name",
      "known",
      "cluster",
      "value",
      "exact",
      "configured",
      "vcenter.",
      "synchronization",
      "action",
      "updates",
      "communicating",
      "last",
      "synchronized",
      "field",
      "overview",
      "tab",
      "indicates",
      "time",
      "local",
      "client",
      "completed",
      "successfully.",
      "enabled",
      "availability"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "provider resource pools",
    "contentLower": "resource pools create an association between resources that can be provisioned by a provider and subscriptions. you can create a resource pool on a provider to represent a pool of resources associated with that provider. for example, you can create a resource pool on a vmware vcenter resource provider that corresponds to a vmware cluster. you can also model an infrastructure orchestration resource pool (a pool of cpu, memory, storage, and networking) as a resource pool. you can decide which provider concepts, if any, you wish to model as resource pools; the resource pool concept may not be applicable to all provider types. when you model resources on a provider in resource pools, you should model them in one of the two fashions described below: a single resource pool on a resource provider that models all resources that can be allocated on this provider multiple resource pools on a resource provider, each of which models its own portion of the total available resources on the provider.",
    "keywordsLower": [
      "provider",
      "resource",
      "pools",
      "tasks",
      "best",
      "practices",
      "create",
      "association",
      "between",
      "resources",
      "provisioned",
      "subscriptions.",
      "pool",
      "represent",
      "associated",
      "provider.",
      "example",
      "vmware",
      "vcenter",
      "corresponds",
      "cluster.",
      "model",
      "infrastructure",
      "orchestration",
      "cpu",
      "memory",
      "storage",
      "networking",
      "pool.",
      "decide",
      "concepts",
      "any",
      "wish",
      "concept",
      "applicable",
      "all",
      "types.",
      "one",
      "two",
      "fashions",
      "described",
      "below",
      "single",
      "models",
      "allocated",
      "multiple",
      "own",
      "portion",
      "total",
      "available",
      "view",
      "selected",
      "see",
      "list",
      "descriptions.",
      "disabled",
      "indicated",
      "label",
      "participate",
      "allocation",
      "processing",
      "new",
      "click",
      "create.",
      "provide",
      "information",
      "listed",
      "following",
      "table.",
      "about",
      "whose",
      "want",
      "view.",
      "item",
      "description",
      "display",
      "name",
      "known",
      "cluster",
      "value",
      "exact",
      "configured",
      "vcenter.",
      "synchronization",
      "action",
      "updates",
      "communicating",
      "last",
      "synchronized",
      "field",
      "overview",
      "tab",
      "indicates",
      "time",
      "local",
      "client",
      "completed",
      "successfully.",
      "enabled",
      "availability"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage provider types",
    "content": "You can perform the following tasks in this area: Create a provider type - Provide the information listed in the table below. Edit a provider type - See the table below for the items you can edit. Delete a provider type - A provider type can't be deleted if any resource offerings or providers of that provider type exist. Out-of-the-box provider types also can't be deleted. Item Description Name A name that's automatically generated, which may be needed when importing topology components. Display Name The display name you provide for the provider type. Description The description you provide for the provider type. Image The default image displayed for the provider type. To change the image, click Change Image select an image and upload it. Supported file extensions include.jpg, .jpeg, .gif, and.png. The recommended image size is 256 by 256 pixels.",
    "url": "providertypemanage",
    "filename": "providertypemanage",
    "headings": [],
    "keywords": [
      "include.jpg",
      "and.png",
      "manage",
      "provider",
      "types",
      "perform",
      "following",
      "tasks",
      "area",
      "create",
      "type",
      "provide",
      "information",
      "listed",
      "table",
      "below.",
      "edit",
      "see",
      "below",
      "items",
      "edit.",
      "delete",
      "deleted",
      "any",
      "resource",
      "offerings",
      "providers",
      "exist.",
      "out-of-the-box",
      "deleted.",
      "item",
      "description",
      "name",
      "automatically",
      "generated",
      "needed",
      "importing",
      "topology",
      "components.",
      "display",
      "type.",
      "image",
      "default",
      "displayed",
      "change",
      "click",
      "select",
      "upload",
      "it.",
      "supported",
      "file",
      "extensions",
      ".jpeg",
      ".gif",
      "and.png.",
      "recommended",
      "size",
      "256",
      "pixels."
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage provider types",
    "contentLower": "you can perform the following tasks in this area: create a provider type - provide the information listed in the table below. edit a provider type - see the table below for the items you can edit. delete a provider type - a provider type can't be deleted if any resource offerings or providers of that provider type exist. out-of-the-box provider types also can't be deleted. item description name a name that's automatically generated, which may be needed when importing topology components. display name the display name you provide for the provider type. description the description you provide for the provider type. image the default image displayed for the provider type. to change the image, click change image select an image and upload it. supported file extensions include.jpg, .jpeg, .gif, and.png. the recommended image size is 256 by 256 pixels.",
    "keywordsLower": [
      "include.jpg",
      "and.png",
      "manage",
      "provider",
      "types",
      "perform",
      "following",
      "tasks",
      "area",
      "create",
      "type",
      "provide",
      "information",
      "listed",
      "table",
      "below.",
      "edit",
      "see",
      "below",
      "items",
      "edit.",
      "delete",
      "deleted",
      "any",
      "resource",
      "offerings",
      "providers",
      "exist.",
      "out-of-the-box",
      "deleted.",
      "item",
      "description",
      "name",
      "automatically",
      "generated",
      "needed",
      "importing",
      "topology",
      "components.",
      "display",
      "type.",
      "image",
      "default",
      "displayed",
      "change",
      "click",
      "select",
      "upload",
      "it.",
      "supported",
      "file",
      "extensions",
      ".jpeg",
      ".gif",
      "and.png.",
      "recommended",
      "size",
      "256",
      "pixels."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Notification history",
    "content": "When the email notification is triggered for a record, the notification history and detailed information will be available on the History page of this record. This is available for all agent users, not only for tenant admin users. If you are a tenant admin and want to view all notification history events, you may go to the Notification History UI. When the associated record of a notification event is deleted, the notification event and event details are also deleted. Tenant admins can use the notification history capability to view email notification events sent by Service Management. It allows the tenant admin to view the recipient details of each notification event and the notification template used by the notification service to send the email. Notification history doesn't capture notifications not associated with a tenant, for example, emails generated during user account activation. To view the Notification History UI, go to Administration > Utilities > Notification History. To sh",
    "url": "notificationhistory",
    "filename": "notificationhistory",
    "headings": [
      "Notification history events page",
      "Notification history event detail page",
      "General",
      "Resending history"
    ],
    "keywords": [
      "notification",
      "history",
      "events",
      "page",
      "event",
      "detail",
      "general",
      "resending",
      "email",
      "triggered",
      "record",
      "detailed",
      "information",
      "available",
      "record.",
      "all",
      "agent",
      "users",
      "tenant",
      "admin",
      "users.",
      "want",
      "view",
      "go",
      "ui.",
      "associated",
      "deleted",
      "details",
      "deleted.",
      "admins",
      "capability",
      "sent",
      "service",
      "management.",
      "allows",
      "recipient",
      "template",
      "send",
      "email.",
      "doesn",
      "capture",
      "notifications",
      "example",
      "emails",
      "generated",
      "during",
      "user",
      "account",
      "activation.",
      "ui",
      "administration",
      "utilities",
      "history.",
      "show",
      "menu",
      "item",
      "turn",
      "enable",
      "configuration",
      "application",
      "settings.",
      "tab",
      "includes",
      "following",
      "pages",
      "lists",
      "events.",
      "shows",
      "selected",
      "event.",
      "comprises",
      "fields",
      "edit",
      "views",
      "ootb",
      "create",
      "manage",
      "own",
      "views.",
      "filters",
      "click",
      "select",
      "any",
      "option",
      "filter",
      "data.",
      "based",
      "creation",
      "time",
      "id",
      "last",
      "update",
      "parent",
      "type",
      "status.",
      "group",
      "primary",
      "secondary",
      "tertiary",
      "field"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "notification history",
    "contentLower": "when the email notification is triggered for a record, the notification history and detailed information will be available on the history page of this record. this is available for all agent users, not only for tenant admin users. if you are a tenant admin and want to view all notification history events, you may go to the notification history ui. when the associated record of a notification event is deleted, the notification event and event details are also deleted. tenant admins can use the notification history capability to view email notification events sent by service management. it allows the tenant admin to view the recipient details of each notification event and the notification template used by the notification service to send the email. notification history doesn't capture notifications not associated with a tenant, for example, emails generated during user account activation. to view the notification history ui, go to administration > utilities > notification history. to sh",
    "keywordsLower": [
      "notification",
      "history",
      "events",
      "page",
      "event",
      "detail",
      "general",
      "resending",
      "email",
      "triggered",
      "record",
      "detailed",
      "information",
      "available",
      "record.",
      "all",
      "agent",
      "users",
      "tenant",
      "admin",
      "users.",
      "want",
      "view",
      "go",
      "ui.",
      "associated",
      "deleted",
      "details",
      "deleted.",
      "admins",
      "capability",
      "sent",
      "service",
      "management.",
      "allows",
      "recipient",
      "template",
      "send",
      "email.",
      "doesn",
      "capture",
      "notifications",
      "example",
      "emails",
      "generated",
      "during",
      "user",
      "account",
      "activation.",
      "ui",
      "administration",
      "utilities",
      "history.",
      "show",
      "menu",
      "item",
      "turn",
      "enable",
      "configuration",
      "application",
      "settings.",
      "tab",
      "includes",
      "following",
      "pages",
      "lists",
      "events.",
      "shows",
      "selected",
      "event.",
      "comprises",
      "fields",
      "edit",
      "views",
      "ootb",
      "create",
      "manage",
      "own",
      "views.",
      "filters",
      "click",
      "select",
      "any",
      "option",
      "filter",
      "data.",
      "based",
      "creation",
      "time",
      "id",
      "last",
      "update",
      "parent",
      "type",
      "status.",
      "group",
      "primary",
      "secondary",
      "tertiary",
      "field"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OO Central license",
    "content": "Operations Orchestration license is available as follows: License for OO in Cloud Management is available out-of-the-box. No action is required to access the OO license. License for OO in Service Management is available as concurrent licenses. This license has to be applied in the AutoPass Licensing Server.",
    "url": "ootenantlicense",
    "filename": "ootenantlicense",
    "headings": [],
    "keywords": [
      "oo",
      "central",
      "license",
      "operations",
      "orchestration",
      "available",
      "follows",
      "cloud",
      "management",
      "out-of-the-box.",
      "action",
      "required",
      "access",
      "license.",
      "service",
      "concurrent",
      "licenses.",
      "applied",
      "autopass",
      "licensing",
      "server."
    ],
    "language": "en",
    "word_count": 31,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "oo central license",
    "contentLower": "operations orchestration license is available as follows: license for oo in cloud management is available out-of-the-box. no action is required to access the oo license. license for oo in service management is available as concurrent licenses. this license has to be applied in the autopass licensing server.",
    "keywordsLower": [
      "oo",
      "central",
      "license",
      "operations",
      "orchestration",
      "available",
      "follows",
      "cloud",
      "management",
      "out-of-the-box.",
      "action",
      "required",
      "access",
      "license.",
      "service",
      "concurrent",
      "licenses.",
      "applied",
      "autopass",
      "licensing",
      "server."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage OO Workflow Designer users",
    "content": "Role Location System Administrator IdM Administration Console of OO Workflow Designer To manage users, click OO Workflow Designer APPLICATION > IdM Administration Console, click the organization name, then click the Users tab. This page displays user name, the first authentication date, and the last authentication date. The user management page lists all users in the organization. You can search, add, edit, and delete the users. Search for an existing user To search for an existing user: In the upper-right corner, enter the user name into the search bar, then click search to perform the search. Add a new user Click on the top right menu to add a user. Enter the user name, display name and password. Choose the user type: REGULAR or SYSTEM Select the Password Policy. To define user attributes, in the User Attributes area: Click Add Attributes to add user attributes. In the Attribute Name box, enter the name In the Value box, enter the valid value Click SAVE. The user attributes gets save",
    "url": "manageusersood",
    "filename": "manageusersood",
    "headings": [
      "Search for an existing user",
      "Add a new user",
      "Edit an existing user",
      "Remove the user password",
      "Add a new password",
      "Change the user password",
      "Delete an existing user",
      "Next",
      "Related topics"
    ],
    "keywords": [
      "manage",
      "oo",
      "workflow",
      "designer",
      "users",
      "search",
      "existing",
      "user",
      "add",
      "new",
      "edit",
      "remove",
      "password",
      "change",
      "delete",
      "next",
      "related",
      "topics",
      "role",
      "location",
      "system",
      "administrator",
      "idm",
      "administration",
      "console",
      "click",
      "application",
      "organization",
      "name",
      "tab.",
      "page",
      "displays",
      "first",
      "authentication",
      "date",
      "last",
      "date.",
      "management",
      "lists",
      "all",
      "organization.",
      "users.",
      "upper-right",
      "corner",
      "enter",
      "bar",
      "perform",
      "search.",
      "top",
      "right",
      "menu",
      "user.",
      "display",
      "password.",
      "choose",
      "type",
      "regular",
      "select",
      "policy.",
      "define",
      "attributes",
      "area",
      "attributes.",
      "attribute",
      "box",
      "value",
      "valid",
      "save.",
      "gets",
      "saved.",
      "whether",
      "passwords",
      "internal",
      "flows",
      "such",
      "ldap",
      "saml.",
      "authentications",
      "create",
      "same",
      "name.",
      "just",
      "action",
      "button",
      "lock",
      "toggle",
      "unlock",
      "attribute.",
      "value.",
      "want",
      "icon",
      "confirm",
      "deletion.",
      "menu.",
      "removal.",
      "after",
      "removing",
      "fields",
      "reset",
      "map"
    ],
    "language": "en",
    "word_count": 121,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage oo workflow designer users",
    "contentLower": "role location system administrator idm administration console of oo workflow designer to manage users, click oo workflow designer application > idm administration console, click the organization name, then click the users tab. this page displays user name, the first authentication date, and the last authentication date. the user management page lists all users in the organization. you can search, add, edit, and delete the users. search for an existing user to search for an existing user: in the upper-right corner, enter the user name into the search bar, then click search to perform the search. add a new user click on the top right menu to add a user. enter the user name, display name and password. choose the user type: regular or system select the password policy. to define user attributes, in the user attributes area: click add attributes to add user attributes. in the attribute name box, enter the name in the value box, enter the valid value click save. the user attributes gets save",
    "keywordsLower": [
      "manage",
      "oo",
      "workflow",
      "designer",
      "users",
      "search",
      "existing",
      "user",
      "add",
      "new",
      "edit",
      "remove",
      "password",
      "change",
      "delete",
      "next",
      "related",
      "topics",
      "role",
      "location",
      "system",
      "administrator",
      "idm",
      "administration",
      "console",
      "click",
      "application",
      "organization",
      "name",
      "tab.",
      "page",
      "displays",
      "first",
      "authentication",
      "date",
      "last",
      "date.",
      "management",
      "lists",
      "all",
      "organization.",
      "users.",
      "upper-right",
      "corner",
      "enter",
      "bar",
      "perform",
      "search.",
      "top",
      "right",
      "menu",
      "user.",
      "display",
      "password.",
      "choose",
      "type",
      "regular",
      "select",
      "policy.",
      "define",
      "attributes",
      "area",
      "attributes.",
      "attribute",
      "box",
      "value",
      "valid",
      "save.",
      "gets",
      "saved.",
      "whether",
      "passwords",
      "internal",
      "flows",
      "such",
      "ldap",
      "saml.",
      "authentications",
      "create",
      "same",
      "name.",
      "just",
      "action",
      "button",
      "lock",
      "toggle",
      "unlock",
      "attribute.",
      "value.",
      "want",
      "icon",
      "confirm",
      "deletion.",
      "menu.",
      "removal.",
      "after",
      "removing",
      "fields",
      "reset",
      "map"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage user groups for OO Workflow Designer",
    "content": "Role Location System Administrator IdM Administration Console of OO Workflow Designer To manage groups, go to OO Workflow Designer APPLICATION > IdM Administration Console, click the organization name, then click on the Groups tab. This page displays the group name and the related roles. Create a new user group Creating user groups helps you to manage what roles and permissions that you want to assign to the users. To create a new group: Click on the menu to add a group. in the Group Settings area, enter the user name, and display name, and choose an appropriate role from the Associated Roles drop-down list box. In the Users area, click the Associated Users drop-down and select an appropriate user from the existing user list. You can also type into a dynamic search field to filter the user results. Click SAVE. Edit an existing group To edit an existing group: Choose a group, then click to edit an existing group. You can change the Display Name of the group, Associated Roles, and Associ",
    "url": "managegroupsood",
    "filename": "managegroupsood",
    "headings": [
      "Create a new user group",
      "Edit an existing group",
      "Delete an existing group",
      "Map a user to a role using the group",
      "Related topics"
    ],
    "keywords": [
      "manage",
      "user",
      "groups",
      "oo",
      "workflow",
      "designer",
      "create",
      "new",
      "group",
      "edit",
      "existing",
      "delete",
      "map",
      "role",
      "related",
      "topics",
      "location",
      "system",
      "administrator",
      "idm",
      "administration",
      "console",
      "go",
      "application",
      "click",
      "organization",
      "name",
      "tab.",
      "page",
      "displays",
      "roles.",
      "creating",
      "helps",
      "what",
      "roles",
      "permissions",
      "want",
      "assign",
      "users.",
      "menu",
      "add",
      "group.",
      "settings",
      "area",
      "enter",
      "display",
      "choose",
      "appropriate",
      "associated",
      "drop-down",
      "list",
      "box.",
      "users",
      "select",
      "list.",
      "type",
      "dynamic",
      "search",
      "field",
      "filter",
      "results.",
      "save.",
      "change",
      "save",
      "apply",
      "changes.",
      "remove.",
      "remove",
      "specific",
      "displays.",
      "mapped",
      "role.",
      "instructions",
      "see"
    ],
    "language": "en",
    "word_count": 116,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage user groups for oo workflow designer",
    "contentLower": "role location system administrator idm administration console of oo workflow designer to manage groups, go to oo workflow designer application > idm administration console, click the organization name, then click on the groups tab. this page displays the group name and the related roles. create a new user group creating user groups helps you to manage what roles and permissions that you want to assign to the users. to create a new group: click on the menu to add a group. in the group settings area, enter the user name, and display name, and choose an appropriate role from the associated roles drop-down list box. in the users area, click the associated users drop-down and select an appropriate user from the existing user list. you can also type into a dynamic search field to filter the user results. click save. edit an existing group to edit an existing group: choose a group, then click to edit an existing group. you can change the display name of the group, associated roles, and associ",
    "keywordsLower": [
      "manage",
      "user",
      "groups",
      "oo",
      "workflow",
      "designer",
      "create",
      "new",
      "group",
      "edit",
      "existing",
      "delete",
      "map",
      "role",
      "related",
      "topics",
      "location",
      "system",
      "administrator",
      "idm",
      "administration",
      "console",
      "go",
      "application",
      "click",
      "organization",
      "name",
      "tab.",
      "page",
      "displays",
      "roles.",
      "creating",
      "helps",
      "what",
      "roles",
      "permissions",
      "want",
      "assign",
      "users.",
      "menu",
      "add",
      "group.",
      "settings",
      "area",
      "enter",
      "display",
      "choose",
      "appropriate",
      "associated",
      "drop-down",
      "list",
      "box.",
      "users",
      "select",
      "list.",
      "type",
      "dynamic",
      "search",
      "field",
      "filter",
      "results.",
      "save.",
      "change",
      "save",
      "apply",
      "changes.",
      "remove.",
      "remove",
      "specific",
      "displays.",
      "mapped",
      "role.",
      "instructions",
      "see"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage roles for OO Workflow Designer",
    "content": "Role Location System Administrator IdM Administration Console of OO Workflow Designer To add a new user and grant permissions to a new user, perform the steps described in the following topics. Add a new role Go to OO Workflow Designer Application > IdM Administration Console > Roles. The page displays the list of roles available in RPA Workflow Designer Click on the top right menu to add a new role and fill the following fields: Enter the Name, Display Name, and Description. Choose the Application, if available. Click Associated Permissions drop-down list box and select one or more appropriate permission/s from the available options. For example, flowDebug. Click SAVE to apply the settings. The new user with defined permission gets created. Edit an existing role Navigate to IdM Administration Console > Roles. Select a role and click edit . In the ROLE EDIT page, you can change the Display Name, Description and add/remove Associated Permissions. Click SAVE to apply the settings. Delete",
    "url": "managerolesood",
    "filename": "managerolesood",
    "headings": [
      "Add a new role",
      "Edit an existing role",
      "Delete an existing role",
      "Related topics"
    ],
    "keywords": [
      "manage",
      "roles",
      "oo",
      "workflow",
      "designer",
      "add",
      "new",
      "role",
      "edit",
      "existing",
      "delete",
      "related",
      "topics",
      "location",
      "system",
      "administrator",
      "idm",
      "administration",
      "console",
      "user",
      "grant",
      "permissions",
      "perform",
      "steps",
      "described",
      "following",
      "topics.",
      "go",
      "application",
      "roles.",
      "page",
      "displays",
      "list",
      "available",
      "rpa",
      "click",
      "top",
      "right",
      "menu",
      "fill",
      "fields",
      "enter",
      "name",
      "display",
      "description.",
      "choose",
      "available.",
      "associated",
      "drop-down",
      "box",
      "select",
      "one",
      "appropriate",
      "permission",
      "options.",
      "example",
      "flowdebug.",
      "save",
      "apply",
      "settings.",
      "defined",
      "gets",
      "created.",
      "navigate",
      "change",
      "description",
      "remove",
      "permissions.",
      "confirm",
      "removal.",
      "instructions",
      "users",
      "see",
      "users.",
      "groups",
      "designer."
    ],
    "language": "en",
    "word_count": 118,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage roles for oo workflow designer",
    "contentLower": "role location system administrator idm administration console of oo workflow designer to add a new user and grant permissions to a new user, perform the steps described in the following topics. add a new role go to oo workflow designer application > idm administration console > roles. the page displays the list of roles available in rpa workflow designer click on the top right menu to add a new role and fill the following fields: enter the name, display name, and description. choose the application, if available. click associated permissions drop-down list box and select one or more appropriate permission/s from the available options. for example, flowdebug. click save to apply the settings. the new user with defined permission gets created. edit an existing role navigate to idm administration console > roles. select a role and click edit . in the role edit page, you can change the display name, description and add/remove associated permissions. click save to apply the settings. delete",
    "keywordsLower": [
      "manage",
      "roles",
      "oo",
      "workflow",
      "designer",
      "add",
      "new",
      "role",
      "edit",
      "existing",
      "delete",
      "related",
      "topics",
      "location",
      "system",
      "administrator",
      "idm",
      "administration",
      "console",
      "user",
      "grant",
      "permissions",
      "perform",
      "steps",
      "described",
      "following",
      "topics.",
      "go",
      "application",
      "roles.",
      "page",
      "displays",
      "list",
      "available",
      "rpa",
      "click",
      "top",
      "right",
      "menu",
      "fill",
      "fields",
      "enter",
      "name",
      "display",
      "description.",
      "choose",
      "available.",
      "associated",
      "drop-down",
      "box",
      "select",
      "one",
      "appropriate",
      "permission",
      "options.",
      "example",
      "flowdebug.",
      "save",
      "apply",
      "settings.",
      "defined",
      "gets",
      "created.",
      "navigate",
      "change",
      "description",
      "remove",
      "permissions.",
      "confirm",
      "removal.",
      "instructions",
      "users",
      "see",
      "users.",
      "groups",
      "designer."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage authentication for OO Workflow Designer",
    "content": "Role Location System Administrator IdM Administration Console of OO Workflow Designer You can configure and manage authentication identity servers for the organization. Create an authentication type From the ORGANIZATION menu, select the Authentication tab, and then click . In the Create Authentication dialog, click the Select authentication type drop-down list box and select from the following authentication types: LDAP SAML OAUTH This topic contains the following sub-topics: Configure LDAP authentication Configure SAML authentication Configure OAuth authentication with Azure AD",
    "url": "manageauthenticationood",
    "filename": "manageauthenticationood",
    "headings": [
      "Create an authentication type"
    ],
    "keywords": [
      "manage",
      "authentication",
      "oo",
      "workflow",
      "designer",
      "create",
      "type",
      "role",
      "location",
      "system",
      "administrator",
      "idm",
      "administration",
      "console",
      "configure",
      "identity",
      "servers",
      "organization.",
      "organization",
      "menu",
      "select",
      "tab",
      "click",
      "dialog",
      "drop-down",
      "list",
      "box",
      "following",
      "types",
      "ldap",
      "saml",
      "oauth",
      "topic",
      "contains",
      "sub-topics",
      "azure",
      "ad"
    ],
    "language": "en",
    "word_count": 62,
    "importance_score": 1.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage authentication for oo workflow designer",
    "contentLower": "role location system administrator idm administration console of oo workflow designer you can configure and manage authentication identity servers for the organization. create an authentication type from the organization menu, select the authentication tab, and then click . in the create authentication dialog, click the select authentication type drop-down list box and select from the following authentication types: ldap saml oauth this topic contains the following sub-topics: configure ldap authentication configure saml authentication configure oauth authentication with azure ad",
    "keywordsLower": [
      "manage",
      "authentication",
      "oo",
      "workflow",
      "designer",
      "create",
      "type",
      "role",
      "location",
      "system",
      "administrator",
      "idm",
      "administration",
      "console",
      "configure",
      "identity",
      "servers",
      "organization.",
      "organization",
      "menu",
      "select",
      "tab",
      "click",
      "dialog",
      "drop-down",
      "list",
      "box",
      "following",
      "types",
      "ldap",
      "saml",
      "oauth",
      "topic",
      "contains",
      "sub-topics",
      "azure",
      "ad"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Network and communication security",
    "content": "This topic describes network and communication security in OO Workflow Designer. Communication channel security Supported Protocols and Configuration OO Workflow Designer supports the TLS protocol. The OO Workflow Designer ports are defined by the administrator during the installation. Channel Security OO Workflow Designer supports the following secure channel: Channel (Directed) Supported Secure Protocol Browser OO Workflow Designer For a secure channel, use the TLS communication for encryption and Client Certificate for authentication. Related topics To replace TLS server certificate in OO Workflow Designer, see User management and authentication in OO Workflow Designer.",
    "url": "networksecuritydesigner",
    "filename": "networksecuritydesigner",
    "headings": [
      "Communication channel security",
      "Related topics"
    ],
    "keywords": [
      "network",
      "communication",
      "security",
      "channel",
      "related",
      "topics",
      "topic",
      "describes",
      "oo",
      "workflow",
      "designer.",
      "supported",
      "protocols",
      "configuration",
      "designer",
      "supports",
      "tls",
      "protocol.",
      "ports",
      "defined",
      "administrator",
      "during",
      "installation.",
      "following",
      "secure",
      "directed",
      "protocol",
      "browser",
      "encryption",
      "client",
      "certificate",
      "authentication.",
      "replace",
      "server",
      "see",
      "user",
      "management",
      "authentication"
    ],
    "language": "en",
    "word_count": 73,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "network and communication security",
    "contentLower": "this topic describes network and communication security in oo workflow designer. communication channel security supported protocols and configuration oo workflow designer supports the tls protocol. the oo workflow designer ports are defined by the administrator during the installation. channel security oo workflow designer supports the following secure channel: channel (directed) supported secure protocol browser oo workflow designer for a secure channel, use the tls communication for encryption and client certificate for authentication. related topics to replace tls server certificate in oo workflow designer, see user management and authentication in oo workflow designer.",
    "keywordsLower": [
      "network",
      "communication",
      "security",
      "channel",
      "related",
      "topics",
      "topic",
      "describes",
      "oo",
      "workflow",
      "designer.",
      "supported",
      "protocols",
      "configuration",
      "designer",
      "supports",
      "tls",
      "protocol.",
      "ports",
      "defined",
      "administrator",
      "during",
      "installation.",
      "following",
      "secure",
      "directed",
      "protocol",
      "browser",
      "encryption",
      "client",
      "certificate",
      "authentication.",
      "replace",
      "server",
      "see",
      "user",
      "management",
      "authentication"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Perform a full reindex for Aviator",
    "content": "To ensure up-to-date and accurate search data when using Aviator, perform a full reindex as necessary. To do this, follow these steps: Download and extract the reindex.sh and ais_migration.sh scripts from the SMA Operation Toolkit folder to a Linux environment that is correctly configured to resolve the suite's FQDN.Refer to the README.md file in the SMAX_search_reindex folder for details on how to use the ais_migrate.sh script. This script involves two operations: Clear the data of existing record types in Aviator: ./ais_migration.sh <suite FQDN> -t <tenantid_1,tenantid_2> -c -P <password of integration user:bo-integration@dummy.com> Reindex the record types. For Aviator, only offerings and articles require reindexing. ./reindex.sh <suite FQDN> -t <tenantid_1,tenantid_2> -P <password of integration user:bo-integration@dummy.com> -T \"Offering,Article\" Related topic Perform a full reindex for Smart Analytics",
    "url": "aisreindex",
    "filename": "aisreindex",
    "headings": [
      "Related topic"
    ],
    "keywords": [
      "dummy.com",
      "reindex.sh",
      "README.md",
      "ais_migrate.sh",
      "ais_migration.sh",
      "perform",
      "full",
      "reindex",
      "aviator",
      "related",
      "topic",
      "ensure",
      "up-to-date",
      "accurate",
      "search",
      "data",
      "necessary.",
      "follow",
      "steps",
      "download",
      "extract",
      "scripts",
      "sma",
      "operation",
      "toolkit",
      "folder",
      "linux",
      "environment",
      "correctly",
      "configured",
      "resolve",
      "suite",
      "fqdn.refer",
      "file",
      "details",
      "script.",
      "script",
      "involves",
      "two",
      "operations",
      "clear",
      "existing",
      "record",
      "types",
      "-t",
      "-c",
      "-p",
      "types.",
      "offerings",
      "articles",
      "require",
      "reindexing.",
      "offering",
      "article",
      "smart",
      "analytics"
    ],
    "language": "en",
    "word_count": 75,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "perform a full reindex for aviator",
    "contentLower": "to ensure up-to-date and accurate search data when using aviator, perform a full reindex as necessary. to do this, follow these steps: download and extract the reindex.sh and ais_migration.sh scripts from the sma operation toolkit folder to a linux environment that is correctly configured to resolve the suite's fqdn.refer to the readme.md file in the smax_search_reindex folder for details on how to use the ais_migrate.sh script. this script involves two operations: clear the data of existing record types in aviator: ./ais_migration.sh <suite fqdn> -t <tenantid_1,tenantid_2> -c -p <password of integration user:bo-integration@dummy.com> reindex the record types. for aviator, only offerings and articles require reindexing. ./reindex.sh <suite fqdn> -t <tenantid_1,tenantid_2> -p <password of integration user:bo-integration@dummy.com> -t \"offering,article\" related topic perform a full reindex for smart analytics",
    "keywordsLower": [
      "dummy.com",
      "reindex.sh",
      "readme.md",
      "ais_migrate.sh",
      "ais_migration.sh",
      "perform",
      "full",
      "reindex",
      "aviator",
      "related",
      "topic",
      "ensure",
      "up-to-date",
      "accurate",
      "search",
      "data",
      "necessary.",
      "follow",
      "steps",
      "download",
      "extract",
      "scripts",
      "sma",
      "operation",
      "toolkit",
      "folder",
      "linux",
      "environment",
      "correctly",
      "configured",
      "resolve",
      "suite",
      "fqdn.refer",
      "file",
      "details",
      "script.",
      "script",
      "involves",
      "two",
      "operations",
      "clear",
      "existing",
      "record",
      "types",
      "-t",
      "-c",
      "-p",
      "types.",
      "offerings",
      "articles",
      "require",
      "reindexing.",
      "offering",
      "article",
      "smart",
      "analytics"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Measure your carbon footprint",
    "content": "As more organizations embark on digital transformation initiatives, there is a growing need to balance innovation and sustainability. Increased IT resource consumption means increased energy consumption, which in turn results in increased CO2 emissions. Now it is critical for enterprises to consider both business transformation and environmental sustainability. The GreenOps solution is introduced to offer a range of features that deliver sustainable operations and help businesses achieve their environmental commitments. GreenOps aims to provide insights into your IT estate, enabling you to identify under/overconsumption and optimize capacity. With this solution applied, you can enable the collection of spending, consumption, and carbon footprint data for cloud (AWS and Azure) and off-cloud IT assets - and deliver the report in a unified view. Through the carbon footprint report, you will be able to: Identify and map sources of carbon in your IT ecosystem.Analyze emissions across data c",
    "url": "carbonfootprintcalculation",
    "filename": "carbonfootprintcalculation",
    "headings": [
      "Prerequisites",
      "Enable carbon footprint calculation",
      "ITAM infrastructures",
      "Input carbon emissions data for asset models",
      "Input carbon emissions data for Device and Infrastructure & Peripheral records",
      "Set carbon intensity of electricity based on location",
      "Cloud infrastructures",
      "View cloud carbon footprint calculation results",
      "Create carbon footprint reports",
      "Create ITAM carbon footprint reports via OData",
      "Create cloud carbon footprint reports from Power BI",
      "Download sample reports from Marketplace"
    ],
    "keywords": [
      "details.In",
      "Configuration.From",
      "https://www.cloudcarbonfootprint.org",
      "Devices.Edit",
      "required.Save",
      "record.In",
      "created.Save",
      "cloudcarbonfootprint.org",
      "Models.Edit",
      "AM.In",
      "location.Save",
      "measure",
      "carbon",
      "footprint",
      "prerequisites",
      "enable",
      "calculation",
      "itam",
      "infrastructures",
      "input",
      "emissions",
      "data",
      "asset",
      "models",
      "device",
      "infrastructure",
      "peripheral",
      "records",
      "set",
      "intensity",
      "electricity",
      "based",
      "location",
      "cloud",
      "view",
      "results",
      "create",
      "reports",
      "via",
      "odata",
      "power",
      "bi",
      "download",
      "sample",
      "marketplace",
      "organizations",
      "embark",
      "digital",
      "transformation",
      "initiatives",
      "there",
      "growing",
      "need",
      "balance",
      "innovation",
      "sustainability.",
      "increased",
      "resource",
      "consumption",
      "means",
      "energy",
      "turn",
      "co2",
      "emissions.",
      "now",
      "critical",
      "enterprises",
      "consider",
      "both",
      "business",
      "environmental",
      "greenops",
      "solution",
      "introduced",
      "offer",
      "range",
      "features",
      "deliver",
      "sustainable",
      "operations",
      "help",
      "businesses",
      "achieve",
      "commitments.",
      "aims",
      "provide",
      "insights",
      "estate",
      "enabling",
      "identify",
      "under",
      "overconsumption",
      "optimize",
      "capacity.",
      "applied",
      "collection",
      "spending",
      "aws",
      "azure",
      "off-cloud"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "measure your carbon footprint",
    "contentLower": "as more organizations embark on digital transformation initiatives, there is a growing need to balance innovation and sustainability. increased it resource consumption means increased energy consumption, which in turn results in increased co2 emissions. now it is critical for enterprises to consider both business transformation and environmental sustainability. the greenops solution is introduced to offer a range of features that deliver sustainable operations and help businesses achieve their environmental commitments. greenops aims to provide insights into your it estate, enabling you to identify under/overconsumption and optimize capacity. with this solution applied, you can enable the collection of spending, consumption, and carbon footprint data for cloud (aws and azure) and off-cloud it assets - and deliver the report in a unified view. through the carbon footprint report, you will be able to: identify and map sources of carbon in your it ecosystem.analyze emissions across data c",
    "keywordsLower": [
      "details.in",
      "configuration.from",
      "https://www.cloudcarbonfootprint.org",
      "devices.edit",
      "required.save",
      "record.in",
      "created.save",
      "cloudcarbonfootprint.org",
      "models.edit",
      "am.in",
      "location.save",
      "measure",
      "carbon",
      "footprint",
      "prerequisites",
      "enable",
      "calculation",
      "itam",
      "infrastructures",
      "input",
      "emissions",
      "data",
      "asset",
      "models",
      "device",
      "infrastructure",
      "peripheral",
      "records",
      "set",
      "intensity",
      "electricity",
      "based",
      "location",
      "cloud",
      "view",
      "results",
      "create",
      "reports",
      "via",
      "odata",
      "power",
      "bi",
      "download",
      "sample",
      "marketplace",
      "organizations",
      "embark",
      "digital",
      "transformation",
      "initiatives",
      "there",
      "growing",
      "need",
      "balance",
      "innovation",
      "sustainability.",
      "increased",
      "resource",
      "consumption",
      "means",
      "energy",
      "turn",
      "co2",
      "emissions.",
      "now",
      "critical",
      "enterprises",
      "consider",
      "both",
      "business",
      "environmental",
      "greenops",
      "solution",
      "introduced",
      "offer",
      "range",
      "features",
      "deliver",
      "sustainable",
      "operations",
      "help",
      "businesses",
      "achieve",
      "commitments.",
      "aims",
      "provide",
      "insights",
      "estate",
      "enabling",
      "identify",
      "under",
      "overconsumption",
      "optimize",
      "capacity.",
      "applied",
      "collection",
      "spending",
      "aws",
      "azure",
      "off-cloud"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "My Home",
    "content": "My Home is available for suite users including business account users and suite administration users. On this page you can manage your personal profile, and view each product instance, tenant, or service you purchased or authorized in the suite. The language setting that you specify in your user profile determines the display language of Suite Administration, My Home, or both, based on your suite authorization. Note Only users with the DB authentication type can change password in My Home.",
    "url": "myhome",
    "filename": "myhome",
    "headings": [],
    "keywords": [
      "home",
      "available",
      "suite",
      "users",
      "including",
      "business",
      "account",
      "administration",
      "users.",
      "page",
      "manage",
      "personal",
      "profile",
      "view",
      "product",
      "instance",
      "tenant",
      "service",
      "purchased",
      "authorized",
      "suite.",
      "language",
      "setting",
      "specify",
      "user",
      "determines",
      "display",
      "both",
      "based",
      "authorization.",
      "note",
      "db",
      "authentication",
      "type",
      "change",
      "password",
      "home."
    ],
    "language": "en",
    "word_count": 47,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "my home",
    "contentLower": "my home is available for suite users including business account users and suite administration users. on this page you can manage your personal profile, and view each product instance, tenant, or service you purchased or authorized in the suite. the language setting that you specify in your user profile determines the display language of suite administration, my home, or both, based on your suite authorization. note only users with the db authentication type can change password in my home.",
    "keywordsLower": [
      "home",
      "available",
      "suite",
      "users",
      "including",
      "business",
      "account",
      "administration",
      "users.",
      "page",
      "manage",
      "personal",
      "profile",
      "view",
      "product",
      "instance",
      "tenant",
      "service",
      "purchased",
      "authorized",
      "suite.",
      "language",
      "setting",
      "specify",
      "user",
      "determines",
      "display",
      "both",
      "based",
      "authorization.",
      "note",
      "db",
      "authentication",
      "type",
      "change",
      "password",
      "home."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Out-of-the-box content for HR Service Management",
    "content": "This topic describes the main types of data that are included in the out-of-the-box Human Resources Content package for HR Service Management. For more information about how to deploy content such as the Human Resources Content package, see Content. Offerings in Service Catalog The following table lists HR Catalogs, Service Definitions, and Offerings in Service Management's out-of-the-box package. Catalog Service Definitions Offering Offering type Benefits and Rewards Benefits Ask about Medical, Dental, Vision, Life and Disability Insurance Benefits HR Support Offering Ask about Retirement Saving Benefits or Pension Benefits HR Support Offering Ask about Social Security / Govt. & Local Benefits HR Support Offering Ask about Educational Assistance or Fitness and Wellness Program HR Support Offering Ask a question about your benefits HR Support Offering Rewards Ask about Referral Bonus Program HR Support Offering Employee Lifecycle Compliance and Policy Review Ask a question about the ca",
    "url": "oobhrsmcontent",
    "filename": "oobhrsmcontent",
    "headings": [
      "Offerings in Service Catalog",
      "Offering bundles",
      "Surveys",
      "Email templates",
      "Smart Virtual Agent intents",
      "HR sample data package"
    ],
    "keywords": [
      "out-of-the-box",
      "content",
      "hr",
      "service",
      "management",
      "offerings",
      "catalog",
      "offering",
      "bundles",
      "surveys",
      "email",
      "templates",
      "smart",
      "virtual",
      "agent",
      "intents",
      "sample",
      "data",
      "package",
      "topic",
      "describes",
      "main",
      "types",
      "included",
      "human",
      "resources",
      "management.",
      "information",
      "about",
      "deploy",
      "such",
      "see",
      "content.",
      "following",
      "table",
      "lists",
      "catalogs",
      "definitions",
      "package.",
      "type",
      "benefits",
      "rewards",
      "ask",
      "medical",
      "dental",
      "vision",
      "life",
      "disability",
      "insurance",
      "support",
      "retirement",
      "saving",
      "pension",
      "social",
      "security",
      "govt.",
      "local",
      "educational",
      "assistance",
      "fitness",
      "wellness",
      "program",
      "question",
      "referral",
      "bonus",
      "employee",
      "lifecycle",
      "compliance",
      "policy",
      "review",
      "candidate",
      "background",
      "check",
      "process",
      "req",
      "id",
      "case",
      "issue",
      "level",
      "checkout",
      "department",
      "team",
      "transfer",
      "job",
      "position",
      "change",
      "office",
      "move",
      "promotion",
      "title",
      "update",
      "i-9",
      "verify",
      "new",
      "letter",
      "request",
      "get",
      "employment",
      "verification",
      "performance"
    ],
    "language": "en",
    "word_count": 122,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "out-of-the-box content for hr service management",
    "contentLower": "this topic describes the main types of data that are included in the out-of-the-box human resources content package for hr service management. for more information about how to deploy content such as the human resources content package, see content. offerings in service catalog the following table lists hr catalogs, service definitions, and offerings in service management's out-of-the-box package. catalog service definitions offering offering type benefits and rewards benefits ask about medical, dental, vision, life and disability insurance benefits hr support offering ask about retirement saving benefits or pension benefits hr support offering ask about social security / govt. & local benefits hr support offering ask about educational assistance or fitness and wellness program hr support offering ask a question about your benefits hr support offering rewards ask about referral bonus program hr support offering employee lifecycle compliance and policy review ask a question about the ca",
    "keywordsLower": [
      "out-of-the-box",
      "content",
      "hr",
      "service",
      "management",
      "offerings",
      "catalog",
      "offering",
      "bundles",
      "surveys",
      "email",
      "templates",
      "smart",
      "virtual",
      "agent",
      "intents",
      "sample",
      "data",
      "package",
      "topic",
      "describes",
      "main",
      "types",
      "included",
      "human",
      "resources",
      "management.",
      "information",
      "about",
      "deploy",
      "such",
      "see",
      "content.",
      "following",
      "table",
      "lists",
      "catalogs",
      "definitions",
      "package.",
      "type",
      "benefits",
      "rewards",
      "ask",
      "medical",
      "dental",
      "vision",
      "life",
      "disability",
      "insurance",
      "support",
      "retirement",
      "saving",
      "pension",
      "social",
      "security",
      "govt.",
      "local",
      "educational",
      "assistance",
      "fitness",
      "wellness",
      "program",
      "question",
      "referral",
      "bonus",
      "employee",
      "lifecycle",
      "compliance",
      "policy",
      "review",
      "candidate",
      "background",
      "check",
      "process",
      "req",
      "id",
      "case",
      "issue",
      "level",
      "checkout",
      "department",
      "team",
      "transfer",
      "job",
      "position",
      "change",
      "office",
      "move",
      "promotion",
      "title",
      "update",
      "i-9",
      "verify",
      "new",
      "letter",
      "request",
      "get",
      "employment",
      "verification",
      "performance"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Privacy controls in HR Service Management",
    "content": "Aviator workflow business rules can automatically analyze whether Requests contain private information, including: Personally identifiable information (such as names and contact details) Financial information (such as credit card numbers and bank account details) Health information (such as medical records and health insurance information) Other sensitive information (such as authentication credentials and confidential business information). The result of the privacy analysis and a short justification are displayed in the Request record together. If Aviator determines that a Request does include personal private information, the Request is automatically assigned to the \"HR Private Sensitive\" group. Enable privacy control in HR Service Management To use the automatic assignment feature of privacy analysis, perform the following steps. Enable Aviator For details, see Configure Aviator. Add a functional group In the agent interface, click Administration > Master Data > People > Groups, an",
    "url": "privacycontrolshrsm",
    "filename": "privacycontrolshrsm",
    "headings": [
      "Enable privacy control in HR Service Management",
      "Enable Aviator",
      "Add a functional group",
      "Add business rules",
      "Additional steps if upgraded from a version earlier than 24.4",
      "Update forms",
      "Add business rules"
    ],
    "keywords": [
      "24.4",
      "privacy",
      "controls",
      "hr",
      "service",
      "management",
      "enable",
      "control",
      "aviator",
      "add",
      "functional",
      "group",
      "business",
      "rules",
      "additional",
      "steps",
      "upgraded",
      "version",
      "earlier",
      "update",
      "forms",
      "workflow",
      "automatically",
      "analyze",
      "whether",
      "requests",
      "contain",
      "private",
      "information",
      "including",
      "personally",
      "identifiable",
      "such",
      "names",
      "contact",
      "details",
      "financial",
      "credit",
      "card",
      "numbers",
      "bank",
      "account",
      "health",
      "medical",
      "records",
      "insurance",
      "sensitive",
      "authentication",
      "credentials",
      "confidential",
      "result",
      "analysis",
      "short",
      "justification",
      "displayed",
      "request",
      "record",
      "together.",
      "determines",
      "include",
      "personal",
      "assigned",
      "group.",
      "automatic",
      "assignment",
      "feature",
      "perform",
      "following",
      "steps.",
      "see",
      "configure",
      "aviator.",
      "agent",
      "interface",
      "click",
      "administration",
      "master",
      "data",
      "people",
      "groups",
      "new",
      "described",
      "below.",
      "groups.",
      "case",
      "type",
      "field",
      "name",
      "user",
      "principal",
      "hrprivatesensitive",
      "email",
      "none",
      "owner",
      "appropriate",
      "users.",
      "main",
      "menu",
      "studio",
      "processes"
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "privacy controls in hr service management",
    "contentLower": "aviator workflow business rules can automatically analyze whether requests contain private information, including: personally identifiable information (such as names and contact details) financial information (such as credit card numbers and bank account details) health information (such as medical records and health insurance information) other sensitive information (such as authentication credentials and confidential business information). the result of the privacy analysis and a short justification are displayed in the request record together. if aviator determines that a request does include personal private information, the request is automatically assigned to the \"hr private sensitive\" group. enable privacy control in hr service management to use the automatic assignment feature of privacy analysis, perform the following steps. enable aviator for details, see configure aviator. add a functional group in the agent interface, click administration > master data > people > groups, an",
    "keywordsLower": [
      "24.4",
      "privacy",
      "controls",
      "hr",
      "service",
      "management",
      "enable",
      "control",
      "aviator",
      "add",
      "functional",
      "group",
      "business",
      "rules",
      "additional",
      "steps",
      "upgraded",
      "version",
      "earlier",
      "update",
      "forms",
      "workflow",
      "automatically",
      "analyze",
      "whether",
      "requests",
      "contain",
      "private",
      "information",
      "including",
      "personally",
      "identifiable",
      "such",
      "names",
      "contact",
      "details",
      "financial",
      "credit",
      "card",
      "numbers",
      "bank",
      "account",
      "health",
      "medical",
      "records",
      "insurance",
      "sensitive",
      "authentication",
      "credentials",
      "confidential",
      "result",
      "analysis",
      "short",
      "justification",
      "displayed",
      "request",
      "record",
      "together.",
      "determines",
      "include",
      "personal",
      "assigned",
      "group.",
      "automatic",
      "assignment",
      "feature",
      "perform",
      "following",
      "steps.",
      "see",
      "configure",
      "aviator.",
      "agent",
      "interface",
      "click",
      "administration",
      "master",
      "data",
      "people",
      "groups",
      "new",
      "described",
      "below.",
      "groups.",
      "case",
      "type",
      "field",
      "name",
      "user",
      "principal",
      "hrprivatesensitive",
      "email",
      "none",
      "owner",
      "appropriate",
      "users.",
      "main",
      "menu",
      "studio",
      "processes"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Patches",
    "content": "This page gives information on patches and how to view them. Overview A patch is a software update or modification designed to fix vulnerabilities, bugs, or other issues in an existing program or system. You can import the patches from Server Automation (SA). Lifecycle of patches Patches have a predefined workflow representing the lifecycle of every instance within Automation Center. The phases of a patch have the following metaphases: DraftActiveRetired The lifecycle of a patch has the following transition phases: Patch Transitions Transition phase Transition type Description New > Action Automatic When you import patches from SA, it moves to Action phase. Action > Retire Manual When you move the patch to Retire phase in Automation Center. When you remove a patch in SA, post synchronization, the phase changes to Retire in Automation Center. Action > Superseded Automatic When a patch supersedes at least one other patch. Supersede > Retire Manual When you move the patch to Retire phase ",
    "url": "patchesac",
    "filename": "patchesac",
    "headings": [
      "Overview",
      "Lifecycle of patches",
      "Draft",
      "Active",
      "Retired",
      "View patches",
      "Filters",
      "Patch details",
      "General tab",
      "CVE Details",
      "Supersedes",
      "Superseded By",
      "Workflow tab",
      "Discussions tab",
      "History tab"
    ],
    "keywords": [
      "24.3",
      "Patches.To",
      "patches",
      "overview",
      "lifecycle",
      "draft",
      "active",
      "retired",
      "view",
      "filters",
      "patch",
      "details",
      "general",
      "tab",
      "cve",
      "supersedes",
      "superseded",
      "workflow",
      "discussions",
      "history",
      "page",
      "gives",
      "information",
      "them.",
      "software",
      "update",
      "modification",
      "designed",
      "fix",
      "vulnerabilities",
      "bugs",
      "issues",
      "existing",
      "program",
      "system.",
      "import",
      "server",
      "automation",
      "sa",
      "predefined",
      "representing",
      "every",
      "instance",
      "center.",
      "phases",
      "following",
      "metaphases",
      "draftactiveretired",
      "transition",
      "transitions",
      "phase",
      "type",
      "description",
      "new",
      "action",
      "automatic",
      "moves",
      "phase.",
      "retire",
      "manual",
      "move",
      "remove",
      "post",
      "synchronization",
      "changes",
      "least",
      "one",
      "patch.",
      "supersede",
      "back",
      "add",
      "again",
      "becomes",
      "entity",
      "sa.",
      "center",
      "new.",
      "metaphase",
      "contains",
      "initial",
      "newly",
      "created",
      "until",
      "required",
      "information.",
      "all",
      "use.",
      "attach",
      "policies.superseded",
      "works",
      "like",
      "there",
      "exists",
      "instance.",
      "deleted",
      "source",
      "longer",
      "interest.",
      "policies.",
      "mega"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "patches",
    "contentLower": "this page gives information on patches and how to view them. overview a patch is a software update or modification designed to fix vulnerabilities, bugs, or other issues in an existing program or system. you can import the patches from server automation (sa). lifecycle of patches patches have a predefined workflow representing the lifecycle of every instance within automation center. the phases of a patch have the following metaphases: draftactiveretired the lifecycle of a patch has the following transition phases: patch transitions transition phase transition type description new > action automatic when you import patches from sa, it moves to action phase. action > retire manual when you move the patch to retire phase in automation center. when you remove a patch in sa, post synchronization, the phase changes to retire in automation center. action > superseded automatic when a patch supersedes at least one other patch. supersede > retire manual when you move the patch to retire phase ",
    "keywordsLower": [
      "24.3",
      "patches.to",
      "patches",
      "overview",
      "lifecycle",
      "draft",
      "active",
      "retired",
      "view",
      "filters",
      "patch",
      "details",
      "general",
      "tab",
      "cve",
      "supersedes",
      "superseded",
      "workflow",
      "discussions",
      "history",
      "page",
      "gives",
      "information",
      "them.",
      "software",
      "update",
      "modification",
      "designed",
      "fix",
      "vulnerabilities",
      "bugs",
      "issues",
      "existing",
      "program",
      "system.",
      "import",
      "server",
      "automation",
      "sa",
      "predefined",
      "representing",
      "every",
      "instance",
      "center.",
      "phases",
      "following",
      "metaphases",
      "draftactiveretired",
      "transition",
      "transitions",
      "phase",
      "type",
      "description",
      "new",
      "action",
      "automatic",
      "moves",
      "phase.",
      "retire",
      "manual",
      "move",
      "remove",
      "post",
      "synchronization",
      "changes",
      "least",
      "one",
      "patch.",
      "supersede",
      "back",
      "add",
      "again",
      "becomes",
      "entity",
      "sa.",
      "center",
      "new.",
      "metaphase",
      "contains",
      "initial",
      "newly",
      "created",
      "until",
      "required",
      "information.",
      "all",
      "use.",
      "attach",
      "policies.superseded",
      "works",
      "like",
      "there",
      "exists",
      "instance.",
      "deleted",
      "source",
      "longer",
      "interest.",
      "policies.",
      "mega"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Policy Implementations",
    "content": "This page details information on policy implementations and how to view them. Overview A policy defines the vulnerabilities you need to remediate and the inherited patches available to apply on targeted systems or the list of patches you want to apply. Policy implementations link the patches defined in a policy (either patches added directly or patches inherited from vulnerabilities) to the Configuration Items (CIs) (Involved CIs) to monitor them. Policy implementations lifecycle Policy implementations have a pre-defined workflow representing the lifecycle of every instance within Automation Center. Policy implementations contain the following metaphases: DraftDefinitionActiveRetired The policy implementation lifecycle has the following transition phases: policy implementations transitions Transition phase Transition type Description New > Define Automatic When you fill in the name and policy, the phase moves from New to Define. Define > Approve Manual This identifies that there is no ",
    "url": "policyimplementationsac",
    "filename": "policyimplementationsac",
    "headings": [
      "Overview",
      "Policy implementations lifecycle",
      "Draft",
      "Definition",
      "Active",
      "Retired",
      "View policy implementations",
      "Filters",
      "Policy implementation details",
      "General tab",
      "Workflow tab",
      "Involved CIs tab",
      "Discussions tab",
      "History tab",
      "Create a policy implementation",
      "Edit a policy implementation",
      "Remediate CIs through a policy implementation"
    ],
    "keywords": [
      "opens.Edit",
      "page.Fill",
      "dropdown.The",
      "24.3",
      "Implementations.To",
      "edit.On",
      "Model.If",
      "policy",
      "implementations",
      "overview",
      "lifecycle",
      "draft",
      "definition",
      "active",
      "retired",
      "view",
      "filters",
      "implementation",
      "details",
      "general",
      "tab",
      "workflow",
      "involved",
      "cis",
      "discussions",
      "history",
      "create",
      "edit",
      "remediate",
      "through",
      "page",
      "information",
      "them.",
      "defines",
      "vulnerabilities",
      "need",
      "inherited",
      "patches",
      "available",
      "apply",
      "targeted",
      "systems",
      "list",
      "want",
      "apply.",
      "link",
      "defined",
      "either",
      "added",
      "directly",
      "configuration",
      "items",
      "monitor",
      "pre-defined",
      "representing",
      "every",
      "instance",
      "automation",
      "center.",
      "contain",
      "following",
      "metaphases",
      "draftdefinitionactiveretired",
      "transition",
      "phases",
      "transitions",
      "phase",
      "type",
      "description",
      "new",
      "define",
      "automatic",
      "fill",
      "name",
      "moves",
      "define.",
      "approve",
      "manual",
      "identifies",
      "there",
      "default",
      "approval",
      "plan",
      "automatically",
      "phase.",
      "request",
      "rejected.",
      "approved.",
      "synchronized",
      "server",
      "integration",
      "synchronization",
      "followed.",
      "suspend",
      "until",
      "changes.",
      "retire",
      "jobs",
      "longer",
      "run"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "policy implementations",
    "contentLower": "this page details information on policy implementations and how to view them. overview a policy defines the vulnerabilities you need to remediate and the inherited patches available to apply on targeted systems or the list of patches you want to apply. policy implementations link the patches defined in a policy (either patches added directly or patches inherited from vulnerabilities) to the configuration items (cis) (involved cis) to monitor them. policy implementations lifecycle policy implementations have a pre-defined workflow representing the lifecycle of every instance within automation center. policy implementations contain the following metaphases: draftdefinitionactiveretired the policy implementation lifecycle has the following transition phases: policy implementations transitions transition phase transition type description new > define automatic when you fill in the name and policy, the phase moves from new to define. define > approve manual this identifies that there is no ",
    "keywordsLower": [
      "opens.edit",
      "page.fill",
      "dropdown.the",
      "24.3",
      "implementations.to",
      "edit.on",
      "model.if",
      "policy",
      "implementations",
      "overview",
      "lifecycle",
      "draft",
      "definition",
      "active",
      "retired",
      "view",
      "filters",
      "implementation",
      "details",
      "general",
      "tab",
      "workflow",
      "involved",
      "cis",
      "discussions",
      "history",
      "create",
      "edit",
      "remediate",
      "through",
      "page",
      "information",
      "them.",
      "defines",
      "vulnerabilities",
      "need",
      "inherited",
      "patches",
      "available",
      "apply",
      "targeted",
      "systems",
      "list",
      "want",
      "apply.",
      "link",
      "defined",
      "either",
      "added",
      "directly",
      "configuration",
      "items",
      "monitor",
      "pre-defined",
      "representing",
      "every",
      "instance",
      "automation",
      "center.",
      "contain",
      "following",
      "metaphases",
      "draftdefinitionactiveretired",
      "transition",
      "phases",
      "transitions",
      "phase",
      "type",
      "description",
      "new",
      "define",
      "automatic",
      "fill",
      "name",
      "moves",
      "define.",
      "approve",
      "manual",
      "identifies",
      "there",
      "default",
      "approval",
      "plan",
      "automatically",
      "phase.",
      "request",
      "rejected.",
      "approved.",
      "synchronized",
      "server",
      "integration",
      "synchronization",
      "followed.",
      "suspend",
      "until",
      "changes.",
      "retire",
      "jobs",
      "longer",
      "run"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Policies",
    "content": "This page gives information on policies and how to edit, create, and view them. Overview A policy contains a set of patches that you can apply to one or more Configuration Items (CIs) or a set of vulnerabilities that you want to fix. You can fix the vulnerabilities if there are patches available for them. Lifecycle of policies Policies have a pre-defined workflow representing the lifecycle of every instance within Automation Center. A policy has the following metaphases: DraftDefinitionActiveRetired The lifecycle of a policy has the following transition phases: policy transitions Transition phase Transition type Description New > Define Automatic When you fill in the name, the phase moves from New to Define. Define > Approve Manual This identifies that there is no default approval plan and moves a policy automatically from Define to Active phase. Approve > Define Manual/ Automatic Definition not approved. Approve > Active Automatic Definition approved. Active > Suspend Manual Suspend p",
    "url": "policiesac",
    "filename": "policiesac",
    "headings": [
      "Overview",
      "Lifecycle of policies",
      "Draft",
      "Definition",
      "Active",
      "Retired",
      "View policies",
      "Filters",
      "Policy details",
      "General tab",
      "Workflow tab",
      "Vulnerabilities tab",
      "Patches tab",
      "Discussions tab",
      "History tab",
      "Create a policy",
      "Edit a policy"
    ],
    "keywords": [
      "opens.Edit",
      "Policies.To",
      "page.Give",
      "24.3",
      "edit.On",
      "policies",
      "overview",
      "lifecycle",
      "draft",
      "definition",
      "active",
      "retired",
      "view",
      "filters",
      "policy",
      "details",
      "general",
      "tab",
      "workflow",
      "vulnerabilities",
      "patches",
      "discussions",
      "history",
      "create",
      "edit",
      "page",
      "gives",
      "information",
      "them.",
      "contains",
      "set",
      "apply",
      "one",
      "configuration",
      "items",
      "cis",
      "want",
      "fix.",
      "fix",
      "there",
      "available",
      "pre-defined",
      "representing",
      "every",
      "instance",
      "automation",
      "center.",
      "following",
      "metaphases",
      "draftdefinitionactiveretired",
      "transition",
      "phases",
      "transitions",
      "phase",
      "type",
      "description",
      "new",
      "define",
      "automatic",
      "fill",
      "name",
      "moves",
      "define.",
      "approve",
      "manual",
      "identifies",
      "default",
      "approval",
      "plan",
      "automatically",
      "phase.",
      "approved.",
      "suspend",
      "until",
      "changes",
      "rules.",
      "retire",
      "implementations",
      "reference",
      "it.",
      "metaphase",
      "initial",
      "newly",
      "created",
      "required",
      "information.",
      "defined",
      "rules.approve",
      "ready",
      "process.",
      "action",
      "approved",
      "use.",
      "change",
      "rules.superseded",
      "suspends",
      "longer",
      "interest.",
      "mega",
      "menu"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "policies",
    "contentLower": "this page gives information on policies and how to edit, create, and view them. overview a policy contains a set of patches that you can apply to one or more configuration items (cis) or a set of vulnerabilities that you want to fix. you can fix the vulnerabilities if there are patches available for them. lifecycle of policies policies have a pre-defined workflow representing the lifecycle of every instance within automation center. a policy has the following metaphases: draftdefinitionactiveretired the lifecycle of a policy has the following transition phases: policy transitions transition phase transition type description new > define automatic when you fill in the name, the phase moves from new to define. define > approve manual this identifies that there is no default approval plan and moves a policy automatically from define to active phase. approve > define manual/ automatic definition not approved. approve > active automatic definition approved. active > suspend manual suspend p",
    "keywordsLower": [
      "opens.edit",
      "policies.to",
      "page.give",
      "24.3",
      "edit.on",
      "policies",
      "overview",
      "lifecycle",
      "draft",
      "definition",
      "active",
      "retired",
      "view",
      "filters",
      "policy",
      "details",
      "general",
      "tab",
      "workflow",
      "vulnerabilities",
      "patches",
      "discussions",
      "history",
      "create",
      "edit",
      "page",
      "gives",
      "information",
      "them.",
      "contains",
      "set",
      "apply",
      "one",
      "configuration",
      "items",
      "cis",
      "want",
      "fix.",
      "fix",
      "there",
      "available",
      "pre-defined",
      "representing",
      "every",
      "instance",
      "automation",
      "center.",
      "following",
      "metaphases",
      "draftdefinitionactiveretired",
      "transition",
      "phases",
      "transitions",
      "phase",
      "type",
      "description",
      "new",
      "define",
      "automatic",
      "fill",
      "name",
      "moves",
      "define.",
      "approve",
      "manual",
      "identifies",
      "default",
      "approval",
      "plan",
      "automatically",
      "phase.",
      "approved.",
      "suspend",
      "until",
      "changes",
      "rules.",
      "retire",
      "implementations",
      "reference",
      "it.",
      "metaphase",
      "initial",
      "newly",
      "created",
      "required",
      "information.",
      "defined",
      "rules.approve",
      "ready",
      "process.",
      "action",
      "approved",
      "use.",
      "change",
      "rules.superseded",
      "suspends",
      "longer",
      "interest.",
      "mega",
      "menu"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan, Build, and Run",
    "content": "This section contains information about the Service Management user interface, how to use the business modules in Service Management, and how to use the DND and CMP FinOps features which are specifically for Cloud Management. Additionally, this section also describes how to use the expression language to construct meaningful expressions when configuring the product. User interface basics Agent user interface My dashboards Reports Logging in to the agent interface Roles and permissions ID numbers for records Task plans Dates and time Models Manage hot topic analytics In-line translations Business modules Plan Build Run Cloud Management specific features Design and Deploy (DND) Cloud Management Platform (CMP) FinOps Advanced usage Expression Language",
    "url": "referencesinusenode",
    "filename": "referencesinusenode",
    "headings": [
      "User interface basics",
      "Business modules",
      "Cloud Management specific features",
      "Advanced usage"
    ],
    "keywords": [
      "plan",
      "build",
      "run",
      "user",
      "interface",
      "basics",
      "business",
      "modules",
      "cloud",
      "management",
      "specific",
      "features",
      "advanced",
      "usage",
      "section",
      "contains",
      "information",
      "about",
      "service",
      "dnd",
      "cmp",
      "finops",
      "specifically",
      "management.",
      "additionally",
      "describes",
      "expression",
      "language",
      "construct",
      "meaningful",
      "expressions",
      "configuring",
      "product.",
      "agent",
      "dashboards",
      "reports",
      "logging",
      "roles",
      "permissions",
      "id",
      "numbers",
      "records",
      "task",
      "plans",
      "dates",
      "time",
      "models",
      "manage",
      "hot",
      "topic",
      "analytics",
      "in-line",
      "translations",
      "design",
      "deploy",
      "platform"
    ],
    "language": "en",
    "word_count": 80,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan, build, and run",
    "contentLower": "this section contains information about the service management user interface, how to use the business modules in service management, and how to use the dnd and cmp finops features which are specifically for cloud management. additionally, this section also describes how to use the expression language to construct meaningful expressions when configuring the product. user interface basics agent user interface my dashboards reports logging in to the agent interface roles and permissions id numbers for records task plans dates and time models manage hot topic analytics in-line translations business modules plan build run cloud management specific features design and deploy (dnd) cloud management platform (cmp) finops advanced usage expression language",
    "keywordsLower": [
      "plan",
      "build",
      "run",
      "user",
      "interface",
      "basics",
      "business",
      "modules",
      "cloud",
      "management",
      "specific",
      "features",
      "advanced",
      "usage",
      "section",
      "contains",
      "information",
      "about",
      "service",
      "dnd",
      "cmp",
      "finops",
      "specifically",
      "management.",
      "additionally",
      "describes",
      "expression",
      "language",
      "construct",
      "meaningful",
      "expressions",
      "configuring",
      "product.",
      "agent",
      "dashboards",
      "reports",
      "logging",
      "roles",
      "permissions",
      "id",
      "numbers",
      "records",
      "task",
      "plans",
      "dates",
      "time",
      "models",
      "manage",
      "hot",
      "topic",
      "analytics",
      "in-line",
      "translations",
      "design",
      "deploy",
      "platform"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Profile and preferences",
    "content": "The profile and preferences part of the user interface includes information about user profile, preferences, contact details, and other user settings. Click your user name in the upper right corner and select User profile to open the Profile & Preferences page. Profile Click your user name in the upper right corner to open the Profile & Preferences page. The following are available there: <Image> Click Change image to select an image to upload, or to change the existing image. The User profile is displayed with the current image. You may drag and drop an image into the User profile. View calendar Click the link to display your on-call shifts. This link only displays if On-Call Schedule management is enabled. For more information, see On-Call Schedule Management. Preferences Property Description Choose language Select the user interface language from the drop-down list. The selected language determines the locale, time display format (12-hour or 24-hour), and number formatting for INTEG",
    "url": "profilepreferences",
    "filename": "profilepreferences",
    "headings": [
      "Profile",
      "<Image>",
      "View calendar",
      "Preferences",
      "Page preferences",
      "Page tabs",
      "Area of expertise",
      "Contact details",
      "Your group memberships",
      "Your employees",
      "Schedule",
      "Notification settings"
    ],
    "keywords": [
      "preferences.To",
      "settings.If",
      "sent.For",
      "profile",
      "preferences",
      "view",
      "calendar",
      "page",
      "tabs",
      "area",
      "expertise",
      "contact",
      "details",
      "group",
      "memberships",
      "employees",
      "schedule",
      "notification",
      "settings",
      "part",
      "user",
      "interface",
      "includes",
      "information",
      "about",
      "settings.",
      "click",
      "name",
      "upper",
      "right",
      "corner",
      "select",
      "open",
      "page.",
      "following",
      "available",
      "there",
      "change",
      "image",
      "upload",
      "existing",
      "image.",
      "displayed",
      "current",
      "drag",
      "drop",
      "profile.",
      "link",
      "display",
      "on-call",
      "shifts.",
      "displays",
      "management",
      "enabled.",
      "see",
      "management.",
      "property",
      "description",
      "choose",
      "language",
      "drop-down",
      "list.",
      "selected",
      "determines",
      "locale",
      "time",
      "format",
      "12-hour",
      "24-hour",
      "number",
      "formatting",
      "integer",
      "double",
      "data",
      "types.",
      "because",
      "known",
      "issue",
      "setting",
      "temporary",
      "automatically",
      "changed",
      "empty",
      "english",
      "later.",
      "want",
      "permanently",
      "administrator",
      "prevent",
      "overwritten",
      "again",
      "through",
      "editing",
      "people",
      "ui.",
      "high",
      "contrast",
      "mode",
      "screen",
      "mode."
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "profile and preferences",
    "contentLower": "the profile and preferences part of the user interface includes information about user profile, preferences, contact details, and other user settings. click your user name in the upper right corner and select user profile to open the profile & preferences page. profile click your user name in the upper right corner to open the profile & preferences page. the following are available there: <image> click change image to select an image to upload, or to change the existing image. the user profile is displayed with the current image. you may drag and drop an image into the user profile. view calendar click the link to display your on-call shifts. this link only displays if on-call schedule management is enabled. for more information, see on-call schedule management. preferences property description choose language select the user interface language from the drop-down list. the selected language determines the locale, time display format (12-hour or 24-hour), and number formatting for integ",
    "keywordsLower": [
      "preferences.to",
      "settings.if",
      "sent.for",
      "profile",
      "preferences",
      "view",
      "calendar",
      "page",
      "tabs",
      "area",
      "expertise",
      "contact",
      "details",
      "group",
      "memberships",
      "employees",
      "schedule",
      "notification",
      "settings",
      "part",
      "user",
      "interface",
      "includes",
      "information",
      "about",
      "settings.",
      "click",
      "name",
      "upper",
      "right",
      "corner",
      "select",
      "open",
      "page.",
      "following",
      "available",
      "there",
      "change",
      "image",
      "upload",
      "existing",
      "image.",
      "displayed",
      "current",
      "drag",
      "drop",
      "profile.",
      "link",
      "display",
      "on-call",
      "shifts.",
      "displays",
      "management",
      "enabled.",
      "see",
      "management.",
      "property",
      "description",
      "choose",
      "language",
      "drop-down",
      "list.",
      "selected",
      "determines",
      "locale",
      "time",
      "format",
      "12-hour",
      "24-hour",
      "number",
      "formatting",
      "integer",
      "double",
      "data",
      "types.",
      "because",
      "known",
      "issue",
      "setting",
      "temporary",
      "automatically",
      "changed",
      "empty",
      "english",
      "later.",
      "want",
      "permanently",
      "administrator",
      "prevent",
      "overwritten",
      "again",
      "through",
      "editing",
      "people",
      "ui.",
      "high",
      "contrast",
      "mode",
      "screen",
      "mode."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "My Dashboards",
    "content": "My Dashboards provides a single point of access to the information you rely on in your daily activities.: Dashboard Tasks queue Approvals queue",
    "url": "mydashboards",
    "filename": "mydashboards",
    "headings": [],
    "keywords": [
      "dashboards",
      "provides",
      "single",
      "point",
      "access",
      "information",
      "rely",
      "daily",
      "activities.",
      "dashboard",
      "tasks",
      "queue",
      "approvals"
    ],
    "language": "en",
    "word_count": 15,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "my dashboards",
    "contentLower": "my dashboards provides a single point of access to the information you rely on in your daily activities.: dashboard tasks queue approvals queue",
    "keywordsLower": [
      "dashboards",
      "provides",
      "single",
      "point",
      "access",
      "information",
      "rely",
      "daily",
      "activities.",
      "dashboard",
      "tasks",
      "queue",
      "approvals"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Overview of dashboards",
    "content": "By default, when accessing the Dashboard menu item, an agent user lands on the System Dashboard Definition, a generic collection of charts representing the set of modules supported by Service Management. Roles and permissions control the charts visible to each user. The system offers a set of out-of-the-box dashboards, which comply with ITIL best practices and target different user roles (such as Incident Manager, and Asset Manager). Agent users can add additional dashboards as desired. Agent users with the appropriate role permissions can create dashboard definitions for self-use or for sharing with specified audiences or all agent users. This way, agent users can view multiple dashboards from their dashboard home page. See dashboard definitions for more information. Without any modifications to roles, an agent without the Tenant Admin role can't modify an existing dashboard or save a private copy. To allow an agent to save a dashboard, the tenant admin needs to enable the relevant pe",
    "url": "dashboard",
    "filename": "dashboard",
    "headings": [
      "Dashboard user interface",
      "Search for a dashboard",
      "Sort the dashboard list",
      "Adjust the content or layout of a dashboard",
      "Want to save your changes?",
      "Set your default dashboard",
      "Work with a widget",
      "Related topics"
    ],
    "keywords": [
      "overview",
      "dashboards",
      "dashboard",
      "user",
      "interface",
      "search",
      "sort",
      "list",
      "adjust",
      "content",
      "layout",
      "want",
      "save",
      "changes",
      "set",
      "default",
      "work",
      "widget",
      "related",
      "topics",
      "accessing",
      "menu",
      "item",
      "agent",
      "lands",
      "system",
      "definition",
      "generic",
      "collection",
      "charts",
      "representing",
      "modules",
      "supported",
      "service",
      "management.",
      "roles",
      "permissions",
      "control",
      "visible",
      "user.",
      "offers",
      "out-of-the-box",
      "comply",
      "itil",
      "best",
      "practices",
      "target",
      "different",
      "such",
      "incident",
      "manager",
      "asset",
      "users",
      "add",
      "additional",
      "desired.",
      "appropriate",
      "role",
      "create",
      "definitions",
      "self-use",
      "sharing",
      "specified",
      "audiences",
      "all",
      "users.",
      "way",
      "view",
      "multiple",
      "home",
      "page.",
      "see",
      "information.",
      "any",
      "modifications",
      "tenant",
      "admin",
      "modify",
      "existing",
      "private",
      "copy.",
      "allow",
      "needs",
      "enable",
      "relevant",
      "record",
      "type.",
      "details",
      "assign",
      "section.",
      "agents",
      "decide",
      "public",
      "group",
      "personalized",
      "copy",
      "number",
      "created",
      "significant.",
      "permission"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "overview of dashboards",
    "contentLower": "by default, when accessing the dashboard menu item, an agent user lands on the system dashboard definition, a generic collection of charts representing the set of modules supported by service management. roles and permissions control the charts visible to each user. the system offers a set of out-of-the-box dashboards, which comply with itil best practices and target different user roles (such as incident manager, and asset manager). agent users can add additional dashboards as desired. agent users with the appropriate role permissions can create dashboard definitions for self-use or for sharing with specified audiences or all agent users. this way, agent users can view multiple dashboards from their dashboard home page. see dashboard definitions for more information. without any modifications to roles, an agent without the tenant admin role can't modify an existing dashboard or save a private copy. to allow an agent to save a dashboard, the tenant admin needs to enable the relevant pe",
    "keywordsLower": [
      "overview",
      "dashboards",
      "dashboard",
      "user",
      "interface",
      "search",
      "sort",
      "list",
      "adjust",
      "content",
      "layout",
      "want",
      "save",
      "changes",
      "set",
      "default",
      "work",
      "widget",
      "related",
      "topics",
      "accessing",
      "menu",
      "item",
      "agent",
      "lands",
      "system",
      "definition",
      "generic",
      "collection",
      "charts",
      "representing",
      "modules",
      "supported",
      "service",
      "management.",
      "roles",
      "permissions",
      "control",
      "visible",
      "user.",
      "offers",
      "out-of-the-box",
      "comply",
      "itil",
      "best",
      "practices",
      "target",
      "different",
      "such",
      "incident",
      "manager",
      "asset",
      "users",
      "add",
      "additional",
      "desired.",
      "appropriate",
      "role",
      "create",
      "definitions",
      "self-use",
      "sharing",
      "specified",
      "audiences",
      "all",
      "users.",
      "way",
      "view",
      "multiple",
      "home",
      "page.",
      "see",
      "information.",
      "any",
      "modifications",
      "tenant",
      "admin",
      "modify",
      "existing",
      "private",
      "copy.",
      "allow",
      "needs",
      "enable",
      "relevant",
      "record",
      "type.",
      "details",
      "assign",
      "section.",
      "agents",
      "decide",
      "public",
      "group",
      "personalized",
      "copy",
      "number",
      "created",
      "significant.",
      "permission"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Override approvals",
    "content": "Some users in Service Management can immediately override all pending approvals or previous denials. This is possible when: You have the correct permission. The specified record is in the Approval phase. The approval strategy is set to All must approve and there are still pending approvals. Note By default, the ability to override approvals is granted only to users having the Tenant admin role. To assign the permission to override approvals to other roles, see How to edit role permissions. A user with the Tenant Admin role can override pending approvals inside Service Management, but not in the Service Portal. In the Service Portal, a manager can override a pending approval by going to YOUR TEAM APPROVALS, and giving approval there. The ability to override existing approvals is possible for the following record types: Request Change Article Idea Proposal Release When properly configured, the OVERRIDE ALL APPROVALS button appears on the Approval form when you double-click (or edit) an a",
    "url": "overrideapprovals",
    "filename": "overrideapprovals",
    "headings": [],
    "keywords": [
      "override",
      "approvals",
      "users",
      "service",
      "management",
      "immediately",
      "all",
      "pending",
      "previous",
      "denials.",
      "possible",
      "correct",
      "permission.",
      "specified",
      "record",
      "approval",
      "phase.",
      "strategy",
      "set",
      "approve",
      "there",
      "still",
      "approvals.",
      "note",
      "default",
      "ability",
      "granted",
      "having",
      "tenant",
      "admin",
      "role.",
      "assign",
      "permission",
      "roles",
      "see",
      "edit",
      "role",
      "permissions.",
      "user",
      "inside",
      "portal.",
      "portal",
      "manager",
      "going",
      "team",
      "giving",
      "there.",
      "existing",
      "following",
      "types",
      "request",
      "change",
      "article",
      "idea",
      "proposal",
      "release",
      "properly",
      "configured",
      "button",
      "appears",
      "form",
      "double-click",
      "tab",
      "specific",
      "record.",
      "after",
      "clicking",
      "confirmation",
      "message",
      "appears.",
      "click",
      "complete",
      "process",
      "refresh",
      "screen",
      "view",
      "updated",
      "queue.",
      "approved",
      "attribute",
      "assigned",
      "denied"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "override approvals",
    "contentLower": "some users in service management can immediately override all pending approvals or previous denials. this is possible when: you have the correct permission. the specified record is in the approval phase. the approval strategy is set to all must approve and there are still pending approvals. note by default, the ability to override approvals is granted only to users having the tenant admin role. to assign the permission to override approvals to other roles, see how to edit role permissions. a user with the tenant admin role can override pending approvals inside service management, but not in the service portal. in the service portal, a manager can override a pending approval by going to your team approvals, and giving approval there. the ability to override existing approvals is possible for the following record types: request change article idea proposal release when properly configured, the override all approvals button appears on the approval form when you double-click (or edit) an a",
    "keywordsLower": [
      "override",
      "approvals",
      "users",
      "service",
      "management",
      "immediately",
      "all",
      "pending",
      "previous",
      "denials.",
      "possible",
      "correct",
      "permission.",
      "specified",
      "record",
      "approval",
      "phase.",
      "strategy",
      "set",
      "approve",
      "there",
      "still",
      "approvals.",
      "note",
      "default",
      "ability",
      "granted",
      "having",
      "tenant",
      "admin",
      "role.",
      "assign",
      "permission",
      "roles",
      "see",
      "edit",
      "role",
      "permissions.",
      "user",
      "inside",
      "portal.",
      "portal",
      "manager",
      "going",
      "team",
      "giving",
      "there.",
      "existing",
      "following",
      "types",
      "request",
      "change",
      "article",
      "idea",
      "proposal",
      "release",
      "properly",
      "configured",
      "button",
      "appears",
      "form",
      "double-click",
      "tab",
      "specific",
      "record.",
      "after",
      "clicking",
      "confirmation",
      "message",
      "appears.",
      "click",
      "complete",
      "process",
      "refresh",
      "screen",
      "view",
      "updated",
      "queue.",
      "approved",
      "attribute",
      "assigned",
      "denied"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Models",
    "content": "A model simplifies the creation of a record. For example, when you have common incidents, it's efficient to design an incident model that you can reuse to simplify the amount of effort required to resolve that same type of incident many times. Models standardize the end-to-end process and maximize efficiency. A Service Management model pre-populates common fields to save time when you create a new record. When you create a new record and select an appropriate model, Service Management automatically populates the relevant fields. For certain record types, a model can even create the tasks necessary to complete a process. Service Management provides a set of default models, and you can add models as required. If you no longer require a model, you can retire that model. For more information, see the following: Module Topics Change Management How to create a change model How to edit a change model Incident Management How to create an incident model How to edit an incident model Knowledge M",
    "url": "models",
    "filename": "models",
    "headings": [],
    "keywords": [
      "models",
      "model",
      "simplifies",
      "creation",
      "record.",
      "example",
      "common",
      "incidents",
      "efficient",
      "design",
      "incident",
      "reuse",
      "simplify",
      "amount",
      "effort",
      "required",
      "resolve",
      "same",
      "type",
      "many",
      "times.",
      "standardize",
      "end-to-end",
      "process",
      "maximize",
      "efficiency.",
      "service",
      "management",
      "pre-populates",
      "fields",
      "save",
      "time",
      "create",
      "new",
      "record",
      "select",
      "appropriate",
      "automatically",
      "populates",
      "relevant",
      "fields.",
      "certain",
      "types",
      "even",
      "tasks",
      "necessary",
      "complete",
      "process.",
      "provides",
      "set",
      "default",
      "add",
      "required.",
      "longer",
      "require",
      "retire",
      "model.",
      "information",
      "see",
      "following",
      "module",
      "topics",
      "change",
      "edit",
      "knowledge",
      "article",
      "release",
      "asset",
      "configuration",
      "software",
      "license",
      "level",
      "agreement"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "models",
    "contentLower": "a model simplifies the creation of a record. for example, when you have common incidents, it's efficient to design an incident model that you can reuse to simplify the amount of effort required to resolve that same type of incident many times. models standardize the end-to-end process and maximize efficiency. a service management model pre-populates common fields to save time when you create a new record. when you create a new record and select an appropriate model, service management automatically populates the relevant fields. for certain record types, a model can even create the tasks necessary to complete a process. service management provides a set of default models, and you can add models as required. if you no longer require a model, you can retire that model. for more information, see the following: module topics change management how to create a change model how to edit a change model incident management how to create an incident model how to edit an incident model knowledge m",
    "keywordsLower": [
      "models",
      "model",
      "simplifies",
      "creation",
      "record.",
      "example",
      "common",
      "incidents",
      "efficient",
      "design",
      "incident",
      "reuse",
      "simplify",
      "amount",
      "effort",
      "required",
      "resolve",
      "same",
      "type",
      "many",
      "times.",
      "standardize",
      "end-to-end",
      "process",
      "maximize",
      "efficiency.",
      "service",
      "management",
      "pre-populates",
      "fields",
      "save",
      "time",
      "create",
      "new",
      "record",
      "select",
      "appropriate",
      "automatically",
      "populates",
      "relevant",
      "fields.",
      "certain",
      "types",
      "even",
      "tasks",
      "necessary",
      "complete",
      "process.",
      "provides",
      "set",
      "default",
      "add",
      "required.",
      "longer",
      "require",
      "retire",
      "model.",
      "information",
      "see",
      "following",
      "module",
      "topics",
      "change",
      "edit",
      "knowledge",
      "article",
      "release",
      "asset",
      "configuration",
      "software",
      "license",
      "level",
      "agreement"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage hot topic analytics",
    "content": "Hot topics are calculated by an algorithm that takes into account the words that appear in a given set of records (for example, in support requests or incidents), and finds the topics in those records by looking for correlations between subsets of the words. The calculation takes into account the textual fields of the records (for example, title, description, and solution). The list of topics is calculated by removing extraneous items such as email addresses, single letters, common and rare words, frequently appearing hyphenated terms, and stop list words. Currently, Hot Topic Analytics supports the following document types: Support requests without offerings User searches User questions Incidents Survey results The resulting calculated number of topics depends on the number of records. Only the hottest 5 topics are presented on the map, and each topic shows three or four most significant terms. Number of records Number of topics calculated Fewer than 50 records 3 topics 50-200 records",
    "url": "managehottopics",
    "filename": "managehottopics",
    "headings": [
      "Add a word to the user-defined stop list",
      "Manage the stop list",
      "Related topics"
    ],
    "keywords": [
      "manage",
      "hot",
      "topic",
      "analytics",
      "add",
      "word",
      "user-defined",
      "stop",
      "list",
      "related",
      "topics",
      "calculated",
      "algorithm",
      "takes",
      "account",
      "words",
      "appear",
      "given",
      "set",
      "records",
      "example",
      "support",
      "requests",
      "incidents",
      "finds",
      "looking",
      "correlations",
      "between",
      "subsets",
      "words.",
      "calculation",
      "textual",
      "fields",
      "title",
      "description",
      "solution",
      "removing",
      "extraneous",
      "items",
      "such",
      "email",
      "addresses",
      "single",
      "letters",
      "common",
      "rare",
      "frequently",
      "appearing",
      "hyphenated",
      "terms",
      "currently",
      "supports",
      "following",
      "document",
      "types",
      "offerings",
      "user",
      "searches",
      "questions",
      "survey",
      "results",
      "resulting",
      "number",
      "depends",
      "records.",
      "hottest",
      "presented",
      "map",
      "shows",
      "three",
      "four",
      "most",
      "significant",
      "terms.",
      "fewer",
      "50",
      "50-200",
      "10",
      "200-500",
      "20",
      "500-1000",
      "30",
      "1000",
      "40",
      "engine",
      "calculates",
      "presents",
      "two",
      "ways",
      "background",
      "learning",
      "000",
      "latest",
      "collected",
      "type",
      "generate",
      "every",
      "12",
      "hours.",
      "minimum"
    ],
    "language": "en",
    "word_count": 107,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage hot topic analytics",
    "contentLower": "hot topics are calculated by an algorithm that takes into account the words that appear in a given set of records (for example, in support requests or incidents), and finds the topics in those records by looking for correlations between subsets of the words. the calculation takes into account the textual fields of the records (for example, title, description, and solution). the list of topics is calculated by removing extraneous items such as email addresses, single letters, common and rare words, frequently appearing hyphenated terms, and stop list words. currently, hot topic analytics supports the following document types: support requests without offerings user searches user questions incidents survey results the resulting calculated number of topics depends on the number of records. only the hottest 5 topics are presented on the map, and each topic shows three or four most significant terms. number of records number of topics calculated fewer than 50 records 3 topics 50-200 records",
    "keywordsLower": [
      "manage",
      "hot",
      "topic",
      "analytics",
      "add",
      "word",
      "user-defined",
      "stop",
      "list",
      "related",
      "topics",
      "calculated",
      "algorithm",
      "takes",
      "account",
      "words",
      "appear",
      "given",
      "set",
      "records",
      "example",
      "support",
      "requests",
      "incidents",
      "finds",
      "looking",
      "correlations",
      "between",
      "subsets",
      "words.",
      "calculation",
      "textual",
      "fields",
      "title",
      "description",
      "solution",
      "removing",
      "extraneous",
      "items",
      "such",
      "email",
      "addresses",
      "single",
      "letters",
      "common",
      "rare",
      "frequently",
      "appearing",
      "hyphenated",
      "terms",
      "currently",
      "supports",
      "following",
      "document",
      "types",
      "offerings",
      "user",
      "searches",
      "questions",
      "survey",
      "results",
      "resulting",
      "number",
      "depends",
      "records.",
      "hottest",
      "presented",
      "map",
      "shows",
      "three",
      "four",
      "most",
      "significant",
      "terms.",
      "fewer",
      "50",
      "50-200",
      "10",
      "200-500",
      "20",
      "500-1000",
      "30",
      "1000",
      "40",
      "engine",
      "calculates",
      "presents",
      "two",
      "ways",
      "background",
      "learning",
      "000",
      "latest",
      "collected",
      "type",
      "generate",
      "every",
      "12",
      "hours.",
      "minimum"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Plan",
    "content": "This part includes the following: Service Portfolio Management Service Catalog Management Time Period Management Service Level Management Contract Management Company Management Idea and Proposal Management Application Portfolio Management Project and Program Management",
    "url": "plan",
    "filename": "plan",
    "headings": [],
    "keywords": [
      "plan",
      "part",
      "includes",
      "following",
      "service",
      "portfolio",
      "management",
      "catalog",
      "time",
      "period",
      "level",
      "contract",
      "company",
      "idea",
      "proposal",
      "application",
      "project",
      "program"
    ],
    "language": "en",
    "word_count": 29,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "plan",
    "contentLower": "this part includes the following: service portfolio management service catalog management time period management service level management contract management company management idea and proposal management application portfolio management project and program management",
    "keywordsLower": [
      "plan",
      "part",
      "includes",
      "following",
      "service",
      "portfolio",
      "management",
      "catalog",
      "time",
      "period",
      "level",
      "contract",
      "company",
      "idea",
      "proposal",
      "application",
      "project",
      "program"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Offerings and pricing",
    "content": "If you have the appropriate permissions, you can configure offerings in Service Management to offer an extensive menu of prices, including packages for different scenarios, all according to your organizational requirements. Price variations Variations to the price may be: Context-based Price can vary according to location. Currency can vary according to location. Option-based Options can increase the price. Total price dynamically calculated based on options selected. Business rules defined on the offering can be used for custom-price formula calculations, for example, to set discounts. Advanced pricing example Assume the basic pricing for the offering, already entered into Service Management, is as follows: Field Value Currency USD($) Price 500 Recurring price 10 Recurring period Monthly You want to set up pricing for the following locations, already entered into Service Management: Europe For this location, you want to use the euro for the currency, based on an exchange rate of 1 dol",
    "url": "offeringspricing",
    "filename": "offeringspricing",
    "headings": [
      "Price variations",
      "Advanced pricing example",
      "User options pricing example",
      "Business rules pricing example",
      "Set up the basic user options",
      "Set up the promotion date fields in user options",
      "Set default values for promotion period and CPU",
      "Set prices",
      "Add rules for the price promotion"
    ],
    "keywords": [
      "entity.Cost",
      "0.9",
      "offerings",
      "pricing",
      "price",
      "variations",
      "advanced",
      "example",
      "user",
      "options",
      "business",
      "rules",
      "set",
      "basic",
      "promotion",
      "date",
      "fields",
      "default",
      "values",
      "period",
      "cpu",
      "prices",
      "add",
      "appropriate",
      "permissions",
      "configure",
      "service",
      "management",
      "offer",
      "extensive",
      "menu",
      "including",
      "packages",
      "different",
      "scenarios",
      "all",
      "according",
      "organizational",
      "requirements.",
      "context-based",
      "vary",
      "location.",
      "currency",
      "option-based",
      "increase",
      "price.",
      "total",
      "dynamically",
      "calculated",
      "based",
      "selected.",
      "defined",
      "offering",
      "custom-price",
      "formula",
      "calculations",
      "discounts.",
      "assume",
      "already",
      "entered",
      "follows",
      "field",
      "value",
      "usd",
      "500",
      "recurring",
      "10",
      "monthly",
      "want",
      "following",
      "locations",
      "europe",
      "location",
      "euro",
      "exchange",
      "rate",
      "dollar",
      "equaling",
      "euro.",
      "middle",
      "east",
      "currency.",
      "however",
      "reflect",
      "increased",
      "costs",
      "delivering",
      "higher.",
      "select",
      "offering.",
      "click",
      "tab.",
      "europe.",
      "complete",
      "details",
      "eur",
      "450",
      "east.",
      "550",
      "11"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "offerings and pricing",
    "contentLower": "if you have the appropriate permissions, you can configure offerings in service management to offer an extensive menu of prices, including packages for different scenarios, all according to your organizational requirements. price variations variations to the price may be: context-based price can vary according to location. currency can vary according to location. option-based options can increase the price. total price dynamically calculated based on options selected. business rules defined on the offering can be used for custom-price formula calculations, for example, to set discounts. advanced pricing example assume the basic pricing for the offering, already entered into service management, is as follows: field value currency usd($) price 500 recurring price 10 recurring period monthly you want to set up pricing for the following locations, already entered into service management: europe for this location, you want to use the euro for the currency, based on an exchange rate of 1 dol",
    "keywordsLower": [
      "entity.cost",
      "0.9",
      "offerings",
      "pricing",
      "price",
      "variations",
      "advanced",
      "example",
      "user",
      "options",
      "business",
      "rules",
      "set",
      "basic",
      "promotion",
      "date",
      "fields",
      "default",
      "values",
      "period",
      "cpu",
      "prices",
      "add",
      "appropriate",
      "permissions",
      "configure",
      "service",
      "management",
      "offer",
      "extensive",
      "menu",
      "including",
      "packages",
      "different",
      "scenarios",
      "all",
      "according",
      "organizational",
      "requirements.",
      "context-based",
      "vary",
      "location.",
      "currency",
      "option-based",
      "increase",
      "price.",
      "total",
      "dynamically",
      "calculated",
      "based",
      "selected.",
      "defined",
      "offering",
      "custom-price",
      "formula",
      "calculations",
      "discounts.",
      "assume",
      "already",
      "entered",
      "follows",
      "field",
      "value",
      "usd",
      "500",
      "recurring",
      "10",
      "monthly",
      "want",
      "following",
      "locations",
      "europe",
      "location",
      "euro",
      "exchange",
      "rate",
      "dollar",
      "equaling",
      "euro.",
      "middle",
      "east",
      "currency.",
      "however",
      "reflect",
      "increased",
      "costs",
      "delivering",
      "higher.",
      "select",
      "offering.",
      "click",
      "tab.",
      "europe.",
      "complete",
      "details",
      "eur",
      "450",
      "east.",
      "550",
      "11"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Offering business rules",
    "content": "You can define business rules within a specific offering. The rules run in the request workflow for request records in which the offering is selected as the Catalog offering. These business rules can be used to make a user option mandatory, hidden, or to control values displayed in the user option fields and any fields in the requests based on the offering. For example, you can use this feature to display a state field if United States is selected as the country field. You can define the rules to run in connection with one of the following process events: Process event Description Before change The rule is run before the data is updated. After change The rule is run after the data is updated. Rendering forms The rule is run when a form is opened. After applying changes The rule is run after the change is committed. SLT Event The rule is run when the Service Level target duration reaches the 0%, 50%, 75%, 90%, or 100% level of the target, as defined by the rule. For information on defin",
    "url": "offeringbizrules",
    "filename": "offeringbizrules",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "ProblemCausedByRequest.Id",
      "offering",
      "business",
      "rules",
      "related",
      "topics",
      "define",
      "specific",
      "offering.",
      "run",
      "request",
      "workflow",
      "records",
      "selected",
      "catalog",
      "make",
      "user",
      "option",
      "mandatory",
      "hidden",
      "control",
      "values",
      "displayed",
      "fields",
      "any",
      "requests",
      "based",
      "example",
      "feature",
      "display",
      "state",
      "field",
      "united",
      "states",
      "country",
      "field.",
      "connection",
      "one",
      "following",
      "process",
      "events",
      "event",
      "description",
      "before",
      "change",
      "rule",
      "data",
      "updated.",
      "after",
      "rendering",
      "forms",
      "form",
      "opened.",
      "applying",
      "changes",
      "committed.",
      "slt",
      "service",
      "level",
      "target",
      "duration",
      "reaches",
      "50",
      "75",
      "90",
      "100",
      "defined",
      "rule.",
      "information",
      "defining",
      "under",
      "see",
      "add",
      "rules.",
      "either",
      "general",
      "record",
      "select",
      "required",
      "tab.",
      "click",
      "links",
      "button",
      "next",
      "event.",
      "template",
      "note",
      "aviod",
      "many-to-many",
      "m2m",
      "records.",
      "set",
      "comments",
      "concat",
      "entity.problemcausedbyrequest.id",
      "cause",
      "error",
      "portal",
      "edits",
      "cart"
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "offering business rules",
    "contentLower": "you can define business rules within a specific offering. the rules run in the request workflow for request records in which the offering is selected as the catalog offering. these business rules can be used to make a user option mandatory, hidden, or to control values displayed in the user option fields and any fields in the requests based on the offering. for example, you can use this feature to display a state field if united states is selected as the country field. you can define the rules to run in connection with one of the following process events: process event description before change the rule is run before the data is updated. after change the rule is run after the data is updated. rendering forms the rule is run when a form is opened. after applying changes the rule is run after the change is committed. slt event the rule is run when the service level target duration reaches the 0%, 50%, 75%, 90%, or 100% level of the target, as defined by the rule. for information on defin",
    "keywordsLower": [
      "problemcausedbyrequest.id",
      "offering",
      "business",
      "rules",
      "related",
      "topics",
      "define",
      "specific",
      "offering.",
      "run",
      "request",
      "workflow",
      "records",
      "selected",
      "catalog",
      "make",
      "user",
      "option",
      "mandatory",
      "hidden",
      "control",
      "values",
      "displayed",
      "fields",
      "any",
      "requests",
      "based",
      "example",
      "feature",
      "display",
      "state",
      "field",
      "united",
      "states",
      "country",
      "field.",
      "connection",
      "one",
      "following",
      "process",
      "events",
      "event",
      "description",
      "before",
      "change",
      "rule",
      "data",
      "updated.",
      "after",
      "rendering",
      "forms",
      "form",
      "opened.",
      "applying",
      "changes",
      "committed.",
      "slt",
      "service",
      "level",
      "target",
      "duration",
      "reaches",
      "50",
      "75",
      "90",
      "100",
      "defined",
      "rule.",
      "information",
      "defining",
      "under",
      "see",
      "add",
      "rules.",
      "either",
      "general",
      "record",
      "select",
      "required",
      "tab.",
      "click",
      "links",
      "button",
      "next",
      "event.",
      "template",
      "note",
      "aviod",
      "many-to-many",
      "m2m",
      "records.",
      "set",
      "comments",
      "concat",
      "entity.problemcausedbyrequest.id",
      "cause",
      "error",
      "portal",
      "edits",
      "cart"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage Service Level Target business rules",
    "content": "For Service Level Target, you can define Service Level Target rules through either: The Processes and Rules section in Studio. This allows you to manage rules for all records controlled by processes in a centralized place. The Rules section in Service Level Target Set records. This allows you to manage rules for records with a specific target set, as a supplement to the rules defined in Studio. There are two types of SLT business rules that can be managed by customers, operational rules (Start/Suspend/Unsuspend/Stop Targets) and notification rules. Notification rules will run when the duration reaches 0%, 15%, 35%, 50%, 75%, 90%, and 100% level of the target. You can configure the rules to set fields or send notifications. By default, Request, Incident, Change, Problem, and Task workflows contain out-of-the-box operational rules, you can either modify these rules or create new rules according to your business needs. If there are multiple rules running the same operation on one target, ",
    "url": "addslteventrules",
    "filename": "addslteventrules",
    "headings": [
      "Define a Service Level Target business rule",
      "SLT operation rules",
      "Rule execution order",
      "Related topics"
    ],
    "keywords": [
      "Resolution.Type",
      "manage",
      "service",
      "level",
      "target",
      "business",
      "rules",
      "define",
      "rule",
      "slt",
      "operation",
      "execution",
      "order",
      "related",
      "topics",
      "through",
      "either",
      "processes",
      "section",
      "studio.",
      "allows",
      "all",
      "records",
      "controlled",
      "centralized",
      "place.",
      "set",
      "records.",
      "specific",
      "supplement",
      "defined",
      "there",
      "two",
      "types",
      "managed",
      "customers",
      "operational",
      "start",
      "suspend",
      "unsuspend",
      "stop",
      "targets",
      "notification",
      "rules.",
      "run",
      "duration",
      "reaches",
      "15",
      "35",
      "50",
      "75",
      "90",
      "100",
      "target.",
      "configure",
      "fields",
      "send",
      "notifications.",
      "default",
      "request",
      "incident",
      "change",
      "problem",
      "task",
      "workflows",
      "contain",
      "out-of-the-box",
      "modify",
      "create",
      "new",
      "according",
      "needs.",
      "multiple",
      "running",
      "same",
      "one",
      "conditions",
      "met",
      "triggered.",
      "however",
      "first",
      "actually",
      "executed",
      "latter",
      "ones",
      "won",
      "any",
      "effects.",
      "example",
      "initial",
      "review",
      "started",
      "triggered",
      "first.",
      "allowed",
      "add",
      "general",
      "like",
      "field",
      "well."
    ],
    "language": "en",
    "word_count": 112,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage service level target business rules",
    "contentLower": "for service level target, you can define service level target rules through either: the processes and rules section in studio. this allows you to manage rules for all records controlled by processes in a centralized place. the rules section in service level target set records. this allows you to manage rules for records with a specific target set, as a supplement to the rules defined in studio. there are two types of slt business rules that can be managed by customers, operational rules (start/suspend/unsuspend/stop targets) and notification rules. notification rules will run when the duration reaches 0%, 15%, 35%, 50%, 75%, 90%, and 100% level of the target. you can configure the rules to set fields or send notifications. by default, request, incident, change, problem, and task workflows contain out-of-the-box operational rules, you can either modify these rules or create new rules according to your business needs. if there are multiple rules running the same operation on one target, ",
    "keywordsLower": [
      "resolution.type",
      "manage",
      "service",
      "level",
      "target",
      "business",
      "rules",
      "define",
      "rule",
      "slt",
      "operation",
      "execution",
      "order",
      "related",
      "topics",
      "through",
      "either",
      "processes",
      "section",
      "studio.",
      "allows",
      "all",
      "records",
      "controlled",
      "centralized",
      "place.",
      "set",
      "records.",
      "specific",
      "supplement",
      "defined",
      "there",
      "two",
      "types",
      "managed",
      "customers",
      "operational",
      "start",
      "suspend",
      "unsuspend",
      "stop",
      "targets",
      "notification",
      "rules.",
      "run",
      "duration",
      "reaches",
      "15",
      "35",
      "50",
      "75",
      "90",
      "100",
      "target.",
      "configure",
      "fields",
      "send",
      "notifications.",
      "default",
      "request",
      "incident",
      "change",
      "problem",
      "task",
      "workflows",
      "contain",
      "out-of-the-box",
      "modify",
      "create",
      "new",
      "according",
      "needs.",
      "multiple",
      "running",
      "same",
      "one",
      "conditions",
      "met",
      "triggered.",
      "however",
      "first",
      "actually",
      "executed",
      "latter",
      "ones",
      "won",
      "any",
      "effects.",
      "example",
      "initial",
      "review",
      "started",
      "triggered",
      "first.",
      "allowed",
      "add",
      "general",
      "like",
      "field",
      "well."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage custom Service Level Target types",
    "content": "To manage custom target types, follow these steps. Create a custom target type in Lists You can go to Lists, open the Target type (TargetType) list, and add custom target type items as needed. Notes: The custom target types created this way are all SLA target types. There is only one OLA target type, which is Time in Group. You can activate or deactivate custom target types in this list. If you deactivate a custom target type, it won't be available when you create/edit a Target Set. However, this Active/Inactive switch doesn't take effect on records that have already applied this target type. For an inactive target type, you need to manually delete its configurations in order to stop the corresponding SLT calculation. When an SLT is already running for a record, if you go to Lists and add a custom target type item and then try to append it to the target set, the newly added custom target type won't show up in the record. Add the custom target type to a Service Level Target Set Add the ",
    "url": "managecustomslttypes",
    "filename": "managecustomslttypes",
    "headings": [
      "Create a custom target type in Lists",
      "Add the custom target type to a Service Level Target Set",
      "Add SLT operation rules for the custom target type"
    ],
    "keywords": [
      "manage",
      "custom",
      "service",
      "level",
      "target",
      "types",
      "create",
      "type",
      "lists",
      "add",
      "set",
      "slt",
      "operation",
      "rules",
      "follow",
      "steps.",
      "go",
      "open",
      "targettype",
      "list",
      "items",
      "needed.",
      "notes",
      "created",
      "way",
      "all",
      "sla",
      "types.",
      "there",
      "one",
      "ola",
      "time",
      "group.",
      "activate",
      "deactivate",
      "list.",
      "won",
      "available",
      "edit",
      "set.",
      "however",
      "active",
      "inactive",
      "switch",
      "doesn",
      "take",
      "effect",
      "records",
      "already",
      "applied",
      "type.",
      "need",
      "manually",
      "delete",
      "configurations",
      "order",
      "stop",
      "corresponding",
      "calculation.",
      "running",
      "record",
      "item",
      "try",
      "append",
      "newly",
      "added",
      "show",
      "record.",
      "field",
      "instructions",
      "business",
      "through",
      "either",
      "studio",
      "workflow",
      "tab",
      "managing",
      "rule",
      "conditions",
      "execute",
      "pay",
      "attention",
      "following",
      "scenarios",
      "want",
      "start",
      "based",
      "metaphase",
      "changes",
      "include",
      "additional",
      "condition",
      "especially",
      "change",
      "default",
      "process",
      "normal",
      "every",
      "first",
      "flow"
    ],
    "language": "en",
    "word_count": 122,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage custom service level target types",
    "contentLower": "to manage custom target types, follow these steps. create a custom target type in lists you can go to lists, open the target type (targettype) list, and add custom target type items as needed. notes: the custom target types created this way are all sla target types. there is only one ola target type, which is time in group. you can activate or deactivate custom target types in this list. if you deactivate a custom target type, it won't be available when you create/edit a target set. however, this active/inactive switch doesn't take effect on records that have already applied this target type. for an inactive target type, you need to manually delete its configurations in order to stop the corresponding slt calculation. when an slt is already running for a record, if you go to lists and add a custom target type item and then try to append it to the target set, the newly added custom target type won't show up in the record. add the custom target type to a service level target set add the ",
    "keywordsLower": [
      "manage",
      "custom",
      "service",
      "level",
      "target",
      "types",
      "create",
      "type",
      "lists",
      "add",
      "set",
      "slt",
      "operation",
      "rules",
      "follow",
      "steps.",
      "go",
      "open",
      "targettype",
      "list",
      "items",
      "needed.",
      "notes",
      "created",
      "way",
      "all",
      "sla",
      "types.",
      "there",
      "one",
      "ola",
      "time",
      "group.",
      "activate",
      "deactivate",
      "list.",
      "won",
      "available",
      "edit",
      "set.",
      "however",
      "active",
      "inactive",
      "switch",
      "doesn",
      "take",
      "effect",
      "records",
      "already",
      "applied",
      "type.",
      "need",
      "manually",
      "delete",
      "configurations",
      "order",
      "stop",
      "corresponding",
      "calculation.",
      "running",
      "record",
      "item",
      "try",
      "append",
      "newly",
      "added",
      "show",
      "record.",
      "field",
      "instructions",
      "business",
      "through",
      "either",
      "studio",
      "workflow",
      "tab",
      "managing",
      "rule",
      "conditions",
      "execute",
      "pay",
      "attention",
      "following",
      "scenarios",
      "want",
      "start",
      "based",
      "metaphase",
      "changes",
      "include",
      "additional",
      "condition",
      "especially",
      "change",
      "default",
      "process",
      "normal",
      "every",
      "first",
      "flow"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage companies",
    "content": "This page lists the topics related to managing companies: Company workflow Add company View or edit company Related company",
    "url": "managecompany",
    "filename": "managecompany",
    "headings": [],
    "keywords": [
      "manage",
      "companies",
      "page",
      "lists",
      "topics",
      "related",
      "managing",
      "company",
      "workflow",
      "add",
      "view",
      "edit"
    ],
    "language": "en",
    "word_count": 17,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage companies",
    "contentLower": "this page lists the topics related to managing companies: company workflow add company view or edit company related company",
    "keywordsLower": [
      "manage",
      "companies",
      "page",
      "lists",
      "topics",
      "related",
      "managing",
      "company",
      "workflow",
      "add",
      "view",
      "edit"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage users and contacts",
    "content": "This page explains the steps to manage users and contacts. Manage users and contacts See Users and contacts to learn about users and contacts and the steps involved in the following: How to view the list of all users and contacts in the applicationHow to add, edit, or delete users in Suite AdministrationHow to manage users in the tenantHow to create and delete users or contactsHow to edit a user or contact recordHow to enable non-tenant admins to manage user lifecycle Make sure the Company field in the Organizational information section of the user record reflects the company they belong to. This is necessary for CSM to work properly. View users and contacts of a company Click > Plan > Company. Click the name of the company for which you want to view users and contacts. Click the Related parties tab.Go to the Users of this company section. This displays all the users and contacts of the company.",
    "url": "manageusers",
    "filename": "manageusers",
    "headings": [
      "Manage users and contacts",
      "View users and contacts of a company"
    ],
    "keywords": [
      "tab.Go",
      "manage",
      "users",
      "contacts",
      "view",
      "company",
      "page",
      "explains",
      "steps",
      "contacts.",
      "see",
      "learn",
      "about",
      "involved",
      "following",
      "list",
      "all",
      "applicationhow",
      "add",
      "edit",
      "delete",
      "suite",
      "administrationhow",
      "tenanthow",
      "create",
      "contactshow",
      "user",
      "contact",
      "recordhow",
      "enable",
      "non-tenant",
      "admins",
      "lifecycle",
      "make",
      "sure",
      "field",
      "organizational",
      "information",
      "section",
      "record",
      "reflects",
      "belong",
      "to.",
      "necessary",
      "csm",
      "work",
      "properly.",
      "click",
      "plan",
      "company.",
      "name",
      "want",
      "related",
      "parties",
      "section.",
      "displays"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage users and contacts",
    "contentLower": "this page explains the steps to manage users and contacts. manage users and contacts see users and contacts to learn about users and contacts and the steps involved in the following: how to view the list of all users and contacts in the applicationhow to add, edit, or delete users in suite administrationhow to manage users in the tenanthow to create and delete users or contactshow to edit a user or contact recordhow to enable non-tenant admins to manage user lifecycle make sure the company field in the organizational information section of the user record reflects the company they belong to. this is necessary for csm to work properly. view users and contacts of a company click > plan > company. click the name of the company for which you want to view users and contacts. click the related parties tab.go to the users of this company section. this displays all the users and contacts of the company.",
    "keywordsLower": [
      "tab.go",
      "manage",
      "users",
      "contacts",
      "view",
      "company",
      "page",
      "explains",
      "steps",
      "contacts.",
      "see",
      "learn",
      "about",
      "involved",
      "following",
      "list",
      "all",
      "applicationhow",
      "add",
      "edit",
      "delete",
      "suite",
      "administrationhow",
      "tenanthow",
      "create",
      "contactshow",
      "user",
      "contact",
      "recordhow",
      "enable",
      "non-tenant",
      "admins",
      "lifecycle",
      "make",
      "sure",
      "field",
      "organizational",
      "information",
      "section",
      "record",
      "reflects",
      "belong",
      "to.",
      "necessary",
      "csm",
      "work",
      "properly.",
      "click",
      "plan",
      "company.",
      "name",
      "want",
      "related",
      "parties",
      "section.",
      "displays"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage product models",
    "content": "This page explains product models and lists the topics related to managing product models. Product model A product model captures the structure and details of what your company offers — whether it’s a standalone product, a bundle of products, or simply a way to group and classify products. Each product model falls under one of the following types: Category - Use this to define logical groupings of products. Categories are not actual products — they help organize your product catalog. Example: Hardware, Software, Services. Product - This represents a specific item your company sells. Example: A software application, or a hardware product. Bundle - A collection of products sold together as a single unit. Product models can also have versions and modules. Topics related to managing product models Product model workflow Add product model View or edit product model Delete product model",
    "url": "manageproductmodel",
    "filename": "manageproductmodel",
    "headings": [
      "Product model",
      "Topics related to managing product models"
    ],
    "keywords": [
      "manage",
      "product",
      "models",
      "model",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "lists",
      "models.",
      "captures",
      "structure",
      "details",
      "what",
      "company",
      "offers",
      "whether",
      "standalone",
      "bundle",
      "products",
      "simply",
      "way",
      "group",
      "classify",
      "products.",
      "falls",
      "under",
      "one",
      "following",
      "types",
      "category",
      "define",
      "logical",
      "groupings",
      "categories",
      "actual",
      "help",
      "organize",
      "catalog.",
      "example",
      "hardware",
      "software",
      "services.",
      "represents",
      "specific",
      "item",
      "sells.",
      "application",
      "product.",
      "collection",
      "sold",
      "together",
      "single",
      "unit.",
      "versions",
      "modules.",
      "workflow",
      "add",
      "view",
      "edit",
      "delete"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage product models",
    "contentLower": "this page explains product models and lists the topics related to managing product models. product model a product model captures the structure and details of what your company offers — whether it’s a standalone product, a bundle of products, or simply a way to group and classify products. each product model falls under one of the following types: category - use this to define logical groupings of products. categories are not actual products — they help organize your product catalog. example: hardware, software, services. product - this represents a specific item your company sells. example: a software application, or a hardware product. bundle - a collection of products sold together as a single unit. product models can also have versions and modules. topics related to managing product models product model workflow add product model view or edit product model delete product model",
    "keywordsLower": [
      "manage",
      "product",
      "models",
      "model",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "lists",
      "models.",
      "captures",
      "structure",
      "details",
      "what",
      "company",
      "offers",
      "whether",
      "standalone",
      "bundle",
      "products",
      "simply",
      "way",
      "group",
      "classify",
      "products.",
      "falls",
      "under",
      "one",
      "following",
      "types",
      "category",
      "define",
      "logical",
      "groupings",
      "categories",
      "actual",
      "help",
      "organize",
      "catalog.",
      "example",
      "hardware",
      "software",
      "services.",
      "represents",
      "specific",
      "item",
      "sells.",
      "application",
      "product.",
      "collection",
      "sold",
      "together",
      "single",
      "unit.",
      "versions",
      "modules.",
      "workflow",
      "add",
      "view",
      "edit",
      "delete"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Product model workflow",
    "content": "This page explains the product model workflow and the steps to perform the following tasks: View workflow and current phase of product model Change phase of product model Product model workflow The product model workflow describes the different metaphases and phases in the lifecycle of a product model. Metaphases represent broad stages in the lifecycle, while phases are the specific steps within each stage. The product model lifecycle consists of the following metaphases: Planning Consumable Retired (End) The following figure shows the product model workflow. The following table describes the phases within each metaphase. Metaphase Phase Description Allowed transitions Transition type Planning Draft After creating the product model, it is in the Draft phase by default. Keep the product model in the Draft phase until it is ready to use. Draft to Active, Draft to Inactive Manual Consumable Active Move the product model to the Active phase when it is ready for use or has active customers.",
    "url": "manageprductwrkflw",
    "filename": "manageprductwrkflw",
    "headings": [
      "Product model workflow",
      "View workflow and current phase of product model",
      "Change phase of product model"
    ],
    "keywords": [
      "product",
      "model",
      "workflow",
      "view",
      "current",
      "phase",
      "change",
      "page",
      "explains",
      "steps",
      "perform",
      "following",
      "tasks",
      "describes",
      "different",
      "metaphases",
      "phases",
      "lifecycle",
      "model.",
      "represent",
      "broad",
      "stages",
      "while",
      "specific",
      "stage.",
      "consists",
      "planning",
      "consumable",
      "retired",
      "end",
      "figure",
      "shows",
      "workflow.",
      "table",
      "metaphase.",
      "metaphase",
      "description",
      "allowed",
      "transitions",
      "transition",
      "type",
      "draft",
      "after",
      "creating",
      "default.",
      "keep",
      "until",
      "ready",
      "use.",
      "active",
      "inactive",
      "manual",
      "move",
      "customers.",
      "longer",
      "support",
      "ended.",
      "click",
      "plan",
      "company.",
      "companies",
      "dropdown",
      "models.",
      "tab.",
      "displays",
      "highlighted",
      "blue.",
      "directs",
      "general",
      "tab",
      "button",
      "upper-right",
      "side",
      "next",
      "select",
      "appropriate",
      "dropdown.",
      "save."
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "product model workflow",
    "contentLower": "this page explains the product model workflow and the steps to perform the following tasks: view workflow and current phase of product model change phase of product model product model workflow the product model workflow describes the different metaphases and phases in the lifecycle of a product model. metaphases represent broad stages in the lifecycle, while phases are the specific steps within each stage. the product model lifecycle consists of the following metaphases: planning consumable retired (end) the following figure shows the product model workflow. the following table describes the phases within each metaphase. metaphase phase description allowed transitions transition type planning draft after creating the product model, it is in the draft phase by default. keep the product model in the draft phase until it is ready to use. draft to active, draft to inactive manual consumable active move the product model to the active phase when it is ready for use or has active customers.",
    "keywordsLower": [
      "product",
      "model",
      "workflow",
      "view",
      "current",
      "phase",
      "change",
      "page",
      "explains",
      "steps",
      "perform",
      "following",
      "tasks",
      "describes",
      "different",
      "metaphases",
      "phases",
      "lifecycle",
      "model.",
      "represent",
      "broad",
      "stages",
      "while",
      "specific",
      "stage.",
      "consists",
      "planning",
      "consumable",
      "retired",
      "end",
      "figure",
      "shows",
      "workflow.",
      "table",
      "metaphase.",
      "metaphase",
      "description",
      "allowed",
      "transitions",
      "transition",
      "type",
      "draft",
      "after",
      "creating",
      "default.",
      "keep",
      "until",
      "ready",
      "use.",
      "active",
      "inactive",
      "manual",
      "move",
      "customers.",
      "longer",
      "support",
      "ended.",
      "click",
      "plan",
      "company.",
      "companies",
      "dropdown",
      "models.",
      "tab.",
      "displays",
      "highlighted",
      "blue.",
      "directs",
      "general",
      "tab",
      "button",
      "upper-right",
      "side",
      "next",
      "select",
      "appropriate",
      "dropdown.",
      "save."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage product modules",
    "content": "This page explains product modules and lists the topics related to managing product modules. Product module A product module is a subset of a product model. It can represent a specific set of features or a specific subcomponent of the product. Product modules help break a product into manageable components, making it easier to classify and route requests effectively during request handling. Topics related to managing product modules Product module workflow Add product module View or edit product module Delete product module",
    "url": "mngprdctmodule",
    "filename": "mngprdctmodule",
    "headings": [
      "Product module",
      "Topics related to managing product modules"
    ],
    "keywords": [
      "manage",
      "product",
      "modules",
      "module",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "lists",
      "modules.",
      "subset",
      "model.",
      "represent",
      "specific",
      "set",
      "features",
      "subcomponent",
      "product.",
      "help",
      "break",
      "manageable",
      "components",
      "making",
      "easier",
      "classify",
      "route",
      "requests",
      "effectively",
      "during",
      "request",
      "handling.",
      "workflow",
      "add",
      "view",
      "edit",
      "delete"
    ],
    "language": "en",
    "word_count": 61,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage product modules",
    "contentLower": "this page explains product modules and lists the topics related to managing product modules. product module a product module is a subset of a product model. it can represent a specific set of features or a specific subcomponent of the product. product modules help break a product into manageable components, making it easier to classify and route requests effectively during request handling. topics related to managing product modules product module workflow add product module view or edit product module delete product module",
    "keywordsLower": [
      "manage",
      "product",
      "modules",
      "module",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "lists",
      "modules.",
      "subset",
      "model.",
      "represent",
      "specific",
      "set",
      "features",
      "subcomponent",
      "product.",
      "help",
      "break",
      "manageable",
      "components",
      "making",
      "easier",
      "classify",
      "route",
      "requests",
      "effectively",
      "during",
      "request",
      "handling.",
      "workflow",
      "add",
      "view",
      "edit",
      "delete"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Product module workflow",
    "content": "This page explains the product module workflow and the steps to perform the following tasks: View workflow and current phase of product module Change phase of product module Product module workflow The product module workflow describes the different metaphases and phases in the lifecycle of a product module. Metaphases represent broad stages in the lifecycle, while phases are the specific steps within each stage. The product module lifecycle consists of the following metaphases: Planning Consumable Retired (End) The following figure shows the product module workflow. The following table describes phases within each metaphase. Metaphase Phase Description Allowed transitions Transition type Planning Draft After creating the product module, it is in the Draft phase by default. Keep the product module in the Draft phase until it is ready to use. Draft to Active, Draft to Inactive Manual Consumable Active Move the product module to the Active phase when it is ready for use or has active cus",
    "url": "manageproductmoduleworkflow",
    "filename": "manageproductmoduleworkflow",
    "headings": [
      "Product module workflow",
      "View workflow and current phase of product module",
      "Change phase of product module"
    ],
    "keywords": [
      "product",
      "module",
      "workflow",
      "view",
      "current",
      "phase",
      "change",
      "page",
      "explains",
      "steps",
      "perform",
      "following",
      "tasks",
      "describes",
      "different",
      "metaphases",
      "phases",
      "lifecycle",
      "module.",
      "represent",
      "broad",
      "stages",
      "while",
      "specific",
      "stage.",
      "consists",
      "planning",
      "consumable",
      "retired",
      "end",
      "figure",
      "shows",
      "workflow.",
      "table",
      "metaphase.",
      "metaphase",
      "description",
      "allowed",
      "transitions",
      "transition",
      "type",
      "draft",
      "after",
      "creating",
      "default.",
      "keep",
      "until",
      "ready",
      "use.",
      "active",
      "inactive",
      "manual",
      "move",
      "customers.",
      "longer",
      "support",
      "ended.",
      "click",
      "plan",
      "company.",
      "companies",
      "dropdown",
      "modules.",
      "id",
      "tab.",
      "displays",
      "highlighted",
      "blue.",
      "button",
      "upper-right",
      "side",
      "next",
      "select",
      "appropriate",
      "dropdown.",
      "save."
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "product module workflow",
    "contentLower": "this page explains the product module workflow and the steps to perform the following tasks: view workflow and current phase of product module change phase of product module product module workflow the product module workflow describes the different metaphases and phases in the lifecycle of a product module. metaphases represent broad stages in the lifecycle, while phases are the specific steps within each stage. the product module lifecycle consists of the following metaphases: planning consumable retired (end) the following figure shows the product module workflow. the following table describes phases within each metaphase. metaphase phase description allowed transitions transition type planning draft after creating the product module, it is in the draft phase by default. keep the product module in the draft phase until it is ready to use. draft to active, draft to inactive manual consumable active move the product module to the active phase when it is ready for use or has active cus",
    "keywordsLower": [
      "product",
      "module",
      "workflow",
      "view",
      "current",
      "phase",
      "change",
      "page",
      "explains",
      "steps",
      "perform",
      "following",
      "tasks",
      "describes",
      "different",
      "metaphases",
      "phases",
      "lifecycle",
      "module.",
      "represent",
      "broad",
      "stages",
      "while",
      "specific",
      "stage.",
      "consists",
      "planning",
      "consumable",
      "retired",
      "end",
      "figure",
      "shows",
      "workflow.",
      "table",
      "metaphase.",
      "metaphase",
      "description",
      "allowed",
      "transitions",
      "transition",
      "type",
      "draft",
      "after",
      "creating",
      "default.",
      "keep",
      "until",
      "ready",
      "use.",
      "active",
      "inactive",
      "manual",
      "move",
      "customers.",
      "longer",
      "support",
      "ended.",
      "click",
      "plan",
      "company.",
      "companies",
      "dropdown",
      "modules.",
      "id",
      "tab.",
      "displays",
      "highlighted",
      "blue.",
      "button",
      "upper-right",
      "side",
      "next",
      "select",
      "appropriate",
      "dropdown.",
      "save."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage product versions",
    "content": "This page lists the topics related to managing versions: Version workflow Add version View or edit version Delete version",
    "url": "manageproductversion",
    "filename": "manageproductversion",
    "headings": [],
    "keywords": [
      "manage",
      "product",
      "versions",
      "page",
      "lists",
      "topics",
      "related",
      "managing",
      "version",
      "workflow",
      "add",
      "view",
      "edit",
      "delete"
    ],
    "language": "en",
    "word_count": 18,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage product versions",
    "contentLower": "this page lists the topics related to managing versions: version workflow add version view or edit version delete version",
    "keywordsLower": [
      "manage",
      "product",
      "versions",
      "page",
      "lists",
      "topics",
      "related",
      "managing",
      "version",
      "workflow",
      "add",
      "view",
      "edit",
      "delete"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage environments",
    "content": "This page explains environments and lists the topics related to managing environments. Environment An environment record captures details of the infrastructure where a customer's product instance is deployed. Environment information is helpful when handling cases related to the product instance. Topics related to managing environments Environment workflowAdd environmentView or edit environmentDelete environment",
    "url": "manageenvironment",
    "filename": "manageenvironment",
    "headings": [
      "Environment",
      "Topics related to managing environments"
    ],
    "keywords": [
      "manage",
      "environments",
      "environment",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "lists",
      "environments.",
      "record",
      "captures",
      "details",
      "infrastructure",
      "customer",
      "product",
      "instance",
      "deployed.",
      "information",
      "helpful",
      "handling",
      "cases",
      "instance.",
      "workflowadd",
      "environmentview",
      "edit",
      "environmentdelete"
    ],
    "language": "en",
    "word_count": 39,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage environments",
    "contentLower": "this page explains environments and lists the topics related to managing environments. environment an environment record captures details of the infrastructure where a customer's product instance is deployed. environment information is helpful when handling cases related to the product instance. topics related to managing environments environment workflowadd environmentview or edit environmentdelete environment",
    "keywordsLower": [
      "manage",
      "environments",
      "environment",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "lists",
      "environments.",
      "record",
      "captures",
      "details",
      "infrastructure",
      "customer",
      "product",
      "instance",
      "deployed.",
      "information",
      "helpful",
      "handling",
      "cases",
      "instance.",
      "workflowadd",
      "environmentview",
      "edit",
      "environmentdelete"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage product instances",
    "content": "This page explains product instances and lists the topics related to managing product instances. Product instance A product instance represents a specific product or bundle purchased by a customer. It captures key details such as the product model (product or bundle) that was purchased, the company that purchased it, the version of the product model, the environment where the product is deployed, and the associated contract. Topics related to managing product instances Product instance workflow Add product instance View or edit product instance Delete product instance",
    "url": "manageproductinstance",
    "filename": "manageproductinstance",
    "headings": [
      "Product instance",
      "Topics related to managing product instances"
    ],
    "keywords": [
      "manage",
      "product",
      "instances",
      "instance",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "lists",
      "instances.",
      "represents",
      "specific",
      "bundle",
      "purchased",
      "customer.",
      "captures",
      "key",
      "details",
      "such",
      "model",
      "company",
      "version",
      "environment",
      "deployed",
      "associated",
      "contract.",
      "workflow",
      "add",
      "view",
      "edit",
      "delete"
    ],
    "language": "en",
    "word_count": 60,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage product instances",
    "contentLower": "this page explains product instances and lists the topics related to managing product instances. product instance a product instance represents a specific product or bundle purchased by a customer. it captures key details such as the product model (product or bundle) that was purchased, the company that purchased it, the version of the product model, the environment where the product is deployed, and the associated contract. topics related to managing product instances product instance workflow add product instance view or edit product instance delete product instance",
    "keywordsLower": [
      "manage",
      "product",
      "instances",
      "instance",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "lists",
      "instances.",
      "represents",
      "specific",
      "bundle",
      "purchased",
      "customer.",
      "captures",
      "key",
      "details",
      "such",
      "model",
      "company",
      "version",
      "environment",
      "deployed",
      "associated",
      "contract.",
      "workflow",
      "add",
      "view",
      "edit",
      "delete"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Product instance workflow",
    "content": "This page explains the product instance workflow and the steps to perform the following tasks: View workflow and current phase of product instance Change phase of product instance Product instance workflow The product instance workflow describes the different metaphases and phases in the lifecycle of a product instance. Metaphases represent broad stages in the lifecycle, while phases are the specific steps within each stage. The product instance lifecycle consists of the following metaphases: Available Usage Ended (End) The following figure shows the product instance workflow. The following sections describe the phases within each metaphase. Available Phase Description Allowed transitions Transition type Ready After creating the product instance, it is in the Ready phase by default. Keep the product instance in the Ready phase if it has been purchased by a customer but is not yet in use. Ready to In use, Ready to Inactive Manual Usage Phase Description Allowed transitions Transition ty",
    "url": "manageproductinstanceworkflow",
    "filename": "manageproductinstanceworkflow",
    "headings": [
      "Product instance workflow",
      "Available",
      "Usage",
      "Ended (End)",
      "View workflow and current phase of product instance",
      "Change phase of product instance"
    ],
    "keywords": [
      "product",
      "instance",
      "workflow",
      "available",
      "usage",
      "ended",
      "end",
      "view",
      "current",
      "phase",
      "change",
      "page",
      "explains",
      "steps",
      "perform",
      "following",
      "tasks",
      "describes",
      "different",
      "metaphases",
      "phases",
      "lifecycle",
      "instance.",
      "represent",
      "broad",
      "stages",
      "while",
      "specific",
      "stage.",
      "consists",
      "figure",
      "shows",
      "workflow.",
      "sections",
      "describe",
      "metaphase.",
      "description",
      "allowed",
      "transitions",
      "transition",
      "type",
      "ready",
      "after",
      "creating",
      "default.",
      "keep",
      "purchased",
      "customer",
      "yet",
      "use.",
      "inactive",
      "manual",
      "move",
      "it.",
      "expiring",
      "nearing",
      "period.",
      "longer",
      "click",
      "plan",
      "company.",
      "companies",
      "dropdown",
      "instances.",
      "id",
      "tab.",
      "displays",
      "highlighted",
      "blue.",
      "directs",
      "general",
      "tab",
      "button",
      "upper-right",
      "side",
      "next",
      "select",
      "appropriate",
      "dropdown.",
      "save."
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "product instance workflow",
    "contentLower": "this page explains the product instance workflow and the steps to perform the following tasks: view workflow and current phase of product instance change phase of product instance product instance workflow the product instance workflow describes the different metaphases and phases in the lifecycle of a product instance. metaphases represent broad stages in the lifecycle, while phases are the specific steps within each stage. the product instance lifecycle consists of the following metaphases: available usage ended (end) the following figure shows the product instance workflow. the following sections describe the phases within each metaphase. available phase description allowed transitions transition type ready after creating the product instance, it is in the ready phase by default. keep the product instance in the ready phase if it has been purchased by a customer but is not yet in use. ready to in use, ready to inactive manual usage phase description allowed transitions transition ty",
    "keywordsLower": [
      "product",
      "instance",
      "workflow",
      "available",
      "usage",
      "ended",
      "end",
      "view",
      "current",
      "phase",
      "change",
      "page",
      "explains",
      "steps",
      "perform",
      "following",
      "tasks",
      "describes",
      "different",
      "metaphases",
      "phases",
      "lifecycle",
      "instance.",
      "represent",
      "broad",
      "stages",
      "while",
      "specific",
      "stage.",
      "consists",
      "figure",
      "shows",
      "workflow.",
      "sections",
      "describe",
      "metaphase.",
      "description",
      "allowed",
      "transitions",
      "transition",
      "type",
      "ready",
      "after",
      "creating",
      "default.",
      "keep",
      "purchased",
      "customer",
      "yet",
      "use.",
      "inactive",
      "manual",
      "move",
      "it.",
      "expiring",
      "nearing",
      "period.",
      "longer",
      "click",
      "plan",
      "company.",
      "companies",
      "dropdown",
      "instances.",
      "id",
      "tab.",
      "displays",
      "highlighted",
      "blue.",
      "directs",
      "general",
      "tab",
      "button",
      "upper-right",
      "side",
      "next",
      "select",
      "appropriate",
      "dropdown.",
      "save."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage entitlements",
    "content": "This page explains entitlements and delegated companies and lists the topics related to managing entitlements. Entitlement An entitlement defines which users from a customer company or its delegated companies can view or create requests for specific product instances in the service portal. In an entitlement, you specify the following: Product instances: The product instances that the customer company has purchased. User permissions: The users who can view requests, create or update requests, and access knowledge articles for the covered product instances. Delegated company A delegated company is a company that the customer company allows to view or create requests for the product instances covered by the entitlement. Topics related to managing entitlements Entitlement workflow Add entitlement View or edit entitlement Delete entitlement",
    "url": "manageentitlement",
    "filename": "manageentitlement",
    "headings": [
      "Entitlement",
      "Delegated company",
      "Topics related to managing entitlements"
    ],
    "keywords": [
      "manage",
      "entitlements",
      "entitlement",
      "delegated",
      "company",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "companies",
      "lists",
      "entitlements.",
      "defines",
      "users",
      "customer",
      "view",
      "create",
      "requests",
      "specific",
      "product",
      "instances",
      "service",
      "portal.",
      "specify",
      "following",
      "purchased.",
      "user",
      "permissions",
      "update",
      "access",
      "knowledge",
      "articles",
      "covered",
      "instances.",
      "allows",
      "entitlement.",
      "workflow",
      "add",
      "edit",
      "delete"
    ],
    "language": "en",
    "word_count": 80,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage entitlements",
    "contentLower": "this page explains entitlements and delegated companies and lists the topics related to managing entitlements. entitlement an entitlement defines which users from a customer company or its delegated companies can view or create requests for specific product instances in the service portal. in an entitlement, you specify the following: product instances: the product instances that the customer company has purchased. user permissions: the users who can view requests, create or update requests, and access knowledge articles for the covered product instances. delegated company a delegated company is a company that the customer company allows to view or create requests for the product instances covered by the entitlement. topics related to managing entitlements entitlement workflow add entitlement view or edit entitlement delete entitlement",
    "keywordsLower": [
      "manage",
      "entitlements",
      "entitlement",
      "delegated",
      "company",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "companies",
      "lists",
      "entitlements.",
      "defines",
      "users",
      "customer",
      "view",
      "create",
      "requests",
      "specific",
      "product",
      "instances",
      "service",
      "portal.",
      "specify",
      "following",
      "purchased.",
      "user",
      "permissions",
      "update",
      "access",
      "knowledge",
      "articles",
      "covered",
      "instances.",
      "allows",
      "entitlement.",
      "workflow",
      "add",
      "edit",
      "delete"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage sold licenses",
    "content": "This page explains the license classifications: Sold license and Purchased license, and lists the topics involved in managing sold licenses. License classifications Purchased Licenses: In the Software Asset Management (SAM) module, you can create license records to track the licenses your organization has purchased. These licenses are referred to as purchased licenses. Sold Licenses: When CSM is enabled, you can create license records in the Company Management module to track the licenses your organization has sold to its customers. These licenses are referred to as sold licenses. Topics related to managing sold licenses Sold license workflow Add sold license View or edit sold license Delete sold license",
    "url": "managelicense",
    "filename": "managelicense",
    "headings": [
      "License classifications",
      "Topics related to managing sold licenses"
    ],
    "keywords": [
      "manage",
      "sold",
      "licenses",
      "license",
      "classifications",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "purchased",
      "lists",
      "involved",
      "licenses.",
      "software",
      "asset",
      "management",
      "sam",
      "module",
      "create",
      "records",
      "track",
      "organization",
      "purchased.",
      "referred",
      "csm",
      "enabled",
      "company",
      "customers.",
      "workflow",
      "add",
      "view",
      "edit",
      "delete"
    ],
    "language": "en",
    "word_count": 74,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage sold licenses",
    "contentLower": "this page explains the license classifications: sold license and purchased license, and lists the topics involved in managing sold licenses. license classifications purchased licenses: in the software asset management (sam) module, you can create license records to track the licenses your organization has purchased. these licenses are referred to as purchased licenses. sold licenses: when csm is enabled, you can create license records in the company management module to track the licenses your organization has sold to its customers. these licenses are referred to as sold licenses. topics related to managing sold licenses sold license workflow add sold license view or edit sold license delete sold license",
    "keywordsLower": [
      "manage",
      "sold",
      "licenses",
      "license",
      "classifications",
      "topics",
      "related",
      "managing",
      "page",
      "explains",
      "purchased",
      "lists",
      "involved",
      "licenses.",
      "software",
      "asset",
      "management",
      "sam",
      "module",
      "create",
      "records",
      "track",
      "organization",
      "purchased.",
      "referred",
      "csm",
      "enabled",
      "company",
      "customers.",
      "workflow",
      "add",
      "view",
      "edit",
      "delete"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Proposals",
    "content": "The Proposals feature enables you to manage the proposal life cycle through refinement, review, and approval. You can create a proposal from a Service Management idea or directly through the Proposals feature. Other project management applications can later implement the approved proposals. Related topics Proposal roles and permissions Proposal workflow How to create a proposal How to edit a proposal",
    "url": "proposalmgmt",
    "filename": "proposalmgmt",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "proposals",
      "related",
      "topics",
      "feature",
      "enables",
      "manage",
      "proposal",
      "life",
      "cycle",
      "through",
      "refinement",
      "review",
      "approval.",
      "create",
      "service",
      "management",
      "idea",
      "directly",
      "feature.",
      "project",
      "applications",
      "later",
      "implement",
      "approved",
      "proposals.",
      "roles",
      "permissions",
      "workflow",
      "edit"
    ],
    "language": "en",
    "word_count": 39,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "proposals",
    "contentLower": "the proposals feature enables you to manage the proposal life cycle through refinement, review, and approval. you can create a proposal from a service management idea or directly through the proposals feature. other project management applications can later implement the approved proposals. related topics proposal roles and permissions proposal workflow how to create a proposal how to edit a proposal",
    "keywordsLower": [
      "proposals",
      "related",
      "topics",
      "feature",
      "enables",
      "manage",
      "proposal",
      "life",
      "cycle",
      "through",
      "refinement",
      "review",
      "approval.",
      "create",
      "service",
      "management",
      "idea",
      "directly",
      "feature.",
      "project",
      "applications",
      "later",
      "implement",
      "approved",
      "proposals.",
      "roles",
      "permissions",
      "workflow",
      "edit"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Proposal roles and permissions",
    "content": "There are specific roles associated with proposals. Service Management uses role based permissions to enable you to complete a task that's appropriate to your role. The tenant administrator manages and assigns these permissions. By default, the following roles are related to a proposal lifecycle. These roles support proposal creation and process ownership. Role Responsibilities Proposal Creator Creates proposals Publishes proposals for review Abandons proposals before publication Proposal Reviewer Reviews existing proposals and provides review feedback Categorizes and prioritizes proposals Approves or rejects proposals after review Portfolio Manager Assesses proposals from multiple dimensions to align with the company strategies and business objectives Defines and configures the workflow process for proposals Manages business objectives and impacts investment decisions Proposal Administrator Creates, updates, and deletes proposals Creates approval definitions for proposals Review the i",
    "url": "proposalroles",
    "filename": "proposalroles",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "proposal",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "proposals.",
      "service",
      "management",
      "uses",
      "role",
      "based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "tenant",
      "administrator",
      "manages",
      "assigns",
      "permissions.",
      "default",
      "following",
      "lifecycle.",
      "support",
      "creation",
      "process",
      "ownership.",
      "responsibilities",
      "creator",
      "creates",
      "proposals",
      "publishes",
      "review",
      "abandons",
      "before",
      "publication",
      "reviewer",
      "reviews",
      "existing",
      "provides",
      "feedback",
      "categorizes",
      "prioritizes",
      "approves",
      "rejects",
      "after",
      "portfolio",
      "manager",
      "assesses",
      "multiple",
      "dimensions",
      "align",
      "company",
      "strategies",
      "business",
      "objectives",
      "defines",
      "configures",
      "workflow",
      "impacts",
      "investment",
      "decisions",
      "updates",
      "deletes",
      "approval",
      "definitions",
      "individual",
      "permission",
      "assignments",
      "administration",
      "master",
      "data",
      "people",
      "roles."
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "proposal roles and permissions",
    "contentLower": "there are specific roles associated with proposals. service management uses role based permissions to enable you to complete a task that's appropriate to your role. the tenant administrator manages and assigns these permissions. by default, the following roles are related to a proposal lifecycle. these roles support proposal creation and process ownership. role responsibilities proposal creator creates proposals publishes proposals for review abandons proposals before publication proposal reviewer reviews existing proposals and provides review feedback categorizes and prioritizes proposals approves or rejects proposals after review portfolio manager assesses proposals from multiple dimensions to align with the company strategies and business objectives defines and configures the workflow process for proposals manages business objectives and impacts investment decisions proposal administrator creates, updates, and deletes proposals creates approval definitions for proposals review the i",
    "keywordsLower": [
      "proposal",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "proposals.",
      "service",
      "management",
      "uses",
      "role",
      "based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "tenant",
      "administrator",
      "manages",
      "assigns",
      "permissions.",
      "default",
      "following",
      "lifecycle.",
      "support",
      "creation",
      "process",
      "ownership.",
      "responsibilities",
      "creator",
      "creates",
      "proposals",
      "publishes",
      "review",
      "abandons",
      "before",
      "publication",
      "reviewer",
      "reviews",
      "existing",
      "provides",
      "feedback",
      "categorizes",
      "prioritizes",
      "approves",
      "rejects",
      "after",
      "portfolio",
      "manager",
      "assesses",
      "multiple",
      "dimensions",
      "align",
      "company",
      "strategies",
      "business",
      "objectives",
      "defines",
      "configures",
      "workflow",
      "impacts",
      "investment",
      "decisions",
      "updates",
      "deletes",
      "approval",
      "definitions",
      "individual",
      "permission",
      "assignments",
      "administration",
      "master",
      "data",
      "people",
      "roles."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Proposal workflow",
    "content": "This section describes the metaphases and subordinate phases in the life cycle of a proposal. The proposal workflow relies on business rules. Rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. For more information about the out-of-the-box business rules defined for the proposal workflow, see Proposal process - Business rules. Metaphase: Refinement The first metaphase in the proposal's lifecycle, where the proposal enters into the Compose phase. Phase Transition Description Compose Manual Compose is the starting point for proposals. The result of the phase is the successful creation of a proposal. When the proposal is ready for review, the proposal creator can manually transition the proposal to the Review phase. If the proposal isn't valid, the proposal creator can also manually transition the proposal to the Abandon phase. Next phase: Review or Abandon Metaphase: Assessment Proposal reviewer reviews and ",
    "url": "proposalwflw",
    "filename": "proposalwflw",
    "headings": [
      "Metaphase: Refinement",
      "Metaphase: Assessment",
      "Metaphase: Done (End)",
      "Related topics"
    ],
    "keywords": [
      "proposal",
      "workflow",
      "metaphase",
      "refinement",
      "assessment",
      "done",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "proposal.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "out-of-the-box",
      "defined",
      "see",
      "process",
      "first",
      "lifecycle",
      "enters",
      "compose",
      "transition",
      "description",
      "manual",
      "starting",
      "point",
      "proposals.",
      "result",
      "successful",
      "creation",
      "ready",
      "review",
      "creator",
      "manually",
      "isn",
      "valid",
      "abandon",
      "next",
      "reviewer",
      "reviews",
      "prioritizes",
      "proposals",
      "needs",
      "decide",
      "proceed",
      "following",
      "actions",
      "approve",
      "reject",
      "back",
      "provide",
      "automatic",
      "automatically",
      "transitioned",
      "close",
      "decision",
      "approve.",
      "need",
      "information.",
      "return",
      "final",
      "stage",
      "workflow.",
      "completed",
      "returned",
      "none",
      "completed.",
      "abandoned",
      "inactive.",
      "wants",
      "reopen",
      "idea"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "proposal workflow",
    "contentLower": "this section describes the metaphases and subordinate phases in the life cycle of a proposal. the proposal workflow relies on business rules. rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. for more information about the out-of-the-box business rules defined for the proposal workflow, see proposal process - business rules. metaphase: refinement the first metaphase in the proposal's lifecycle, where the proposal enters into the compose phase. phase transition description compose manual compose is the starting point for proposals. the result of the phase is the successful creation of a proposal. when the proposal is ready for review, the proposal creator can manually transition the proposal to the review phase. if the proposal isn't valid, the proposal creator can also manually transition the proposal to the abandon phase. next phase: review or abandon metaphase: assessment proposal reviewer reviews and ",
    "keywordsLower": [
      "proposal",
      "workflow",
      "metaphase",
      "refinement",
      "assessment",
      "done",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "proposal.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "out-of-the-box",
      "defined",
      "see",
      "process",
      "first",
      "lifecycle",
      "enters",
      "compose",
      "transition",
      "description",
      "manual",
      "starting",
      "point",
      "proposals.",
      "result",
      "successful",
      "creation",
      "ready",
      "review",
      "creator",
      "manually",
      "isn",
      "valid",
      "abandon",
      "next",
      "reviewer",
      "reviews",
      "prioritizes",
      "proposals",
      "needs",
      "decide",
      "proceed",
      "following",
      "actions",
      "approve",
      "reject",
      "back",
      "provide",
      "automatic",
      "automatically",
      "transitioned",
      "close",
      "decision",
      "approve.",
      "need",
      "information.",
      "return",
      "final",
      "stage",
      "workflow.",
      "completed",
      "returned",
      "none",
      "completed.",
      "abandoned",
      "inactive.",
      "wants",
      "reopen",
      "idea"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Proposal Analytics",
    "content": "The Proposal Analytics feature enables you, the portfolio managers, to analyze proposals and evaluate their business values when you select the candidate projects. To access Proposal Analytics From the main menu, select Plan > Idea & Proposal > Proposal Analytics. User interface elements The Proposal Analytics user interface (UI) includes the following: Filter panel You can find the Filter panel on the upper-left side of the UI. By default, the following filter options are selected: Filter option Value Phase Id Review Business value >0 Cost >0 Risk rating >0 You can click the Clear all button to clear the default filter options and customize the filter options by clicking the Filter button. A list of fields in the proposal record is displayed. For detailed explanation of these fields, see How to edit a proposal. Proposal list panel The Proposal list panel is on the left-hand side. The following elements are included: UI element Description Sort by To change the sorting option, select a",
    "url": "proposalanalytics",
    "filename": "proposalanalytics",
    "headings": [
      "Filter panel",
      "Proposal list panel",
      "Bubble chart",
      "Summary panel",
      "Related topics"
    ],
    "keywords": [
      "proposal",
      "analytics",
      "filter",
      "panel",
      "list",
      "bubble",
      "chart",
      "summary",
      "related",
      "topics",
      "feature",
      "enables",
      "portfolio",
      "managers",
      "analyze",
      "proposals",
      "evaluate",
      "business",
      "values",
      "select",
      "candidate",
      "projects.",
      "access",
      "main",
      "menu",
      "plan",
      "idea",
      "analytics.",
      "user",
      "interface",
      "elements",
      "ui",
      "includes",
      "following",
      "find",
      "upper-left",
      "side",
      "ui.",
      "default",
      "options",
      "selected",
      "option",
      "value",
      "phase",
      "id",
      "review",
      "cost",
      "risk",
      "rating",
      "click",
      "clear",
      "all",
      "button",
      "customize",
      "clicking",
      "button.",
      "fields",
      "record",
      "displayed.",
      "detailed",
      "explanation",
      "see",
      "edit",
      "proposal.",
      "left-hand",
      "side.",
      "included",
      "element",
      "description",
      "sort",
      "change",
      "sorting",
      "field",
      "drop-down",
      "list.",
      "displayed",
      "ascending",
      "order.",
      "reverse",
      "order",
      "descending",
      "title",
      "check",
      "box",
      "before",
      "title.",
      "meet",
      "criteria",
      "box.",
      "view",
      "details",
      "id.",
      "display",
      "100",
      "maximum.",
      "exceeded",
      "aren",
      "chart.",
      "highlighted",
      "bubbles"
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "proposal analytics",
    "contentLower": "the proposal analytics feature enables you, the portfolio managers, to analyze proposals and evaluate their business values when you select the candidate projects. to access proposal analytics from the main menu, select plan > idea & proposal > proposal analytics. user interface elements the proposal analytics user interface (ui) includes the following: filter panel you can find the filter panel on the upper-left side of the ui. by default, the following filter options are selected: filter option value phase id review business value >0 cost >0 risk rating >0 you can click the clear all button to clear the default filter options and customize the filter options by clicking the filter button. a list of fields in the proposal record is displayed. for detailed explanation of these fields, see how to edit a proposal. proposal list panel the proposal list panel is on the left-hand side. the following elements are included: ui element description sort by to change the sorting option, select a",
    "keywordsLower": [
      "proposal",
      "analytics",
      "filter",
      "panel",
      "list",
      "bubble",
      "chart",
      "summary",
      "related",
      "topics",
      "feature",
      "enables",
      "portfolio",
      "managers",
      "analyze",
      "proposals",
      "evaluate",
      "business",
      "values",
      "select",
      "candidate",
      "projects.",
      "access",
      "main",
      "menu",
      "plan",
      "idea",
      "analytics.",
      "user",
      "interface",
      "elements",
      "ui",
      "includes",
      "following",
      "find",
      "upper-left",
      "side",
      "ui.",
      "default",
      "options",
      "selected",
      "option",
      "value",
      "phase",
      "id",
      "review",
      "cost",
      "risk",
      "rating",
      "click",
      "clear",
      "all",
      "button",
      "customize",
      "clicking",
      "button.",
      "fields",
      "record",
      "displayed.",
      "detailed",
      "explanation",
      "see",
      "edit",
      "proposal.",
      "left-hand",
      "side.",
      "included",
      "element",
      "description",
      "sort",
      "change",
      "sorting",
      "field",
      "drop-down",
      "list.",
      "displayed",
      "ascending",
      "order.",
      "reverse",
      "order",
      "descending",
      "title",
      "check",
      "box",
      "before",
      "title.",
      "meet",
      "criteria",
      "box.",
      "view",
      "details",
      "id.",
      "display",
      "100",
      "maximum.",
      "exceeded",
      "aren",
      "chart.",
      "highlighted",
      "bubbles"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Portfolios",
    "content": "A portfolio is a collection of applications that have related initiatives, services, resources, projects, and programs. A portfolio can attain wide reaching benefits and impact. The collection of applications in an IT portfolio is more business oriented than technical. The Portfolios feature enables you to manage the portfolios of your company. You can view the content of the applications under each portfolio, manage their roadmaps, and analyze and optimize the portfolio infrastructure. Related topics Application Portfolio Management How to create a portfolio record How to edit a portfolio record",
    "url": "portfoliooverview",
    "filename": "portfoliooverview",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "portfolios",
      "related",
      "topics",
      "portfolio",
      "collection",
      "applications",
      "initiatives",
      "services",
      "resources",
      "projects",
      "programs.",
      "attain",
      "wide",
      "reaching",
      "benefits",
      "impact.",
      "business",
      "oriented",
      "technical.",
      "feature",
      "enables",
      "manage",
      "company.",
      "view",
      "content",
      "under",
      "roadmaps",
      "analyze",
      "optimize",
      "infrastructure.",
      "application",
      "management",
      "create",
      "record",
      "edit"
    ],
    "language": "en",
    "word_count": 50,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "portfolios",
    "contentLower": "a portfolio is a collection of applications that have related initiatives, services, resources, projects, and programs. a portfolio can attain wide reaching benefits and impact. the collection of applications in an it portfolio is more business oriented than technical. the portfolios feature enables you to manage the portfolios of your company. you can view the content of the applications under each portfolio, manage their roadmaps, and analyze and optimize the portfolio infrastructure. related topics application portfolio management how to create a portfolio record how to edit a portfolio record",
    "keywordsLower": [
      "portfolios",
      "related",
      "topics",
      "portfolio",
      "collection",
      "applications",
      "initiatives",
      "services",
      "resources",
      "projects",
      "programs.",
      "attain",
      "wide",
      "reaching",
      "benefits",
      "impact.",
      "business",
      "oriented",
      "technical.",
      "feature",
      "enables",
      "manage",
      "company.",
      "view",
      "content",
      "under",
      "roadmaps",
      "analyze",
      "optimize",
      "infrastructure.",
      "application",
      "management",
      "create",
      "record",
      "edit"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Optimizations",
    "content": "The Optimizations feature enables you to optimize portfolios and select the suitable cloud delivery model that suits your company's needs. This feature allows you to optimize your application portfolios by providing preliminary guidance for each application. Based on this, you can create proposals to migrate and modernize selected applications to the cloud. Application cloud optimization survey are provided to collect information on each criteria. A pre-defined survey (the Application Cloudification Survey) is provided for you to collect basic information of each application from the application owners. This feature assesses and analyzes the survey feedback and displays the findings as a graphical report. The assessment is intended to provide preliminary guidance as to fitness both from a business and a technical point of view and to provide recommendations for transforming the application to the cloud platform. The application portfolio owner can then create a proposal based on the re",
    "url": "optimoverview",
    "filename": "optimoverview",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "optimizations",
      "related",
      "topics",
      "feature",
      "enables",
      "optimize",
      "portfolios",
      "select",
      "suitable",
      "cloud",
      "delivery",
      "model",
      "suits",
      "company",
      "needs.",
      "allows",
      "application",
      "providing",
      "preliminary",
      "guidance",
      "application.",
      "based",
      "create",
      "proposals",
      "migrate",
      "modernize",
      "selected",
      "applications",
      "cloud.",
      "optimization",
      "survey",
      "provided",
      "collect",
      "information",
      "criteria.",
      "pre-defined",
      "cloudification",
      "basic",
      "owners.",
      "assesses",
      "analyzes",
      "feedback",
      "displays",
      "findings",
      "graphical",
      "report.",
      "assessment",
      "intended",
      "provide",
      "fitness",
      "both",
      "business",
      "technical",
      "point",
      "view",
      "recommendations",
      "transforming",
      "platform.",
      "portfolio",
      "owner",
      "proposal",
      "report",
      "implement",
      "transformation.",
      "important",
      "understand",
      "assessments",
      "made",
      "current",
      "broad",
      "risk",
      "appetite",
      "parameters",
      "aligned",
      "keeping",
      "regulated",
      "sensitive",
      "data",
      "data-center",
      "environment.",
      "isn",
      "definitive",
      "answer",
      "whether",
      "move",
      "potential",
      "suitability",
      "view.",
      "management",
      "record",
      "edit",
      "workflow",
      "send",
      "out-of-the-box"
    ],
    "language": "en",
    "word_count": 85,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "optimizations",
    "contentLower": "the optimizations feature enables you to optimize portfolios and select the suitable cloud delivery model that suits your company's needs. this feature allows you to optimize your application portfolios by providing preliminary guidance for each application. based on this, you can create proposals to migrate and modernize selected applications to the cloud. application cloud optimization survey are provided to collect information on each criteria. a pre-defined survey (the application cloudification survey) is provided for you to collect basic information of each application from the application owners. this feature assesses and analyzes the survey feedback and displays the findings as a graphical report. the assessment is intended to provide preliminary guidance as to fitness both from a business and a technical point of view and to provide recommendations for transforming the application to the cloud platform. the application portfolio owner can then create a proposal based on the re",
    "keywordsLower": [
      "optimizations",
      "related",
      "topics",
      "feature",
      "enables",
      "optimize",
      "portfolios",
      "select",
      "suitable",
      "cloud",
      "delivery",
      "model",
      "suits",
      "company",
      "needs.",
      "allows",
      "application",
      "providing",
      "preliminary",
      "guidance",
      "application.",
      "based",
      "create",
      "proposals",
      "migrate",
      "modernize",
      "selected",
      "applications",
      "cloud.",
      "optimization",
      "survey",
      "provided",
      "collect",
      "information",
      "criteria.",
      "pre-defined",
      "cloudification",
      "basic",
      "owners.",
      "assesses",
      "analyzes",
      "feedback",
      "displays",
      "findings",
      "graphical",
      "report.",
      "assessment",
      "intended",
      "provide",
      "fitness",
      "both",
      "business",
      "technical",
      "point",
      "view",
      "recommendations",
      "transforming",
      "platform.",
      "portfolio",
      "owner",
      "proposal",
      "report",
      "implement",
      "transformation.",
      "important",
      "understand",
      "assessments",
      "made",
      "current",
      "broad",
      "risk",
      "appetite",
      "parameters",
      "aligned",
      "keeping",
      "regulated",
      "sensitive",
      "data",
      "data-center",
      "environment.",
      "isn",
      "definitive",
      "answer",
      "whether",
      "move",
      "potential",
      "suitability",
      "view.",
      "management",
      "record",
      "edit",
      "workflow",
      "send",
      "out-of-the-box"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Optimization workflow",
    "content": "This section describes the metaphases and subordinate phases in the life cycle of an optimization record. The optimization workflow relies on business rules. Rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. For more information about the default business rules defined for the optimization workflow, see Optimization process - Business rules. Metaphase: Creation The optimization record is created. Phase Transition Description Create Manual This is the starting point for the basic optimization workflow. The optimization record is in this phase after being created. Move the optimization record to the Send survey phase when the application analyst is ready to send a survey. Next phase: Send survey Metaphase: Preparation A survey is sent to the application owners and responses are collected. Phase Transition Description Send survey Automatic The survey is sent to the application owners. The record is automati",
    "url": "optimwflw",
    "filename": "optimwflw",
    "headings": [
      "Metaphase: Creation",
      "Metaphase: Preparation",
      "Metaphase: Assessment",
      "Metaphase: Done (End)",
      "Related topics"
    ],
    "keywords": [
      "optimization",
      "workflow",
      "metaphase",
      "creation",
      "preparation",
      "assessment",
      "done",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "record.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "default",
      "defined",
      "see",
      "process",
      "record",
      "created.",
      "transition",
      "description",
      "create",
      "manual",
      "starting",
      "point",
      "basic",
      "workflow.",
      "after",
      "move",
      "send",
      "survey",
      "application",
      "analyst",
      "ready",
      "survey.",
      "next",
      "sent",
      "owners",
      "responses",
      "collected.",
      "automatic",
      "owners.",
      "automatically",
      "transitioned",
      "wait",
      "response",
      "successfully.",
      "resend",
      "failed",
      "collected",
      "assess",
      "complete",
      "moved",
      "try",
      "sending",
      "again.",
      "data",
      "assessed",
      "through",
      "algorithm.",
      "begins",
      "moves",
      "wants",
      "report",
      "viewed",
      "proposal",
      "abandoned.",
      "none",
      "complete.",
      "created",
      "optimizations",
      "edit",
      "optimize",
      "applications"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "optimization workflow",
    "contentLower": "this section describes the metaphases and subordinate phases in the life cycle of an optimization record. the optimization workflow relies on business rules. rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. for more information about the default business rules defined for the optimization workflow, see optimization process - business rules. metaphase: creation the optimization record is created. phase transition description create manual this is the starting point for the basic optimization workflow. the optimization record is in this phase after being created. move the optimization record to the send survey phase when the application analyst is ready to send a survey. next phase: send survey metaphase: preparation a survey is sent to the application owners and responses are collected. phase transition description send survey automatic the survey is sent to the application owners. the record is automati",
    "keywordsLower": [
      "optimization",
      "workflow",
      "metaphase",
      "creation",
      "preparation",
      "assessment",
      "done",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "record.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "information",
      "about",
      "default",
      "defined",
      "see",
      "process",
      "record",
      "created.",
      "transition",
      "description",
      "create",
      "manual",
      "starting",
      "point",
      "basic",
      "workflow.",
      "after",
      "move",
      "send",
      "survey",
      "application",
      "analyst",
      "ready",
      "survey.",
      "next",
      "sent",
      "owners",
      "responses",
      "collected.",
      "automatic",
      "owners.",
      "automatically",
      "transitioned",
      "wait",
      "response",
      "successfully.",
      "resend",
      "failed",
      "collected",
      "assess",
      "complete",
      "moved",
      "try",
      "sending",
      "again.",
      "data",
      "assessed",
      "through",
      "algorithm.",
      "begins",
      "moves",
      "wants",
      "report",
      "viewed",
      "proposal",
      "abandoned.",
      "none",
      "complete.",
      "created",
      "optimizations",
      "edit",
      "optimize",
      "applications"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Optimize applications",
    "content": "To assess applications in a portfolio and determine the suitable cloud delivery model for the applications, follow these steps: From the main menu, select Plan > Application Portfolio > Optimizations. Service Management displays a list of optimization records. Click New to create an application record in Service Management. Complete the following fields: Field Description Title Enter the title of the portfolio. Description Enter a description that captures the details of the portfolio. Target portfolio Select the portfolio that you want to optimize. Optimization type Select an optimization type from the drop-down list. Currently Application cloudification is the only optimization type supported. It provides users with a quick way to identify the optimal cloud-based delivery model of an application within the target portfolio. Click Save when you are finished. The optimization record is created with the General tab displayed. For more information on creating an optimization record, see ",
    "url": "optimapps",
    "filename": "optimapps",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "optimize",
      "applications",
      "related",
      "topics",
      "assess",
      "portfolio",
      "determine",
      "suitable",
      "cloud",
      "delivery",
      "model",
      "follow",
      "steps",
      "main",
      "menu",
      "select",
      "plan",
      "application",
      "optimizations.",
      "service",
      "management",
      "displays",
      "list",
      "optimization",
      "records.",
      "click",
      "new",
      "create",
      "record",
      "management.",
      "complete",
      "following",
      "fields",
      "field",
      "description",
      "title",
      "enter",
      "portfolio.",
      "captures",
      "details",
      "target",
      "want",
      "optimize.",
      "type",
      "drop-down",
      "list.",
      "currently",
      "cloudification",
      "supported.",
      "provides",
      "users",
      "quick",
      "way",
      "identify",
      "optimal",
      "cloud-based",
      "save",
      "finished.",
      "created",
      "general",
      "tab",
      "displayed.",
      "information",
      "creating",
      "see",
      "record.",
      "send",
      "survey",
      "owners.",
      "options",
      "section",
      "email",
      "template",
      "due",
      "date",
      "specify",
      "survey.",
      "make",
      "sure",
      "all",
      "owners",
      "portal",
      "users.",
      "ensure",
      "able",
      "take",
      "portal.",
      "roles",
      "permissions",
      "people.",
      "optional",
      "preview",
      "sent",
      "link",
      "upper-right",
      "corner.",
      "through",
      "moved",
      "wait",
      "response"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "optimize applications",
    "contentLower": "to assess applications in a portfolio and determine the suitable cloud delivery model for the applications, follow these steps: from the main menu, select plan > application portfolio > optimizations. service management displays a list of optimization records. click new to create an application record in service management. complete the following fields: field description title enter the title of the portfolio. description enter a description that captures the details of the portfolio. target portfolio select the portfolio that you want to optimize. optimization type select an optimization type from the drop-down list. currently application cloudification is the only optimization type supported. it provides users with a quick way to identify the optimal cloud-based delivery model of an application within the target portfolio. click save when you are finished. the optimization record is created with the general tab displayed. for more information on creating an optimization record, see ",
    "keywordsLower": [
      "optimize",
      "applications",
      "related",
      "topics",
      "assess",
      "portfolio",
      "determine",
      "suitable",
      "cloud",
      "delivery",
      "model",
      "follow",
      "steps",
      "main",
      "menu",
      "select",
      "plan",
      "application",
      "optimizations.",
      "service",
      "management",
      "displays",
      "list",
      "optimization",
      "records.",
      "click",
      "new",
      "create",
      "record",
      "management.",
      "complete",
      "following",
      "fields",
      "field",
      "description",
      "title",
      "enter",
      "portfolio.",
      "captures",
      "details",
      "target",
      "want",
      "optimize.",
      "type",
      "drop-down",
      "list.",
      "currently",
      "cloudification",
      "supported.",
      "provides",
      "users",
      "quick",
      "way",
      "identify",
      "optimal",
      "cloud-based",
      "save",
      "finished.",
      "created",
      "general",
      "tab",
      "displayed.",
      "information",
      "creating",
      "see",
      "record.",
      "send",
      "survey",
      "owners.",
      "options",
      "section",
      "email",
      "template",
      "due",
      "date",
      "specify",
      "survey.",
      "make",
      "sure",
      "all",
      "owners",
      "portal",
      "users.",
      "ensure",
      "able",
      "take",
      "portal.",
      "roles",
      "permissions",
      "people.",
      "optional",
      "preview",
      "sent",
      "link",
      "upper-right",
      "corner.",
      "through",
      "moved",
      "wait",
      "response"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Out-of-the-box optimization survey",
    "content": "Service Management provides an out-of-the-box survey that you use to collect basic application information that's used to provide cloud solution recommendations. Note You can't change or delete the questions in this survey. Questions are included in the following areas: Regulatory Questions 1 and 2 are used to identify regulatory restrictions that would prevent this application from moving into a public cloud environment. Question Description 1 Is the application subject to regulatory requirements such as FISMA, HIPAA, or other such regulations or required to run on a government certified operating system image? Possible answers are: Yes No 2 Are there regulatory requirements requiring the data to be restricted to a specific geographic region? Possible answers are: Yes No Vendor Support Questions 3 and 4 are used to identify vendor support issues that would prevent this application from moving into a public cloud environment. Question Description 3 Is the application fully vendor suppo",
    "url": "ootbsurvey",
    "filename": "ootbsurvey",
    "headings": [
      "Regulatory",
      "Vendor Support",
      "Geography",
      "Service Availability",
      "Workload Variability",
      "Security",
      "Infrastructure Compatibility",
      "Cloud Operating Model",
      "Internet Suitability",
      "Performance",
      "Architecture",
      "Interfaces",
      "Related topics"
    ],
    "keywords": [
      "99.95",
      "5.26",
      "4.4",
      "99.999",
      "8.76",
      "52.6",
      "99.99",
      "99.9",
      "3.65",
      "out-of-the-box",
      "optimization",
      "survey",
      "regulatory",
      "vendor",
      "support",
      "geography",
      "service",
      "availability",
      "workload",
      "variability",
      "security",
      "infrastructure",
      "compatibility",
      "cloud",
      "operating",
      "model",
      "internet",
      "suitability",
      "performance",
      "architecture",
      "interfaces",
      "related",
      "topics",
      "management",
      "provides",
      "collect",
      "basic",
      "application",
      "information",
      "provide",
      "solution",
      "recommendations.",
      "note",
      "change",
      "delete",
      "questions",
      "survey.",
      "included",
      "following",
      "areas",
      "identify",
      "restrictions",
      "prevent",
      "moving",
      "public",
      "environment.",
      "question",
      "description",
      "subject",
      "requirements",
      "such",
      "fisma",
      "hipaa",
      "regulations",
      "required",
      "run",
      "government",
      "certified",
      "system",
      "image",
      "possible",
      "answers",
      "there",
      "requiring",
      "data",
      "restricted",
      "specific",
      "geographic",
      "region",
      "issues",
      "fully",
      "supported",
      "virtualized",
      "environment",
      "any",
      "impediments",
      "location",
      "user",
      "base",
      "factor",
      "hosting",
      "whether",
      "level",
      "alignment",
      "most",
      "major",
      "providers",
      "date.",
      "what",
      "99"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "out-of-the-box optimization survey",
    "contentLower": "service management provides an out-of-the-box survey that you use to collect basic application information that's used to provide cloud solution recommendations. note you can't change or delete the questions in this survey. questions are included in the following areas: regulatory questions 1 and 2 are used to identify regulatory restrictions that would prevent this application from moving into a public cloud environment. question description 1 is the application subject to regulatory requirements such as fisma, hipaa, or other such regulations or required to run on a government certified operating system image? possible answers are: yes no 2 are there regulatory requirements requiring the data to be restricted to a specific geographic region? possible answers are: yes no vendor support questions 3 and 4 are used to identify vendor support issues that would prevent this application from moving into a public cloud environment. question description 3 is the application fully vendor suppo",
    "keywordsLower": [
      "99.95",
      "5.26",
      "4.4",
      "99.999",
      "8.76",
      "52.6",
      "99.99",
      "99.9",
      "3.65",
      "out-of-the-box",
      "optimization",
      "survey",
      "regulatory",
      "vendor",
      "support",
      "geography",
      "service",
      "availability",
      "workload",
      "variability",
      "security",
      "infrastructure",
      "compatibility",
      "cloud",
      "operating",
      "model",
      "internet",
      "suitability",
      "performance",
      "architecture",
      "interfaces",
      "related",
      "topics",
      "management",
      "provides",
      "collect",
      "basic",
      "application",
      "information",
      "provide",
      "solution",
      "recommendations.",
      "note",
      "change",
      "delete",
      "questions",
      "survey.",
      "included",
      "following",
      "areas",
      "identify",
      "restrictions",
      "prevent",
      "moving",
      "public",
      "environment.",
      "question",
      "description",
      "subject",
      "requirements",
      "such",
      "fisma",
      "hipaa",
      "regulations",
      "required",
      "run",
      "government",
      "certified",
      "system",
      "image",
      "possible",
      "answers",
      "there",
      "requiring",
      "data",
      "restricted",
      "specific",
      "geographic",
      "region",
      "issues",
      "fully",
      "supported",
      "virtualized",
      "environment",
      "any",
      "impediments",
      "location",
      "user",
      "base",
      "factor",
      "hosting",
      "whether",
      "level",
      "alignment",
      "most",
      "major",
      "providers",
      "date.",
      "what",
      "99"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Optimization survey report",
    "content": "The answers to the Application Cloudification Survey are automatically compiled into a report after the survey is taken by the application owner. Prerequisite: The survey is taken by the application owner through the Service Portal. For more information about sending the Application Cloudification Survey, see How to send a survey to collect application information. From the main menu, select Plan > Application Portfolio > Optimizations. Service Management displays a list of optimization records. Select the optimization record that you want to assess. To filter the record list, click the Add filter button. For more information, see Filters. Click the optimization record identifier in the ID column to display the selected record, or click Edit in the optimization preview panel. By default, the optimization record is displayed with the General tab selected. Go to the Report tab and move the record to the Assess phase. The link to the survey is displayed above the optimization report table",
    "url": "ootbsurveyreport",
    "filename": "ootbsurveyreport",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "optimization",
      "survey",
      "report",
      "related",
      "topics",
      "answers",
      "application",
      "cloudification",
      "automatically",
      "compiled",
      "after",
      "taken",
      "owner.",
      "prerequisite",
      "owner",
      "through",
      "service",
      "portal.",
      "information",
      "about",
      "sending",
      "see",
      "send",
      "collect",
      "information.",
      "main",
      "menu",
      "select",
      "plan",
      "portfolio",
      "optimizations.",
      "management",
      "displays",
      "list",
      "records.",
      "record",
      "want",
      "assess.",
      "filter",
      "click",
      "add",
      "button.",
      "filters.",
      "identifier",
      "id",
      "column",
      "display",
      "selected",
      "edit",
      "preview",
      "panel.",
      "default",
      "displayed",
      "general",
      "tab",
      "selected.",
      "go",
      "move",
      "assess",
      "phase.",
      "link",
      "above",
      "table.",
      "survey.",
      "opened.",
      "reports",
      "view",
      "report.",
      "note",
      "takes",
      "approximately",
      "hour",
      "generate",
      "alternatively",
      "find",
      "selecting",
      "build",
      "menu.",
      "following",
      "name",
      "label",
      "question",
      "answer",
      "surveys",
      "management.",
      "optimizations",
      "out-of-the-box"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "optimization survey report",
    "contentLower": "the answers to the application cloudification survey are automatically compiled into a report after the survey is taken by the application owner. prerequisite: the survey is taken by the application owner through the service portal. for more information about sending the application cloudification survey, see how to send a survey to collect application information. from the main menu, select plan > application portfolio > optimizations. service management displays a list of optimization records. select the optimization record that you want to assess. to filter the record list, click the add filter button. for more information, see filters. click the optimization record identifier in the id column to display the selected record, or click edit in the optimization preview panel. by default, the optimization record is displayed with the general tab selected. go to the report tab and move the record to the assess phase. the link to the survey is displayed above the optimization report table",
    "keywordsLower": [
      "optimization",
      "survey",
      "report",
      "related",
      "topics",
      "answers",
      "application",
      "cloudification",
      "automatically",
      "compiled",
      "after",
      "taken",
      "owner.",
      "prerequisite",
      "owner",
      "through",
      "service",
      "portal.",
      "information",
      "about",
      "sending",
      "see",
      "send",
      "collect",
      "information.",
      "main",
      "menu",
      "select",
      "plan",
      "portfolio",
      "optimizations.",
      "management",
      "displays",
      "list",
      "records.",
      "record",
      "want",
      "assess.",
      "filter",
      "click",
      "add",
      "button.",
      "filters.",
      "identifier",
      "id",
      "column",
      "display",
      "selected",
      "edit",
      "preview",
      "panel.",
      "default",
      "displayed",
      "general",
      "tab",
      "selected.",
      "go",
      "move",
      "assess",
      "phase.",
      "link",
      "above",
      "table.",
      "survey.",
      "opened.",
      "reports",
      "view",
      "report.",
      "note",
      "takes",
      "approximately",
      "hour",
      "generate",
      "alternatively",
      "find",
      "selecting",
      "build",
      "menu.",
      "following",
      "name",
      "label",
      "question",
      "answer",
      "surveys",
      "management.",
      "optimizations",
      "out-of-the-box"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Optimization types",
    "content": "Each optimization record is associated with a optimization type. The optimization type defines the algorithm logic and directs how the optimization result is calculated. Currently, Application cloudification is the only optimization type supported. It provides users with a quick way to identify the optimal cloud-based delivery model of an application within the target portfolio. The analysis is divided into two primary categories: business suitability and technical suitability. Each category is composed of six dimensions. For details of these dimensions, see the Configuration tab. The following tabs are included: General tab This tab contains the following basic information of the optimization type. Field Description Title The title of the optimization type. Currently, Application cloudification is the only optimization type supported. Description A description that captures the details of the record. Model A classification applied to optimization type. Currently, Application cloudific",
    "url": "optimtypeoverview",
    "filename": "optimtypeoverview",
    "headings": [
      "General tab",
      "Configuration tab",
      "Sample report tab",
      "Related topics"
    ],
    "keywords": [
      "optimization",
      "types",
      "general",
      "tab",
      "configuration",
      "sample",
      "report",
      "related",
      "topics",
      "record",
      "associated",
      "type.",
      "type",
      "defines",
      "algorithm",
      "logic",
      "directs",
      "result",
      "calculated.",
      "currently",
      "application",
      "cloudification",
      "supported.",
      "provides",
      "users",
      "quick",
      "way",
      "identify",
      "optimal",
      "cloud-based",
      "delivery",
      "model",
      "target",
      "portfolio.",
      "analysis",
      "divided",
      "two",
      "primary",
      "categories",
      "business",
      "suitability",
      "technical",
      "suitability.",
      "category",
      "composed",
      "six",
      "dimensions.",
      "details",
      "dimensions",
      "see",
      "tab.",
      "following",
      "tabs",
      "included",
      "contains",
      "basic",
      "information",
      "field",
      "description",
      "title",
      "captures",
      "record.",
      "classification",
      "applied",
      "created",
      "name",
      "user",
      "creation",
      "time",
      "created.",
      "shows",
      "recommendation",
      "calculated",
      "dimension",
      "alignment",
      "regulatory",
      "whether",
      "restrictions",
      "identified",
      "prevent",
      "moving",
      "public-cloud",
      "environment.",
      "security",
      "there",
      "constraints",
      "application.",
      "workload",
      "variability",
      "any",
      "issues",
      "service",
      "availability",
      "level",
      "required",
      "aligned",
      "most",
      "major",
      "public",
      "cloud"
    ],
    "language": "en",
    "word_count": 89,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "optimization types",
    "contentLower": "each optimization record is associated with a optimization type. the optimization type defines the algorithm logic and directs how the optimization result is calculated. currently, application cloudification is the only optimization type supported. it provides users with a quick way to identify the optimal cloud-based delivery model of an application within the target portfolio. the analysis is divided into two primary categories: business suitability and technical suitability. each category is composed of six dimensions. for details of these dimensions, see the configuration tab. the following tabs are included: general tab this tab contains the following basic information of the optimization type. field description title the title of the optimization type. currently, application cloudification is the only optimization type supported. description a description that captures the details of the record. model a classification applied to optimization type. currently, application cloudific",
    "keywordsLower": [
      "optimization",
      "types",
      "general",
      "tab",
      "configuration",
      "sample",
      "report",
      "related",
      "topics",
      "record",
      "associated",
      "type.",
      "type",
      "defines",
      "algorithm",
      "logic",
      "directs",
      "result",
      "calculated.",
      "currently",
      "application",
      "cloudification",
      "supported.",
      "provides",
      "users",
      "quick",
      "way",
      "identify",
      "optimal",
      "cloud-based",
      "delivery",
      "model",
      "target",
      "portfolio.",
      "analysis",
      "divided",
      "two",
      "primary",
      "categories",
      "business",
      "suitability",
      "technical",
      "suitability.",
      "category",
      "composed",
      "six",
      "dimensions.",
      "details",
      "dimensions",
      "see",
      "tab.",
      "following",
      "tabs",
      "included",
      "contains",
      "basic",
      "information",
      "field",
      "description",
      "title",
      "captures",
      "record.",
      "classification",
      "applied",
      "created",
      "name",
      "user",
      "creation",
      "time",
      "created.",
      "shows",
      "recommendation",
      "calculated",
      "dimension",
      "alignment",
      "regulatory",
      "whether",
      "restrictions",
      "identified",
      "prevent",
      "moving",
      "public-cloud",
      "environment.",
      "security",
      "there",
      "constraints",
      "application.",
      "workload",
      "variability",
      "any",
      "issues",
      "service",
      "availability",
      "level",
      "required",
      "aligned",
      "most",
      "major",
      "public",
      "cloud"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Project and Program Management",
    "content": "The Project and Program Management module is the place to track and manage IT projects, programs, and project portfolios. You can plan and manage the resource demand, financial aspects, benefit, and business value for the projects and programs, and track their progress and health, and optimize the resource and budget plan in order to maximize business return. This module works closely with the Idea and Proposal module, where a proposal can be the basis of a project or program once technical and business review is performed and the necessary resources and budget are allocated. Related topics Category Links Administer Fields Forms Roles Data domain segmentation Use Projects Programs Project Portfolios Scenarios Executive Summary Project and Program Management roles and permissions Develop Single record APIs Record bulk update and collection APIs",
    "url": "projectprogrammgmt",
    "filename": "projectprogrammgmt",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "project",
      "program",
      "management",
      "related",
      "topics",
      "module",
      "place",
      "track",
      "manage",
      "projects",
      "programs",
      "portfolios.",
      "plan",
      "resource",
      "demand",
      "financial",
      "aspects",
      "benefit",
      "business",
      "value",
      "progress",
      "health",
      "optimize",
      "budget",
      "order",
      "maximize",
      "return.",
      "works",
      "closely",
      "idea",
      "proposal",
      "basis",
      "once",
      "technical",
      "review",
      "performed",
      "necessary",
      "resources",
      "allocated.",
      "category",
      "links",
      "administer",
      "fields",
      "forms",
      "roles",
      "data",
      "domain",
      "segmentation",
      "portfolios",
      "scenarios",
      "executive",
      "summary",
      "permissions",
      "develop",
      "single",
      "record",
      "apis",
      "bulk",
      "update",
      "collection"
    ],
    "language": "en",
    "word_count": 87,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "project and program management",
    "contentLower": "the project and program management module is the place to track and manage it projects, programs, and project portfolios. you can plan and manage the resource demand, financial aspects, benefit, and business value for the projects and programs, and track their progress and health, and optimize the resource and budget plan in order to maximize business return. this module works closely with the idea and proposal module, where a proposal can be the basis of a project or program once technical and business review is performed and the necessary resources and budget are allocated. related topics category links administer fields forms roles data domain segmentation use projects programs project portfolios scenarios executive summary project and program management roles and permissions develop single record apis record bulk update and collection apis",
    "keywordsLower": [
      "project",
      "program",
      "management",
      "related",
      "topics",
      "module",
      "place",
      "track",
      "manage",
      "projects",
      "programs",
      "portfolios.",
      "plan",
      "resource",
      "demand",
      "financial",
      "aspects",
      "benefit",
      "business",
      "value",
      "progress",
      "health",
      "optimize",
      "budget",
      "order",
      "maximize",
      "return.",
      "works",
      "closely",
      "idea",
      "proposal",
      "basis",
      "once",
      "technical",
      "review",
      "performed",
      "necessary",
      "resources",
      "allocated.",
      "category",
      "links",
      "administer",
      "fields",
      "forms",
      "roles",
      "data",
      "domain",
      "segmentation",
      "portfolios",
      "scenarios",
      "executive",
      "summary",
      "permissions",
      "develop",
      "single",
      "record",
      "apis",
      "bulk",
      "update",
      "collection"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Project and Program Management roles and permissions",
    "content": "There are specific roles associated with project portfolios and their components. Service Management uses role based permissions to enable you to complete a task that's appropriate to your role. The tenant administrator manages and assigns these permissions. By default, the following roles are related to various events in a project portfolio's lifecycle. These roles support portfolio creation and management, and process ownership. Role Responsibilities Project Portfolio Manager Owner of project portfolios Allowed to create and add content to project portfolios Responsible for high level governance of a collection of projects or programs Able to add, edit, or delete budget relevant financial data such as budget and budget lines in Project Portfolios Program Manager Owner of programs Allowed to create and add content to programs Responsible for managing related projects in a coordinated way to obtain benefits and control not available from managing the projects individually Able to add, ",
    "url": "pprolespermissions",
    "filename": "pprolespermissions",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "project",
      "program",
      "management",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "portfolios",
      "components.",
      "service",
      "uses",
      "role",
      "based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "tenant",
      "administrator",
      "manages",
      "assigns",
      "permissions.",
      "default",
      "following",
      "various",
      "events",
      "portfolio",
      "lifecycle.",
      "support",
      "creation",
      "process",
      "ownership.",
      "responsibilities",
      "manager",
      "owner",
      "allowed",
      "create",
      "add",
      "content",
      "responsible",
      "high",
      "level",
      "governance",
      "collection",
      "projects",
      "programs",
      "able",
      "edit",
      "delete",
      "budget",
      "relevant",
      "financial",
      "data",
      "such",
      "lines",
      "managing",
      "coordinated",
      "way",
      "obtain",
      "benefits",
      "control",
      "available",
      "individually",
      "all",
      "aspects",
      "success",
      "proposal",
      "reviewer",
      "reviewing",
      "proposals",
      "approving",
      "creating",
      "maintains",
      "type",
      "list",
      "metadata",
      "including",
      "calendar",
      "chart",
      "accounts",
      "review",
      "individual",
      "permission",
      "assignments",
      "administration",
      "master",
      "people",
      "roles."
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "project and program management roles and permissions",
    "contentLower": "there are specific roles associated with project portfolios and their components. service management uses role based permissions to enable you to complete a task that's appropriate to your role. the tenant administrator manages and assigns these permissions. by default, the following roles are related to various events in a project portfolio's lifecycle. these roles support portfolio creation and management, and process ownership. role responsibilities project portfolio manager owner of project portfolios allowed to create and add content to project portfolios responsible for high level governance of a collection of projects or programs able to add, edit, or delete budget relevant financial data such as budget and budget lines in project portfolios program manager owner of programs allowed to create and add content to programs responsible for managing related projects in a coordinated way to obtain benefits and control not available from managing the projects individually able to add, ",
    "keywordsLower": [
      "project",
      "program",
      "management",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "portfolios",
      "components.",
      "service",
      "uses",
      "role",
      "based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "tenant",
      "administrator",
      "manages",
      "assigns",
      "permissions.",
      "default",
      "following",
      "various",
      "events",
      "portfolio",
      "lifecycle.",
      "support",
      "creation",
      "process",
      "ownership.",
      "responsibilities",
      "manager",
      "owner",
      "allowed",
      "create",
      "add",
      "content",
      "responsible",
      "high",
      "level",
      "governance",
      "collection",
      "projects",
      "programs",
      "able",
      "edit",
      "delete",
      "budget",
      "relevant",
      "financial",
      "data",
      "such",
      "lines",
      "managing",
      "coordinated",
      "way",
      "obtain",
      "benefits",
      "control",
      "available",
      "individually",
      "all",
      "aspects",
      "success",
      "proposal",
      "reviewer",
      "reviewing",
      "proposals",
      "approving",
      "creating",
      "maintains",
      "type",
      "list",
      "metadata",
      "including",
      "calendar",
      "chart",
      "accounts",
      "review",
      "individual",
      "permission",
      "assignments",
      "administration",
      "master",
      "people",
      "roles."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Projects",
    "content": "Projects are a collection of interrelated tasks to be run over a fixed period of time and within certain cost and other limitations, in order to reach a specific goal. For example, a complicated change such as upgrading network bandwidth to 1000 MB/second could be managed as a project if many tasks are required to achieve this, or the tasks have a long duration or have a large financial cost. Project management helps you to create, manage, and track your project list, and manage its various attributes and relationships. Related topics Project workflow How to create a project How to edit a project",
    "url": "projects",
    "filename": "projects",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "projects",
      "related",
      "topics",
      "collection",
      "interrelated",
      "tasks",
      "run",
      "over",
      "fixed",
      "period",
      "time",
      "certain",
      "cost",
      "limitations",
      "order",
      "reach",
      "specific",
      "goal.",
      "example",
      "complicated",
      "change",
      "such",
      "upgrading",
      "network",
      "bandwidth",
      "1000",
      "mb",
      "second",
      "managed",
      "project",
      "many",
      "required",
      "achieve",
      "long",
      "duration",
      "large",
      "financial",
      "cost.",
      "management",
      "helps",
      "create",
      "manage",
      "track",
      "list",
      "various",
      "attributes",
      "relationships.",
      "workflow",
      "edit"
    ],
    "language": "en",
    "word_count": 59,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "projects",
    "contentLower": "projects are a collection of interrelated tasks to be run over a fixed period of time and within certain cost and other limitations, in order to reach a specific goal. for example, a complicated change such as upgrading network bandwidth to 1000 mb/second could be managed as a project if many tasks are required to achieve this, or the tasks have a long duration or have a large financial cost. project management helps you to create, manage, and track your project list, and manage its various attributes and relationships. related topics project workflow how to create a project how to edit a project",
    "keywordsLower": [
      "projects",
      "related",
      "topics",
      "collection",
      "interrelated",
      "tasks",
      "run",
      "over",
      "fixed",
      "period",
      "time",
      "certain",
      "cost",
      "limitations",
      "order",
      "reach",
      "specific",
      "goal.",
      "example",
      "complicated",
      "change",
      "such",
      "upgrading",
      "network",
      "bandwidth",
      "1000",
      "mb",
      "second",
      "managed",
      "project",
      "many",
      "required",
      "achieve",
      "long",
      "duration",
      "large",
      "financial",
      "cost.",
      "management",
      "helps",
      "create",
      "manage",
      "track",
      "list",
      "various",
      "attributes",
      "relationships.",
      "workflow",
      "edit"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Project workflow",
    "content": "This section describes the metaphases and subordinate phases in the life cycle of a project record. The project workflow doesn't rely on any business rules. All transitions between phases are manually triggered. Metaphase: Initiation This is the beginning phase of a project, where the initial project scope, project resources, and stakeholders are identified. If a project manager has not been selected, one is assigned as well. The project is officially authorized in this phase. Phase Transition Description Initialize Manual This is the starting point for the basic project workflow. A project record is in this phase after being created. Move the project record to the Plan phase when the project has been approved or can be executed. Next phase: Plan Metaphase: Planning In this phase, the project manager clarifies the project's scope and schedules, defines project management plan and project objectives, and fulfills the resources needed for the project. Phase Transition Description Plan Ma",
    "url": "projectwflw",
    "filename": "projectwflw",
    "headings": [
      "Metaphase: Initiation",
      "Metaphase: Planning",
      "Metaphase: Execution",
      "Metaphase: Closure (End)",
      "Related topics"
    ],
    "keywords": [
      "project",
      "workflow",
      "metaphase",
      "initiation",
      "planning",
      "execution",
      "closure",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "record.",
      "doesn",
      "rely",
      "any",
      "business",
      "rules.",
      "all",
      "transitions",
      "between",
      "manually",
      "triggered.",
      "beginning",
      "phase",
      "initial",
      "scope",
      "resources",
      "stakeholders",
      "identified.",
      "manager",
      "selected",
      "one",
      "assigned",
      "well.",
      "officially",
      "authorized",
      "phase.",
      "transition",
      "description",
      "initialize",
      "manual",
      "starting",
      "point",
      "basic",
      "workflow.",
      "record",
      "after",
      "created.",
      "move",
      "plan",
      "approved",
      "executed.",
      "next",
      "clarifies",
      "schedules",
      "defines",
      "management",
      "objectives",
      "fulfills",
      "needed",
      "project.",
      "schedule",
      "timeline",
      "gets",
      "approval",
      "budget.",
      "execute",
      "begins.",
      "executed",
      "managed",
      "manager.",
      "projects",
      "put",
      "hold",
      "valid",
      "reasons",
      "continue",
      "blockers",
      "removed.",
      "blocked",
      "longer",
      "close",
      "either",
      "completed",
      "canceled.",
      "example",
      "there",
      "budget",
      "issues",
      "investment",
      "changed.",
      "back",
      "resumes.",
      "forward"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "project workflow",
    "contentLower": "this section describes the metaphases and subordinate phases in the life cycle of a project record. the project workflow doesn't rely on any business rules. all transitions between phases are manually triggered. metaphase: initiation this is the beginning phase of a project, where the initial project scope, project resources, and stakeholders are identified. if a project manager has not been selected, one is assigned as well. the project is officially authorized in this phase. phase transition description initialize manual this is the starting point for the basic project workflow. a project record is in this phase after being created. move the project record to the plan phase when the project has been approved or can be executed. next phase: plan metaphase: planning in this phase, the project manager clarifies the project's scope and schedules, defines project management plan and project objectives, and fulfills the resources needed for the project. phase transition description plan ma",
    "keywordsLower": [
      "project",
      "workflow",
      "metaphase",
      "initiation",
      "planning",
      "execution",
      "closure",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "record.",
      "doesn",
      "rely",
      "any",
      "business",
      "rules.",
      "all",
      "transitions",
      "between",
      "manually",
      "triggered.",
      "beginning",
      "phase",
      "initial",
      "scope",
      "resources",
      "stakeholders",
      "identified.",
      "manager",
      "selected",
      "one",
      "assigned",
      "well.",
      "officially",
      "authorized",
      "phase.",
      "transition",
      "description",
      "initialize",
      "manual",
      "starting",
      "point",
      "basic",
      "workflow.",
      "record",
      "after",
      "created.",
      "move",
      "plan",
      "approved",
      "executed.",
      "next",
      "clarifies",
      "schedules",
      "defines",
      "management",
      "objectives",
      "fulfills",
      "needed",
      "project.",
      "schedule",
      "timeline",
      "gets",
      "approval",
      "budget.",
      "execute",
      "begins.",
      "executed",
      "managed",
      "manager.",
      "projects",
      "put",
      "hold",
      "valid",
      "reasons",
      "continue",
      "blockers",
      "removed.",
      "blocked",
      "longer",
      "close",
      "either",
      "completed",
      "canceled.",
      "example",
      "there",
      "budget",
      "issues",
      "investment",
      "changed.",
      "back",
      "resumes.",
      "forward"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Programs",
    "content": "A program consists of projects and proposals, which are normally funded or managed together to achieve a long-term business objective, which can't be achieved by a single project. For example, building an IT service is a program that consists of various projects to build, test, and release the IT service to production. Program management helps you to create, manage, and track your program list, maintain its content, and view the various aspects of the program that are aggregated from its content. Related topics Program workflow How to create a program How to edit a program",
    "url": "programs",
    "filename": "programs",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "programs",
      "related",
      "topics",
      "program",
      "consists",
      "projects",
      "proposals",
      "normally",
      "funded",
      "managed",
      "together",
      "achieve",
      "long-term",
      "business",
      "objective",
      "achieved",
      "single",
      "project.",
      "example",
      "building",
      "service",
      "various",
      "build",
      "test",
      "release",
      "production.",
      "management",
      "helps",
      "create",
      "manage",
      "track",
      "list",
      "maintain",
      "content",
      "view",
      "aspects",
      "aggregated",
      "content.",
      "workflow",
      "edit"
    ],
    "language": "en",
    "word_count": 53,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "programs",
    "contentLower": "a program consists of projects and proposals, which are normally funded or managed together to achieve a long-term business objective, which can't be achieved by a single project. for example, building an it service is a program that consists of various projects to build, test, and release the it service to production. program management helps you to create, manage, and track your program list, maintain its content, and view the various aspects of the program that are aggregated from its content. related topics program workflow how to create a program how to edit a program",
    "keywordsLower": [
      "programs",
      "related",
      "topics",
      "program",
      "consists",
      "projects",
      "proposals",
      "normally",
      "funded",
      "managed",
      "together",
      "achieve",
      "long-term",
      "business",
      "objective",
      "achieved",
      "single",
      "project.",
      "example",
      "building",
      "service",
      "various",
      "build",
      "test",
      "release",
      "production.",
      "management",
      "helps",
      "create",
      "manage",
      "track",
      "list",
      "maintain",
      "content",
      "view",
      "aspects",
      "aggregated",
      "content.",
      "workflow",
      "edit"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Program workflow",
    "content": "This section describes the metaphases and subordinate phases in the life cycle of a program record. The program workflow doesn't rely on any business rules. All transitions between phases are manually triggered. Metaphase: Initiation This is the beginning phase of a program, where the initial program scope, program resources, and stakeholders are identified. If a program manager has not been selected, one is assigned as well. The program is officially authorized in this phase. Phase Transition Description Initialize Manual This is the starting point for the basic program workflow. A program record is in this phase after being created. Move the program record to the Plan phase when the program has been authorized or can be executed. Next phase: Plan Metaphase: Planning In this phase, the program manager clarifies the program's scope and schedules, defines projects and program objectives, and fulfills the resources needed for the entire program. Phase Transition Description Plan Manual I",
    "url": "programwflw",
    "filename": "programwflw",
    "headings": [
      "Metaphase: Initiation",
      "Metaphase: Planning",
      "Metaphase: Execution",
      "Metaphase: Closure (End)",
      "Related topics"
    ],
    "keywords": [
      "program",
      "workflow",
      "metaphase",
      "initiation",
      "planning",
      "execution",
      "closure",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "record.",
      "doesn",
      "rely",
      "any",
      "business",
      "rules.",
      "all",
      "transitions",
      "between",
      "manually",
      "triggered.",
      "beginning",
      "phase",
      "initial",
      "scope",
      "resources",
      "stakeholders",
      "identified.",
      "manager",
      "selected",
      "one",
      "assigned",
      "well.",
      "officially",
      "authorized",
      "phase.",
      "transition",
      "description",
      "initialize",
      "manual",
      "starting",
      "point",
      "basic",
      "workflow.",
      "record",
      "after",
      "created.",
      "move",
      "plan",
      "executed.",
      "next",
      "clarifies",
      "schedules",
      "defines",
      "projects",
      "objectives",
      "fulfills",
      "needed",
      "entire",
      "program.",
      "duration",
      "timeline",
      "required",
      "defined.",
      "execute",
      "begins.",
      "executed",
      "managed",
      "project",
      "manager.",
      "programs",
      "put",
      "hold",
      "valid",
      "reasons",
      "continue",
      "blockers",
      "removed.",
      "blocked",
      "longer",
      "close",
      "either",
      "completed",
      "canceled.",
      "example",
      "there",
      "budget",
      "issues",
      "investment",
      "changed.",
      "back",
      "resumes.",
      "forward"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "program workflow",
    "contentLower": "this section describes the metaphases and subordinate phases in the life cycle of a program record. the program workflow doesn't rely on any business rules. all transitions between phases are manually triggered. metaphase: initiation this is the beginning phase of a program, where the initial program scope, program resources, and stakeholders are identified. if a program manager has not been selected, one is assigned as well. the program is officially authorized in this phase. phase transition description initialize manual this is the starting point for the basic program workflow. a program record is in this phase after being created. move the program record to the plan phase when the program has been authorized or can be executed. next phase: plan metaphase: planning in this phase, the program manager clarifies the program's scope and schedules, defines projects and program objectives, and fulfills the resources needed for the entire program. phase transition description plan manual i",
    "keywordsLower": [
      "program",
      "workflow",
      "metaphase",
      "initiation",
      "planning",
      "execution",
      "closure",
      "end",
      "related",
      "topics",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "record.",
      "doesn",
      "rely",
      "any",
      "business",
      "rules.",
      "all",
      "transitions",
      "between",
      "manually",
      "triggered.",
      "beginning",
      "phase",
      "initial",
      "scope",
      "resources",
      "stakeholders",
      "identified.",
      "manager",
      "selected",
      "one",
      "assigned",
      "well.",
      "officially",
      "authorized",
      "phase.",
      "transition",
      "description",
      "initialize",
      "manual",
      "starting",
      "point",
      "basic",
      "workflow.",
      "record",
      "after",
      "created.",
      "move",
      "plan",
      "executed.",
      "next",
      "clarifies",
      "schedules",
      "defines",
      "projects",
      "objectives",
      "fulfills",
      "needed",
      "entire",
      "program.",
      "duration",
      "timeline",
      "required",
      "defined.",
      "execute",
      "begins.",
      "executed",
      "managed",
      "project",
      "manager.",
      "programs",
      "put",
      "hold",
      "valid",
      "reasons",
      "continue",
      "blockers",
      "removed.",
      "blocked",
      "longer",
      "close",
      "either",
      "completed",
      "canceled.",
      "example",
      "there",
      "budget",
      "issues",
      "investment",
      "changed.",
      "back",
      "resumes.",
      "forward"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Project Portfolios",
    "content": "A project portfolio is a collection of programs, projects, and proposals that are managed as a group. A project portfolio's components aren't necessarily dependent on or even related to each other. But they're managed together as a group to achieve strategic objectives. Project and Program Management helps you to Create, manage, and track your project portfolio list Keep the portfolio's content View the various aspects of the portfolio that are aggregated from its content You can also manually or automatically optimize the portfolio to achieve the maximum business value without breaking any financial or resource constraints. Related topics How to create a project portfolio How to edit a project portfolio",
    "url": "projectportfolios",
    "filename": "projectportfolios",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "project",
      "portfolios",
      "related",
      "topics",
      "portfolio",
      "collection",
      "programs",
      "projects",
      "proposals",
      "managed",
      "group.",
      "components",
      "aren",
      "necessarily",
      "dependent",
      "even",
      "other.",
      "re",
      "together",
      "group",
      "achieve",
      "strategic",
      "objectives.",
      "program",
      "management",
      "helps",
      "create",
      "manage",
      "track",
      "list",
      "keep",
      "content",
      "view",
      "various",
      "aspects",
      "aggregated",
      "manually",
      "automatically",
      "optimize",
      "maximum",
      "business",
      "value",
      "breaking",
      "any",
      "financial",
      "resource",
      "constraints.",
      "edit"
    ],
    "language": "en",
    "word_count": 69,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "project portfolios",
    "contentLower": "a project portfolio is a collection of programs, projects, and proposals that are managed as a group. a project portfolio's components aren't necessarily dependent on or even related to each other. but they're managed together as a group to achieve strategic objectives. project and program management helps you to create, manage, and track your project portfolio list keep the portfolio's content view the various aspects of the portfolio that are aggregated from its content you can also manually or automatically optimize the portfolio to achieve the maximum business value without breaking any financial or resource constraints. related topics how to create a project portfolio how to edit a project portfolio",
    "keywordsLower": [
      "project",
      "portfolios",
      "related",
      "topics",
      "portfolio",
      "collection",
      "programs",
      "projects",
      "proposals",
      "managed",
      "group.",
      "components",
      "aren",
      "necessarily",
      "dependent",
      "even",
      "other.",
      "re",
      "together",
      "group",
      "achieve",
      "strategic",
      "objectives.",
      "program",
      "management",
      "helps",
      "create",
      "manage",
      "track",
      "list",
      "keep",
      "content",
      "view",
      "various",
      "aspects",
      "aggregated",
      "manually",
      "automatically",
      "optimize",
      "maximum",
      "business",
      "value",
      "breaking",
      "any",
      "financial",
      "resource",
      "constraints.",
      "edit"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Optimize a scenario automatically using the embedded optimization engine",
    "content": "The purpose of optimization is to maximize business value of programs, projects, or proposals, without breaking resource and budget constraints, within the time frame that you specify. Note If you specify that certain content items must be included and these items break the constraints that you set, no data is displayed. Do one of the following: From the main menu, select Plan > Project & Program > Project Portfolios. Service Management displays a list of project portfolios. Click the Scenarios tab and select the portfolio that contains the scenario you want to optimize. From the main menu, select Plan > Project & Program > Portfolio Scenarios. Service Management displays a list of project portfolios. Select the portfolio that contains the scenario you want to optimize. Click Optimize it to optimize the scenario according to the selected settings. Service Management analyzes the proposals and displays results that include only those items that meet the desired business objective. When ",
    "url": "optimscenario",
    "filename": "optimscenario",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "optimize",
      "scenario",
      "automatically",
      "embedded",
      "optimization",
      "engine",
      "related",
      "topics",
      "purpose",
      "maximize",
      "business",
      "value",
      "programs",
      "projects",
      "proposals",
      "breaking",
      "resource",
      "budget",
      "constraints",
      "time",
      "frame",
      "specify.",
      "note",
      "specify",
      "certain",
      "content",
      "items",
      "included",
      "break",
      "set",
      "data",
      "displayed.",
      "one",
      "following",
      "main",
      "menu",
      "select",
      "plan",
      "project",
      "program",
      "portfolios.",
      "service",
      "management",
      "displays",
      "list",
      "click",
      "scenarios",
      "tab",
      "portfolio",
      "contains",
      "want",
      "optimize.",
      "scenarios.",
      "according",
      "selected",
      "settings.",
      "analyzes",
      "results",
      "include",
      "meet",
      "desired",
      "objective.",
      "calculation",
      "finished",
      "preview",
      "window",
      "appears",
      "comparing",
      "original",
      "optimized",
      "values",
      "cost",
      "demand",
      "total",
      "items.",
      "satisfied",
      "continue.",
      "page.",
      "aren",
      "result",
      "cancel.",
      "discarded",
      "page",
      "selections",
      "before",
      "running",
      "optimization.",
      "whether",
      "continue",
      "cancel",
      "saved",
      "name",
      "portfolioname",
      "timetag",
      "further",
      "use.",
      "save",
      "finished.",
      "different",
      "future"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "optimize a scenario automatically using the embedded optimization engine",
    "contentLower": "the purpose of optimization is to maximize business value of programs, projects, or proposals, without breaking resource and budget constraints, within the time frame that you specify. note if you specify that certain content items must be included and these items break the constraints that you set, no data is displayed. do one of the following: from the main menu, select plan > project & program > project portfolios. service management displays a list of project portfolios. click the scenarios tab and select the portfolio that contains the scenario you want to optimize. from the main menu, select plan > project & program > portfolio scenarios. service management displays a list of project portfolios. select the portfolio that contains the scenario you want to optimize. click optimize it to optimize the scenario according to the selected settings. service management analyzes the proposals and displays results that include only those items that meet the desired business objective. when ",
    "keywordsLower": [
      "optimize",
      "scenario",
      "automatically",
      "embedded",
      "optimization",
      "engine",
      "related",
      "topics",
      "purpose",
      "maximize",
      "business",
      "value",
      "programs",
      "projects",
      "proposals",
      "breaking",
      "resource",
      "budget",
      "constraints",
      "time",
      "frame",
      "specify.",
      "note",
      "specify",
      "certain",
      "content",
      "items",
      "included",
      "break",
      "set",
      "data",
      "displayed.",
      "one",
      "following",
      "main",
      "menu",
      "select",
      "plan",
      "project",
      "program",
      "portfolios.",
      "service",
      "management",
      "displays",
      "list",
      "click",
      "scenarios",
      "tab",
      "portfolio",
      "contains",
      "want",
      "optimize.",
      "scenarios.",
      "according",
      "selected",
      "settings.",
      "analyzes",
      "results",
      "include",
      "meet",
      "desired",
      "objective.",
      "calculation",
      "finished",
      "preview",
      "window",
      "appears",
      "comparing",
      "original",
      "optimized",
      "values",
      "cost",
      "demand",
      "total",
      "items.",
      "satisfied",
      "continue.",
      "page.",
      "aren",
      "result",
      "cancel.",
      "discarded",
      "page",
      "selections",
      "before",
      "running",
      "optimization.",
      "whether",
      "continue",
      "cancel",
      "saved",
      "name",
      "portfolioname",
      "timetag",
      "further",
      "use.",
      "save",
      "finished.",
      "different",
      "future"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Optimization use case",
    "content": "This use case is based on the images shown below. Carl Hulman is a project portfolio manager. He manages investments for enhancing the F-Generation application in his company. He creates a project portfolio 16747: F-Generation for that purpose, and sets himself as the owner. Carl’s portfolio contains four proposals (new business initiatives that bring new business value), one program and two projects. Project 16843: SAP Integration is currently in planning phrase, while Project 16852: Maps everywherehas already begun and is in execution phrase. The planned cost or resource demand and the business value are also recorded in the proposal. Take proposal 16928: BI - Phase II as an example: The planned cost is $9,050,000 and the business value is $50,000,000. This is shown on the proposal detail page. Carl can also see details in the Financial summary tab, such as planned cost per month. The proposal's resource demand is displayed on the Resource demand tab, showing the kinds of resources t",
    "url": "ppusecase",
    "filename": "ppusecase",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "238.13",
      "103.25",
      "57.69",
      "48.2",
      "optimization",
      "case",
      "related",
      "topics",
      "based",
      "images",
      "shown",
      "below.",
      "carl",
      "hulman",
      "project",
      "portfolio",
      "manager.",
      "manages",
      "investments",
      "enhancing",
      "f-generation",
      "application",
      "company.",
      "creates",
      "16747",
      "purpose",
      "sets",
      "himself",
      "owner.",
      "contains",
      "four",
      "proposals",
      "new",
      "business",
      "initiatives",
      "bring",
      "value",
      "one",
      "program",
      "two",
      "projects.",
      "16843",
      "sap",
      "integration",
      "currently",
      "planning",
      "phrase",
      "while",
      "16852",
      "maps",
      "everywherehas",
      "already",
      "begun",
      "execution",
      "phrase.",
      "planned",
      "cost",
      "resource",
      "demand",
      "recorded",
      "proposal.",
      "take",
      "proposal",
      "16928",
      "bi",
      "phase",
      "ii",
      "example",
      "050",
      "000",
      "50",
      "000.",
      "detail",
      "page.",
      "see",
      "details",
      "financial",
      "summary",
      "tab",
      "such",
      "per",
      "month.",
      "displayed",
      "showing",
      "kinds",
      "resources",
      "required",
      "complete",
      "budgets",
      "portfolio.",
      "strategic",
      "investment",
      "30",
      "covers",
      "january",
      "2016",
      "through",
      "june",
      "2016.",
      "budget"
    ],
    "language": "en",
    "word_count": 112,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "optimization use case",
    "contentLower": "this use case is based on the images shown below. carl hulman is a project portfolio manager. he manages investments for enhancing the f-generation application in his company. he creates a project portfolio 16747: f-generation for that purpose, and sets himself as the owner. carl’s portfolio contains four proposals (new business initiatives that bring new business value), one program and two projects. project 16843: sap integration is currently in planning phrase, while project 16852: maps everywherehas already begun and is in execution phrase. the planned cost or resource demand and the business value are also recorded in the proposal. take proposal 16928: bi - phase ii as an example: the planned cost is $9,050,000 and the business value is $50,000,000. this is shown on the proposal detail page. carl can also see details in the financial summary tab, such as planned cost per month. the proposal's resource demand is displayed on the resource demand tab, showing the kinds of resources t",
    "keywordsLower": [
      "238.13",
      "103.25",
      "57.69",
      "48.2",
      "optimization",
      "case",
      "related",
      "topics",
      "based",
      "images",
      "shown",
      "below.",
      "carl",
      "hulman",
      "project",
      "portfolio",
      "manager.",
      "manages",
      "investments",
      "enhancing",
      "f-generation",
      "application",
      "company.",
      "creates",
      "16747",
      "purpose",
      "sets",
      "himself",
      "owner.",
      "contains",
      "four",
      "proposals",
      "new",
      "business",
      "initiatives",
      "bring",
      "value",
      "one",
      "program",
      "two",
      "projects.",
      "16843",
      "sap",
      "integration",
      "currently",
      "planning",
      "phrase",
      "while",
      "16852",
      "maps",
      "everywherehas",
      "already",
      "begun",
      "execution",
      "phrase.",
      "planned",
      "cost",
      "resource",
      "demand",
      "recorded",
      "proposal.",
      "take",
      "proposal",
      "16928",
      "bi",
      "phase",
      "ii",
      "example",
      "050",
      "000",
      "50",
      "000.",
      "detail",
      "page.",
      "see",
      "details",
      "financial",
      "summary",
      "tab",
      "such",
      "per",
      "month.",
      "displayed",
      "showing",
      "kinds",
      "resources",
      "required",
      "complete",
      "budgets",
      "portfolio.",
      "strategic",
      "investment",
      "30",
      "covers",
      "january",
      "2016",
      "through",
      "june",
      "2016.",
      "budget"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Moderate questions and answers",
    "content": "Q&A enables you to do one or all of the following: Fix questions that have been reported as having problems Edit or delete questions and answers Post new answers The Q&A tab displays the following: Unanswered questions Questions that haven't yet received any answers Questions with problems Questions that have been flagged as having problems. Such questions might contain incorrect or offensive content. All questions All questions Assign moderator permissions Note To moderate questions and answers, you must have one of the following out-of-the-box roles: Knowledge Contributor Knowledge Publisher Knowledge Administrator Alternatively, you can create additional roles that include Moderate user questions and answers permission. For more information, see How to edit role permissions. Service Portal users don't have Q&A moderation permissions as such, but by default can ask, review, and respond to questions and answers that other Service Portal users have submitted. For more information about",
    "url": "qamoderation",
    "filename": "qamoderation",
    "headings": [
      "Assign moderator permissions",
      "Review questions",
      "Related topics"
    ],
    "keywords": [
      "moderate",
      "questions",
      "answers",
      "assign",
      "moderator",
      "permissions",
      "review",
      "related",
      "topics",
      "enables",
      "one",
      "all",
      "following",
      "fix",
      "reported",
      "having",
      "problems",
      "edit",
      "delete",
      "post",
      "new",
      "tab",
      "displays",
      "unanswered",
      "haven",
      "yet",
      "received",
      "any",
      "flagged",
      "problems.",
      "such",
      "contain",
      "incorrect",
      "offensive",
      "content.",
      "note",
      "out-of-the-box",
      "roles",
      "knowledge",
      "contributor",
      "publisher",
      "administrator",
      "alternatively",
      "create",
      "additional",
      "include",
      "user",
      "permission.",
      "information",
      "see",
      "role",
      "permissions.",
      "service",
      "portal",
      "users",
      "don",
      "moderation",
      "default",
      "ask",
      "respond",
      "submitted.",
      "about",
      "authorize",
      "handling",
      "portal.",
      "main",
      "menu",
      "click",
      "build",
      "a.",
      "questions.",
      "sorted",
      "descending",
      "order",
      "starting",
      "highest",
      "number",
      "requestors.",
      "select",
      "question",
      "view",
      "full",
      "description.",
      "answer.",
      "question.",
      "management",
      "procedures",
      "news",
      "article",
      "workflow"
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "moderate questions and answers",
    "contentLower": "q&a enables you to do one or all of the following: fix questions that have been reported as having problems edit or delete questions and answers post new answers the q&a tab displays the following: unanswered questions questions that haven't yet received any answers questions with problems questions that have been flagged as having problems. such questions might contain incorrect or offensive content. all questions all questions assign moderator permissions note to moderate questions and answers, you must have one of the following out-of-the-box roles: knowledge contributor knowledge publisher knowledge administrator alternatively, you can create additional roles that include moderate user questions and answers permission. for more information, see how to edit role permissions. service portal users don't have q&a moderation permissions as such, but by default can ask, review, and respond to questions and answers that other service portal users have submitted. for more information about",
    "keywordsLower": [
      "moderate",
      "questions",
      "answers",
      "assign",
      "moderator",
      "permissions",
      "review",
      "related",
      "topics",
      "enables",
      "one",
      "all",
      "following",
      "fix",
      "reported",
      "having",
      "problems",
      "edit",
      "delete",
      "post",
      "new",
      "tab",
      "displays",
      "unanswered",
      "haven",
      "yet",
      "received",
      "any",
      "flagged",
      "problems.",
      "such",
      "contain",
      "incorrect",
      "offensive",
      "content.",
      "note",
      "out-of-the-box",
      "roles",
      "knowledge",
      "contributor",
      "publisher",
      "administrator",
      "alternatively",
      "create",
      "additional",
      "include",
      "user",
      "permission.",
      "information",
      "see",
      "role",
      "permissions.",
      "service",
      "portal",
      "users",
      "don",
      "moderation",
      "default",
      "ask",
      "respond",
      "submitted.",
      "about",
      "authorize",
      "handling",
      "portal.",
      "main",
      "menu",
      "click",
      "build",
      "a.",
      "questions.",
      "sorted",
      "descending",
      "order",
      "starting",
      "highest",
      "number",
      "requestors.",
      "select",
      "question",
      "view",
      "full",
      "description.",
      "answer.",
      "question.",
      "management",
      "procedures",
      "news",
      "article",
      "workflow"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Packaging and importing knowledge articles",
    "content": "Service Management enables you to perform a mass import of knowledge articles to populate the Knowledge Management module. Articles are imported in a standard package created by the Knowledge Packaging Tool. The packaging process: Retrieves articles from their source using a simple article retriever plugin that retrieves articles from a hierarchy of folders on a file system. Converts the articles to an HTML format using an article retriever plugin. Note Conversion capability of different file formats is NOT provided with the tool and is the responsibility of the customer to implement. Parses the articles to find links to images and other articles. Downloads images that are embedded in the articles. Rewrites links that refer to other articles to avoid broken links when the articles are imported into Service Management. Packages all articles and images together, so that after importing to Service Management, images are visible and links between articles work. The Knowledge Packaging tool",
    "url": "pckgimportarticles",
    "filename": "pckgimportarticles",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "packaging",
      "importing",
      "knowledge",
      "articles",
      "related",
      "topics",
      "service",
      "management",
      "enables",
      "perform",
      "mass",
      "import",
      "populate",
      "module.",
      "imported",
      "standard",
      "package",
      "created",
      "tool.",
      "process",
      "retrieves",
      "source",
      "simple",
      "article",
      "retriever",
      "plugin",
      "hierarchy",
      "folders",
      "file",
      "system.",
      "converts",
      "html",
      "format",
      "plugin.",
      "note",
      "conversion",
      "capability",
      "different",
      "formats",
      "provided",
      "tool",
      "responsibility",
      "customer",
      "implement.",
      "parses",
      "find",
      "links",
      "images",
      "articles.",
      "downloads",
      "embedded",
      "rewrites",
      "refer",
      "avoid",
      "broken",
      "management.",
      "packages",
      "all",
      "together",
      "after",
      "visible",
      "between",
      "work.",
      "includes",
      "metadata",
      "such",
      "author",
      "tags.",
      "because",
      "content",
      "located",
      "anywhere",
      "sharepoint",
      "site",
      "wiki",
      "system",
      "requires",
      "implementation",
      "code",
      "extracts",
      "data",
      "while",
      "running",
      "run",
      "complete",
      "later",
      "information",
      "see",
      "quickstart",
      "advanced",
      "users.",
      "sources",
      "various",
      "formats.",
      "packages.",
      "create",
      "update",
      "news",
      "integrate",
      "confluence"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "packaging and importing knowledge articles",
    "contentLower": "service management enables you to perform a mass import of knowledge articles to populate the knowledge management module. articles are imported in a standard package created by the knowledge packaging tool. the packaging process: retrieves articles from their source using a simple article retriever plugin that retrieves articles from a hierarchy of folders on a file system. converts the articles to an html format using an article retriever plugin. note conversion capability of different file formats is not provided with the tool and is the responsibility of the customer to implement. parses the articles to find links to images and other articles. downloads images that are embedded in the articles. rewrites links that refer to other articles to avoid broken links when the articles are imported into service management. packages all articles and images together, so that after importing to service management, images are visible and links between articles work. the knowledge packaging tool",
    "keywordsLower": [
      "packaging",
      "importing",
      "knowledge",
      "articles",
      "related",
      "topics",
      "service",
      "management",
      "enables",
      "perform",
      "mass",
      "import",
      "populate",
      "module.",
      "imported",
      "standard",
      "package",
      "created",
      "tool.",
      "process",
      "retrieves",
      "source",
      "simple",
      "article",
      "retriever",
      "plugin",
      "hierarchy",
      "folders",
      "file",
      "system.",
      "converts",
      "html",
      "format",
      "plugin.",
      "note",
      "conversion",
      "capability",
      "different",
      "formats",
      "provided",
      "tool",
      "responsibility",
      "customer",
      "implement.",
      "parses",
      "find",
      "links",
      "images",
      "articles.",
      "downloads",
      "embedded",
      "rewrites",
      "refer",
      "avoid",
      "broken",
      "management.",
      "packages",
      "all",
      "together",
      "after",
      "visible",
      "between",
      "work.",
      "includes",
      "metadata",
      "such",
      "author",
      "tags.",
      "because",
      "content",
      "located",
      "anywhere",
      "sharepoint",
      "site",
      "wiki",
      "system",
      "requires",
      "implementation",
      "code",
      "extracts",
      "data",
      "while",
      "running",
      "run",
      "complete",
      "later",
      "information",
      "see",
      "quickstart",
      "advanced",
      "users.",
      "sources",
      "various",
      "formats.",
      "packages.",
      "create",
      "update",
      "news",
      "integrate",
      "confluence"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Model examples",
    "content": "The following graphics are examples of how SACM models are displayed. A model of an HR service A simplified model of an HR service A model of a Store service",
    "url": "modelingexamples",
    "filename": "modelingexamples",
    "headings": [],
    "keywords": [
      "model",
      "examples",
      "following",
      "graphics",
      "sacm",
      "models",
      "displayed.",
      "hr",
      "service",
      "simplified",
      "store"
    ],
    "language": "en",
    "word_count": 18,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "model examples",
    "contentLower": "the following graphics are examples of how sacm models are displayed. a model of an hr service a simplified model of an hr service a model of a store service",
    "keywordsLower": [
      "model",
      "examples",
      "following",
      "graphics",
      "sacm",
      "models",
      "displayed.",
      "hr",
      "service",
      "simplified",
      "store"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Native SACM",
    "content": "Native SACM is one single solution for the entire Service Asset & Configuration Management (SACM) process with instant access to the data and embedded graphical widgets for CI topology representation. Instead of a loosely coupled and single-directional data replication, Native SACM is a natively connected solution that enables bi-directional data federation between Service Management and UCMDB, which is achieved by building a new technology called UCMDB Gateway. After UCMDB Gateway is set up and ready, you can enable Native SACM in the tenant application settings. Please be aware that this operation can't be reverted, once you have enabled Native SACM, you can no longer use OPB to connect Service Management and UCMDB. Compared with the traditional Service Management-UCMDB integration through OPB, Native SACM brings the following benefits: Up-to-date services and inventory Bi-directional data flow One service model and CI topology End-to-end lifecycle Auditing including federated attrib",
    "url": "nativesacm",
    "filename": "nativesacm",
    "headings": [
      "Scope of Native SACM",
      "Related topics"
    ],
    "keywords": [
      "native",
      "sacm",
      "scope",
      "related",
      "topics",
      "one",
      "single",
      "solution",
      "entire",
      "service",
      "asset",
      "configuration",
      "management",
      "process",
      "instant",
      "access",
      "data",
      "embedded",
      "graphical",
      "widgets",
      "ci",
      "topology",
      "representation.",
      "instead",
      "loosely",
      "coupled",
      "single-directional",
      "replication",
      "natively",
      "connected",
      "enables",
      "bi-directional",
      "federation",
      "between",
      "ucmdb",
      "achieved",
      "building",
      "new",
      "technology",
      "called",
      "gateway.",
      "after",
      "gateway",
      "set",
      "ready",
      "enable",
      "tenant",
      "application",
      "settings.",
      "please",
      "aware",
      "operation",
      "reverted",
      "once",
      "enabled",
      "longer",
      "opb",
      "connect",
      "ucmdb.",
      "compared",
      "traditional",
      "management-ucmdb",
      "integration",
      "through",
      "brings",
      "following",
      "benefits",
      "up-to-date",
      "services",
      "inventory",
      "flow",
      "model",
      "end-to-end",
      "lifecycle",
      "auditing",
      "including",
      "federated",
      "attributes",
      "enriched",
      "information",
      "codeless",
      "implemented",
      "record",
      "types",
      "actual",
      "devices",
      "component",
      "system",
      "element",
      "note",
      "infrastructure",
      "peripherals",
      "currently",
      "cis.",
      "active",
      "any",
      "objects",
      "created",
      "joined",
      "identical"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "native sacm",
    "contentLower": "native sacm is one single solution for the entire service asset & configuration management (sacm) process with instant access to the data and embedded graphical widgets for ci topology representation. instead of a loosely coupled and single-directional data replication, native sacm is a natively connected solution that enables bi-directional data federation between service management and ucmdb, which is achieved by building a new technology called ucmdb gateway. after ucmdb gateway is set up and ready, you can enable native sacm in the tenant application settings. please be aware that this operation can't be reverted, once you have enabled native sacm, you can no longer use opb to connect service management and ucmdb. compared with the traditional service management-ucmdb integration through opb, native sacm brings the following benefits: up-to-date services and inventory bi-directional data flow one service model and ci topology end-to-end lifecycle auditing including federated attrib",
    "keywordsLower": [
      "native",
      "sacm",
      "scope",
      "related",
      "topics",
      "one",
      "single",
      "solution",
      "entire",
      "service",
      "asset",
      "configuration",
      "management",
      "process",
      "instant",
      "access",
      "data",
      "embedded",
      "graphical",
      "widgets",
      "ci",
      "topology",
      "representation.",
      "instead",
      "loosely",
      "coupled",
      "single-directional",
      "replication",
      "natively",
      "connected",
      "enables",
      "bi-directional",
      "federation",
      "between",
      "ucmdb",
      "achieved",
      "building",
      "new",
      "technology",
      "called",
      "gateway.",
      "after",
      "gateway",
      "set",
      "ready",
      "enable",
      "tenant",
      "application",
      "settings.",
      "please",
      "aware",
      "operation",
      "reverted",
      "once",
      "enabled",
      "longer",
      "opb",
      "connect",
      "ucmdb.",
      "compared",
      "traditional",
      "management-ucmdb",
      "integration",
      "through",
      "brings",
      "following",
      "benefits",
      "up-to-date",
      "services",
      "inventory",
      "flow",
      "model",
      "end-to-end",
      "lifecycle",
      "auditing",
      "including",
      "federated",
      "attributes",
      "enriched",
      "information",
      "codeless",
      "implemented",
      "record",
      "types",
      "actual",
      "devices",
      "component",
      "system",
      "element",
      "note",
      "infrastructure",
      "peripherals",
      "currently",
      "cis.",
      "active",
      "any",
      "objects",
      "created",
      "joined",
      "identical"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Native SACM overview",
    "content": "Native SACM vs. the traditional SACM module After Native SACM is enabled, the general configurations and functionalities of the SACM module are still valid, with the following behavior changes: Data is federated with UCMDB in a bi-directional manner; Service Management and UCMDB are tightly coupled. Specifically, actual services, service components, and devices that are created through Service Management UI will appear in UCMDB immediately. Likewise, they can be created in UCMDB and appear in Service Management immediately. In the traditional Service Management SACM module, you can create several services with the same name. This is no longer possible in the Native SACM solution since the reconciliation engine in UCMDB prevents creating several services with the same name. In case it's required to create several services with the same name, we recommend that you deactivate the identification rule (set the UCMDB Identification field to No identification) for the Business and Infrastruct",
    "url": "native_sacm_overview",
    "filename": "native_sacm_overview",
    "headings": [
      "Native SACM vs. the traditional SACM module",
      "Create and edit new records",
      "Manage system elements",
      "Delete records",
      "Service modeling",
      "Related CIs, CI relationships, and Impact Analysis",
      "Native SACM limitations"
    ],
    "keywords": [
      "native",
      "sacm",
      "overview",
      "vs.",
      "traditional",
      "module",
      "create",
      "edit",
      "new",
      "records",
      "manage",
      "system",
      "elements",
      "delete",
      "service",
      "modeling",
      "related",
      "cis",
      "ci",
      "relationships",
      "impact",
      "analysis",
      "limitations",
      "after",
      "enabled",
      "general",
      "configurations",
      "functionalities",
      "still",
      "valid",
      "following",
      "behavior",
      "changes",
      "data",
      "federated",
      "ucmdb",
      "bi-directional",
      "manner",
      "management",
      "tightly",
      "coupled.",
      "specifically",
      "actual",
      "services",
      "components",
      "devices",
      "created",
      "through",
      "ui",
      "appear",
      "immediately.",
      "likewise",
      "several",
      "same",
      "name.",
      "longer",
      "possible",
      "solution",
      "since",
      "reconciliation",
      "engine",
      "prevents",
      "creating",
      "case",
      "required",
      "name",
      "recommend",
      "deactivate",
      "identification",
      "rule",
      "set",
      "field",
      "business",
      "infrastructure",
      "types.",
      "because",
      "underlying",
      "types",
      "different",
      "prerequisites.",
      "therefore",
      "need",
      "discovered",
      "synchronized",
      "management.",
      "mostly",
      "maintained",
      "rather",
      "way",
      "rules.",
      "all",
      "directly",
      "managed",
      "either",
      "automatically",
      "via",
      "discovery",
      "manually.",
      "topology",
      "displayed"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "native sacm overview",
    "contentLower": "native sacm vs. the traditional sacm module after native sacm is enabled, the general configurations and functionalities of the sacm module are still valid, with the following behavior changes: data is federated with ucmdb in a bi-directional manner; service management and ucmdb are tightly coupled. specifically, actual services, service components, and devices that are created through service management ui will appear in ucmdb immediately. likewise, they can be created in ucmdb and appear in service management immediately. in the traditional service management sacm module, you can create several services with the same name. this is no longer possible in the native sacm solution since the reconciliation engine in ucmdb prevents creating several services with the same name. in case it's required to create several services with the same name, we recommend that you deactivate the identification rule (set the ucmdb identification field to no identification) for the business and infrastruct",
    "keywordsLower": [
      "native",
      "sacm",
      "overview",
      "vs.",
      "traditional",
      "module",
      "create",
      "edit",
      "new",
      "records",
      "manage",
      "system",
      "elements",
      "delete",
      "service",
      "modeling",
      "related",
      "cis",
      "ci",
      "relationships",
      "impact",
      "analysis",
      "limitations",
      "after",
      "enabled",
      "general",
      "configurations",
      "functionalities",
      "still",
      "valid",
      "following",
      "behavior",
      "changes",
      "data",
      "federated",
      "ucmdb",
      "bi-directional",
      "manner",
      "management",
      "tightly",
      "coupled.",
      "specifically",
      "actual",
      "services",
      "components",
      "devices",
      "created",
      "through",
      "ui",
      "appear",
      "immediately.",
      "likewise",
      "several",
      "same",
      "name.",
      "longer",
      "possible",
      "solution",
      "since",
      "reconciliation",
      "engine",
      "prevents",
      "creating",
      "case",
      "required",
      "name",
      "recommend",
      "deactivate",
      "identification",
      "rule",
      "set",
      "field",
      "business",
      "infrastructure",
      "types.",
      "because",
      "underlying",
      "types",
      "different",
      "prerequisites.",
      "therefore",
      "need",
      "discovered",
      "synchronized",
      "management.",
      "mostly",
      "maintained",
      "rather",
      "way",
      "rules.",
      "all",
      "directly",
      "managed",
      "either",
      "automatically",
      "via",
      "discovery",
      "manually.",
      "topology",
      "displayed"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Native SACM reconciliation",
    "content": "This section outlines some theories, recommendations, and considerations to ensure the reconciliation between existing CIs and CIs that get updated throughout the lifecycle (discovery, data import, manual change) works successfully in the Native SACM setup. With native SACM enabled, a federation is established and shows UD/UCMDB data in Service Management. The reconciliation of CIs is fully based on the reconciliation engine defined in UD/UCMDB, the Service Management internal reconciliation engine is no longer used in the Native SACM setup. How does reconciliation work in UD/UCMDB Let’s start with some theories first. Reconciliation is defined per CI type in UD/UCMDB. Each CI type may have different reconciliation rules. The reconciliation rule can be found on the Details tab of each CI type. You can find the full documentation about how reconciliation in UD/UCMDB works by searching for \"Data reconciliation\" in the UCMDB documentation. Reconciliation types Based on key attributes This",
    "url": "nativesacmreconciliation",
    "filename": "nativesacmreconciliation",
    "headings": [
      "How does reconciliation work in UD/UCMDB",
      "Reconciliation types",
      "Based on key attributes",
      "Based on the identification rule",
      "Considerations for Service Management",
      "Actual Service",
      "Business Service",
      "Infrastructure Service",
      "Service Component",
      "System Element",
      "Device"
    ],
    "keywords": [
      "native",
      "sacm",
      "reconciliation",
      "work",
      "ud",
      "ucmdb",
      "types",
      "based",
      "key",
      "attributes",
      "identification",
      "rule",
      "considerations",
      "service",
      "management",
      "actual",
      "business",
      "infrastructure",
      "component",
      "system",
      "element",
      "device",
      "section",
      "outlines",
      "theories",
      "recommendations",
      "ensure",
      "between",
      "existing",
      "cis",
      "get",
      "updated",
      "throughout",
      "lifecycle",
      "discovery",
      "data",
      "import",
      "manual",
      "change",
      "works",
      "successfully",
      "setup.",
      "enabled",
      "federation",
      "established",
      "shows",
      "management.",
      "fully",
      "engine",
      "defined",
      "internal",
      "longer",
      "let",
      "start",
      "first.",
      "per",
      "ci",
      "type",
      "ucmdb.",
      "different",
      "rules.",
      "found",
      "details",
      "tab",
      "type.",
      "find",
      "full",
      "documentation",
      "about",
      "searching",
      "documentation.",
      "simple",
      "way",
      "define",
      "reconciled.",
      "simply",
      "select",
      "scope.",
      "option",
      "allows",
      "comprehensive",
      "logic.",
      "logic",
      "xml",
      "script",
      "consists",
      "following",
      "elements.",
      "broken",
      "two",
      "parts",
      "follows",
      "outlined",
      "process.",
      "set",
      "criteria",
      "all",
      "possible",
      "conditions",
      "candidate"
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "native sacm reconciliation",
    "contentLower": "this section outlines some theories, recommendations, and considerations to ensure the reconciliation between existing cis and cis that get updated throughout the lifecycle (discovery, data import, manual change) works successfully in the native sacm setup. with native sacm enabled, a federation is established and shows ud/ucmdb data in service management. the reconciliation of cis is fully based on the reconciliation engine defined in ud/ucmdb, the service management internal reconciliation engine is no longer used in the native sacm setup. how does reconciliation work in ud/ucmdb let’s start with some theories first. reconciliation is defined per ci type in ud/ucmdb. each ci type may have different reconciliation rules. the reconciliation rule can be found on the details tab of each ci type. you can find the full documentation about how reconciliation in ud/ucmdb works by searching for \"data reconciliation\" in the ucmdb documentation. reconciliation types based on key attributes this",
    "keywordsLower": [
      "native",
      "sacm",
      "reconciliation",
      "work",
      "ud",
      "ucmdb",
      "types",
      "based",
      "key",
      "attributes",
      "identification",
      "rule",
      "considerations",
      "service",
      "management",
      "actual",
      "business",
      "infrastructure",
      "component",
      "system",
      "element",
      "device",
      "section",
      "outlines",
      "theories",
      "recommendations",
      "ensure",
      "between",
      "existing",
      "cis",
      "get",
      "updated",
      "throughout",
      "lifecycle",
      "discovery",
      "data",
      "import",
      "manual",
      "change",
      "works",
      "successfully",
      "setup.",
      "enabled",
      "federation",
      "established",
      "shows",
      "management.",
      "fully",
      "engine",
      "defined",
      "internal",
      "longer",
      "let",
      "start",
      "first.",
      "per",
      "ci",
      "type",
      "ucmdb.",
      "different",
      "rules.",
      "found",
      "details",
      "tab",
      "type.",
      "find",
      "full",
      "documentation",
      "about",
      "searching",
      "documentation.",
      "simple",
      "way",
      "define",
      "reconciled.",
      "simply",
      "select",
      "scope.",
      "option",
      "allows",
      "comprehensive",
      "logic.",
      "logic",
      "xml",
      "script",
      "consists",
      "following",
      "elements.",
      "broken",
      "two",
      "parts",
      "follows",
      "outlined",
      "process.",
      "set",
      "criteria",
      "all",
      "possible",
      "conditions",
      "candidate"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Publish an asset model as offering",
    "content": "From the main menu, select Build > Service Asset & Configuration. From SACM Home, select Asset Models. Open the asset model record which you want to publish as an offering. Click More > Publish as offering on the toolbar. If the asset model type you want to publish is an Enterprise Asset with User options, there will be a pop-up message to ask you if you want to copy the user options and the default values to the offerings. Note that no matter you click Yes or NO, the rules and any default values defined as an expression won't get published to the offering. You may want to make adjustments to the published offering as needed. A New Offering pop-up window will open for you to input more information. Click Save and a new offering is published. If you are using Enterprise Assets for the first time, there will be more Service Portal users generating requests related to Enterprise Assets. Ensure that you reevaluate the deployment size based on the potential number of concurrent users. Refer",
    "url": "publishassetmodel",
    "filename": "publishassetmodel",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "publish",
      "asset",
      "model",
      "offering",
      "related",
      "topics",
      "main",
      "menu",
      "select",
      "build",
      "service",
      "configuration.",
      "sacm",
      "home",
      "models.",
      "open",
      "record",
      "want",
      "offering.",
      "click",
      "toolbar.",
      "type",
      "enterprise",
      "user",
      "options",
      "there",
      "pop-up",
      "message",
      "ask",
      "copy",
      "default",
      "values",
      "offerings.",
      "note",
      "matter",
      "rules",
      "any",
      "defined",
      "expression",
      "won",
      "get",
      "published",
      "make",
      "adjustments",
      "needed.",
      "new",
      "window",
      "input",
      "information.",
      "save",
      "published.",
      "assets",
      "first",
      "time",
      "portal",
      "users",
      "generating",
      "requests",
      "assets.",
      "ensure",
      "reevaluate",
      "deployment",
      "size",
      "based",
      "potential",
      "number",
      "concurrent",
      "users.",
      "refer",
      "sizing",
      "documents",
      "according",
      "platform",
      "re",
      "increase",
      "hardware",
      "resources",
      "example",
      "cpu",
      "memory",
      "accordingly.",
      "models",
      "view",
      "records",
      "edit",
      "retire"
    ],
    "language": "en",
    "word_count": 102,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "publish an asset model as offering",
    "contentLower": "from the main menu, select build > service asset & configuration. from sacm home, select asset models. open the asset model record which you want to publish as an offering. click more > publish as offering on the toolbar. if the asset model type you want to publish is an enterprise asset with user options, there will be a pop-up message to ask you if you want to copy the user options and the default values to the offerings. note that no matter you click yes or no, the rules and any default values defined as an expression won't get published to the offering. you may want to make adjustments to the published offering as needed. a new offering pop-up window will open for you to input more information. click save and a new offering is published. if you are using enterprise assets for the first time, there will be more service portal users generating requests related to enterprise assets. ensure that you reevaluate the deployment size based on the potential number of concurrent users. refer",
    "keywordsLower": [
      "publish",
      "asset",
      "model",
      "offering",
      "related",
      "topics",
      "main",
      "menu",
      "select",
      "build",
      "service",
      "configuration.",
      "sacm",
      "home",
      "models.",
      "open",
      "record",
      "want",
      "offering.",
      "click",
      "toolbar.",
      "type",
      "enterprise",
      "user",
      "options",
      "there",
      "pop-up",
      "message",
      "ask",
      "copy",
      "default",
      "values",
      "offerings.",
      "note",
      "matter",
      "rules",
      "any",
      "defined",
      "expression",
      "won",
      "get",
      "published",
      "make",
      "adjustments",
      "needed.",
      "new",
      "window",
      "input",
      "information.",
      "save",
      "published.",
      "assets",
      "first",
      "time",
      "portal",
      "users",
      "generating",
      "requests",
      "assets.",
      "ensure",
      "reevaluate",
      "deployment",
      "size",
      "based",
      "potential",
      "number",
      "concurrent",
      "users.",
      "refer",
      "sizing",
      "documents",
      "according",
      "platform",
      "re",
      "increase",
      "hardware",
      "resources",
      "example",
      "cpu",
      "memory",
      "accordingly.",
      "models",
      "view",
      "records",
      "edit",
      "retire"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prepare a survey for conduction",
    "content": "After a survey has been created, various configurations can be made before the survey is conducted. Specify start and end dates for a survey The Survey Editor can optionally specify start and end dates for automatic progression of the survey through the workflow. From the main menu, select Build > Survey. Service Management displays the available surveys. Click the record identifier in the ID column to display the selected record. In the Details section, specify the Active From and Active until dates for the survey. These dates constrain the time period when the survey can be active and data can be collected. If an Active from date is specified, the workflow automatically transitions to Execute phase when this date is reached. In this phase only, business rules are active and answers are being collected. If an Active until date is specified, the workflow automatically transitions to Assess phase when this date is reached. In this phase, no further data is collected. Create a report Sur",
    "url": "preparesurvey",
    "filename": "preparesurvey",
    "headings": [
      "Specify start and end dates for a survey",
      "Create a report",
      "Localize the surveys",
      "Related topics"
    ],
    "keywords": [
      "SurveyQuestion_translations.zip",
      "Survey_fr.zip",
      "CN.csv",
      "Survey_translations.zip",
      "SurveyQuestion_fr.csv",
      "Survey_fr.csv",
      "Survey_tranlations.zip",
      "prepare",
      "survey",
      "conduction",
      "specify",
      "start",
      "end",
      "dates",
      "create",
      "report",
      "localize",
      "surveys",
      "related",
      "topics",
      "after",
      "created",
      "various",
      "configurations",
      "made",
      "before",
      "conducted.",
      "editor",
      "optionally",
      "automatic",
      "progression",
      "through",
      "workflow.",
      "main",
      "menu",
      "select",
      "build",
      "survey.",
      "service",
      "management",
      "displays",
      "available",
      "surveys.",
      "click",
      "record",
      "identifier",
      "id",
      "column",
      "display",
      "selected",
      "record.",
      "details",
      "section",
      "active",
      "until",
      "constrain",
      "time",
      "period",
      "data",
      "collected.",
      "date",
      "specified",
      "workflow",
      "automatically",
      "transitions",
      "execute",
      "phase",
      "reached.",
      "business",
      "rules",
      "answers",
      "assess",
      "further",
      "reports",
      "analytic",
      "over",
      "answers.",
      "all",
      "non-textual",
      "questions",
      "fields",
      "during",
      "creation",
      "included",
      "tenant",
      "quota",
      "calculation.",
      "note",
      "publisher",
      "role",
      "reports.",
      "information",
      "see",
      "tab",
      "add",
      "report.",
      "module",
      "opens.",
      "new",
      "finished"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prepare a survey for conduction",
    "contentLower": "after a survey has been created, various configurations can be made before the survey is conducted. specify start and end dates for a survey the survey editor can optionally specify start and end dates for automatic progression of the survey through the workflow. from the main menu, select build > survey. service management displays the available surveys. click the record identifier in the id column to display the selected record. in the details section, specify the active from and active until dates for the survey. these dates constrain the time period when the survey can be active and data can be collected. if an active from date is specified, the workflow automatically transitions to execute phase when this date is reached. in this phase only, business rules are active and answers are being collected. if an active until date is specified, the workflow automatically transitions to assess phase when this date is reached. in this phase, no further data is collected. create a report sur",
    "keywordsLower": [
      "surveyquestion_translations.zip",
      "survey_fr.zip",
      "cn.csv",
      "survey_translations.zip",
      "surveyquestion_fr.csv",
      "survey_fr.csv",
      "survey_tranlations.zip",
      "prepare",
      "survey",
      "conduction",
      "specify",
      "start",
      "end",
      "dates",
      "create",
      "report",
      "localize",
      "surveys",
      "related",
      "topics",
      "after",
      "created",
      "various",
      "configurations",
      "made",
      "before",
      "conducted.",
      "editor",
      "optionally",
      "automatic",
      "progression",
      "through",
      "workflow.",
      "main",
      "menu",
      "select",
      "build",
      "survey.",
      "service",
      "management",
      "displays",
      "available",
      "surveys.",
      "click",
      "record",
      "identifier",
      "id",
      "column",
      "display",
      "selected",
      "record.",
      "details",
      "section",
      "active",
      "until",
      "constrain",
      "time",
      "period",
      "data",
      "collected.",
      "date",
      "specified",
      "workflow",
      "automatically",
      "transitions",
      "execute",
      "phase",
      "reached.",
      "business",
      "rules",
      "answers",
      "assess",
      "further",
      "reports",
      "analytic",
      "over",
      "answers.",
      "all",
      "non-textual",
      "questions",
      "fields",
      "during",
      "creation",
      "included",
      "tenant",
      "quota",
      "calculation.",
      "note",
      "publisher",
      "role",
      "reports.",
      "information",
      "see",
      "tab",
      "add",
      "report.",
      "module",
      "opens.",
      "new",
      "finished"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Procurement Management",
    "content": "Procurement Management provides you with the following capabilities: Creating and managing vendor catalogs Creating and managing purchase orders Creating a purchase order via a purchase request Receiving assets Related topics Category Links Administer Fields Forms Roles Use Procurement Management roles and permissions Create and manage vendor catalogs Create and manage vendor catalog items Create and manage purchase orders Create and manage purchase order lines Purchase order workflow Purchase order transition rules Create and manage receiving slips View and update receiving lines \"Create purchase order\" business rule End-to-end workflow to fulfill a purchase order created on a request for a vendor catalog item Develop Single record APIs Record bulk update and collection APIs",
    "url": "procurementmgmt",
    "filename": "procurementmgmt",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "procurement",
      "management",
      "related",
      "topics",
      "provides",
      "following",
      "capabilities",
      "creating",
      "managing",
      "vendor",
      "catalogs",
      "purchase",
      "orders",
      "order",
      "via",
      "request",
      "receiving",
      "assets",
      "category",
      "links",
      "administer",
      "fields",
      "forms",
      "roles",
      "permissions",
      "create",
      "manage",
      "catalog",
      "items",
      "lines",
      "workflow",
      "transition",
      "rules",
      "slips",
      "view",
      "update",
      "business",
      "rule",
      "end-to-end",
      "fulfill",
      "created",
      "item",
      "develop",
      "single",
      "record",
      "apis",
      "bulk",
      "collection"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "procurement management",
    "contentLower": "procurement management provides you with the following capabilities: creating and managing vendor catalogs creating and managing purchase orders creating a purchase order via a purchase request receiving assets related topics category links administer fields forms roles use procurement management roles and permissions create and manage vendor catalogs create and manage vendor catalog items create and manage purchase orders create and manage purchase order lines purchase order workflow purchase order transition rules create and manage receiving slips view and update receiving lines \"create purchase order\" business rule end-to-end workflow to fulfill a purchase order created on a request for a vendor catalog item develop single record apis record bulk update and collection apis",
    "keywordsLower": [
      "procurement",
      "management",
      "related",
      "topics",
      "provides",
      "following",
      "capabilities",
      "creating",
      "managing",
      "vendor",
      "catalogs",
      "purchase",
      "orders",
      "order",
      "via",
      "request",
      "receiving",
      "assets",
      "category",
      "links",
      "administer",
      "fields",
      "forms",
      "roles",
      "permissions",
      "create",
      "manage",
      "catalog",
      "items",
      "lines",
      "workflow",
      "transition",
      "rules",
      "slips",
      "view",
      "update",
      "business",
      "rule",
      "end-to-end",
      "fulfill",
      "created",
      "item",
      "develop",
      "single",
      "record",
      "apis",
      "bulk",
      "collection"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Procurement Management roles and permissions",
    "content": "There are specific roles associated with Procurement Management. Service Management uses role-based permissions to enable you to complete a task that's appropriate to your role. The tenant administrator manages and assigns these permissions. By default, the following OOB roles are provided with Procurement Management. Role Responsibilities Procurement Manager create and manage purchase orders acknowledge received goods Vendor Catalog Administrator create and manage vendor catalogs publish and link vendor catalog items as product offerings Review the individual permission assignments for each role in Administration > Master Data > People > Roles. Related topics Roles",
    "url": "procurementroles",
    "filename": "procurementroles",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "procurement",
      "management",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "management.",
      "service",
      "uses",
      "role-based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "tenant",
      "administrator",
      "manages",
      "assigns",
      "permissions.",
      "default",
      "following",
      "oob",
      "provided",
      "role",
      "responsibilities",
      "manager",
      "create",
      "manage",
      "purchase",
      "orders",
      "acknowledge",
      "received",
      "goods",
      "vendor",
      "catalog",
      "catalogs",
      "publish",
      "link",
      "items",
      "product",
      "offerings",
      "review",
      "individual",
      "permission",
      "assignments",
      "administration",
      "master",
      "data",
      "people",
      "roles."
    ],
    "language": "en",
    "word_count": 71,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "procurement management roles and permissions",
    "contentLower": "there are specific roles associated with procurement management. service management uses role-based permissions to enable you to complete a task that's appropriate to your role. the tenant administrator manages and assigns these permissions. by default, the following oob roles are provided with procurement management. role responsibilities procurement manager create and manage purchase orders acknowledge received goods vendor catalog administrator create and manage vendor catalogs publish and link vendor catalog items as product offerings review the individual permission assignments for each role in administration > master data > people > roles. related topics roles",
    "keywordsLower": [
      "procurement",
      "management",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "management.",
      "service",
      "uses",
      "role-based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "tenant",
      "administrator",
      "manages",
      "assigns",
      "permissions.",
      "default",
      "following",
      "oob",
      "provided",
      "role",
      "responsibilities",
      "manager",
      "create",
      "manage",
      "purchase",
      "orders",
      "acknowledge",
      "received",
      "goods",
      "vendor",
      "catalog",
      "catalogs",
      "publish",
      "link",
      "items",
      "product",
      "offerings",
      "review",
      "individual",
      "permission",
      "assignments",
      "administration",
      "master",
      "data",
      "people",
      "roles."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "managevendorcatalogitems",
    "content": "This section provides instructions about the following: View a vendor catalog item To view a vendor catalog item, follow these steps: From the main menu, select Build > Procurement > Vendor Catalog Items. Service Management displays a list of vendor catalog items. Select the check box of a record to view the summary on the right-handed preview pane, or click the ID link to view the details. Create a new vendor catalog item To create a new vendor catalog item, follow these steps: From the main menu, select Build > Procurement > Vendor Catalog Items. Service Management displays a list of vendor catalog items, if any. Click New. Fill in the form. Click SAVE, SAVE & ADD ANOTHER, SAVE & EDIT or CANCEL as needed. Alternatively, you can create a new vendor catalog item from a vendor catalog. To do this, follow these steps: From the main menu, select Build > Procurement > Vendor Catalogs. Service Management displays a list of vendor catalogs. Open the vendor catalog record from which you want ",
    "url": "managevendorcatalogitems",
    "filename": "managevendorcatalogitems",
    "headings": [
      "View a vendor catalog item",
      "Create a new vendor catalog item",
      "Update a vendor catalog item"
    ],
    "keywords": [
      "managevendorcatalogitems",
      "view",
      "vendor",
      "catalog",
      "item",
      "create",
      "new",
      "update",
      "section",
      "provides",
      "instructions",
      "about",
      "following",
      "follow",
      "steps",
      "main",
      "menu",
      "select",
      "build",
      "procurement",
      "items.",
      "service",
      "management",
      "displays",
      "list",
      "check",
      "box",
      "record",
      "summary",
      "right-handed",
      "preview",
      "pane",
      "click",
      "id",
      "link",
      "details.",
      "items",
      "any.",
      "new.",
      "fill",
      "form.",
      "save",
      "add",
      "another",
      "edit",
      "cancel",
      "needed.",
      "alternatively",
      "catalog.",
      "catalogs.",
      "open",
      "want",
      "item.",
      "tab.",
      "save.",
      "single",
      "desired",
      "record.",
      "default",
      "displayed",
      "general",
      "tab",
      "selected.",
      "description",
      "information",
      "current",
      "see",
      "section.",
      "related",
      "offerings",
      "published",
      "through",
      "publish",
      "offering",
      "function.",
      "offering.",
      "discussions",
      "any",
      "relevant",
      "conversations",
      "discussions.",
      "history",
      "changes",
      "selected",
      "history.",
      "details",
      "includes",
      "fields",
      "title",
      "name",
      "brief",
      "included.",
      "brand",
      "asset",
      "model",
      "asset.",
      "normally",
      "built",
      "manufacturer.",
      "owner"
    ],
    "language": "en",
    "word_count": 114,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "managevendorcatalogitems",
    "contentLower": "this section provides instructions about the following: view a vendor catalog item to view a vendor catalog item, follow these steps: from the main menu, select build > procurement > vendor catalog items. service management displays a list of vendor catalog items. select the check box of a record to view the summary on the right-handed preview pane, or click the id link to view the details. create a new vendor catalog item to create a new vendor catalog item, follow these steps: from the main menu, select build > procurement > vendor catalog items. service management displays a list of vendor catalog items, if any. click new. fill in the form. click save, save & add another, save & edit or cancel as needed. alternatively, you can create a new vendor catalog item from a vendor catalog. to do this, follow these steps: from the main menu, select build > procurement > vendor catalogs. service management displays a list of vendor catalogs. open the vendor catalog record from which you want ",
    "keywordsLower": [
      "managevendorcatalogitems",
      "view",
      "vendor",
      "catalog",
      "item",
      "create",
      "new",
      "update",
      "section",
      "provides",
      "instructions",
      "about",
      "following",
      "follow",
      "steps",
      "main",
      "menu",
      "select",
      "build",
      "procurement",
      "items.",
      "service",
      "management",
      "displays",
      "list",
      "check",
      "box",
      "record",
      "summary",
      "right-handed",
      "preview",
      "pane",
      "click",
      "id",
      "link",
      "details.",
      "items",
      "any.",
      "new.",
      "fill",
      "form.",
      "save",
      "add",
      "another",
      "edit",
      "cancel",
      "needed.",
      "alternatively",
      "catalog.",
      "catalogs.",
      "open",
      "want",
      "item.",
      "tab.",
      "save.",
      "single",
      "desired",
      "record.",
      "default",
      "displayed",
      "general",
      "tab",
      "selected.",
      "description",
      "information",
      "current",
      "see",
      "section.",
      "related",
      "offerings",
      "published",
      "through",
      "publish",
      "offering",
      "function.",
      "offering.",
      "discussions",
      "any",
      "relevant",
      "conversations",
      "discussions.",
      "history",
      "changes",
      "selected",
      "history.",
      "details",
      "includes",
      "fields",
      "title",
      "name",
      "brief",
      "included.",
      "brand",
      "asset",
      "model",
      "asset.",
      "normally",
      "built",
      "manufacturer.",
      "owner"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Publish a vendor catalog item as offering",
    "content": "You can publish an offering from a vendor catalog item. Both of the following conditions should be met for a catalog item to appear in the product offering catalog: Publishable to product offering item is enabled for the catalog item. The catalog item should be Active Note Publishing an offering from a vendor catalog item is only required when end users are requesting the service offering. Procurement managers can place an order for a vendor catalog item without publishing it as offering. To publish a vendor catalog item as an offering, follow these steps: From the main menu, select Build > Procurement > Vendor Catalog Items. Open the desired record. By default, the General tab is displayed. On the record toolbar, click More > Publish as offering. The New Offering definition form opens. Fill in the form as needed. Note By default, the display label of the offering is automatically populated and in the format of \"Request <vendor catalog item title>\". Certain information of the vendor ca",
    "url": "publishvendorcatalog",
    "filename": "publishvendorcatalog",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "publish",
      "vendor",
      "catalog",
      "item",
      "offering",
      "related",
      "topics",
      "item.",
      "both",
      "following",
      "conditions",
      "met",
      "appear",
      "product",
      "publishable",
      "enabled",
      "active",
      "note",
      "publishing",
      "required",
      "end",
      "users",
      "requesting",
      "service",
      "offering.",
      "procurement",
      "managers",
      "place",
      "order",
      "follow",
      "steps",
      "main",
      "menu",
      "select",
      "build",
      "items.",
      "open",
      "desired",
      "record.",
      "default",
      "general",
      "tab",
      "displayed.",
      "record",
      "toolbar",
      "click",
      "new",
      "definition",
      "form",
      "opens.",
      "fill",
      "needed.",
      "display",
      "label",
      "automatically",
      "populated",
      "format",
      "request",
      "certain",
      "information",
      "example",
      "value",
      "price",
      "section",
      "synced",
      "once",
      "created",
      "successfully.",
      "there",
      "further",
      "synch",
      "after",
      "initial",
      "publication.",
      "preferred",
      "set",
      "save.",
      "find",
      "link",
      "offerings",
      "asset",
      "model"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "publish a vendor catalog item as offering",
    "contentLower": "you can publish an offering from a vendor catalog item. both of the following conditions should be met for a catalog item to appear in the product offering catalog: publishable to product offering item is enabled for the catalog item. the catalog item should be active note publishing an offering from a vendor catalog item is only required when end users are requesting the service offering. procurement managers can place an order for a vendor catalog item without publishing it as offering. to publish a vendor catalog item as an offering, follow these steps: from the main menu, select build > procurement > vendor catalog items. open the desired record. by default, the general tab is displayed. on the record toolbar, click more > publish as offering. the new offering definition form opens. fill in the form as needed. note by default, the display label of the offering is automatically populated and in the format of \"request <vendor catalog item title>\". certain information of the vendor ca",
    "keywordsLower": [
      "publish",
      "vendor",
      "catalog",
      "item",
      "offering",
      "related",
      "topics",
      "item.",
      "both",
      "following",
      "conditions",
      "met",
      "appear",
      "product",
      "publishable",
      "enabled",
      "active",
      "note",
      "publishing",
      "required",
      "end",
      "users",
      "requesting",
      "service",
      "offering.",
      "procurement",
      "managers",
      "place",
      "order",
      "follow",
      "steps",
      "main",
      "menu",
      "select",
      "build",
      "items.",
      "open",
      "desired",
      "record.",
      "default",
      "general",
      "tab",
      "displayed.",
      "record",
      "toolbar",
      "click",
      "new",
      "definition",
      "form",
      "opens.",
      "fill",
      "needed.",
      "display",
      "label",
      "automatically",
      "populated",
      "format",
      "request",
      "certain",
      "information",
      "example",
      "value",
      "price",
      "section",
      "synced",
      "once",
      "created",
      "successfully.",
      "there",
      "further",
      "synch",
      "after",
      "initial",
      "publication.",
      "preferred",
      "set",
      "save.",
      "find",
      "link",
      "offerings",
      "asset",
      "model"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Purchase orders",
    "content": "This section includes the following: Purchase order workflow Purchase order transition rules Create and manage purchase orders Purchase order line workflow Purchase order line transition rules Create and manage purchase order lines \"Create purchase order\" business rule",
    "url": "purchaseorderhead",
    "filename": "purchaseorderhead",
    "headings": [],
    "keywords": [
      "purchase",
      "orders",
      "section",
      "includes",
      "following",
      "order",
      "workflow",
      "transition",
      "rules",
      "create",
      "manage",
      "line",
      "lines",
      "business",
      "rule"
    ],
    "language": "en",
    "word_count": 35,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "purchase orders",
    "contentLower": "this section includes the following: purchase order workflow purchase order transition rules create and manage purchase orders purchase order line workflow purchase order line transition rules create and manage purchase order lines \"create purchase order\" business rule",
    "keywordsLower": [
      "purchase",
      "orders",
      "section",
      "includes",
      "following",
      "order",
      "workflow",
      "transition",
      "rules",
      "create",
      "manage",
      "line",
      "lines",
      "business",
      "rule"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Purchase order workflow",
    "content": "This section describes the metaphases and subordinate phases in the life cycle of a purchase order. The workflow relies on business rules. Rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. Metaphase: New The purchase order is entered into the system and still awaiting being issued. Phase Transition to next phase Description New Manual transition to any of the following: Issued Suspended Canceled New is a starting point. If the purchase order is reviewed and approved, you can manually move the record to the Issued phase. Note You can't issue a purchase order without any active purchase order lines: A purchase order with new purchase order lines can be issued. A purchase order with both canceled and new purchase order lines can be issued. A purchase order with canceled purchase order lines can't be issued. If the purchase order is pending further confirmation or updates, you can manually move the record to",
    "url": "powflw",
    "filename": "powflw",
    "headings": [
      "Metaphase: New",
      "Metaphase: Order",
      "Metaphase: Fulfilled (End)"
    ],
    "keywords": [
      "purchase",
      "order",
      "workflow",
      "metaphase",
      "new",
      "fulfilled",
      "end",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "order.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "entered",
      "system",
      "still",
      "awaiting",
      "issued.",
      "transition",
      "next",
      "description",
      "manual",
      "any",
      "following",
      "issued",
      "suspended",
      "canceled",
      "starting",
      "point.",
      "reviewed",
      "approved",
      "manually",
      "move",
      "record",
      "note",
      "issue",
      "active",
      "lines",
      "both",
      "pending",
      "further",
      "confirmation",
      "updates",
      "decision",
      "cancel",
      "either",
      "automatic",
      "partially",
      "received",
      "fully",
      "receiving",
      "slip",
      "created",
      "upon",
      "automatically",
      "moved",
      "depending",
      "status.",
      "certain",
      "reasons",
      "example",
      "details",
      "subject",
      "change.",
      "alternatively",
      "moving",
      "go",
      "back",
      "action.",
      "reached",
      "life.",
      "items",
      "ordered",
      "received.",
      "line",
      "once",
      "quantity",
      "equals",
      "quantity.",
      "applicable",
      "all",
      "vendor"
    ],
    "language": "en",
    "word_count": 108,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "purchase order workflow",
    "contentLower": "this section describes the metaphases and subordinate phases in the life cycle of a purchase order. the workflow relies on business rules. rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. metaphase: new the purchase order is entered into the system and still awaiting being issued. phase transition to next phase description new manual transition to any of the following: issued suspended canceled new is a starting point. if the purchase order is reviewed and approved, you can manually move the record to the issued phase. note you can't issue a purchase order without any active purchase order lines: a purchase order with new purchase order lines can be issued. a purchase order with both canceled and new purchase order lines can be issued. a purchase order with canceled purchase order lines can't be issued. if the purchase order is pending further confirmation or updates, you can manually move the record to",
    "keywordsLower": [
      "purchase",
      "order",
      "workflow",
      "metaphase",
      "new",
      "fulfilled",
      "end",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "order.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "entered",
      "system",
      "still",
      "awaiting",
      "issued.",
      "transition",
      "next",
      "description",
      "manual",
      "any",
      "following",
      "issued",
      "suspended",
      "canceled",
      "starting",
      "point.",
      "reviewed",
      "approved",
      "manually",
      "move",
      "record",
      "note",
      "issue",
      "active",
      "lines",
      "both",
      "pending",
      "further",
      "confirmation",
      "updates",
      "decision",
      "cancel",
      "either",
      "automatic",
      "partially",
      "received",
      "fully",
      "receiving",
      "slip",
      "created",
      "upon",
      "automatically",
      "moved",
      "depending",
      "status.",
      "certain",
      "reasons",
      "example",
      "details",
      "subject",
      "change.",
      "alternatively",
      "moving",
      "go",
      "back",
      "action.",
      "reached",
      "life.",
      "items",
      "ordered",
      "received.",
      "line",
      "once",
      "quantity",
      "equals",
      "quantity.",
      "applicable",
      "all",
      "vendor"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Purchase order line workflow",
    "content": "This section describes the metaphases and subordinate phases in the life cycle of a purchase order line. The workflow relies on business rules. Rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. . Metaphase: New The purchase order line is entered into the system and still awaiting being issued. Phase Transition to next phase Description New Manual transition to: Canceled. Automatic transition to any of the following: Issued Suspended Canceled New is a starting point. If the decision is to cancel the purchase order line, you can manually move the record to the Canceled phase. If the associated purchase order is changed to Issued, Suspended or Canceled, the purchase order line is then automatically moved to Issued, Suspended or Canceled respectively. Metaphase: Order Phase Transition to next phase Description Issued Manual transition to either of the following: Canceled Automatic transition to any of the fo",
    "url": "polwflw",
    "filename": "polwflw",
    "headings": [
      "Metaphase: New",
      "Metaphase: Order",
      "Metaphase: Fulfilled (End)"
    ],
    "keywords": [
      "purchase",
      "order",
      "line",
      "workflow",
      "metaphase",
      "new",
      "fulfilled",
      "end",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "line.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "entered",
      "system",
      "still",
      "awaiting",
      "issued.",
      "transition",
      "next",
      "description",
      "manual",
      "canceled.",
      "automatic",
      "any",
      "following",
      "issued",
      "suspended",
      "canceled",
      "starting",
      "point.",
      "decision",
      "cancel",
      "manually",
      "move",
      "record",
      "associated",
      "changed",
      "automatically",
      "moved",
      "respectively.",
      "either",
      "partially",
      "received",
      "fully",
      "receiving",
      "slip",
      "created",
      "upon",
      "depending",
      "status.",
      "certain",
      "reasons",
      "example",
      "details",
      "subject",
      "change.",
      "reached",
      "life.",
      "items",
      "ordered",
      "received.",
      "once",
      "quantity",
      "equals",
      "quantity.",
      "applicable",
      "all",
      "vendor",
      "fulfill",
      "purchaser",
      "order."
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "purchase order line workflow",
    "contentLower": "this section describes the metaphases and subordinate phases in the life cycle of a purchase order line. the workflow relies on business rules. rules repeat from one phase to another when the end user can make a change to a field affected by a business rule during that phase. . metaphase: new the purchase order line is entered into the system and still awaiting being issued. phase transition to next phase description new manual transition to: canceled. automatic transition to any of the following: issued suspended canceled new is a starting point. if the decision is to cancel the purchase order line, you can manually move the record to the canceled phase. if the associated purchase order is changed to issued, suspended or canceled, the purchase order line is then automatically moved to issued, suspended or canceled respectively. metaphase: order phase transition to next phase description issued manual transition to either of the following: canceled automatic transition to any of the fo",
    "keywordsLower": [
      "purchase",
      "order",
      "line",
      "workflow",
      "metaphase",
      "new",
      "fulfilled",
      "end",
      "section",
      "describes",
      "metaphases",
      "subordinate",
      "phases",
      "life",
      "cycle",
      "line.",
      "relies",
      "business",
      "rules.",
      "rules",
      "repeat",
      "one",
      "phase",
      "another",
      "user",
      "make",
      "change",
      "field",
      "affected",
      "rule",
      "during",
      "phase.",
      "entered",
      "system",
      "still",
      "awaiting",
      "issued.",
      "transition",
      "next",
      "description",
      "manual",
      "canceled.",
      "automatic",
      "any",
      "following",
      "issued",
      "suspended",
      "canceled",
      "starting",
      "point.",
      "decision",
      "cancel",
      "manually",
      "move",
      "record",
      "associated",
      "changed",
      "automatically",
      "moved",
      "respectively.",
      "either",
      "partially",
      "received",
      "fully",
      "receiving",
      "slip",
      "created",
      "upon",
      "depending",
      "status.",
      "certain",
      "reasons",
      "example",
      "details",
      "subject",
      "change.",
      "reached",
      "life.",
      "items",
      "ordered",
      "received.",
      "once",
      "quantity",
      "equals",
      "quantity.",
      "applicable",
      "all",
      "vendor",
      "fulfill",
      "purchaser",
      "order."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "managereceivingslip",
    "content": "This section provides instructions about the following: View a receiving slip To view a receiving slip, follow these steps: From the main menu, select Build > Procurement > Receiving Slips. Service Management displays a list of receiving slips. Select the check box of a record to view the summary on the right-handed preview pane, or click the ID link to view the details. Create a new receiving slip You can create a new receiving slip in either way: Create a receiving slip from the Receiving Slips page To create a new receiving slip from the Receiving Slips page, follow these steps: From the main menu, select Build > Procurement > Receiving Slips. Click Receiving on PO. From the Purchase Order drop-down list that opens, select a purchase order on which you want to create the receiving slip. Fill in the form. Section Description General details Specify general information about the current record. Receiving slip title. Mandatory. By default, the receiving slip title is automatically popu",
    "url": "managereceivingslip",
    "filename": "managereceivingslip",
    "headings": [
      "View a receiving slip",
      "Create a new receiving slip",
      "Create a receiving slip from the Receiving Slips page",
      "Add receiving lines form",
      "Create a receiving slip from the Purchase Orders page",
      "Update a receiving slip"
    ],
    "keywords": [
      "managereceivingslip",
      "view",
      "receiving",
      "slip",
      "create",
      "new",
      "slips",
      "page",
      "add",
      "lines",
      "form",
      "purchase",
      "orders",
      "update",
      "section",
      "provides",
      "instructions",
      "about",
      "following",
      "follow",
      "steps",
      "main",
      "menu",
      "select",
      "build",
      "procurement",
      "slips.",
      "service",
      "management",
      "displays",
      "list",
      "check",
      "box",
      "record",
      "summary",
      "right-handed",
      "preview",
      "pane",
      "click",
      "id",
      "link",
      "details.",
      "either",
      "way",
      "po.",
      "order",
      "drop-down",
      "opens",
      "want",
      "slip.",
      "fill",
      "form.",
      "description",
      "general",
      "details",
      "specify",
      "information",
      "current",
      "record.",
      "title.",
      "mandatory.",
      "default",
      "title",
      "automatically",
      "populated",
      "starting",
      "received",
      "format",
      "modify",
      "suit",
      "needs.",
      "receive",
      "reference",
      "type",
      "field",
      "read-only",
      "value",
      "order.",
      "vendor.",
      "vendor",
      "specified",
      "associated",
      "line.",
      "method",
      "direct",
      "stock",
      "deliveries",
      "use.",
      "selecting",
      "displayed",
      "status",
      "items",
      "enterprise",
      "asset",
      "process",
      "stationary",
      "item",
      "ready",
      "license",
      "created"
    ],
    "language": "en",
    "word_count": 110,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "managereceivingslip",
    "contentLower": "this section provides instructions about the following: view a receiving slip to view a receiving slip, follow these steps: from the main menu, select build > procurement > receiving slips. service management displays a list of receiving slips. select the check box of a record to view the summary on the right-handed preview pane, or click the id link to view the details. create a new receiving slip you can create a new receiving slip in either way: create a receiving slip from the receiving slips page to create a new receiving slip from the receiving slips page, follow these steps: from the main menu, select build > procurement > receiving slips. click receiving on po. from the purchase order drop-down list that opens, select a purchase order on which you want to create the receiving slip. fill in the form. section description general details specify general information about the current record. receiving slip title. mandatory. by default, the receiving slip title is automatically popu",
    "keywordsLower": [
      "managereceivingslip",
      "view",
      "receiving",
      "slip",
      "create",
      "new",
      "slips",
      "page",
      "add",
      "lines",
      "form",
      "purchase",
      "orders",
      "update",
      "section",
      "provides",
      "instructions",
      "about",
      "following",
      "follow",
      "steps",
      "main",
      "menu",
      "select",
      "build",
      "procurement",
      "slips.",
      "service",
      "management",
      "displays",
      "list",
      "check",
      "box",
      "record",
      "summary",
      "right-handed",
      "preview",
      "pane",
      "click",
      "id",
      "link",
      "details.",
      "either",
      "way",
      "po.",
      "order",
      "drop-down",
      "opens",
      "want",
      "slip.",
      "fill",
      "form.",
      "description",
      "general",
      "details",
      "specify",
      "information",
      "current",
      "record.",
      "title.",
      "mandatory.",
      "default",
      "title",
      "automatically",
      "populated",
      "starting",
      "received",
      "format",
      "modify",
      "suit",
      "needs.",
      "receive",
      "reference",
      "type",
      "field",
      "read-only",
      "value",
      "order.",
      "vendor.",
      "vendor",
      "specified",
      "associated",
      "line.",
      "method",
      "direct",
      "stock",
      "deliveries",
      "use.",
      "selecting",
      "displayed",
      "status",
      "items",
      "enterprise",
      "asset",
      "process",
      "stationary",
      "item",
      "ready",
      "license",
      "created"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Open an existing request in Live Support",
    "content": "Live Support can open existing requests. To go to the Live Support page, select Run > Service Request > Live Support. To open an existing request: Find the request. If the request you want isn't automatically displayed, you can find it in one of the following ways: Type the ID in the Request ID box at the top of the page. Select the user, and choose from the list of recent requests. Select the user, and type the ID or keywords in the search box at the top of the list of recent requests. Select the request. Click . Service Management displays the request details. Related topics How to create a request in Live Support How to view the caller's details in Live Support Keyboard shortcuts Live Support How to use the chat capability",
    "url": "livesupportopenrequest",
    "filename": "livesupportopenrequest",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "open",
      "existing",
      "request",
      "live",
      "support",
      "related",
      "topics",
      "requests.",
      "go",
      "page",
      "select",
      "run",
      "service",
      "support.",
      "find",
      "request.",
      "want",
      "isn",
      "automatically",
      "displayed",
      "one",
      "following",
      "ways",
      "type",
      "id",
      "box",
      "top",
      "page.",
      "user",
      "choose",
      "list",
      "recent",
      "keywords",
      "search",
      "click",
      "management",
      "displays",
      "details.",
      "create",
      "view",
      "caller",
      "details",
      "keyboard",
      "shortcuts",
      "chat",
      "capability"
    ],
    "language": "en",
    "word_count": 85,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "open an existing request in live support",
    "contentLower": "live support can open existing requests. to go to the live support page, select run > service request > live support. to open an existing request: find the request. if the request you want isn't automatically displayed, you can find it in one of the following ways: type the id in the request id box at the top of the page. select the user, and choose from the list of recent requests. select the user, and type the id or keywords in the search box at the top of the list of recent requests. select the request. click . service management displays the request details. related topics how to create a request in live support how to view the caller's details in live support keyboard shortcuts live support how to use the chat capability",
    "keywordsLower": [
      "open",
      "existing",
      "request",
      "live",
      "support",
      "related",
      "topics",
      "requests.",
      "go",
      "page",
      "select",
      "run",
      "service",
      "support.",
      "find",
      "request.",
      "want",
      "isn",
      "automatically",
      "displayed",
      "one",
      "following",
      "ways",
      "type",
      "id",
      "box",
      "top",
      "page.",
      "user",
      "choose",
      "list",
      "recent",
      "keywords",
      "search",
      "click",
      "management",
      "displays",
      "details.",
      "create",
      "view",
      "caller",
      "details",
      "keyboard",
      "shortcuts",
      "chat",
      "capability"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Public requests",
    "content": "When configured, this feature allows a user to create a public request. Depending on the Public scope setting, in the portal, users with the same issue can see the request, and ask to follow it. Subject to configuration: Public requests can be shared with, and tracked by, users affected by the same issue. Public requests can be followed by multiple users. Public requests can be solved by other users. Solved requests can be leveraged as knowledge. Note In addition, a user can create a request, and add other users as followers, without having to make the request a public request. Notifications to followers A follower receives notifications about comments on the request, and any solution. This notification must be configured by special business rules for comments in the request workflow. To view the rules: From the Main menu, select Administration > Configuration > Studio > Processes and Rules. In the drop-down at the top of the page, select Request. In the pane at the left of the page, s",
    "url": "publicrequests",
    "filename": "publicrequests",
    "headings": [
      "Notifications to followers",
      "Benefits",
      "Display",
      "Public scope",
      "Location",
      "Organization",
      "Location and organization",
      "Related topics"
    ],
    "keywords": [
      "public",
      "requests",
      "notifications",
      "followers",
      "benefits",
      "display",
      "scope",
      "location",
      "organization",
      "related",
      "topics",
      "configured",
      "feature",
      "allows",
      "user",
      "create",
      "request.",
      "depending",
      "setting",
      "portal",
      "users",
      "same",
      "issue",
      "see",
      "request",
      "ask",
      "follow",
      "it.",
      "subject",
      "configuration",
      "shared",
      "tracked",
      "affected",
      "issue.",
      "followed",
      "multiple",
      "users.",
      "solved",
      "leveraged",
      "knowledge.",
      "note",
      "addition",
      "add",
      "having",
      "make",
      "follower",
      "receives",
      "about",
      "comments",
      "any",
      "solution.",
      "notification",
      "special",
      "business",
      "rules",
      "workflow.",
      "view",
      "main",
      "menu",
      "select",
      "administration",
      "studio",
      "processes",
      "rules.",
      "drop-down",
      "top",
      "page",
      "pane",
      "left",
      "open",
      "after",
      "applying",
      "changes",
      "section",
      "following",
      "send",
      "followers.",
      "change",
      "solutions",
      "information",
      "notifications.",
      "comment",
      "contribute",
      "include",
      "reduction",
      "elimination",
      "duplicate",
      "unnecessary",
      "tickets.",
      "stakeholders",
      "track",
      "enabling",
      "crowd",
      "sourcing",
      "solutions.",
      "portal.",
      "perform",
      "searches",
      "service",
      "similar"
    ],
    "language": "en",
    "word_count": 95,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "public requests",
    "contentLower": "when configured, this feature allows a user to create a public request. depending on the public scope setting, in the portal, users with the same issue can see the request, and ask to follow it. subject to configuration: public requests can be shared with, and tracked by, users affected by the same issue. public requests can be followed by multiple users. public requests can be solved by other users. solved requests can be leveraged as knowledge. note in addition, a user can create a request, and add other users as followers, without having to make the request a public request. notifications to followers a follower receives notifications about comments on the request, and any solution. this notification must be configured by special business rules for comments in the request workflow. to view the rules: from the main menu, select administration > configuration > studio > processes and rules. in the drop-down at the top of the page, select request. in the pane at the left of the page, s",
    "keywordsLower": [
      "public",
      "requests",
      "notifications",
      "followers",
      "benefits",
      "display",
      "scope",
      "location",
      "organization",
      "related",
      "topics",
      "configured",
      "feature",
      "allows",
      "user",
      "create",
      "request.",
      "depending",
      "setting",
      "portal",
      "users",
      "same",
      "issue",
      "see",
      "request",
      "ask",
      "follow",
      "it.",
      "subject",
      "configuration",
      "shared",
      "tracked",
      "affected",
      "issue.",
      "followed",
      "multiple",
      "users.",
      "solved",
      "leveraged",
      "knowledge.",
      "note",
      "addition",
      "add",
      "having",
      "make",
      "follower",
      "receives",
      "about",
      "comments",
      "any",
      "solution.",
      "notification",
      "special",
      "business",
      "rules",
      "workflow.",
      "view",
      "main",
      "menu",
      "select",
      "administration",
      "studio",
      "processes",
      "rules.",
      "drop-down",
      "top",
      "page",
      "pane",
      "left",
      "open",
      "after",
      "applying",
      "changes",
      "section",
      "following",
      "send",
      "followers.",
      "change",
      "solutions",
      "information",
      "notifications.",
      "comment",
      "contribute",
      "include",
      "reduction",
      "elimination",
      "duplicate",
      "unnecessary",
      "tickets.",
      "stakeholders",
      "track",
      "enabling",
      "crowd",
      "sourcing",
      "solutions.",
      "portal.",
      "perform",
      "searches",
      "service",
      "similar"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Problem Management",
    "content": "Problem Management is a process that minimizes the effect of errors in infrastructure or service delivery on customers. It enables you to diagnose and correct flaws in your Information Technology (IT) infrastructure to obtain the highest possible stability in IT Service Delivery. You can use Service Management Problem Management to identify the underlying reasons for one or more related incidents. You can create workarounds, identify known errors, and define permanent solutions that minimize the effects of these related incidents. Problem Management can't only reduce the volume of incidents but also save time and money. Problem Management enables you to: Identify, record, track, and resolve problems. Prevent recurrence of the same problems. Create automatic alerts and notifications when a problem, task, or known error creates a new problem, or the problem owner or status changes. Escalate problems automatically when they aren't resolved timely. Record solutions and make them available ",
    "url": "problemmgmt",
    "filename": "problemmgmt",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "problem",
      "management",
      "related",
      "topics",
      "process",
      "minimizes",
      "effect",
      "errors",
      "infrastructure",
      "service",
      "delivery",
      "customers.",
      "enables",
      "diagnose",
      "correct",
      "flaws",
      "information",
      "technology",
      "obtain",
      "highest",
      "possible",
      "stability",
      "delivery.",
      "identify",
      "underlying",
      "reasons",
      "one",
      "incidents.",
      "create",
      "workarounds",
      "known",
      "define",
      "permanent",
      "solutions",
      "minimize",
      "effects",
      "reduce",
      "volume",
      "incidents",
      "save",
      "time",
      "money.",
      "record",
      "track",
      "resolve",
      "problems.",
      "prevent",
      "recurrence",
      "same",
      "automatic",
      "alerts",
      "notifications",
      "task",
      "error",
      "creates",
      "new",
      "owner",
      "status",
      "changes.",
      "escalate",
      "problems",
      "automatically",
      "aren",
      "resolved",
      "timely.",
      "make",
      "available",
      "affected",
      "user",
      "groups.",
      "find",
      "opportunities",
      "improvements.",
      "react",
      "aggressively",
      "relate",
      "proactively",
      "issues",
      "before",
      "occur.",
      "number",
      "assets",
      "configuration",
      "items.",
      "note",
      "work",
      "data",
      "domains",
      "want",
      "manually",
      "change",
      "existing",
      "domain",
      "assignments",
      "first",
      "add",
      "field",
      "relevant",
      "forms.",
      "adding"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "problem management",
    "contentLower": "problem management is a process that minimizes the effect of errors in infrastructure or service delivery on customers. it enables you to diagnose and correct flaws in your information technology (it) infrastructure to obtain the highest possible stability in it service delivery. you can use service management problem management to identify the underlying reasons for one or more related incidents. you can create workarounds, identify known errors, and define permanent solutions that minimize the effects of these related incidents. problem management can't only reduce the volume of incidents but also save time and money. problem management enables you to: identify, record, track, and resolve problems. prevent recurrence of the same problems. create automatic alerts and notifications when a problem, task, or known error creates a new problem, or the problem owner or status changes. escalate problems automatically when they aren't resolved timely. record solutions and make them available ",
    "keywordsLower": [
      "problem",
      "management",
      "related",
      "topics",
      "process",
      "minimizes",
      "effect",
      "errors",
      "infrastructure",
      "service",
      "delivery",
      "customers.",
      "enables",
      "diagnose",
      "correct",
      "flaws",
      "information",
      "technology",
      "obtain",
      "highest",
      "possible",
      "stability",
      "delivery.",
      "identify",
      "underlying",
      "reasons",
      "one",
      "incidents.",
      "create",
      "workarounds",
      "known",
      "define",
      "permanent",
      "solutions",
      "minimize",
      "effects",
      "reduce",
      "volume",
      "incidents",
      "save",
      "time",
      "money.",
      "record",
      "track",
      "resolve",
      "problems.",
      "prevent",
      "recurrence",
      "same",
      "automatic",
      "alerts",
      "notifications",
      "task",
      "error",
      "creates",
      "new",
      "owner",
      "status",
      "changes.",
      "escalate",
      "problems",
      "automatically",
      "aren",
      "resolved",
      "timely.",
      "make",
      "available",
      "affected",
      "user",
      "groups.",
      "find",
      "opportunities",
      "improvements.",
      "react",
      "aggressively",
      "relate",
      "proactively",
      "issues",
      "before",
      "occur.",
      "number",
      "assets",
      "configuration",
      "items.",
      "note",
      "work",
      "data",
      "domains",
      "want",
      "manually",
      "change",
      "existing",
      "domain",
      "assignments",
      "first",
      "add",
      "field",
      "relevant",
      "forms.",
      "adding"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Problem Management roles and permissions",
    "content": "There are specific roles associated with Problem Management. Service Management uses role-based permissions to enable you to complete a task that's appropriate to your role. System Administrators manage and assign these permissions. By default, there are the following roles assigned to Problem Management. Role Responsibilities Problem Analyst Investigate and diagnose assigned problems for workarounds or root causes. Review, then accept or reject assigned known errors. Investigate and diagnose assigned known errors and propose solutions and workarounds. Implement corrective actions and close known errors. Problem Coordinator Periodically analyze whether to create new problem records. Create problem records. Assign work to Problem Analysts and coordinate root cause analysis. Create known error records. Communicate with the Problem Manager. Assign known error to the Problem Analyst. Validate proposed solutions to known errors. Validate outcome of closed changes and close known errors. Val",
    "url": "pmroles",
    "filename": "pmroles",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "problem",
      "management",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "management.",
      "service",
      "uses",
      "role-based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "system",
      "administrators",
      "manage",
      "assign",
      "permissions.",
      "default",
      "following",
      "assigned",
      "role",
      "responsibilities",
      "analyst",
      "investigate",
      "diagnose",
      "problems",
      "workarounds",
      "root",
      "causes.",
      "review",
      "accept",
      "reject",
      "known",
      "errors.",
      "errors",
      "propose",
      "solutions",
      "workarounds.",
      "implement",
      "corrective",
      "actions",
      "close",
      "coordinator",
      "periodically",
      "analyze",
      "whether",
      "create",
      "new",
      "records.",
      "work",
      "analysts",
      "coordinate",
      "cause",
      "analysis.",
      "error",
      "communicate",
      "manager.",
      "analyst.",
      "validate",
      "proposed",
      "outcome",
      "closed",
      "changes",
      "solved.",
      "manager",
      "prioritize",
      "plan",
      "created",
      "coordinators.",
      "stakeholders",
      "required.",
      "change",
      "defer",
      "necessary.",
      "investigation",
      "request",
      "requests",
      "solve",
      "conduct",
      "reviews",
      "document",
      "lessons",
      "learned.",
      "stakeholders.",
      "monitor",
      "problems.",
      "process",
      "owner",
      "sponsors",
      "designs",
      "process.",
      "ensures",
      "compliance",
      "enterprise"
    ],
    "language": "en",
    "word_count": 109,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "problem management roles and permissions",
    "contentLower": "there are specific roles associated with problem management. service management uses role-based permissions to enable you to complete a task that's appropriate to your role. system administrators manage and assign these permissions. by default, there are the following roles assigned to problem management. role responsibilities problem analyst investigate and diagnose assigned problems for workarounds or root causes. review, then accept or reject assigned known errors. investigate and diagnose assigned known errors and propose solutions and workarounds. implement corrective actions and close known errors. problem coordinator periodically analyze whether to create new problem records. create problem records. assign work to problem analysts and coordinate root cause analysis. create known error records. communicate with the problem manager. assign known error to the problem analyst. validate proposed solutions to known errors. validate outcome of closed changes and close known errors. val",
    "keywordsLower": [
      "problem",
      "management",
      "roles",
      "permissions",
      "related",
      "topics",
      "there",
      "specific",
      "associated",
      "management.",
      "service",
      "uses",
      "role-based",
      "enable",
      "complete",
      "task",
      "appropriate",
      "role.",
      "system",
      "administrators",
      "manage",
      "assign",
      "permissions.",
      "default",
      "following",
      "assigned",
      "role",
      "responsibilities",
      "analyst",
      "investigate",
      "diagnose",
      "problems",
      "workarounds",
      "root",
      "causes.",
      "review",
      "accept",
      "reject",
      "known",
      "errors.",
      "errors",
      "propose",
      "solutions",
      "workarounds.",
      "implement",
      "corrective",
      "actions",
      "close",
      "coordinator",
      "periodically",
      "analyze",
      "whether",
      "create",
      "new",
      "records.",
      "work",
      "analysts",
      "coordinate",
      "cause",
      "analysis.",
      "error",
      "communicate",
      "manager.",
      "analyst.",
      "validate",
      "proposed",
      "outcome",
      "closed",
      "changes",
      "solved.",
      "manager",
      "prioritize",
      "plan",
      "created",
      "coordinators.",
      "stakeholders",
      "required.",
      "change",
      "defer",
      "necessary.",
      "investigation",
      "request",
      "requests",
      "solve",
      "conduct",
      "reviews",
      "document",
      "lessons",
      "learned.",
      "stakeholders.",
      "monitor",
      "problems.",
      "process",
      "owner",
      "sponsors",
      "designs",
      "process.",
      "ensures",
      "compliance",
      "enterprise"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Problem workflow",
    "content": "Problem management investigates incidents, determines causes, and provides solutions. It's a process that minimizes the impact on customers of errors in infrastructure, services, and external events. The focus is to diagnose and rectify faults in the IT infrastructure, to obtain the highest possible stability in IT Service Delivery. In Service Management, a workflow is the end-to-end process of problem management, from the problem creation to the problem closure. The Service Management problem workflow includes all necessary steps to log and resolve a problem, and is a best practice for problem management, reflecting ITILv3 and process recommendations. The building blocks of the workflow are metaphases, phases and transitions. Service Management displays a graphic view of the workflow where you can see the current phase and the transitions that connect the current phase to all other phases. The Problem Management workflow contains four metaphases and six phases that lead to closure. Wh",
    "url": "pmwflw",
    "filename": "pmwflw",
    "headings": [
      "Metaphase: Classification",
      "Metaphase: Resolution",
      "Metaphase: Validation",
      "Metaphase: Done (End)",
      "Related topics"
    ],
    "keywords": [
      "problem",
      "workflow",
      "metaphase",
      "classification",
      "resolution",
      "validation",
      "done",
      "end",
      "related",
      "topics",
      "management",
      "investigates",
      "incidents",
      "determines",
      "causes",
      "provides",
      "solutions.",
      "process",
      "minimizes",
      "impact",
      "customers",
      "errors",
      "infrastructure",
      "services",
      "external",
      "events.",
      "focus",
      "diagnose",
      "rectify",
      "faults",
      "obtain",
      "highest",
      "possible",
      "stability",
      "service",
      "delivery.",
      "end-to-end",
      "creation",
      "closure.",
      "includes",
      "all",
      "necessary",
      "steps",
      "log",
      "resolve",
      "best",
      "practice",
      "reflecting",
      "itilv3",
      "recommendations.",
      "building",
      "blocks",
      "metaphases",
      "phases",
      "transitions.",
      "displays",
      "graphic",
      "view",
      "see",
      "current",
      "phase",
      "transitions",
      "connect",
      "phases.",
      "contains",
      "four",
      "six",
      "lead",
      "update",
      "record",
      "assign",
      "new",
      "status",
      "transition",
      "one",
      "next",
      "automatically.",
      "description",
      "automatic",
      "analyst",
      "enters",
      "detailed",
      "information",
      "about",
      "problem.",
      "classify",
      "manual",
      "manager",
      "verifies",
      "provided",
      "performs",
      "initial",
      "analysis",
      "selects",
      "category",
      "assigns",
      "analyst.",
      "investigate",
      "abandon",
      "analyzes"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "problem workflow",
    "contentLower": "problem management investigates incidents, determines causes, and provides solutions. it's a process that minimizes the impact on customers of errors in infrastructure, services, and external events. the focus is to diagnose and rectify faults in the it infrastructure, to obtain the highest possible stability in it service delivery. in service management, a workflow is the end-to-end process of problem management, from the problem creation to the problem closure. the service management problem workflow includes all necessary steps to log and resolve a problem, and is a best practice for problem management, reflecting itilv3 and process recommendations. the building blocks of the workflow are metaphases, phases and transitions. service management displays a graphic view of the workflow where you can see the current phase and the transitions that connect the current phase to all other phases. the problem management workflow contains four metaphases and six phases that lead to closure. wh",
    "keywordsLower": [
      "problem",
      "workflow",
      "metaphase",
      "classification",
      "resolution",
      "validation",
      "done",
      "end",
      "related",
      "topics",
      "management",
      "investigates",
      "incidents",
      "determines",
      "causes",
      "provides",
      "solutions.",
      "process",
      "minimizes",
      "impact",
      "customers",
      "errors",
      "infrastructure",
      "services",
      "external",
      "events.",
      "focus",
      "diagnose",
      "rectify",
      "faults",
      "obtain",
      "highest",
      "possible",
      "stability",
      "service",
      "delivery.",
      "end-to-end",
      "creation",
      "closure.",
      "includes",
      "all",
      "necessary",
      "steps",
      "log",
      "resolve",
      "best",
      "practice",
      "reflecting",
      "itilv3",
      "recommendations.",
      "building",
      "blocks",
      "metaphases",
      "phases",
      "transitions.",
      "displays",
      "graphic",
      "view",
      "see",
      "current",
      "phase",
      "transitions",
      "connect",
      "phases.",
      "contains",
      "four",
      "six",
      "lead",
      "update",
      "record",
      "assign",
      "new",
      "status",
      "transition",
      "one",
      "next",
      "automatically.",
      "description",
      "automatic",
      "analyst",
      "enters",
      "detailed",
      "information",
      "about",
      "problem.",
      "classify",
      "manual",
      "manager",
      "verifies",
      "provided",
      "performs",
      "initial",
      "analysis",
      "selects",
      "category",
      "assigns",
      "analyst.",
      "investigate",
      "abandon",
      "analyzes"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Problem notification rules",
    "content": "Service Management sends an email notification to designated users when a business rule triggers a notification event. The following table describes the event trigger, identifies the email recipient, and identifies the information contained in the notification. The default business rules define the recipients according to the user or group identified in the problem record, but an administrator must first do the following: Assign the appropriate Problem Management roles to the named users Populate groups with users who also have the appropriate roles to add, change, or update problems. Event Conditions Recipients Email contains this information Open Classification > Classify ActualService.Owner Group upn 'Problem_Manager_Group' Priority, service, category, status, symptoms Assign group Resolution / Validation / Classification > Classify Problem owner changed or Problem owning group changed. Problem owner is null and Problem owing group is not null. Problem owning group Priority, service",
    "url": "pmnotifs",
    "filename": "pmnotifs",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "problem",
      "notification",
      "rules",
      "related",
      "topics",
      "service",
      "management",
      "sends",
      "email",
      "designated",
      "users",
      "business",
      "rule",
      "triggers",
      "event.",
      "following",
      "table",
      "describes",
      "event",
      "trigger",
      "identifies",
      "recipient",
      "information",
      "contained",
      "notification.",
      "default",
      "define",
      "recipients",
      "according",
      "user",
      "group",
      "identified",
      "record",
      "administrator",
      "first",
      "assign",
      "appropriate",
      "roles",
      "named",
      "populate",
      "groups",
      "add",
      "change",
      "update",
      "problems.",
      "conditions",
      "contains",
      "open",
      "classification",
      "classify",
      "actualservice.owner",
      "upn",
      "priority",
      "category",
      "status",
      "symptoms",
      "resolution",
      "validation",
      "owner",
      "changed",
      "owning",
      "changed.",
      "null",
      "owing",
      "null.",
      "pending",
      "assignment",
      "workaround",
      "new",
      "value",
      "problemcausedbyrequest",
      "active",
      "true",
      ".assignedtoperson",
      "incidentcausedbyproblem",
      ".assignedperson",
      "expectedresolutiontime",
      "expected",
      "date",
      "resolved",
      "review",
      "root",
      "cause",
      "solution",
      "close",
      "done",
      "recorded",
      "results",
      "abandon",
      "note",
      "means",
      "caused",
      "incident.",
      "request",
      "problem.",
      "about",
      "customizing",
      "templates",
      "see",
      "notifications."
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "problem notification rules",
    "contentLower": "service management sends an email notification to designated users when a business rule triggers a notification event. the following table describes the event trigger, identifies the email recipient, and identifies the information contained in the notification. the default business rules define the recipients according to the user or group identified in the problem record, but an administrator must first do the following: assign the appropriate problem management roles to the named users populate groups with users who also have the appropriate roles to add, change, or update problems. event conditions recipients email contains this information open classification > classify actualservice.owner group upn 'problem_manager_group' priority, service, category, status, symptoms assign group resolution / validation / classification > classify problem owner changed or problem owning group changed. problem owner is null and problem owing group is not null. problem owning group priority, service",
    "keywordsLower": [
      "problem",
      "notification",
      "rules",
      "related",
      "topics",
      "service",
      "management",
      "sends",
      "email",
      "designated",
      "users",
      "business",
      "rule",
      "triggers",
      "event.",
      "following",
      "table",
      "describes",
      "event",
      "trigger",
      "identifies",
      "recipient",
      "information",
      "contained",
      "notification.",
      "default",
      "define",
      "recipients",
      "according",
      "user",
      "group",
      "identified",
      "record",
      "administrator",
      "first",
      "assign",
      "appropriate",
      "roles",
      "named",
      "populate",
      "groups",
      "add",
      "change",
      "update",
      "problems.",
      "conditions",
      "contains",
      "open",
      "classification",
      "classify",
      "actualservice.owner",
      "upn",
      "priority",
      "category",
      "status",
      "symptoms",
      "resolution",
      "validation",
      "owner",
      "changed",
      "owning",
      "changed.",
      "null",
      "owing",
      "null.",
      "pending",
      "assignment",
      "workaround",
      "new",
      "value",
      "problemcausedbyrequest",
      "active",
      "true",
      ".assignedtoperson",
      "incidentcausedbyproblem",
      ".assignedperson",
      "expectedresolutiontime",
      "expected",
      "date",
      "resolved",
      "review",
      "root",
      "cause",
      "solution",
      "close",
      "done",
      "recorded",
      "results",
      "abandon",
      "note",
      "means",
      "caused",
      "incident.",
      "request",
      "problem.",
      "about",
      "customizing",
      "templates",
      "see",
      "notifications."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Problem Management procedures",
    "content": "Problem Management procedures help you deal with the complete lifecycle of problem records. Related topics How to create a problem record How to edit a problem record How to relate problem records How to create a duplicate problem link How to create a record from a problem",
    "url": "pmbasictasks",
    "filename": "pmbasictasks",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "problem",
      "management",
      "procedures",
      "related",
      "topics",
      "help",
      "deal",
      "complete",
      "lifecycle",
      "records.",
      "create",
      "record",
      "edit",
      "relate",
      "records",
      "duplicate",
      "link"
    ],
    "language": "en",
    "word_count": 30,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "problem management procedures",
    "contentLower": "problem management procedures help you deal with the complete lifecycle of problem records. related topics how to create a problem record how to edit a problem record how to relate problem records how to create a duplicate problem link how to create a record from a problem",
    "keywordsLower": [
      "problem",
      "management",
      "procedures",
      "related",
      "topics",
      "help",
      "deal",
      "complete",
      "lifecycle",
      "records.",
      "create",
      "record",
      "edit",
      "relate",
      "records",
      "duplicate",
      "link"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Problem templates",
    "content": "A Service Management template is a group of pre-populated fields that you can use as a quick start to create a new record or apply to an existing record. You can use a template to achieve consistency when you process records that have the same workflow. You can create a template with either of these methods: Save an existing record (such as an incident, problem, or change) as a template. For example, you can save a change record as a change template. Create a new template, where you decide which fields should have pre-populated values. Related topics How to create a problem template How to edit a problem template How to delete a problem template How to apply a problem template",
    "url": "pblmtemplates",
    "filename": "pblmtemplates",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "problem",
      "templates",
      "related",
      "topics",
      "service",
      "management",
      "template",
      "group",
      "pre-populated",
      "fields",
      "quick",
      "start",
      "create",
      "new",
      "record",
      "apply",
      "existing",
      "record.",
      "achieve",
      "consistency",
      "process",
      "records",
      "same",
      "workflow.",
      "either",
      "methods",
      "save",
      "such",
      "incident",
      "change",
      "template.",
      "example",
      "decide",
      "values.",
      "edit",
      "delete"
    ],
    "language": "en",
    "word_count": 62,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "problem templates",
    "contentLower": "a service management template is a group of pre-populated fields that you can use as a quick start to create a new record or apply to an existing record. you can use a template to achieve consistency when you process records that have the same workflow. you can create a template with either of these methods: save an existing record (such as an incident, problem, or change) as a template. for example, you can save a change record as a change template. create a new template, where you decide which fields should have pre-populated values. related topics how to create a problem template how to edit a problem template how to delete a problem template how to apply a problem template",
    "keywordsLower": [
      "problem",
      "templates",
      "related",
      "topics",
      "service",
      "management",
      "template",
      "group",
      "pre-populated",
      "fields",
      "quick",
      "start",
      "create",
      "new",
      "record",
      "apply",
      "existing",
      "record.",
      "achieve",
      "consistency",
      "process",
      "records",
      "same",
      "workflow.",
      "either",
      "methods",
      "save",
      "such",
      "incident",
      "change",
      "template.",
      "example",
      "decide",
      "values.",
      "edit",
      "delete"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Problem Management and ITIL",
    "content": "The ITIL Service Operation publication describes Problem Management as the process responsible for managing the lifecycle of all problems. The main benefits of Problem Management are improved service quality and reliability. As incidents are resolved, Service Management captures information about their resolution. Service Management uses this information to identify and resolve similar new incidents quickly, and to identify and fix the root cause of those incidents. Problem Management functions both reactively and proactively. Reactive Problem Management occurs when: The Service Desk creates a problem record when there is a suspected (or detected) cause of one or more Incidents. A technical support group analyzes an incident and concludes that an underlying problem exists, or is likely to exist. You create a problem record when Service Management automatically creates an incident that's triggered by event or alert tools that detected an application or infrastructure fault. A supplier o",
    "url": "pmitil",
    "filename": "pmitil",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "problem",
      "management",
      "itil",
      "related",
      "topics",
      "service",
      "operation",
      "publication",
      "describes",
      "process",
      "responsible",
      "managing",
      "lifecycle",
      "all",
      "problems.",
      "main",
      "benefits",
      "improved",
      "quality",
      "reliability.",
      "incidents",
      "resolved",
      "captures",
      "information",
      "about",
      "resolution.",
      "uses",
      "identify",
      "resolve",
      "similar",
      "new",
      "quickly",
      "fix",
      "root",
      "cause",
      "incidents.",
      "functions",
      "both",
      "reactively",
      "proactively.",
      "reactive",
      "occurs",
      "desk",
      "creates",
      "record",
      "there",
      "suspected",
      "detected",
      "one",
      "technical",
      "support",
      "group",
      "analyzes",
      "incident",
      "concludes",
      "underlying",
      "exists",
      "likely",
      "exist.",
      "create",
      "automatically",
      "triggered",
      "event",
      "alert",
      "tools",
      "application",
      "infrastructure",
      "fault.",
      "supplier",
      "contractor",
      "notifies",
      "resolved.",
      "regular",
      "analysis",
      "fault",
      "investigated.",
      "resolves",
      "situations",
      "generally",
      "part",
      "based",
      "history.",
      "proactive",
      "identifies",
      "solves",
      "issues",
      "known",
      "errors",
      "before",
      "occur.",
      "initiative",
      "continual",
      "improvement.",
      "prevent",
      "happening",
      "instead",
      "reacting",
      "after",
      "occur",
      "organization"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "problem management and itil",
    "contentLower": "the itil service operation publication describes problem management as the process responsible for managing the lifecycle of all problems. the main benefits of problem management are improved service quality and reliability. as incidents are resolved, service management captures information about their resolution. service management uses this information to identify and resolve similar new incidents quickly, and to identify and fix the root cause of those incidents. problem management functions both reactively and proactively. reactive problem management occurs when: the service desk creates a problem record when there is a suspected (or detected) cause of one or more incidents. a technical support group analyzes an incident and concludes that an underlying problem exists, or is likely to exist. you create a problem record when service management automatically creates an incident that's triggered by event or alert tools that detected an application or infrastructure fault. a supplier o",
    "keywordsLower": [
      "problem",
      "management",
      "itil",
      "related",
      "topics",
      "service",
      "operation",
      "publication",
      "describes",
      "process",
      "responsible",
      "managing",
      "lifecycle",
      "all",
      "problems.",
      "main",
      "benefits",
      "improved",
      "quality",
      "reliability.",
      "incidents",
      "resolved",
      "captures",
      "information",
      "about",
      "resolution.",
      "uses",
      "identify",
      "resolve",
      "similar",
      "new",
      "quickly",
      "fix",
      "root",
      "cause",
      "incidents.",
      "functions",
      "both",
      "reactively",
      "proactively.",
      "reactive",
      "occurs",
      "desk",
      "creates",
      "record",
      "there",
      "suspected",
      "detected",
      "one",
      "technical",
      "support",
      "group",
      "analyzes",
      "incident",
      "concludes",
      "underlying",
      "exists",
      "likely",
      "exist.",
      "create",
      "automatically",
      "triggered",
      "event",
      "alert",
      "tools",
      "application",
      "infrastructure",
      "fault.",
      "supplier",
      "contractor",
      "notifies",
      "resolved.",
      "regular",
      "analysis",
      "fault",
      "investigated.",
      "resolves",
      "situations",
      "generally",
      "part",
      "based",
      "history.",
      "proactive",
      "identifies",
      "solves",
      "issues",
      "known",
      "errors",
      "before",
      "occur.",
      "initiative",
      "continual",
      "improvement.",
      "prevent",
      "happening",
      "instead",
      "reacting",
      "after",
      "occur",
      "organization"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Problem Management input and output",
    "content": "This table describes how problems are created and the possible output of the Problem Management process. Input sources Output Notification from a supplier or product manager that a problem exists. For example, a notification from a development team or a supplier of a known error in a database. Potential security breaches in products that you deploy in the IT environment. For example, products from suppliers or security analysts. Incident Management Incidents classified as problem candidates. Trend analysis and review of closed incidents that were resolved with a workaround. Incident trend and summary reports. Incidents with unknown causes or incidents that are likely to recur (from incident Management). Incidents that reveal an underlying problem. For example, an application error or defect. Change Management RFC and change request status, approval and closure. Configuration Management Configuration details and service model relationships. Event Management Trend analysis and review of ",
    "url": "pminputoutput",
    "filename": "pminputoutput",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "problem",
      "management",
      "input",
      "output",
      "related",
      "topics",
      "table",
      "describes",
      "problems",
      "created",
      "possible",
      "process.",
      "sources",
      "notification",
      "supplier",
      "product",
      "manager",
      "exists.",
      "example",
      "development",
      "team",
      "known",
      "error",
      "database.",
      "potential",
      "security",
      "breaches",
      "products",
      "deploy",
      "environment.",
      "suppliers",
      "analysts.",
      "incident",
      "incidents",
      "classified",
      "candidates.",
      "trend",
      "analysis",
      "review",
      "closed",
      "resolved",
      "workaround.",
      "summary",
      "reports.",
      "unknown",
      "causes",
      "likely",
      "recur",
      "reveal",
      "underlying",
      "problem.",
      "application",
      "defect.",
      "change",
      "rfc",
      "request",
      "status",
      "approval",
      "closure.",
      "configuration",
      "details",
      "service",
      "model",
      "relationships.",
      "event",
      "events.",
      "performance.",
      "logs.",
      "require",
      "intervention.",
      "errors",
      "workarounds",
      "reports",
      "updates",
      "trends",
      "performance",
      "note",
      "affected",
      "required",
      "support",
      "services",
      "need",
      "receive",
      "information",
      "about",
      "permanent",
      "fixes",
      "progress.",
      "itil",
      "kpis"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "problem management input and output",
    "contentLower": "this table describes how problems are created and the possible output of the problem management process. input sources output notification from a supplier or product manager that a problem exists. for example, a notification from a development team or a supplier of a known error in a database. potential security breaches in products that you deploy in the it environment. for example, products from suppliers or security analysts. incident management incidents classified as problem candidates. trend analysis and review of closed incidents that were resolved with a workaround. incident trend and summary reports. incidents with unknown causes or incidents that are likely to recur (from incident management). incidents that reveal an underlying problem. for example, an application error or defect. change management rfc and change request status, approval and closure. configuration management configuration details and service model relationships. event management trend analysis and review of ",
    "keywordsLower": [
      "problem",
      "management",
      "input",
      "output",
      "related",
      "topics",
      "table",
      "describes",
      "problems",
      "created",
      "possible",
      "process.",
      "sources",
      "notification",
      "supplier",
      "product",
      "manager",
      "exists.",
      "example",
      "development",
      "team",
      "known",
      "error",
      "database.",
      "potential",
      "security",
      "breaches",
      "products",
      "deploy",
      "environment.",
      "suppliers",
      "analysts.",
      "incident",
      "incidents",
      "classified",
      "candidates.",
      "trend",
      "analysis",
      "review",
      "closed",
      "resolved",
      "workaround.",
      "summary",
      "reports.",
      "unknown",
      "causes",
      "likely",
      "recur",
      "reveal",
      "underlying",
      "problem.",
      "application",
      "defect.",
      "change",
      "rfc",
      "request",
      "status",
      "approval",
      "closure.",
      "configuration",
      "details",
      "service",
      "model",
      "relationships.",
      "event",
      "events.",
      "performance.",
      "logs.",
      "require",
      "intervention.",
      "errors",
      "workarounds",
      "reports",
      "updates",
      "trends",
      "performance",
      "note",
      "affected",
      "required",
      "support",
      "services",
      "need",
      "receive",
      "information",
      "about",
      "permanent",
      "fixes",
      "progress.",
      "itil",
      "kpis"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Problem Management KPIs",
    "content": "Key Performance Indicators (KPIs) are useful for evaluating your Problem Management processes. In addition to the data provided by Service Management, you may need additional tools to report all of your KPI requirements. To visualize trend information, it's useful to graph KPI data. Metric Description Average time to diagnose Average time to diagnose problems and pinpoint the root cause and the known errors in a specified time period. Average time to fix Average time to fix known errors. Number of new problems Total number of problems recorded in a specified time period. Number of solved problems Total number of problems solved in a specified time period. Incidents caused by problems The number of incidents occurring before the problem is resolved in a specified time period. ITIL V3 KPIs ITIL V3 recommends that you measure these performance indicators: Total number of problems recorded in the period (as a control measure). Percentage of problems resolved within Service Level Targets. P",
    "url": "pmkpis",
    "filename": "pmkpis",
    "headings": [
      "ITIL V3 KPIs",
      "COBIT 4.1 KPIs",
      "RACI Matrix KPIs",
      "Related topics"
    ],
    "keywords": [
      "4.7",
      "4.4",
      "4.1",
      "4.6",
      "4.5",
      "4.2",
      "4.8",
      "4.3",
      "4.9",
      "problem",
      "management",
      "kpis",
      "itil",
      "v3",
      "cobit",
      "raci",
      "matrix",
      "related",
      "topics",
      "key",
      "performance",
      "indicators",
      "useful",
      "evaluating",
      "processes.",
      "addition",
      "data",
      "provided",
      "service",
      "need",
      "additional",
      "tools",
      "report",
      "all",
      "kpi",
      "requirements.",
      "visualize",
      "trend",
      "information",
      "graph",
      "data.",
      "metric",
      "description",
      "average",
      "time",
      "diagnose",
      "problems",
      "pinpoint",
      "root",
      "cause",
      "known",
      "errors",
      "specified",
      "period.",
      "fix",
      "errors.",
      "number",
      "new",
      "total",
      "recorded",
      "solved",
      "incidents",
      "caused",
      "occurring",
      "before",
      "resolved",
      "recommends",
      "measure",
      "period",
      "control",
      "percentage",
      "level",
      "targets.",
      "exceed",
      "target",
      "resolution",
      "times.",
      "backlog",
      "existing",
      "static",
      "declining",
      "increasing",
      "cost",
      "handling",
      "problem.",
      "major",
      "including",
      "opened",
      "closed",
      "backlog.",
      "successful",
      "reviews.",
      "added",
      "error",
      "database.",
      "accuracy",
      "database",
      "based",
      "audits",
      "reviews"
    ],
    "language": "en",
    "word_count": 112,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "problem management kpis",
    "contentLower": "key performance indicators (kpis) are useful for evaluating your problem management processes. in addition to the data provided by service management, you may need additional tools to report all of your kpi requirements. to visualize trend information, it's useful to graph kpi data. metric description average time to diagnose average time to diagnose problems and pinpoint the root cause and the known errors in a specified time period. average time to fix average time to fix known errors. number of new problems total number of problems recorded in a specified time period. number of solved problems total number of problems solved in a specified time period. incidents caused by problems the number of incidents occurring before the problem is resolved in a specified time period. itil v3 kpis itil v3 recommends that you measure these performance indicators: total number of problems recorded in the period (as a control measure). percentage of problems resolved within service level targets. p",
    "keywordsLower": [
      "4.7",
      "4.4",
      "4.1",
      "4.6",
      "4.5",
      "4.2",
      "4.8",
      "4.3",
      "4.9",
      "problem",
      "management",
      "kpis",
      "itil",
      "v3",
      "cobit",
      "raci",
      "matrix",
      "related",
      "topics",
      "key",
      "performance",
      "indicators",
      "useful",
      "evaluating",
      "processes.",
      "addition",
      "data",
      "provided",
      "service",
      "need",
      "additional",
      "tools",
      "report",
      "all",
      "kpi",
      "requirements.",
      "visualize",
      "trend",
      "information",
      "graph",
      "data.",
      "metric",
      "description",
      "average",
      "time",
      "diagnose",
      "problems",
      "pinpoint",
      "root",
      "cause",
      "known",
      "errors",
      "specified",
      "period.",
      "fix",
      "errors.",
      "number",
      "new",
      "total",
      "recorded",
      "solved",
      "incidents",
      "caused",
      "occurring",
      "before",
      "resolved",
      "recommends",
      "measure",
      "period",
      "control",
      "percentage",
      "level",
      "targets.",
      "exceed",
      "target",
      "resolution",
      "times.",
      "backlog",
      "existing",
      "static",
      "declining",
      "increasing",
      "cost",
      "handling",
      "problem.",
      "major",
      "including",
      "opened",
      "closed",
      "backlog.",
      "successful",
      "reviews.",
      "added",
      "error",
      "database.",
      "accuracy",
      "database",
      "based",
      "audits",
      "reviews"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "On-Call Schedule Management",
    "content": "This feature enables you to manage work schedules for the groups and individuals who deal with tickets. On-Call Schedule Management can help you: Easily assign tickets to on-call agentsSet different assignment strategies for groupsProvide the help desk and management with easily accessible information about who is on call You can only view this feature if it has been enabled, and you have the appropriate permission. Shifts, working hours, and vacations On-Call Schedule Management uses a structure of shifts, working hours, and vacations. An on-call shift is set for a group, and defines the times when one or more group members (up to three) are on call, and responsible for dealing with request and incident tickets and notifications. On-call members can be automatically rotated in recurring shifts. For example, a group might have an on-call shift from Monday to Friday, 9.00 AM to 6.00 PM. Working hours is the time when an individual user is scheduled to work. For example, one or more user",
    "url": "oncallschedulemgmt",
    "filename": "oncallschedulemgmt",
    "headings": [
      "Shifts, working hours, and vacations",
      "Limitation",
      "Related topics"
    ],
    "keywords": [
      "10.00",
      "4.00",
      "6.00",
      "9.00",
      "3.00",
      "on-call",
      "schedule",
      "management",
      "shifts",
      "working",
      "hours",
      "vacations",
      "limitation",
      "related",
      "topics",
      "feature",
      "enables",
      "manage",
      "work",
      "schedules",
      "groups",
      "individuals",
      "deal",
      "tickets.",
      "help",
      "easily",
      "assign",
      "tickets",
      "agentsset",
      "different",
      "assignment",
      "strategies",
      "groupsprovide",
      "desk",
      "accessible",
      "information",
      "about",
      "call",
      "view",
      "enabled",
      "appropriate",
      "permission.",
      "uses",
      "structure",
      "vacations.",
      "shift",
      "set",
      "group",
      "defines",
      "times",
      "one",
      "members",
      "three",
      "responsible",
      "dealing",
      "request",
      "incident",
      "notifications.",
      "automatically",
      "rotated",
      "recurring",
      "shifts.",
      "example",
      "monday",
      "friday",
      "am",
      "pm.",
      "time",
      "individual",
      "user",
      "scheduled",
      "work.",
      "users",
      "pm",
      "words",
      "individually",
      "don",
      "need",
      "conform",
      "vacation",
      "off",
      "adjusted",
      "reflect",
      "continue",
      "appear",
      "shift.",
      "manually",
      "adjust",
      "absence.",
      "category",
      "link",
      "get",
      "started",
      "solution",
      "planning",
      "service",
      "managementworking",
      "formsdev2prod",
      "administer",
      "edit"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 4.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "on-call schedule management",
    "contentLower": "this feature enables you to manage work schedules for the groups and individuals who deal with tickets. on-call schedule management can help you: easily assign tickets to on-call agentsset different assignment strategies for groupsprovide the help desk and management with easily accessible information about who is on call you can only view this feature if it has been enabled, and you have the appropriate permission. shifts, working hours, and vacations on-call schedule management uses a structure of shifts, working hours, and vacations. an on-call shift is set for a group, and defines the times when one or more group members (up to three) are on call, and responsible for dealing with request and incident tickets and notifications. on-call members can be automatically rotated in recurring shifts. for example, a group might have an on-call shift from monday to friday, 9.00 am to 6.00 pm. working hours is the time when an individual user is scheduled to work. for example, one or more user",
    "keywordsLower": [
      "10.00",
      "4.00",
      "6.00",
      "9.00",
      "3.00",
      "on-call",
      "schedule",
      "management",
      "shifts",
      "working",
      "hours",
      "vacations",
      "limitation",
      "related",
      "topics",
      "feature",
      "enables",
      "manage",
      "work",
      "schedules",
      "groups",
      "individuals",
      "deal",
      "tickets.",
      "help",
      "easily",
      "assign",
      "tickets",
      "agentsset",
      "different",
      "assignment",
      "strategies",
      "groupsprovide",
      "desk",
      "accessible",
      "information",
      "about",
      "call",
      "view",
      "enabled",
      "appropriate",
      "permission.",
      "uses",
      "structure",
      "vacations.",
      "shift",
      "set",
      "group",
      "defines",
      "times",
      "one",
      "members",
      "three",
      "responsible",
      "dealing",
      "request",
      "incident",
      "notifications.",
      "automatically",
      "rotated",
      "recurring",
      "shifts.",
      "example",
      "monday",
      "friday",
      "am",
      "pm.",
      "time",
      "individual",
      "user",
      "scheduled",
      "work.",
      "users",
      "pm",
      "words",
      "individually",
      "don",
      "need",
      "conform",
      "vacation",
      "off",
      "adjusted",
      "reflect",
      "continue",
      "appear",
      "shift.",
      "manually",
      "adjust",
      "absence.",
      "category",
      "link",
      "get",
      "started",
      "solution",
      "planning",
      "service",
      "managementworking",
      "formsdev2prod",
      "administer",
      "edit"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "On-Call Schedule Management user interface",
    "content": "If this feature is enabled, and you have the appropriate permission, the On-Call Schedule Management feature allows you to view all the groups currently on-call, and the members of the groups. For more information, see How to set up On-Call Schedule Management. To view On-Call Schedule Management, from the main menu, select Run > On-Call Schedule. The On-Call Schedule landing page is displayed: User interface Description View group calendar Click this to display the calendar for the group selected in the main section. See the Group calendar section. Main section (group list) Lists the available groups according to the selected view. For more information about groups, see Groups. The right panel As you select a group from the list, the right panel displays: Current members who are on call.Group members. If a member is on vacation, they still appear under On-call right now (current members who are on call). This is a known limitation. However, their vacation information is displayed next",
    "url": "ocsuserinterface",
    "filename": "ocsuserinterface",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "on-call",
      "schedule",
      "management",
      "user",
      "interface",
      "related",
      "topics",
      "feature",
      "enabled",
      "appropriate",
      "permission",
      "allows",
      "view",
      "all",
      "groups",
      "currently",
      "members",
      "groups.",
      "information",
      "see",
      "set",
      "management.",
      "main",
      "menu",
      "select",
      "run",
      "schedule.",
      "landing",
      "page",
      "displayed",
      "description",
      "group",
      "calendar",
      "click",
      "display",
      "selected",
      "section.",
      "section",
      "list",
      "lists",
      "available",
      "according",
      "view.",
      "about",
      "right",
      "panel",
      "displays",
      "current",
      "call.group",
      "members.",
      "member",
      "vacation",
      "still",
      "appear",
      "under",
      "now",
      "call",
      "known",
      "limitation.",
      "however",
      "next",
      "name",
      "list.",
      "icon",
      "beside",
      "member.",
      "access",
      "calendar.",
      "shifts",
      "group.",
      "out-of-the-box",
      "administrator",
      "left",
      "contains",
      "two",
      "sections",
      "call.",
      "time",
      "due",
      "name.",
      "appears",
      "add",
      "shift.",
      "shifts.",
      "date",
      "range",
      "three",
      "months",
      "ahead",
      "one",
      "month",
      "back.",
      "shift",
      "open",
      "it.",
      "recurring",
      "option",
      "shifts.click",
      "day",
      "week"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "on-call schedule management user interface",
    "contentLower": "if this feature is enabled, and you have the appropriate permission, the on-call schedule management feature allows you to view all the groups currently on-call, and the members of the groups. for more information, see how to set up on-call schedule management. to view on-call schedule management, from the main menu, select run > on-call schedule. the on-call schedule landing page is displayed: user interface description view group calendar click this to display the calendar for the group selected in the main section. see the group calendar section. main section (group list) lists the available groups according to the selected view. for more information about groups, see groups. the right panel as you select a group from the list, the right panel displays: current members who are on call.group members. if a member is on vacation, they still appear under on-call right now (current members who are on call). this is a known limitation. however, their vacation information is displayed next",
    "keywordsLower": [
      "on-call",
      "schedule",
      "management",
      "user",
      "interface",
      "related",
      "topics",
      "feature",
      "enabled",
      "appropriate",
      "permission",
      "allows",
      "view",
      "all",
      "groups",
      "currently",
      "members",
      "groups.",
      "information",
      "see",
      "set",
      "management.",
      "main",
      "menu",
      "select",
      "run",
      "schedule.",
      "landing",
      "page",
      "displayed",
      "description",
      "group",
      "calendar",
      "click",
      "display",
      "selected",
      "section.",
      "section",
      "list",
      "lists",
      "available",
      "according",
      "view.",
      "about",
      "right",
      "panel",
      "displays",
      "current",
      "call.group",
      "members.",
      "member",
      "vacation",
      "still",
      "appear",
      "under",
      "now",
      "call",
      "known",
      "limitation.",
      "however",
      "next",
      "name",
      "list.",
      "icon",
      "beside",
      "member.",
      "access",
      "calendar.",
      "shifts",
      "group.",
      "out-of-the-box",
      "administrator",
      "left",
      "contains",
      "two",
      "sections",
      "call.",
      "time",
      "due",
      "name.",
      "appears",
      "add",
      "shift.",
      "shifts.",
      "date",
      "range",
      "three",
      "months",
      "ahead",
      "one",
      "month",
      "back.",
      "shift",
      "open",
      "it.",
      "recurring",
      "option",
      "shifts.click",
      "day",
      "week"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Original SAM",
    "content": "The SAM capability provides the following features when the new SAM solution isn't enabled: Licenses License metrics License models Software titles",
    "url": "originalsam",
    "filename": "originalsam",
    "headings": [],
    "keywords": [
      "original",
      "sam",
      "capability",
      "provides",
      "following",
      "features",
      "new",
      "solution",
      "isn",
      "enabled",
      "licenses",
      "license",
      "metrics",
      "models",
      "software",
      "titles"
    ],
    "language": "en",
    "word_count": 20,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "original sam",
    "contentLower": "the sam capability provides the following features when the new sam solution isn't enabled: licenses license metrics license models software titles",
    "keywordsLower": [
      "original",
      "sam",
      "capability",
      "provides",
      "following",
      "features",
      "new",
      "solution",
      "isn",
      "enabled",
      "licenses",
      "license",
      "metrics",
      "models",
      "software",
      "titles"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "New SAM",
    "content": "You should assign the Asset Management license to the tenant to access the new SAM capability. To retrieve host information from UCMDB, SAM also requires the Premium Discovery (UD Full) license. The new SAM capability provides the following features: SAM Home Compliance Analysis Licenses License models License usage License metrics License rules Products Configurations To view these menus, sign in to Service Management, and then choose Run > Software Asset. Note that to access the SAM menus after you install and deploy SAM, you must have the required permissions. Out of the box, Service Management provides a Software manager role with all the privileges needed for using the SAM capability. For more information about how to assign permissions to a role, see Roles.",
    "url": "newsam",
    "filename": "newsam",
    "headings": [],
    "keywords": [
      "new",
      "sam",
      "assign",
      "asset",
      "management",
      "license",
      "tenant",
      "access",
      "capability.",
      "retrieve",
      "host",
      "information",
      "ucmdb",
      "requires",
      "premium",
      "discovery",
      "ud",
      "full",
      "license.",
      "capability",
      "provides",
      "following",
      "features",
      "home",
      "compliance",
      "analysis",
      "licenses",
      "models",
      "usage",
      "metrics",
      "rules",
      "products",
      "configurations",
      "view",
      "menus",
      "sign",
      "service",
      "choose",
      "run",
      "software",
      "asset.",
      "note",
      "after",
      "install",
      "deploy",
      "required",
      "permissions.",
      "out",
      "box",
      "manager",
      "role",
      "all",
      "privileges",
      "needed",
      "about",
      "permissions",
      "see",
      "roles."
    ],
    "language": "en",
    "word_count": 82,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "new sam",
    "contentLower": "you should assign the asset management license to the tenant to access the new sam capability. to retrieve host information from ucmdb, sam also requires the premium discovery (ud full) license. the new sam capability provides the following features: sam home compliance analysis licenses license models license usage license metrics license rules products configurations to view these menus, sign in to service management, and then choose run > software asset. note that to access the sam menus after you install and deploy sam, you must have the required permissions. out of the box, service management provides a software manager role with all the privileges needed for using the sam capability. for more information about how to assign permissions to a role, see roles.",
    "keywordsLower": [
      "new",
      "sam",
      "assign",
      "asset",
      "management",
      "license",
      "tenant",
      "access",
      "capability.",
      "retrieve",
      "host",
      "information",
      "ucmdb",
      "requires",
      "premium",
      "discovery",
      "ud",
      "full",
      "license.",
      "capability",
      "provides",
      "following",
      "features",
      "home",
      "compliance",
      "analysis",
      "licenses",
      "models",
      "usage",
      "metrics",
      "rules",
      "products",
      "configurations",
      "view",
      "menus",
      "sign",
      "service",
      "choose",
      "run",
      "software",
      "asset.",
      "note",
      "after",
      "install",
      "deploy",
      "required",
      "permissions.",
      "out",
      "box",
      "manager",
      "role",
      "all",
      "privileges",
      "needed",
      "about",
      "permissions",
      "see",
      "roles."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Products",
    "content": "With the Products page, you can manage two types of products in SAM: on-premises products and SaaS products. The main difference between these product types lies in how SAM collects the required data for license compliance calculation: For SaaS products, SAM collects required data through integrations. For on-premises products, SAM can manage two categories of products: products discovered from your IT infrastructure by UCMDB and synced to SAM for management (called \"discovered products\"), and products that are manually created and whose compliance calculation is done based on data retrieved through integrations (called \"integrated products\"). Click the tabs at the top of the page to switch between the product types. On-premises products Product details The following describes the fields on the Details page of a product. General tab Details This section provides detailed information about the selected product. These fields only apply to discovered products: Auto-built compatibility mat",
    "url": "samproducts",
    "filename": "samproducts",
    "headings": [
      "On-premises products",
      "Product details",
      "General tab",
      "Compliance calculation settings tab",
      "Deployment tab",
      "Exempted devices tab",
      "License compatibility matrix tab",
      "License tab",
      "Report history tab",
      "Add/delete a product",
      "Synchronize the product family",
      "About updating the commercial model",
      "About updating the Managed product setting for commercial products",
      "Synchronize the infrastructure information",
      "Configure license rules for a product (discovered products only)",
      "Exempt devices from license consumption calculation (discovered products only)",
      "View the exempted devices",
      "Add/remove an exempted device",
      "Check the license consumption status of the exempted devices",
      "Maintain license compatibility matrix"
    ],
    "keywords": [
      "7.6",
      "7.5",
      "8.0",
      "products",
      "on-premises",
      "product",
      "details",
      "general",
      "tab",
      "compliance",
      "calculation",
      "settings",
      "deployment",
      "exempted",
      "devices",
      "license",
      "compatibility",
      "matrix",
      "report",
      "history",
      "add",
      "delete",
      "synchronize",
      "family",
      "about",
      "updating",
      "commercial",
      "model",
      "managed",
      "setting",
      "infrastructure",
      "information",
      "configure",
      "rules",
      "discovered",
      "exempt",
      "consumption",
      "view",
      "remove",
      "device",
      "check",
      "status",
      "maintain",
      "downgrade",
      "relationship",
      "cancel",
      "upgrade",
      "automatically",
      "build",
      "calculate",
      "define",
      "scheduler",
      "import",
      "related",
      "data",
      "export",
      "summary",
      "csv",
      "xls",
      "saas",
      "topics",
      "page",
      "manage",
      "two",
      "types",
      "sam",
      "products.",
      "main",
      "difference",
      "between",
      "lies",
      "collects",
      "required",
      "through",
      "integrations.",
      "categories",
      "ucmdb",
      "synced",
      "management",
      "called",
      "manually",
      "created",
      "whose",
      "done",
      "based",
      "retrieved",
      "integrations",
      "integrated",
      "click",
      "tabs",
      "top",
      "switch",
      "types.",
      "following",
      "describes",
      "fields",
      "product.",
      "section",
      "provides",
      "detailed"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "products",
    "contentLower": "with the products page, you can manage two types of products in sam: on-premises products and saas products. the main difference between these product types lies in how sam collects the required data for license compliance calculation: for saas products, sam collects required data through integrations. for on-premises products, sam can manage two categories of products: products discovered from your it infrastructure by ucmdb and synced to sam for management (called \"discovered products\"), and products that are manually created and whose compliance calculation is done based on data retrieved through integrations (called \"integrated products\"). click the tabs at the top of the page to switch between the product types. on-premises products product details the following describes the fields on the details page of a product. general tab details this section provides detailed information about the selected product. these fields only apply to discovered products: auto-built compatibility mat",
    "keywordsLower": [
      "7.6",
      "7.5",
      "8.0",
      "products",
      "on-premises",
      "product",
      "details",
      "general",
      "tab",
      "compliance",
      "calculation",
      "settings",
      "deployment",
      "exempted",
      "devices",
      "license",
      "compatibility",
      "matrix",
      "report",
      "history",
      "add",
      "delete",
      "synchronize",
      "family",
      "about",
      "updating",
      "commercial",
      "model",
      "managed",
      "setting",
      "infrastructure",
      "information",
      "configure",
      "rules",
      "discovered",
      "exempt",
      "consumption",
      "view",
      "remove",
      "device",
      "check",
      "status",
      "maintain",
      "downgrade",
      "relationship",
      "cancel",
      "upgrade",
      "automatically",
      "build",
      "calculate",
      "define",
      "scheduler",
      "import",
      "related",
      "data",
      "export",
      "summary",
      "csv",
      "xls",
      "saas",
      "topics",
      "page",
      "manage",
      "two",
      "types",
      "sam",
      "products.",
      "main",
      "difference",
      "between",
      "lies",
      "collects",
      "required",
      "through",
      "integrations.",
      "categories",
      "ucmdb",
      "synced",
      "management",
      "called",
      "manually",
      "created",
      "whose",
      "done",
      "based",
      "retrieved",
      "integrations",
      "integrated",
      "click",
      "tabs",
      "top",
      "switch",
      "types.",
      "following",
      "describes",
      "fields",
      "product.",
      "section",
      "provides",
      "detailed"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage Office 365 subscription with SAM",
    "content": "With the support of Microsoft 365 integration, the support for integrating SAM with legacy Office 365 is deprecated. If you're using the legacy Office 365 and want to manage its subscription information with SAM, follow these steps: Log in to Suite Administration, click CONFIGURATIONS, click the Proxy Settings tab, and then complete the settings. For more information, see Proxy Settings tab. Sign in to Service Management, choose Run > Software Asset Management > Products, click the SaaS tab, and then select Office 365. Note that you might need to unselect the Managed Products filter. Click Configuration. In the dialog box that appears, update the fields in the following table: Field Description Server URL Enter the production Azure AD authentication endpoint, where authorizing the SAM application starts. In the Endpoints list, you might find multiple endpoints. Use the OAuth 2.0 Token endpoint (v2) option as the server URL. Server Resource URL Default: https://graph.microsoft.com/beta/",
    "url": "manageoffice365withsam",
    "filename": "manageoffice365withsam",
    "headings": [],
    "keywords": [
      "Reports.Read",
      "https://graph.microsoft.com/.default",
      "Directory.Read",
      "https://graph.microsoft.com/beta",
      "microsoft.com",
      "2.0",
      "manage",
      "office",
      "365",
      "subscription",
      "sam",
      "support",
      "microsoft",
      "integration",
      "integrating",
      "legacy",
      "deprecated.",
      "re",
      "want",
      "information",
      "follow",
      "steps",
      "log",
      "suite",
      "administration",
      "click",
      "configurations",
      "proxy",
      "settings",
      "tab",
      "complete",
      "settings.",
      "see",
      "tab.",
      "sign",
      "service",
      "management",
      "choose",
      "run",
      "software",
      "asset",
      "products",
      "saas",
      "select",
      "365.",
      "note",
      "need",
      "unselect",
      "managed",
      "filter.",
      "configuration.",
      "dialog",
      "box",
      "appears",
      "update",
      "fields",
      "following",
      "table",
      "field",
      "description",
      "server",
      "url",
      "enter",
      "production",
      "azure",
      "ad",
      "authentication",
      "endpoint",
      "authorizing",
      "application",
      "starts.",
      "endpoints",
      "list",
      "find",
      "multiple",
      "endpoints.",
      "oauth",
      "token",
      "v2",
      "option",
      "url.",
      "resource",
      "default",
      "https",
      "graph.microsoft.com",
      "beta",
      "keep",
      "value",
      "unchanged.",
      "client",
      "id",
      "app",
      "registration",
      "portal",
      "assigned",
      "registered",
      "application.",
      "make",
      "sure",
      "go"
    ],
    "language": "en",
    "word_count": 107,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage office 365 subscription with sam",
    "contentLower": "with the support of microsoft 365 integration, the support for integrating sam with legacy office 365 is deprecated. if you're using the legacy office 365 and want to manage its subscription information with sam, follow these steps: log in to suite administration, click configurations, click the proxy settings tab, and then complete the settings. for more information, see proxy settings tab. sign in to service management, choose run > software asset management > products, click the saas tab, and then select office 365. note that you might need to unselect the managed products filter. click configuration. in the dialog box that appears, update the fields in the following table: field description server url enter the production azure ad authentication endpoint, where authorizing the sam application starts. in the endpoints list, you might find multiple endpoints. use the oauth 2.0 token endpoint (v2) option as the server url. server resource url default: https://graph.microsoft.com/beta/",
    "keywordsLower": [
      "reports.read",
      "https://graph.microsoft.com/.default",
      "directory.read",
      "https://graph.microsoft.com/beta",
      "microsoft.com",
      "2.0",
      "manage",
      "office",
      "365",
      "subscription",
      "sam",
      "support",
      "microsoft",
      "integration",
      "integrating",
      "legacy",
      "deprecated.",
      "re",
      "want",
      "information",
      "follow",
      "steps",
      "log",
      "suite",
      "administration",
      "click",
      "configurations",
      "proxy",
      "settings",
      "tab",
      "complete",
      "settings.",
      "see",
      "tab.",
      "sign",
      "service",
      "management",
      "choose",
      "run",
      "software",
      "asset",
      "products",
      "saas",
      "select",
      "365.",
      "note",
      "need",
      "unselect",
      "managed",
      "filter.",
      "configuration.",
      "dialog",
      "box",
      "appears",
      "update",
      "fields",
      "following",
      "table",
      "field",
      "description",
      "server",
      "url",
      "enter",
      "production",
      "azure",
      "ad",
      "authentication",
      "endpoint",
      "authorizing",
      "application",
      "starts.",
      "endpoints",
      "list",
      "find",
      "multiple",
      "endpoints.",
      "oauth",
      "token",
      "v2",
      "option",
      "url.",
      "resource",
      "default",
      "https",
      "graph.microsoft.com",
      "beta",
      "keep",
      "value",
      "unchanged.",
      "client",
      "id",
      "app",
      "registration",
      "portal",
      "assigned",
      "registered",
      "application.",
      "make",
      "sure",
      "go"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Net Value Simulation",
    "content": "Service Management's Net Value Simulation feature offers a future value simulation of fixed assets based on the available data, and also predicts the net value trends in a graphical manner. In addition, you can filter the devices, infrastructures, and licenses based on the available fields to sharpen your simulation. The Net Value Simulation feature can be applied to various scenarios. The following are two examples: A data center manager can find out the net values of missing or stolen assets in the data center. An IT agent can decide when and whether a device can retire based on the financial policy. For example, a high-end server can retire when its net value is less than 100 US dollars. How to access Net Value Simulation To access Net Value Simulation, from the main menu, select Run > Financials > Net Value Simulation. User interface elements The following are details of the user interface (UI) elements. Left panel The left panel includes the following elements. Services bar option",
    "url": "valuesimulation",
    "filename": "valuesimulation",
    "headings": [
      "How to access Net Value Simulation",
      "User interface elements",
      "Left panel",
      "Right panel",
      "Related topics"
    ],
    "keywords": [
      "net",
      "value",
      "simulation",
      "access",
      "user",
      "interface",
      "elements",
      "left",
      "panel",
      "right",
      "related",
      "topics",
      "service",
      "management",
      "feature",
      "offers",
      "future",
      "fixed",
      "assets",
      "based",
      "available",
      "data",
      "predicts",
      "trends",
      "graphical",
      "manner.",
      "addition",
      "filter",
      "devices",
      "infrastructures",
      "licenses",
      "fields",
      "sharpen",
      "simulation.",
      "applied",
      "various",
      "scenarios.",
      "following",
      "two",
      "examples",
      "center",
      "manager",
      "find",
      "out",
      "values",
      "missing",
      "stolen",
      "center.",
      "agent",
      "decide",
      "whether",
      "device",
      "retire",
      "financial",
      "policy.",
      "example",
      "high-end",
      "server",
      "less",
      "100",
      "dollars.",
      "main",
      "menu",
      "select",
      "run",
      "financials",
      "details",
      "ui",
      "elements.",
      "includes",
      "services",
      "bar",
      "option",
      "description",
      "criteria",
      "asset",
      "type",
      "infrastructure",
      "peripheral",
      "click",
      "button",
      "criteria.",
      "narrows",
      "scope.",
      "date",
      "want",
      "view.",
      "information",
      "about",
      "options",
      "see",
      "view",
      "displays",
      "result",
      "table",
      "title",
      "shows",
      "number",
      "meet",
      "total"
    ],
    "language": "en",
    "word_count": 109,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "net value simulation",
    "contentLower": "service management's net value simulation feature offers a future value simulation of fixed assets based on the available data, and also predicts the net value trends in a graphical manner. in addition, you can filter the devices, infrastructures, and licenses based on the available fields to sharpen your simulation. the net value simulation feature can be applied to various scenarios. the following are two examples: a data center manager can find out the net values of missing or stolen assets in the data center. an it agent can decide when and whether a device can retire based on the financial policy. for example, a high-end server can retire when its net value is less than 100 us dollars. how to access net value simulation to access net value simulation, from the main menu, select run > financials > net value simulation. user interface elements the following are details of the user interface (ui) elements. left panel the left panel includes the following elements. services bar option",
    "keywordsLower": [
      "net",
      "value",
      "simulation",
      "access",
      "user",
      "interface",
      "elements",
      "left",
      "panel",
      "right",
      "related",
      "topics",
      "service",
      "management",
      "feature",
      "offers",
      "future",
      "fixed",
      "assets",
      "based",
      "available",
      "data",
      "predicts",
      "trends",
      "graphical",
      "manner.",
      "addition",
      "filter",
      "devices",
      "infrastructures",
      "licenses",
      "fields",
      "sharpen",
      "simulation.",
      "applied",
      "various",
      "scenarios.",
      "following",
      "two",
      "examples",
      "center",
      "manager",
      "find",
      "out",
      "values",
      "missing",
      "stolen",
      "center.",
      "agent",
      "decide",
      "whether",
      "device",
      "retire",
      "financial",
      "policy.",
      "example",
      "high-end",
      "server",
      "less",
      "100",
      "dollars.",
      "main",
      "menu",
      "select",
      "run",
      "financials",
      "details",
      "ui",
      "elements.",
      "includes",
      "services",
      "bar",
      "option",
      "description",
      "criteria",
      "asset",
      "type",
      "infrastructure",
      "peripheral",
      "click",
      "button",
      "criteria.",
      "narrows",
      "scope.",
      "date",
      "want",
      "view.",
      "information",
      "about",
      "options",
      "see",
      "view",
      "displays",
      "result",
      "table",
      "title",
      "shows",
      "number",
      "meet",
      "total"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Navigate OO Central",
    "content": "Operations Orchestration (OO) Central contains three workspace, so you can perform end to end tasks from the same place. A workspace is a unit that holds all the screens that belong to the same end to end task group. Run Management–used for running flows, scheduling flows, monitoring runs, and troubleshooting runs. Content Management–used for promotion tasks, such as deploying new content, setting permissions on flows, setting up configuration items, and rolling back to earlier versions of content packs. System Configuration–used to configure topology and personalize the Central UI appearance. Run management Click the Run Management button to display the Run Management workspace. This workspace includes the following modules: Run explorer In the RUN EXPLORER tab, the Suite/Tenant/OO Central user can monitor their running flows and the flows that have finished running. You can track flow runs, export in a CSV format, monitor their progress, and perform actions on flow runs, such as paus",
    "url": "visualoverviewoo",
    "filename": "visualoverviewoo",
    "headings": [
      "Run management",
      "Run explorer",
      "Flow launcher",
      "Scheduler",
      "Content Management",
      "Flow library",
      "Content packs",
      "Configuration items",
      "System configuration",
      "Viewing the workspace",
      "Adjusting the display of panes in the workspace"
    ],
    "keywords": [
      "navigate",
      "oo",
      "central",
      "run",
      "management",
      "explorer",
      "flow",
      "launcher",
      "scheduler",
      "content",
      "library",
      "packs",
      "configuration",
      "items",
      "system",
      "viewing",
      "workspace",
      "adjusting",
      "display",
      "panes",
      "operations",
      "orchestration",
      "contains",
      "three",
      "perform",
      "end",
      "tasks",
      "same",
      "place.",
      "unit",
      "holds",
      "all",
      "screens",
      "belong",
      "task",
      "group.",
      "running",
      "flows",
      "scheduling",
      "monitoring",
      "runs",
      "troubleshooting",
      "runs.",
      "promotion",
      "such",
      "deploying",
      "new",
      "setting",
      "permissions",
      "rolling",
      "back",
      "earlier",
      "versions",
      "packs.",
      "configure",
      "topology",
      "personalize",
      "ui",
      "appearance.",
      "click",
      "button",
      "workspace.",
      "includes",
      "following",
      "modules",
      "tab",
      "suite",
      "tenant",
      "user",
      "monitor",
      "finished",
      "running.",
      "track",
      "export",
      "csv",
      "format",
      "progress",
      "actions",
      "pausing",
      "resuming",
      "canceling",
      "them.",
      "fails",
      "want",
      "troubleshoot",
      "drill",
      "detailed",
      "information.",
      "many",
      "time",
      "filters",
      "locate",
      "need.",
      "make",
      "sure",
      "disable",
      "pop-up",
      "blocker",
      "browser",
      "format."
    ],
    "language": "en",
    "word_count": 109,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "navigate oo central",
    "contentLower": "operations orchestration (oo) central contains three workspace, so you can perform end to end tasks from the same place. a workspace is a unit that holds all the screens that belong to the same end to end task group. run management–used for running flows, scheduling flows, monitoring runs, and troubleshooting runs. content management–used for promotion tasks, such as deploying new content, setting permissions on flows, setting up configuration items, and rolling back to earlier versions of content packs. system configuration–used to configure topology and personalize the central ui appearance. run management click the run management button to display the run management workspace. this workspace includes the following modules: run explorer in the run explorer tab, the suite/tenant/oo central user can monitor their running flows and the flows that have finished running. you can track flow runs, export in a csv format, monitor their progress, and perform actions on flow runs, such as paus",
    "keywordsLower": [
      "navigate",
      "oo",
      "central",
      "run",
      "management",
      "explorer",
      "flow",
      "launcher",
      "scheduler",
      "content",
      "library",
      "packs",
      "configuration",
      "items",
      "system",
      "viewing",
      "workspace",
      "adjusting",
      "display",
      "panes",
      "operations",
      "orchestration",
      "contains",
      "three",
      "perform",
      "end",
      "tasks",
      "same",
      "place.",
      "unit",
      "holds",
      "all",
      "screens",
      "belong",
      "task",
      "group.",
      "running",
      "flows",
      "scheduling",
      "monitoring",
      "runs",
      "troubleshooting",
      "runs.",
      "promotion",
      "such",
      "deploying",
      "new",
      "setting",
      "permissions",
      "rolling",
      "back",
      "earlier",
      "versions",
      "packs.",
      "configure",
      "topology",
      "personalize",
      "ui",
      "appearance.",
      "click",
      "button",
      "workspace.",
      "includes",
      "following",
      "modules",
      "tab",
      "suite",
      "tenant",
      "user",
      "monitor",
      "finished",
      "running.",
      "track",
      "export",
      "csv",
      "format",
      "progress",
      "actions",
      "pausing",
      "resuming",
      "canceling",
      "them.",
      "fails",
      "want",
      "troubleshoot",
      "drill",
      "detailed",
      "information.",
      "many",
      "time",
      "filters",
      "locate",
      "need.",
      "make",
      "sure",
      "disable",
      "pop-up",
      "blocker",
      "browser",
      "format."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Overview of running and monitoring a flow",
    "content": "This topic provides an overview of running and monitoring the flows in Operations Orchestration (OO) Central. After the content packs are deployed in OO Central, the Tenant Admin or an end user can run and monitor the flow runs. Note that this is just an high level look at the workflow, and there are many options that aren't described here. For more detailed information about any of the steps, see the related topics section for relevant topics. Step 1: Find the flow that you want to run The Tenant Admin/End User locates the flow either from the FLOW LIBRARY or the FLOW LAUNCHER. Go to the Content Management > FLOW LIBRARY module or to the Run Management > FLOW LAUNCHER module. Step 2: Run the flow The Tenant Admin/End User runs the flow. Step 3: Monitor the flow run The Tenant Admin/End User tracks the flow run in the RUN EXPLORER module. If required, the Tenant Admin/End User performs actions on flow runs, such as pausing, resuming, and canceling flow runs handing off, reassigning own",
    "url": "runningflowsteps",
    "filename": "runningflowsteps",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "overview",
      "running",
      "monitoring",
      "flow",
      "related",
      "topics",
      "topic",
      "provides",
      "flows",
      "operations",
      "orchestration",
      "oo",
      "central.",
      "after",
      "content",
      "packs",
      "deployed",
      "central",
      "tenant",
      "admin",
      "end",
      "user",
      "run",
      "monitor",
      "runs.",
      "note",
      "just",
      "high",
      "level",
      "look",
      "workflow",
      "there",
      "many",
      "options",
      "aren",
      "described",
      "here.",
      "detailed",
      "information",
      "about",
      "any",
      "steps",
      "see",
      "section",
      "relevant",
      "topics.",
      "step",
      "find",
      "want",
      "locates",
      "either",
      "library",
      "launcher.",
      "go",
      "management",
      "module",
      "launcher",
      "module.",
      "runs",
      "flow.",
      "tracks",
      "explorer",
      "required",
      "performs",
      "actions",
      "such",
      "pausing",
      "resuming",
      "canceling",
      "handing",
      "off",
      "reassigning",
      "ownership.",
      "troubleshoot",
      "investigates",
      "problems",
      "run.",
      "create",
      "reports",
      "generates",
      "report",
      "progress.",
      "track",
      "manage",
      "test"
    ],
    "language": "en",
    "word_count": 124,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "overview of running and monitoring a flow",
    "contentLower": "this topic provides an overview of running and monitoring the flows in operations orchestration (oo) central. after the content packs are deployed in oo central, the tenant admin or an end user can run and monitor the flow runs. note that this is just an high level look at the workflow, and there are many options that aren't described here. for more detailed information about any of the steps, see the related topics section for relevant topics. step 1: find the flow that you want to run the tenant admin/end user locates the flow either from the flow library or the flow launcher. go to the content management > flow library module or to the run management > flow launcher module. step 2: run the flow the tenant admin/end user runs the flow. step 3: monitor the flow run the tenant admin/end user tracks the flow run in the run explorer module. if required, the tenant admin/end user performs actions on flow runs, such as pausing, resuming, and canceling flow runs handing off, reassigning own",
    "keywordsLower": [
      "overview",
      "running",
      "monitoring",
      "flow",
      "related",
      "topics",
      "topic",
      "provides",
      "flows",
      "operations",
      "orchestration",
      "oo",
      "central.",
      "after",
      "content",
      "packs",
      "deployed",
      "central",
      "tenant",
      "admin",
      "end",
      "user",
      "run",
      "monitor",
      "runs.",
      "note",
      "just",
      "high",
      "level",
      "look",
      "workflow",
      "there",
      "many",
      "options",
      "aren",
      "described",
      "here.",
      "detailed",
      "information",
      "about",
      "any",
      "steps",
      "see",
      "section",
      "relevant",
      "topics.",
      "step",
      "find",
      "want",
      "locates",
      "either",
      "library",
      "launcher.",
      "go",
      "management",
      "module",
      "launcher",
      "module.",
      "runs",
      "flow.",
      "tracks",
      "explorer",
      "required",
      "performs",
      "actions",
      "such",
      "pausing",
      "resuming",
      "canceling",
      "handing",
      "off",
      "reassigning",
      "ownership.",
      "troubleshoot",
      "investigates",
      "problems",
      "run.",
      "create",
      "reports",
      "generates",
      "report",
      "progress.",
      "track",
      "manage",
      "test"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Promote content packs",
    "content": "The Tenant Admin is usually the one responsible for promoting content packs. The following topics describe the steps involved in promoting the content packs. Overview of promoting content pack Deploy and manage content packs Manage the flow library Set up configuration items for a content pack",
    "url": "promotecp",
    "filename": "promotecp",
    "headings": [],
    "keywords": [
      "promote",
      "content",
      "packs",
      "tenant",
      "admin",
      "usually",
      "one",
      "responsible",
      "promoting",
      "packs.",
      "following",
      "topics",
      "describe",
      "steps",
      "involved",
      "overview",
      "pack",
      "deploy",
      "manage",
      "flow",
      "library",
      "set",
      "configuration",
      "items"
    ],
    "language": "en",
    "word_count": 35,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "promote content packs",
    "contentLower": "the tenant admin is usually the one responsible for promoting content packs. the following topics describe the steps involved in promoting the content packs. overview of promoting content pack deploy and manage content packs manage the flow library set up configuration items for a content pack",
    "keywordsLower": [
      "promote",
      "content",
      "packs",
      "tenant",
      "admin",
      "usually",
      "one",
      "responsible",
      "promoting",
      "packs.",
      "following",
      "topics",
      "describe",
      "steps",
      "involved",
      "overview",
      "pack",
      "deploy",
      "manage",
      "flow",
      "library",
      "set",
      "configuration",
      "items"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Overview of promoting content pack",
    "content": "This topic gives an overview of promoting a content pack in Operations Orchestration (OO) Central. Promotion Promotion is the process of deploying content packs across several environments, in order to ensure that results are predictable and without (unknown) risks. This is in particular important for maintaining a stable production environment that contains content that has been tested and validated prior to deployment. For example, deploying the content pack on the following Central environments: Development, QA, Staging, Production. The final aim of promotion is to deploy a new content pack to the Central Production server, to make the flows available to users. Promotion versus deployment Deployment is part of a promotion. However, the promotion process also includes other tasks, such as: Configuring the content pack: configuring worker group aliases, mapping system accounts, and more Testing and troubleshooting the flows in the content pack Content pack A content pack is a file con",
    "url": "promotecpsteps",
    "filename": "promotecpsteps",
    "headings": [
      "Promotion",
      "Promotion versus deployment",
      "Content pack",
      "When do you promote a content pack?",
      "Who does the promotion?",
      "What is a content pack patch?",
      "Promotion steps",
      "Related topics"
    ],
    "keywords": [
      "overview",
      "promoting",
      "content",
      "pack",
      "promotion",
      "versus",
      "deployment",
      "promote",
      "what",
      "patch",
      "steps",
      "related",
      "topics",
      "topic",
      "gives",
      "operations",
      "orchestration",
      "oo",
      "central.",
      "process",
      "deploying",
      "packs",
      "across",
      "several",
      "environments",
      "order",
      "ensure",
      "results",
      "predictable",
      "unknown",
      "risks.",
      "particular",
      "important",
      "maintaining",
      "stable",
      "production",
      "environment",
      "contains",
      "tested",
      "validated",
      "prior",
      "deployment.",
      "example",
      "following",
      "central",
      "development",
      "qa",
      "staging",
      "production.",
      "final",
      "aim",
      "deploy",
      "new",
      "server",
      "make",
      "flows",
      "available",
      "users.",
      "part",
      "promotion.",
      "however",
      "includes",
      "tasks",
      "such",
      "configuring",
      "worker",
      "group",
      "aliases",
      "mapping",
      "system",
      "accounts",
      "testing",
      "troubleshooting",
      "file",
      "containing",
      "actions",
      "configuration",
      "items.",
      "flow",
      "authors",
      "complete",
      "project",
      "package",
      "server.",
      "cloudslang",
      "java",
      "based",
      "python",
      "based.",
      "native",
      "java-based",
      ".net",
      "there",
      "need",
      "environment.",
      "fixed",
      "bug",
      "upload",
      "version",
      "add"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "overview of promoting content pack",
    "contentLower": "this topic gives an overview of promoting a content pack in operations orchestration (oo) central. promotion promotion is the process of deploying content packs across several environments, in order to ensure that results are predictable and without (unknown) risks. this is in particular important for maintaining a stable production environment that contains content that has been tested and validated prior to deployment. for example, deploying the content pack on the following central environments: development, qa, staging, production. the final aim of promotion is to deploy a new content pack to the central production server, to make the flows available to users. promotion versus deployment deployment is part of a promotion. however, the promotion process also includes other tasks, such as: configuring the content pack: configuring worker group aliases, mapping system accounts, and more testing and troubleshooting the flows in the content pack content pack a content pack is a file con",
    "keywordsLower": [
      "overview",
      "promoting",
      "content",
      "pack",
      "promotion",
      "versus",
      "deployment",
      "promote",
      "what",
      "patch",
      "steps",
      "related",
      "topics",
      "topic",
      "gives",
      "operations",
      "orchestration",
      "oo",
      "central.",
      "process",
      "deploying",
      "packs",
      "across",
      "several",
      "environments",
      "order",
      "ensure",
      "results",
      "predictable",
      "unknown",
      "risks.",
      "particular",
      "important",
      "maintaining",
      "stable",
      "production",
      "environment",
      "contains",
      "tested",
      "validated",
      "prior",
      "deployment.",
      "example",
      "following",
      "central",
      "development",
      "qa",
      "staging",
      "production.",
      "final",
      "aim",
      "deploy",
      "new",
      "server",
      "make",
      "flows",
      "available",
      "users.",
      "part",
      "promotion.",
      "however",
      "includes",
      "tasks",
      "such",
      "configuring",
      "worker",
      "group",
      "aliases",
      "mapping",
      "system",
      "accounts",
      "testing",
      "troubleshooting",
      "file",
      "containing",
      "actions",
      "configuration",
      "items.",
      "flow",
      "authors",
      "complete",
      "project",
      "package",
      "server.",
      "cloudslang",
      "java",
      "based",
      "python",
      "based.",
      "native",
      "java-based",
      ".net",
      "there",
      "need",
      "environment.",
      "fixed",
      "bug",
      "upload",
      "version",
      "add"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage the flow library",
    "content": "After a content pack has been deployed, you can see the flows inside it in the Flow Library. From here, you can browse or filter flows, to find the one that you need. You can view the flow meta data, view reports about the last time that a flow was run, configure the persistence level and run timeout, and set the content permissions. You will only be able to edit the settings here if you have a role with the Manage Content Settings permission. To access the FLOW LIBRARY, go to Content Management > FLOW LIBRARY tab. The icons of CloudSlang flows and Native OO flows are different and are as below: CloudSlang flow: . OO Native flow: . When you select a flow in the flow library, information about that flow displays in the information pane to the right. This information includes a description of the flow, if one exists, the path to the location where the flow is stored, the flow version, UUID, and ROI. Reference material FLOW LIBRARY pane GUI item Description Filter By To locate the flow yo",
    "url": "manageooflowlibrary",
    "filename": "manageooflowlibrary",
    "headings": [
      "Reference material",
      "FLOW LIBRARY pane",
      "Edit Permissions dialog box",
      "Configuring the Flow Run Settings for flows",
      "Set permissions for content",
      "Display flow information",
      "Display information about the last time a flow was run",
      "Set up content permission for a flow or folder",
      "Set up the timeout for a flow",
      "Related topic"
    ],
    "keywords": [
      "manage",
      "flow",
      "library",
      "reference",
      "material",
      "pane",
      "edit",
      "permissions",
      "dialog",
      "box",
      "configuring",
      "run",
      "settings",
      "flows",
      "set",
      "content",
      "display",
      "information",
      "about",
      "last",
      "time",
      "permission",
      "folder",
      "timeout",
      "related",
      "topic",
      "after",
      "pack",
      "deployed",
      "see",
      "inside",
      "library.",
      "here",
      "browse",
      "filter",
      "find",
      "one",
      "need.",
      "view",
      "meta",
      "data",
      "reports",
      "configure",
      "persistence",
      "level",
      "permissions.",
      "able",
      "role",
      "permission.",
      "access",
      "go",
      "management",
      "tab.",
      "icons",
      "cloudslang",
      "native",
      "oo",
      "different",
      "below",
      "select",
      "displays",
      "right.",
      "includes",
      "description",
      "exists",
      "path",
      "location",
      "stored",
      "version",
      "uuid",
      "roi.",
      "gui",
      "item",
      "locate",
      "need",
      "enter",
      "part",
      "full",
      "text",
      "flows.",
      "button",
      "click",
      "selected",
      "flow.",
      "report",
      "details",
      "what",
      "happened",
      "run.",
      "available",
      "least",
      "once.",
      "open",
      "graph",
      "graphical",
      "representation",
      "id",
      "ids",
      "format.",
      "unique."
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage the flow library",
    "contentLower": "after a content pack has been deployed, you can see the flows inside it in the flow library. from here, you can browse or filter flows, to find the one that you need. you can view the flow meta data, view reports about the last time that a flow was run, configure the persistence level and run timeout, and set the content permissions. you will only be able to edit the settings here if you have a role with the manage content settings permission. to access the flow library, go to content management > flow library tab. the icons of cloudslang flows and native oo flows are different and are as below: cloudslang flow: . oo native flow: . when you select a flow in the flow library, information about that flow displays in the information pane to the right. this information includes a description of the flow, if one exists, the path to the location where the flow is stored, the flow version, uuid, and roi. reference material flow library pane gui item description filter by to locate the flow yo",
    "keywordsLower": [
      "manage",
      "flow",
      "library",
      "reference",
      "material",
      "pane",
      "edit",
      "permissions",
      "dialog",
      "box",
      "configuring",
      "run",
      "settings",
      "flows",
      "set",
      "content",
      "display",
      "information",
      "about",
      "last",
      "time",
      "permission",
      "folder",
      "timeout",
      "related",
      "topic",
      "after",
      "pack",
      "deployed",
      "see",
      "inside",
      "library.",
      "here",
      "browse",
      "filter",
      "find",
      "one",
      "need.",
      "view",
      "meta",
      "data",
      "reports",
      "configure",
      "persistence",
      "level",
      "permissions.",
      "able",
      "role",
      "permission.",
      "access",
      "go",
      "management",
      "tab.",
      "icons",
      "cloudslang",
      "native",
      "oo",
      "different",
      "below",
      "select",
      "displays",
      "right.",
      "includes",
      "description",
      "exists",
      "path",
      "location",
      "stored",
      "version",
      "uuid",
      "roi.",
      "gui",
      "item",
      "locate",
      "need",
      "enter",
      "part",
      "full",
      "text",
      "flows.",
      "button",
      "click",
      "selected",
      "flow.",
      "report",
      "details",
      "what",
      "happened",
      "run.",
      "available",
      "least",
      "once.",
      "open",
      "graph",
      "graphical",
      "representation",
      "id",
      "ids",
      "format.",
      "unique."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage Scripts",
    "content": "From the Manage Scripts dialog you can perform the following actions: Download a script: Click a script name to download the script to your system. Edit a script: Click the edit icon to edit the script. Delete a script: Click the trashcan icon to delete the script. A confirmation dialog will be displayed. If the script is currently in use, these uses will be displayed in the confirmation dialog. Upload a new script: Click the Upload button, then select a JavaScript file. Create a script: To create a script in the UI, click the Create script button and use the JavaScript editor to author the script. Select a script: Click the row for a script and then click Select. Close the dialog: Click the Close button. Only JavaScript files are displayed in this dialog. The JavaScript files for Dynamic Entry lists and for Input Validation are stored separately. For example: When accessing Manage Scripts for Input Validation, only scripts that have been uploaded for input validation will be displayed",
    "url": "managescripts",
    "filename": "managescripts",
    "headings": [],
    "keywords": [
      "manage",
      "scripts",
      "dialog",
      "perform",
      "following",
      "actions",
      "download",
      "script",
      "click",
      "name",
      "system.",
      "edit",
      "icon",
      "script.",
      "delete",
      "trashcan",
      "confirmation",
      "displayed.",
      "currently",
      "uses",
      "displayed",
      "dialog.",
      "upload",
      "new",
      "button",
      "select",
      "javascript",
      "file.",
      "create",
      "ui",
      "editor",
      "author",
      "row",
      "select.",
      "close",
      "button.",
      "files",
      "dynamic",
      "entry",
      "lists",
      "input",
      "validation",
      "stored",
      "separately.",
      "example",
      "accessing",
      "uploaded"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage scripts",
    "contentLower": "from the manage scripts dialog you can perform the following actions: download a script: click a script name to download the script to your system. edit a script: click the edit icon to edit the script. delete a script: click the trashcan icon to delete the script. a confirmation dialog will be displayed. if the script is currently in use, these uses will be displayed in the confirmation dialog. upload a new script: click the upload button, then select a javascript file. create a script: to create a script in the ui, click the create script button and use the javascript editor to author the script. select a script: click the row for a script and then click select. close the dialog: click the close button. only javascript files are displayed in this dialog. the javascript files for dynamic entry lists and for input validation are stored separately. for example: when accessing manage scripts for input validation, only scripts that have been uploaded for input validation will be displayed",
    "keywordsLower": [
      "manage",
      "scripts",
      "dialog",
      "perform",
      "following",
      "actions",
      "download",
      "script",
      "click",
      "name",
      "system.",
      "edit",
      "icon",
      "script.",
      "delete",
      "trashcan",
      "confirmation",
      "displayed.",
      "currently",
      "uses",
      "displayed",
      "dialog.",
      "upload",
      "new",
      "button",
      "select",
      "javascript",
      "file.",
      "create",
      "ui",
      "editor",
      "author",
      "row",
      "select.",
      "close",
      "button.",
      "files",
      "dynamic",
      "entry",
      "lists",
      "input",
      "validation",
      "stored",
      "separately.",
      "example",
      "accessing",
      "uploaded"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Provider selection panel",
    "content": "For more information about associating resource offerings to service component templates, see the topic on Associate resource offerings with component templates. After adding a resource offering to a component template, you can create one or more provider selection actions for that resource offering. You can also choose not to specify provider selection actions, in which case a provider associated to the resource offering will be randomly selected, honoring any environment to catalog associations that may be configured. The provider selection actions run during the Before phase of the Reserving lifecycle stage; the lifecycle stage can't be changed. By default internal actions are available to help with provider selection. For more information about these provider selection internal actions, see Provider selection internal actions. Tasks Navigate to or View (the Provider Selection panel) — From the Sequenced Designs page, select a service design, and then select the design version. Clic",
    "url": "comptmpltproviderselection",
    "filename": "comptmpltproviderselection",
    "headings": [
      "Tasks",
      "Related topics"
    ],
    "keywords": [
      "provider",
      "selection",
      "panel",
      "tasks",
      "related",
      "topics",
      "information",
      "about",
      "associating",
      "resource",
      "offerings",
      "service",
      "component",
      "templates",
      "see",
      "topic",
      "associate",
      "templates.",
      "after",
      "adding",
      "offering",
      "template",
      "create",
      "one",
      "actions",
      "offering.",
      "choose",
      "specify",
      "case",
      "associated",
      "randomly",
      "selected",
      "honoring",
      "any",
      "environment",
      "catalog",
      "associations",
      "configured.",
      "run",
      "during",
      "before",
      "phase",
      "reserving",
      "lifecycle",
      "stage",
      "changed.",
      "default",
      "internal",
      "available",
      "help",
      "selection.",
      "actions.",
      "navigate",
      "view",
      "sequenced",
      "designs",
      "page",
      "select",
      "design",
      "version.",
      "click",
      "designer",
      "tab.",
      "want",
      "add",
      "action.",
      "right",
      "pane",
      "gear",
      "icon",
      "edit",
      "tab",
      "name",
      "right.",
      "action",
      "add.",
      "configured",
      "middle",
      "panel.",
      "enter",
      "described",
      "wizard",
      "created",
      "part",
      "published",
      "design.",
      "next",
      "edit.",
      "update",
      "properties",
      "edited",
      "delete",
      "delete.",
      "set",
      "processing",
      "order",
      "relative",
      "run.",
      "drag",
      "change"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "provider selection panel",
    "contentLower": "for more information about associating resource offerings to service component templates, see the topic on associate resource offerings with component templates. after adding a resource offering to a component template, you can create one or more provider selection actions for that resource offering. you can also choose not to specify provider selection actions, in which case a provider associated to the resource offering will be randomly selected, honoring any environment to catalog associations that may be configured. the provider selection actions run during the before phase of the reserving lifecycle stage; the lifecycle stage can't be changed. by default internal actions are available to help with provider selection. for more information about these provider selection internal actions, see provider selection internal actions. tasks navigate to or view (the provider selection panel) — from the sequenced designs page, select a service design, and then select the design version. clic",
    "keywordsLower": [
      "provider",
      "selection",
      "panel",
      "tasks",
      "related",
      "topics",
      "information",
      "about",
      "associating",
      "resource",
      "offerings",
      "service",
      "component",
      "templates",
      "see",
      "topic",
      "associate",
      "templates.",
      "after",
      "adding",
      "offering",
      "template",
      "create",
      "one",
      "actions",
      "offering.",
      "choose",
      "specify",
      "case",
      "associated",
      "randomly",
      "selected",
      "honoring",
      "any",
      "environment",
      "catalog",
      "associations",
      "configured.",
      "run",
      "during",
      "before",
      "phase",
      "reserving",
      "lifecycle",
      "stage",
      "changed.",
      "default",
      "internal",
      "available",
      "help",
      "selection.",
      "actions.",
      "navigate",
      "view",
      "sequenced",
      "designs",
      "page",
      "select",
      "design",
      "version.",
      "click",
      "designer",
      "tab.",
      "want",
      "add",
      "action.",
      "right",
      "pane",
      "gear",
      "icon",
      "edit",
      "tab",
      "name",
      "right.",
      "action",
      "add.",
      "configured",
      "middle",
      "panel.",
      "enter",
      "described",
      "wizard",
      "created",
      "part",
      "published",
      "design.",
      "next",
      "edit.",
      "update",
      "properties",
      "edited",
      "delete",
      "delete.",
      "set",
      "processing",
      "order",
      "relative",
      "run.",
      "drag",
      "change"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Provider selection internal actions",
    "content": "For more information about provider selection actions, see Provider selection panel. By default internal actions are available to help with provider selection. If you have not configured resource pools on a provider, then the two provider selection actions most likely to be configured on a resource offering are the Build Resource Provider List action and the Select Resource Provider action, which should run in that order. To select the provider already selected by the parent service component, use the Select Resource Provider from Parent action instead of the two previously mentioned actions. If you have configured resource pools on a provider, then the two provider selection actions most likely to be configured on a resource offering are the Build Resource Provider and Pool List action and the Select Resource Provider and Pool action, which should run in that order. To select the provider and pool already selected by the parent service component, use the Select Resource Provider and P",
    "url": "comptmpltproviderselectionintaction",
    "filename": "comptmpltproviderselectionintaction",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "csa.log",
      "provider",
      "selection",
      "internal",
      "actions",
      "related",
      "topics",
      "information",
      "about",
      "see",
      "panel.",
      "default",
      "available",
      "help",
      "selection.",
      "configured",
      "resource",
      "pools",
      "two",
      "most",
      "likely",
      "offering",
      "build",
      "list",
      "action",
      "select",
      "run",
      "order.",
      "already",
      "selected",
      "parent",
      "service",
      "component",
      "instead",
      "previously",
      "mentioned",
      "actions.",
      "pool",
      "accounting",
      "offerings",
      "tokens.",
      "following",
      "table",
      "descriptions",
      "out-of-the-box",
      "description",
      "applies",
      "builds",
      "candidate",
      "providers",
      "associated",
      "meet",
      "requirements",
      "support",
      "offering.",
      "availability",
      "enabled.",
      "references",
      "design",
      "catalog",
      "environments",
      "further",
      "restricted",
      "include",
      "one",
      "environments.",
      "sufficient",
      "capacity.",
      "determine",
      "consider",
      "all",
      "measurable",
      "properties",
      "tab",
      "well",
      "optional",
      "multiplier",
      "property",
      "name",
      "field.",
      "enough",
      "capacity",
      "requires",
      "necessary",
      "type",
      "example",
      "cpu",
      "memory",
      "storage",
      "based",
      "either",
      "unlimited",
      "available.",
      "difference",
      "between",
      "total",
      "current",
      "utilization",
      "requirements.",
      "clone"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "provider selection internal actions",
    "contentLower": "for more information about provider selection actions, see provider selection panel. by default internal actions are available to help with provider selection. if you have not configured resource pools on a provider, then the two provider selection actions most likely to be configured on a resource offering are the build resource provider list action and the select resource provider action, which should run in that order. to select the provider already selected by the parent service component, use the select resource provider from parent action instead of the two previously mentioned actions. if you have configured resource pools on a provider, then the two provider selection actions most likely to be configured on a resource offering are the build resource provider and pool list action and the select resource provider and pool action, which should run in that order. to select the provider and pool already selected by the parent service component, use the select resource provider and p",
    "keywordsLower": [
      "csa.log",
      "provider",
      "selection",
      "internal",
      "actions",
      "related",
      "topics",
      "information",
      "about",
      "see",
      "panel.",
      "default",
      "available",
      "help",
      "selection.",
      "configured",
      "resource",
      "pools",
      "two",
      "most",
      "likely",
      "offering",
      "build",
      "list",
      "action",
      "select",
      "run",
      "order.",
      "already",
      "selected",
      "parent",
      "service",
      "component",
      "instead",
      "previously",
      "mentioned",
      "actions.",
      "pool",
      "accounting",
      "offerings",
      "tokens.",
      "following",
      "table",
      "descriptions",
      "out-of-the-box",
      "description",
      "applies",
      "builds",
      "candidate",
      "providers",
      "associated",
      "meet",
      "requirements",
      "support",
      "offering.",
      "availability",
      "enabled.",
      "references",
      "design",
      "catalog",
      "environments",
      "further",
      "restricted",
      "include",
      "one",
      "environments.",
      "sufficient",
      "capacity.",
      "determine",
      "consider",
      "all",
      "measurable",
      "properties",
      "tab",
      "well",
      "optional",
      "multiplier",
      "property",
      "name",
      "field.",
      "enough",
      "capacity",
      "requires",
      "necessary",
      "type",
      "example",
      "cpu",
      "memory",
      "storage",
      "based",
      "either",
      "unlimited",
      "available.",
      "difference",
      "between",
      "total",
      "current",
      "utilization",
      "requirements.",
      "clone"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Measurable properties panel",
    "content": "For more information about associating resource offerings to service components, see topic Associate resource offerings with component templates. A measurable property is an integer service component property that has a configured Resource Type and Unit (see the topic Service component properties). When you configure measurable properties on a resource offering, you create references to the corresponding service component measurable properties. To view the value of a measurable property, see the corresponding service component property in the Properties tab. Measurable properties are used by the following out-of-the-box actions to assist in provider and pool selection and in resource accounting: Build Resource Provider and Pool List Increase Resource Utilization Decrease Resource Utilization If you configure any of these actions on a resource offering in the Provider Selection or enabled Resource Accounting, you must also configure the measurable properties for that resource offering i",
    "url": "comptmpltromeasurableproperties",
    "filename": "comptmpltromeasurableproperties",
    "headings": [
      "Tasks",
      "Related topics"
    ],
    "keywords": [
      "measurable",
      "properties",
      "panel",
      "tasks",
      "related",
      "topics",
      "information",
      "about",
      "associating",
      "resource",
      "offerings",
      "service",
      "components",
      "see",
      "topic",
      "associate",
      "component",
      "templates.",
      "property",
      "integer",
      "configured",
      "type",
      "unit",
      "configure",
      "offering",
      "create",
      "references",
      "corresponding",
      "properties.",
      "view",
      "value",
      "tab.",
      "following",
      "out-of-the-box",
      "actions",
      "assist",
      "provider",
      "pool",
      "selection",
      "accounting",
      "build",
      "list",
      "increase",
      "utilization",
      "decrease",
      "any",
      "enabled",
      "panel.",
      "navigate",
      "left",
      "pane",
      "sequenced",
      "area",
      "select",
      "tag",
      "associated",
      "template.",
      "templates",
      "tab",
      "template",
      "contains",
      "whose",
      "want",
      "view.",
      "click",
      "name",
      "finally",
      "right.",
      "add",
      "add.",
      "doesn",
      "contain",
      "empty.",
      "added",
      "part",
      "published",
      "design.",
      "delete",
      "de-select",
      "remove.",
      "deleted"
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "measurable properties panel",
    "contentLower": "for more information about associating resource offerings to service components, see topic associate resource offerings with component templates. a measurable property is an integer service component property that has a configured resource type and unit (see the topic service component properties). when you configure measurable properties on a resource offering, you create references to the corresponding service component measurable properties. to view the value of a measurable property, see the corresponding service component property in the properties tab. measurable properties are used by the following out-of-the-box actions to assist in provider and pool selection and in resource accounting: build resource provider and pool list increase resource utilization decrease resource utilization if you configure any of these actions on a resource offering in the provider selection or enabled resource accounting, you must also configure the measurable properties for that resource offering i",
    "keywordsLower": [
      "measurable",
      "properties",
      "panel",
      "tasks",
      "related",
      "topics",
      "information",
      "about",
      "associating",
      "resource",
      "offerings",
      "service",
      "components",
      "see",
      "topic",
      "associate",
      "component",
      "templates.",
      "property",
      "integer",
      "configured",
      "type",
      "unit",
      "configure",
      "offering",
      "create",
      "references",
      "corresponding",
      "properties.",
      "view",
      "value",
      "tab.",
      "following",
      "out-of-the-box",
      "actions",
      "assist",
      "provider",
      "pool",
      "selection",
      "accounting",
      "build",
      "list",
      "increase",
      "utilization",
      "decrease",
      "any",
      "enabled",
      "panel.",
      "navigate",
      "left",
      "pane",
      "sequenced",
      "area",
      "select",
      "tag",
      "associated",
      "template.",
      "templates",
      "tab",
      "template",
      "contains",
      "whose",
      "want",
      "view.",
      "click",
      "name",
      "finally",
      "right.",
      "add",
      "add.",
      "doesn",
      "contain",
      "empty.",
      "added",
      "part",
      "published",
      "design.",
      "delete",
      "de-select",
      "remove.",
      "deleted"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Property mappings panel",
    "content": "Use the Property Mappings panel to view and configure mapping between resource offering properties and component properties. Property mappings enable property values to be passed between the component and the resource offering. Tasks Navigate to or View (the Property Mappings panel) — In the left pane of the Sequenced Components area, select the tag associated with the component template. Select the component type and, in the Templates tab, select the component template. In the Resource Offerings tab, click the name of the resource offering whose property mappings you want to view. Finally, select the Property Mappings panel on the right. Auto-Configure — Automatically map each resource offering property to a best-match component property. If no best match is found, a new property is automatically created. Configure Property Mapping — Click the gear icon next to a mapped resource offering property and then select Configure Mapping. Select one of the following options in the Configure P",
    "url": "comptmpltropropertymappings",
    "filename": "comptmpltropropertymappings",
    "headings": [
      "Tasks"
    ],
    "keywords": [
      "property",
      "mappings",
      "panel",
      "tasks",
      "view",
      "configure",
      "mapping",
      "between",
      "resource",
      "offering",
      "properties",
      "component",
      "properties.",
      "enable",
      "values",
      "passed",
      "offering.",
      "navigate",
      "left",
      "pane",
      "sequenced",
      "components",
      "area",
      "select",
      "tag",
      "associated",
      "template.",
      "type",
      "templates",
      "tab",
      "offerings",
      "click",
      "name",
      "whose",
      "want",
      "view.",
      "finally",
      "right.",
      "auto-configure",
      "automatically",
      "map",
      "best-match",
      "property.",
      "best",
      "match",
      "found",
      "new",
      "created.",
      "gear",
      "icon",
      "next",
      "mapped",
      "mapping.",
      "one",
      "following",
      "options",
      "dialog",
      "isn",
      "component.",
      "maps",
      "created",
      "automatically.",
      "list.",
      "save.",
      "delete",
      "confirm.",
      "deleting",
      "doesn",
      "service",
      "deleted",
      "part",
      "published",
      "design."
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "property mappings panel",
    "contentLower": "use the property mappings panel to view and configure mapping between resource offering properties and component properties. property mappings enable property values to be passed between the component and the resource offering. tasks navigate to or view (the property mappings panel) — in the left pane of the sequenced components area, select the tag associated with the component template. select the component type and, in the templates tab, select the component template. in the resource offerings tab, click the name of the resource offering whose property mappings you want to view. finally, select the property mappings panel on the right. auto-configure — automatically map each resource offering property to a best-match component property. if no best match is found, a new property is automatically created. configure property mapping — click the gear icon next to a mapped resource offering property and then select configure mapping. select one of the following options in the configure p",
    "keywordsLower": [
      "property",
      "mappings",
      "panel",
      "tasks",
      "view",
      "configure",
      "mapping",
      "between",
      "resource",
      "offering",
      "properties",
      "component",
      "properties.",
      "enable",
      "values",
      "passed",
      "offering.",
      "navigate",
      "left",
      "pane",
      "sequenced",
      "components",
      "area",
      "select",
      "tag",
      "associated",
      "template.",
      "type",
      "templates",
      "tab",
      "offerings",
      "click",
      "name",
      "whose",
      "want",
      "view.",
      "finally",
      "right.",
      "auto-configure",
      "automatically",
      "map",
      "best-match",
      "property.",
      "best",
      "match",
      "found",
      "new",
      "created.",
      "gear",
      "icon",
      "next",
      "mapped",
      "mapping.",
      "one",
      "following",
      "options",
      "dialog",
      "isn",
      "component.",
      "maps",
      "created",
      "automatically.",
      "list.",
      "save.",
      "delete",
      "confirm.",
      "deleting",
      "doesn",
      "service",
      "deleted",
      "part",
      "published",
      "design."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Overview tab for service design components",
    "content": "The Edit Component Overview tab allows you to modify attributes of a service component, including its Display Name, Description, Processing Order, and Consumer Visible and Pattern settings. For more information about these properties, see topics Edit a service component and Create a component type. Tasks Edit - If a field is editable, enter data or select/clear a check box. For a published design, you can't edit the following attributes: Display Name — The name you provide for the service component. Description — The description you provide for the service component. Processing Order (default value is 1, minimum value is 1, maximum value is 99) — Select a number to specify the deployment processing order for this service component relative to its sibling service components (that is, service components who share the same parent). Service components are processed in ascending order during service deployment and in descending order during undeployment. Image — An image appears for the com",
    "url": "mgmtconssdeditcompoverview",
    "filename": "mgmtconssdeditcompoverview",
    "headings": [
      "Tasks",
      "Related topics"
    ],
    "keywords": [
      "overview",
      "tab",
      "service",
      "design",
      "components",
      "tasks",
      "related",
      "topics",
      "edit",
      "component",
      "allows",
      "modify",
      "attributes",
      "including",
      "display",
      "name",
      "description",
      "processing",
      "order",
      "consumer",
      "visible",
      "pattern",
      "settings.",
      "information",
      "about",
      "properties",
      "see",
      "create",
      "type.",
      "field",
      "editable",
      "enter",
      "data",
      "select",
      "clear",
      "check",
      "box.",
      "published",
      "following",
      "provide",
      "component.",
      "default",
      "value",
      "minimum",
      "maximum",
      "99",
      "number",
      "specify",
      "deployment",
      "relative",
      "sibling",
      "share",
      "same",
      "parent",
      "processed",
      "ascending",
      "during",
      "descending",
      "undeployment.",
      "image",
      "appears",
      "click",
      "change",
      "desired.",
      "dialog",
      "allowing",
      "choose",
      "variety",
      "available",
      "images",
      "upload",
      "add",
      "own",
      "image.",
      "false",
      "consumer.",
      "de-select",
      "visible.",
      "lifecycle",
      "actions",
      "configured",
      "displayed",
      "associated",
      "itself",
      "visibility",
      "doesn",
      "affect",
      "child",
      "necessary",
      "box",
      "mark",
      "pattern.",
      "indicates",
      "automatically",
      "engine.",
      "clone",
      "action",
      "example",
      "out-of-the-box",
      "type"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "overview tab for service design components",
    "contentLower": "the edit component overview tab allows you to modify attributes of a service component, including its display name, description, processing order, and consumer visible and pattern settings. for more information about these properties, see topics edit a service component and create a component type. tasks edit - if a field is editable, enter data or select/clear a check box. for a published design, you can't edit the following attributes: display name — the name you provide for the service component. description — the description you provide for the service component. processing order (default value is 1, minimum value is 1, maximum value is 99) — select a number to specify the deployment processing order for this service component relative to its sibling service components (that is, service components who share the same parent). service components are processed in ascending order during service deployment and in descending order during undeployment. image — an image appears for the com",
    "keywordsLower": [
      "overview",
      "tab",
      "service",
      "design",
      "components",
      "tasks",
      "related",
      "topics",
      "edit",
      "component",
      "allows",
      "modify",
      "attributes",
      "including",
      "display",
      "name",
      "description",
      "processing",
      "order",
      "consumer",
      "visible",
      "pattern",
      "settings.",
      "information",
      "about",
      "properties",
      "see",
      "create",
      "type.",
      "field",
      "editable",
      "enter",
      "data",
      "select",
      "clear",
      "check",
      "box.",
      "published",
      "following",
      "provide",
      "component.",
      "default",
      "value",
      "minimum",
      "maximum",
      "99",
      "number",
      "specify",
      "deployment",
      "relative",
      "sibling",
      "share",
      "same",
      "parent",
      "processed",
      "ascending",
      "during",
      "descending",
      "undeployment.",
      "image",
      "appears",
      "click",
      "change",
      "desired.",
      "dialog",
      "allowing",
      "choose",
      "variety",
      "available",
      "images",
      "upload",
      "add",
      "own",
      "image.",
      "false",
      "consumer.",
      "de-select",
      "visible.",
      "lifecycle",
      "actions",
      "configured",
      "displayed",
      "associated",
      "itself",
      "visibility",
      "doesn",
      "affect",
      "child",
      "necessary",
      "box",
      "mark",
      "pattern.",
      "indicates",
      "automatically",
      "engine.",
      "clone",
      "action",
      "example",
      "out-of-the-box",
      "type"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Provider selection panel",
    "content": "After adding a resource offering to a component template, you can create one or more provider selection actions for that resource offering. You can also choose not to specify provider selection actions, in which case a provider associated to the resource offering will be randomly selected, honoring any environment to catalog associations that may be configured. The provider selection actions run during the Before phase of the Reserving lifecycle stage; the lifecycle stage can't be changed. The product ships with internal actions to help with provider selection. For more information about these provider selection internal actions, see Provider selection internal actions. Tasks Navigate to or View (the Provider Selection panel) — Select a service design version. Click the Designer tab. Select the service component on which you want to add a provider selection action. In the right pane, click the gear icon, select Edit Component, and select the Resource Offerings tab. In the Resource Offe",
    "url": "mgmtconssdroproviderselection",
    "filename": "mgmtconssdroproviderselection",
    "headings": [
      "Tasks"
    ],
    "keywords": [
      "provider",
      "selection",
      "panel",
      "tasks",
      "after",
      "adding",
      "resource",
      "offering",
      "component",
      "template",
      "create",
      "one",
      "actions",
      "offering.",
      "choose",
      "specify",
      "case",
      "associated",
      "randomly",
      "selected",
      "honoring",
      "any",
      "environment",
      "catalog",
      "associations",
      "configured.",
      "run",
      "during",
      "before",
      "phase",
      "reserving",
      "lifecycle",
      "stage",
      "changed.",
      "product",
      "ships",
      "internal",
      "help",
      "selection.",
      "information",
      "about",
      "see",
      "actions.",
      "navigate",
      "view",
      "select",
      "service",
      "design",
      "version.",
      "click",
      "designer",
      "tab.",
      "want",
      "add",
      "action.",
      "right",
      "pane",
      "gear",
      "icon",
      "edit",
      "offerings",
      "tab",
      "name",
      "right.",
      "action",
      "add.",
      "configured",
      "middle",
      "panel.",
      "enter",
      "described",
      "wizard",
      "sequenced",
      "designs.",
      "created",
      "part",
      "published",
      "design.",
      "next",
      "edit.",
      "update",
      "properties",
      "edited",
      "delete",
      "delete.",
      "set",
      "processing",
      "order",
      "relative",
      "run.",
      "drag",
      "change"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "provider selection panel",
    "contentLower": "after adding a resource offering to a component template, you can create one or more provider selection actions for that resource offering. you can also choose not to specify provider selection actions, in which case a provider associated to the resource offering will be randomly selected, honoring any environment to catalog associations that may be configured. the provider selection actions run during the before phase of the reserving lifecycle stage; the lifecycle stage can't be changed. the product ships with internal actions to help with provider selection. for more information about these provider selection internal actions, see provider selection internal actions. tasks navigate to or view (the provider selection panel) — select a service design version. click the designer tab. select the service component on which you want to add a provider selection action. in the right pane, click the gear icon, select edit component, and select the resource offerings tab. in the resource offe",
    "keywordsLower": [
      "provider",
      "selection",
      "panel",
      "tasks",
      "after",
      "adding",
      "resource",
      "offering",
      "component",
      "template",
      "create",
      "one",
      "actions",
      "offering.",
      "choose",
      "specify",
      "case",
      "associated",
      "randomly",
      "selected",
      "honoring",
      "any",
      "environment",
      "catalog",
      "associations",
      "configured.",
      "run",
      "during",
      "before",
      "phase",
      "reserving",
      "lifecycle",
      "stage",
      "changed.",
      "product",
      "ships",
      "internal",
      "help",
      "selection.",
      "information",
      "about",
      "see",
      "actions.",
      "navigate",
      "view",
      "select",
      "service",
      "design",
      "version.",
      "click",
      "designer",
      "tab.",
      "want",
      "add",
      "action.",
      "right",
      "pane",
      "gear",
      "icon",
      "edit",
      "offerings",
      "tab",
      "name",
      "right.",
      "action",
      "add.",
      "configured",
      "middle",
      "panel.",
      "enter",
      "described",
      "wizard",
      "sequenced",
      "designs.",
      "created",
      "part",
      "published",
      "design.",
      "next",
      "edit.",
      "update",
      "properties",
      "edited",
      "delete",
      "delete.",
      "set",
      "processing",
      "order",
      "relative",
      "run.",
      "drag",
      "change"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Provider selection internal actions",
    "content": "Internal actions are shipped with the product to help with provider selection. If you have not configured resource pools on a provider, then the two provider selection actions most likely to be configured on a resource offering are the Build Resource Provider List action and the Select Resource Provider action, which should run in that order. To select the provider already selected by the parent service component, use the Select Resource Provider from Parent action instead of the two previously mentioned actions. If you have configured resource pools on a provider, then the two provider selection actions most likely to be configured on a resource offering are the Build Resource Provider and Pool List action and the Select Resource Provider and Pool action, which should run in that order. To select the provider and pool already selected by the parent service component, use the Select Resource Provider and Pool from Parent action instead of the two previously mentioned actions. For infor",
    "url": "mgmtconssdprovselectintact",
    "filename": "mgmtconssdprovselectintact",
    "headings": [],
    "keywords": [
      "csa.log",
      "provider",
      "selection",
      "internal",
      "actions",
      "shipped",
      "product",
      "help",
      "selection.",
      "configured",
      "resource",
      "pools",
      "two",
      "most",
      "likely",
      "offering",
      "build",
      "list",
      "action",
      "select",
      "run",
      "order.",
      "already",
      "selected",
      "parent",
      "service",
      "component",
      "instead",
      "previously",
      "mentioned",
      "actions.",
      "pool",
      "information",
      "about",
      "accounting",
      "offerings",
      "see",
      "tokens.",
      "following",
      "table",
      "descriptions",
      "out-of-the-box",
      "description",
      "applies",
      "builds",
      "candidate",
      "providers",
      "associated",
      "meet",
      "requirements",
      "support",
      "offering.",
      "availability",
      "enabled.",
      "references",
      "design",
      "catalog",
      "environments",
      "further",
      "restricted",
      "include",
      "one",
      "environments.",
      "sufficient",
      "capacity.",
      "determine",
      "consider",
      "all",
      "measurable",
      "properties",
      "tab",
      "well",
      "optional",
      "multiplier",
      "property",
      "name",
      "field.",
      "enough",
      "capacity",
      "requires",
      "necessary",
      "type",
      "example",
      "cpu",
      "memory",
      "storage",
      "based",
      "either",
      "unlimited",
      "available.",
      "available",
      "difference",
      "between",
      "total",
      "csa",
      "current",
      "utilization",
      "requirements.",
      "clone",
      "pattern"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "provider selection internal actions",
    "contentLower": "internal actions are shipped with the product to help with provider selection. if you have not configured resource pools on a provider, then the two provider selection actions most likely to be configured on a resource offering are the build resource provider list action and the select resource provider action, which should run in that order. to select the provider already selected by the parent service component, use the select resource provider from parent action instead of the two previously mentioned actions. if you have configured resource pools on a provider, then the two provider selection actions most likely to be configured on a resource offering are the build resource provider and pool list action and the select resource provider and pool action, which should run in that order. to select the provider and pool already selected by the parent service component, use the select resource provider and pool from parent action instead of the two previously mentioned actions. for infor",
    "keywordsLower": [
      "csa.log",
      "provider",
      "selection",
      "internal",
      "actions",
      "shipped",
      "product",
      "help",
      "selection.",
      "configured",
      "resource",
      "pools",
      "two",
      "most",
      "likely",
      "offering",
      "build",
      "list",
      "action",
      "select",
      "run",
      "order.",
      "already",
      "selected",
      "parent",
      "service",
      "component",
      "instead",
      "previously",
      "mentioned",
      "actions.",
      "pool",
      "information",
      "about",
      "accounting",
      "offerings",
      "see",
      "tokens.",
      "following",
      "table",
      "descriptions",
      "out-of-the-box",
      "description",
      "applies",
      "builds",
      "candidate",
      "providers",
      "associated",
      "meet",
      "requirements",
      "support",
      "offering.",
      "availability",
      "enabled.",
      "references",
      "design",
      "catalog",
      "environments",
      "further",
      "restricted",
      "include",
      "one",
      "environments.",
      "sufficient",
      "capacity.",
      "determine",
      "consider",
      "all",
      "measurable",
      "properties",
      "tab",
      "well",
      "optional",
      "multiplier",
      "property",
      "name",
      "field.",
      "enough",
      "capacity",
      "requires",
      "necessary",
      "type",
      "example",
      "cpu",
      "memory",
      "storage",
      "based",
      "either",
      "unlimited",
      "available.",
      "available",
      "difference",
      "between",
      "total",
      "csa",
      "current",
      "utilization",
      "requirements.",
      "clone",
      "pattern"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Measurable properties panel",
    "content": "A measurable property is an integer service component property that has a configured resource type and unit (see Service component properties). When you configure measurable properties on a resource offering, you create references to the corresponding service component measurable properties. To view the value of a measurable property, see the corresponding service component property in the Properties tab. Measurable properties are used by the following out-of-the-box actions to assist in provider and pool selection and in resource accounting: Build Resource Provider and Pool List Increase Resource Utilization Decrease Resource Utilization If you configure any of these actions on a resource offering in the Provider Selection or enabled Resource Accounting, you must also configure the measurable properties for that resource offering in the Measurable Properties panel. Tasks Navigate to or View (the Measurable Properties panel) — In the left pane of the All Design area, select the tag ass",
    "url": "mgmtconssdromeasurableproperties",
    "filename": "mgmtconssdromeasurableproperties",
    "headings": [
      "Tasks"
    ],
    "keywords": [
      "measurable",
      "properties",
      "panel",
      "tasks",
      "property",
      "integer",
      "service",
      "component",
      "configured",
      "resource",
      "type",
      "unit",
      "see",
      "configure",
      "offering",
      "create",
      "references",
      "corresponding",
      "properties.",
      "view",
      "value",
      "tab.",
      "following",
      "out-of-the-box",
      "actions",
      "assist",
      "provider",
      "pool",
      "selection",
      "accounting",
      "build",
      "list",
      "increase",
      "utilization",
      "decrease",
      "any",
      "enabled",
      "panel.",
      "navigate",
      "left",
      "pane",
      "all",
      "design",
      "area",
      "select",
      "tag",
      "associated",
      "want",
      "view.",
      "designer",
      "tab",
      "contains",
      "whose",
      "right",
      "click",
      "gear",
      "icon",
      "edit",
      "component.",
      "offerings",
      "name",
      "finally",
      "right.",
      "add",
      "add.",
      "doesn",
      "contain",
      "empty.",
      "added",
      "part",
      "published",
      "design.",
      "delete",
      "de-select",
      "remove.",
      "deleted"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "measurable properties panel",
    "contentLower": "a measurable property is an integer service component property that has a configured resource type and unit (see service component properties). when you configure measurable properties on a resource offering, you create references to the corresponding service component measurable properties. to view the value of a measurable property, see the corresponding service component property in the properties tab. measurable properties are used by the following out-of-the-box actions to assist in provider and pool selection and in resource accounting: build resource provider and pool list increase resource utilization decrease resource utilization if you configure any of these actions on a resource offering in the provider selection or enabled resource accounting, you must also configure the measurable properties for that resource offering in the measurable properties panel. tasks navigate to or view (the measurable properties panel) — in the left pane of the all design area, select the tag ass",
    "keywordsLower": [
      "measurable",
      "properties",
      "panel",
      "tasks",
      "property",
      "integer",
      "service",
      "component",
      "configured",
      "resource",
      "type",
      "unit",
      "see",
      "configure",
      "offering",
      "create",
      "references",
      "corresponding",
      "properties.",
      "view",
      "value",
      "tab.",
      "following",
      "out-of-the-box",
      "actions",
      "assist",
      "provider",
      "pool",
      "selection",
      "accounting",
      "build",
      "list",
      "increase",
      "utilization",
      "decrease",
      "any",
      "enabled",
      "panel.",
      "navigate",
      "left",
      "pane",
      "all",
      "design",
      "area",
      "select",
      "tag",
      "associated",
      "want",
      "view.",
      "designer",
      "tab",
      "contains",
      "whose",
      "right",
      "click",
      "gear",
      "icon",
      "edit",
      "component.",
      "offerings",
      "name",
      "finally",
      "right.",
      "add",
      "add.",
      "doesn",
      "contain",
      "empty.",
      "added",
      "part",
      "published",
      "design.",
      "delete",
      "de-select",
      "remove.",
      "deleted"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Property mappings",
    "content": "Property mappings enable property values to be passed between the component and the resource offering. Tasks View property mapping — In the left pane of the All Design area, select the tag associated with the design you want to view. Select the service design and, in the Designer tab, select the service component that contains the resource offering whose property mappings you want to view. In the right pane, click the gear icon and select Edit Component. In the Resource Offerings tab, click the name of the resource offering whose property mappings you want to view. Select the Property Mappings panel on the right. Automatically configure property mapping — Automatically map each resource offering property to a best-match component property. If no best match is found, a new property is automatically created. Configure property mapping — Click the gear icon next to a mapped resource offering property and then select Configure Mapping. Select one of the following options in the dialog: Not",
    "url": "mgmtconssdropropertymappings",
    "filename": "mgmtconssdropropertymappings",
    "headings": [
      "Tasks"
    ],
    "keywords": [
      "property",
      "mappings",
      "tasks",
      "enable",
      "values",
      "passed",
      "between",
      "component",
      "resource",
      "offering.",
      "view",
      "mapping",
      "left",
      "pane",
      "all",
      "design",
      "area",
      "select",
      "tag",
      "associated",
      "want",
      "view.",
      "service",
      "designer",
      "tab",
      "contains",
      "offering",
      "whose",
      "right",
      "click",
      "gear",
      "icon",
      "edit",
      "component.",
      "offerings",
      "name",
      "panel",
      "right.",
      "automatically",
      "configure",
      "map",
      "best-match",
      "property.",
      "best",
      "match",
      "found",
      "new",
      "created.",
      "next",
      "mapped",
      "mapping.",
      "one",
      "following",
      "options",
      "dialog",
      "isn",
      "auto-configure",
      "maps",
      "created",
      "automatically.",
      "list.",
      "delete",
      "confirm.",
      "deleting",
      "doesn",
      "deleted",
      "part",
      "published",
      "design."
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "property mappings",
    "contentLower": "property mappings enable property values to be passed between the component and the resource offering. tasks view property mapping — in the left pane of the all design area, select the tag associated with the design you want to view. select the service design and, in the designer tab, select the service component that contains the resource offering whose property mappings you want to view. in the right pane, click the gear icon and select edit component. in the resource offerings tab, click the name of the resource offering whose property mappings you want to view. select the property mappings panel on the right. automatically configure property mapping — automatically map each resource offering property to a best-match component property. if no best match is found, a new property is automatically created. configure property mapping — click the gear icon next to a mapped resource offering property and then select configure mapping. select one of the following options in the dialog: not",
    "keywordsLower": [
      "property",
      "mappings",
      "tasks",
      "enable",
      "values",
      "passed",
      "between",
      "component",
      "resource",
      "offering.",
      "view",
      "mapping",
      "left",
      "pane",
      "all",
      "design",
      "area",
      "select",
      "tag",
      "associated",
      "want",
      "view.",
      "service",
      "designer",
      "tab",
      "contains",
      "offering",
      "whose",
      "right",
      "click",
      "gear",
      "icon",
      "edit",
      "component.",
      "offerings",
      "name",
      "panel",
      "right.",
      "automatically",
      "configure",
      "map",
      "best-match",
      "property.",
      "best",
      "match",
      "found",
      "new",
      "created.",
      "next",
      "mapped",
      "mapping.",
      "one",
      "following",
      "options",
      "dialog",
      "isn",
      "auto-configure",
      "maps",
      "created",
      "automatically.",
      "list.",
      "delete",
      "confirm.",
      "deleting",
      "doesn",
      "deleted",
      "part",
      "published",
      "design."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Property mapping",
    "content": "You can create a property mapping between the properties of two components in a design when the property of one component should obtain its value from the property of another component. Using property mapping, you can share a single property value between multiple components in a design without the need to duplicate the value of the property across those components. For example, if your design includes three components that represent servers, and you need each server to be provisioned with the same number of CPUs, you can define the number of CPUs on one component and then create a property mapping between the CPU properties of the other components and the component that stores that common value. A property mapping can only be created between properties of the same type. A property mapping can't be created between two properties on the same component, or where the mapping creates a loop. For example, a property mapping where property A is mapped to obtain its value from property B, whi",
    "url": "mgmtconssdservicecompropmapping",
    "filename": "mgmtconssdservicecompropmapping",
    "headings": [
      "Tasks"
    ],
    "keywords": [
      "property",
      "mapping",
      "tasks",
      "create",
      "between",
      "properties",
      "two",
      "components",
      "design",
      "one",
      "component",
      "obtain",
      "value",
      "another",
      "component.",
      "share",
      "single",
      "multiple",
      "need",
      "duplicate",
      "across",
      "components.",
      "example",
      "includes",
      "three",
      "represent",
      "servers",
      "server",
      "provisioned",
      "same",
      "number",
      "cpus",
      "define",
      "cpu",
      "stores",
      "common",
      "value.",
      "created",
      "type.",
      "creates",
      "loop.",
      "mapped",
      "isn",
      "valid.",
      "select",
      "service",
      "want",
      "add",
      "mapping.",
      "pane",
      "click",
      "gear",
      "icon",
      "next",
      "map",
      "property.",
      "dialog",
      "different",
      "compatible",
      "selected",
      "flow",
      "displayed",
      "bottom",
      "dialog.",
      "delete",
      "view",
      "view.",
      "chain",
      "right.",
      "edit",
      "update",
      "highlighted",
      "right",
      "pane."
    ],
    "language": "en",
    "word_count": 92,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "property mapping",
    "contentLower": "you can create a property mapping between the properties of two components in a design when the property of one component should obtain its value from the property of another component. using property mapping, you can share a single property value between multiple components in a design without the need to duplicate the value of the property across those components. for example, if your design includes three components that represent servers, and you need each server to be provisioned with the same number of cpus, you can define the number of cpus on one component and then create a property mapping between the cpu properties of the other components and the component that stores that common value. a property mapping can only be created between properties of the same type. a property mapping can't be created between two properties on the same component, or where the mapping creates a loop. for example, a property mapping where property a is mapped to obtain its value from property b, whi",
    "keywordsLower": [
      "property",
      "mapping",
      "tasks",
      "create",
      "between",
      "properties",
      "two",
      "components",
      "design",
      "one",
      "component",
      "obtain",
      "value",
      "another",
      "component.",
      "share",
      "single",
      "multiple",
      "need",
      "duplicate",
      "across",
      "components.",
      "example",
      "includes",
      "three",
      "represent",
      "servers",
      "server",
      "provisioned",
      "same",
      "number",
      "cpus",
      "define",
      "cpu",
      "stores",
      "common",
      "value.",
      "created",
      "type.",
      "creates",
      "loop.",
      "mapped",
      "isn",
      "valid.",
      "select",
      "service",
      "want",
      "add",
      "mapping.",
      "pane",
      "click",
      "gear",
      "icon",
      "next",
      "map",
      "property.",
      "dialog",
      "different",
      "compatible",
      "selected",
      "flow",
      "displayed",
      "bottom",
      "dialog.",
      "delete",
      "view",
      "view.",
      "chain",
      "right.",
      "edit",
      "update",
      "highlighted",
      "right",
      "pane."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Overview tab",
    "content": "Tasks Edit a resource offering — To edit the resource offering, in the resource offering's details page, click the gear icon and choose Edit. Edit the display name or description and click Save. Delete a resource offering — To delete the resource offering, in the resource offering's details page, click the gear icon and choose Delete. Caution An offering can't be deleted if it's used in a service design. When an offering is deleted, its associations to providers are automatically removed. Copy a resource offering (Save As) — To copy a resource offering, in the resource offering's details page, click the gear icon and choose Save As. Enter a display name and description and click Save. Export a resource offering — To export a resource offering, in the resource offering's details page, click the gear icon and click Export. See Import and export a resource offering. Related topics Import and export a resource offering",
    "url": "mgmtconsrooverviewtab",
    "filename": "mgmtconsrooverviewtab",
    "headings": [
      "Tasks",
      "Related topics"
    ],
    "keywords": [
      "overview",
      "tab",
      "tasks",
      "related",
      "topics",
      "edit",
      "resource",
      "offering",
      "details",
      "page",
      "click",
      "gear",
      "icon",
      "choose",
      "edit.",
      "display",
      "name",
      "description",
      "save.",
      "delete",
      "delete.",
      "caution",
      "deleted",
      "service",
      "design.",
      "associations",
      "providers",
      "automatically",
      "removed.",
      "copy",
      "save",
      "as.",
      "enter",
      "export",
      "export.",
      "see",
      "import",
      "offering."
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "overview tab",
    "contentLower": "tasks edit a resource offering — to edit the resource offering, in the resource offering's details page, click the gear icon and choose edit. edit the display name or description and click save. delete a resource offering — to delete the resource offering, in the resource offering's details page, click the gear icon and choose delete. caution an offering can't be deleted if it's used in a service design. when an offering is deleted, its associations to providers are automatically removed. copy a resource offering (save as) — to copy a resource offering, in the resource offering's details page, click the gear icon and choose save as. enter a display name and description and click save. export a resource offering — to export a resource offering, in the resource offering's details page, click the gear icon and click export. see import and export a resource offering. related topics import and export a resource offering",
    "keywordsLower": [
      "overview",
      "tab",
      "tasks",
      "related",
      "topics",
      "edit",
      "resource",
      "offering",
      "details",
      "page",
      "click",
      "gear",
      "icon",
      "choose",
      "edit.",
      "display",
      "name",
      "description",
      "save.",
      "delete",
      "delete.",
      "caution",
      "deleted",
      "service",
      "design.",
      "associations",
      "providers",
      "automatically",
      "removed.",
      "copy",
      "save",
      "as.",
      "enter",
      "export",
      "export.",
      "see",
      "import",
      "offering."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage resource category",
    "content": "Resource offering categories allow you to classify resource offerings for improved filtering and identification. The product includes some predefined categories out-of-the-box (shown below) or you can create your own. A category is associated with a resource offering and can be used when assigning resource offerings to service designs. Service components within a service design can accept offering assignments only for those offerings with a category that's supported by the service component type. For example, a Server service component can be associated with offerings from a category of Compute (among others), while a Software Application Service component can be associated with offerings with a category of Application. Resource categories are available by selecting By Category from the list in the left navigation pane. In the resource offering's landing page, click the gear icon and chose Manage Resource Category. Use the add, edit, or delete icons to add, edit, or delete the category",
    "url": "mgmtconsroresourcecategory",
    "filename": "mgmtconsroresourcecategory",
    "headings": [
      "Tasks",
      "Create a resource category",
      "Edit a resource category",
      "Delete a resource category"
    ],
    "keywords": [
      "manage",
      "resource",
      "category",
      "tasks",
      "create",
      "edit",
      "delete",
      "offering",
      "categories",
      "allow",
      "classify",
      "offerings",
      "improved",
      "filtering",
      "identification.",
      "product",
      "includes",
      "predefined",
      "out-of-the-box",
      "shown",
      "below",
      "own.",
      "associated",
      "assigning",
      "service",
      "designs.",
      "components",
      "design",
      "accept",
      "assignments",
      "supported",
      "component",
      "type.",
      "example",
      "server",
      "compute",
      "among",
      "others",
      "while",
      "software",
      "application",
      "application.",
      "available",
      "selecting",
      "list",
      "left",
      "navigation",
      "pane.",
      "landing",
      "page",
      "click",
      "gear",
      "icon",
      "chose",
      "category.",
      "add",
      "icons",
      "select",
      "dropdown",
      "properties",
      "create.",
      "item",
      "description",
      "display",
      "name",
      "provide",
      "image",
      "displays",
      "change",
      "image.",
      "choose",
      "want",
      "select.",
      "upload",
      "own",
      "file",
      "extensions",
      "include",
      ".jpg",
      ".jpeg",
      ".gif",
      ".png.",
      "recommended",
      "size",
      "256",
      "pixels",
      "scaled",
      "appropriate",
      "size.",
      "filter",
      "edit.",
      "save",
      "done.",
      "deleted",
      "them.",
      "re",
      "out-of-the-box.",
      "delete.",
      "confirm",
      "deletion"
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage resource category",
    "contentLower": "resource offering categories allow you to classify resource offerings for improved filtering and identification. the product includes some predefined categories out-of-the-box (shown below) or you can create your own. a category is associated with a resource offering and can be used when assigning resource offerings to service designs. service components within a service design can accept offering assignments only for those offerings with a category that's supported by the service component type. for example, a server service component can be associated with offerings from a category of compute (among others), while a software application service component can be associated with offerings with a category of application. resource categories are available by selecting by category from the list in the left navigation pane. in the resource offering's landing page, click the gear icon and chose manage resource category. use the add, edit, or delete icons to add, edit, or delete the category",
    "keywordsLower": [
      "manage",
      "resource",
      "category",
      "tasks",
      "create",
      "edit",
      "delete",
      "offering",
      "categories",
      "allow",
      "classify",
      "offerings",
      "improved",
      "filtering",
      "identification.",
      "product",
      "includes",
      "predefined",
      "out-of-the-box",
      "shown",
      "below",
      "own.",
      "associated",
      "assigning",
      "service",
      "designs.",
      "components",
      "design",
      "accept",
      "assignments",
      "supported",
      "component",
      "type.",
      "example",
      "server",
      "compute",
      "among",
      "others",
      "while",
      "software",
      "application",
      "application.",
      "available",
      "selecting",
      "list",
      "left",
      "navigation",
      "pane.",
      "landing",
      "page",
      "click",
      "gear",
      "icon",
      "chose",
      "category.",
      "add",
      "icons",
      "select",
      "dropdown",
      "properties",
      "create.",
      "item",
      "description",
      "display",
      "name",
      "provide",
      "image",
      "displays",
      "change",
      "image.",
      "choose",
      "want",
      "select.",
      "upload",
      "own",
      "file",
      "extensions",
      "include",
      ".jpg",
      ".jpeg",
      ".gif",
      ".png.",
      "recommended",
      "size",
      "256",
      "pixels",
      "scaled",
      "appropriate",
      "size.",
      "filter",
      "edit.",
      "save",
      "done.",
      "deleted",
      "them.",
      "re",
      "out-of-the-box.",
      "delete.",
      "confirm",
      "deletion"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Navigate OO Workflow Designer",
    "content": "The OO Workflow Designer user interface includes several sections: PROJECTS pane The Projects pane on the top left shows the project you are currently working on. It displays the Content Generator for REST API , Project tree, editable flows, and Configuration items that you can use in the project. DEPENDENCIES pane The DEPENDENCIES pane on the top left shows the imported content packs. You can import an existing content pack and use its flows, operations, and configuration items in your flows. Authoring pane The Authoring pane is a rectangular box in the middle. It contains the canvas to work on flows. You can launch a flow onto the canvas area by double clicking a flow from the Projects pane. If you are working on multiple flows, they're indicated in tabs with the corresponding flow names. Some of these tabs are hidden from view and are displayed in the top right corner of the authoring pane with a number indicating the number of hidden tabs . Click the arrow to view a list of hidden ",
    "url": "designernavigation",
    "filename": "designernavigation",
    "headings": [
      "PROJECTS pane",
      "DEPENDENCIES pane",
      "Authoring pane",
      "PROPERTIES tab",
      "GRAPH pane",
      "DEBUG pane",
      "SCM pane",
      "Related topics"
    ],
    "keywords": [
      "navigate",
      "oo",
      "workflow",
      "designer",
      "projects",
      "pane",
      "dependencies",
      "authoring",
      "properties",
      "tab",
      "graph",
      "debug",
      "scm",
      "related",
      "topics",
      "user",
      "interface",
      "includes",
      "several",
      "sections",
      "top",
      "left",
      "shows",
      "project",
      "currently",
      "working",
      "on.",
      "displays",
      "content",
      "generator",
      "rest",
      "api",
      "tree",
      "editable",
      "flows",
      "configuration",
      "items",
      "project.",
      "imported",
      "packs.",
      "import",
      "existing",
      "pack",
      "operations",
      "flows.",
      "rectangular",
      "box",
      "middle.",
      "contains",
      "canvas",
      "work",
      "launch",
      "flow",
      "onto",
      "area",
      "double",
      "clicking",
      "pane.",
      "multiple",
      "re",
      "indicated",
      "tabs",
      "corresponding",
      "names.",
      "hidden",
      "view",
      "displayed",
      "right",
      "corner",
      "number",
      "indicating",
      "click",
      "arrow",
      "list",
      "point",
      "select",
      "open.",
      "close",
      "next",
      "name.",
      "inputs",
      "outputs",
      "results",
      "description",
      "about",
      "current",
      "flow.",
      "display",
      "add",
      "configure",
      "descriptions.",
      "enter",
      "graphical",
      "representation",
      "tabs.",
      "relevant",
      "step",
      "bottom",
      "operation",
      "inputs."
    ],
    "language": "en",
    "word_count": 103,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "navigate oo workflow designer",
    "contentLower": "the oo workflow designer user interface includes several sections: projects pane the projects pane on the top left shows the project you are currently working on. it displays the content generator for rest api , project tree, editable flows, and configuration items that you can use in the project. dependencies pane the dependencies pane on the top left shows the imported content packs. you can import an existing content pack and use its flows, operations, and configuration items in your flows. authoring pane the authoring pane is a rectangular box in the middle. it contains the canvas to work on flows. you can launch a flow onto the canvas area by double clicking a flow from the projects pane. if you are working on multiple flows, they're indicated in tabs with the corresponding flow names. some of these tabs are hidden from view and are displayed in the top right corner of the authoring pane with a number indicating the number of hidden tabs . click the arrow to view a list of hidden ",
    "keywordsLower": [
      "navigate",
      "oo",
      "workflow",
      "designer",
      "projects",
      "pane",
      "dependencies",
      "authoring",
      "properties",
      "tab",
      "graph",
      "debug",
      "scm",
      "related",
      "topics",
      "user",
      "interface",
      "includes",
      "several",
      "sections",
      "top",
      "left",
      "shows",
      "project",
      "currently",
      "working",
      "on.",
      "displays",
      "content",
      "generator",
      "rest",
      "api",
      "tree",
      "editable",
      "flows",
      "configuration",
      "items",
      "project.",
      "imported",
      "packs.",
      "import",
      "existing",
      "pack",
      "operations",
      "flows.",
      "rectangular",
      "box",
      "middle.",
      "contains",
      "canvas",
      "work",
      "launch",
      "flow",
      "onto",
      "area",
      "double",
      "clicking",
      "pane.",
      "multiple",
      "re",
      "indicated",
      "tabs",
      "corresponding",
      "names.",
      "hidden",
      "view",
      "displayed",
      "right",
      "corner",
      "number",
      "indicating",
      "click",
      "arrow",
      "list",
      "point",
      "select",
      "open.",
      "close",
      "next",
      "name.",
      "inputs",
      "outputs",
      "results",
      "description",
      "about",
      "current",
      "flow.",
      "display",
      "add",
      "configure",
      "descriptions.",
      "enter",
      "graphical",
      "representation",
      "tabs.",
      "relevant",
      "step",
      "bottom",
      "operation",
      "inputs."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Object property expressions",
    "content": "When you form an object property expression you refer to the current record by the term entity. The phrase ${entity} returns the current record itself. The phrase ${entity.Id} returns the Id property of the current record. The same Expression Language phrase can be used in business rules in a workflow, in tasks, or in notifications. In each instance, the term entity refers to the current record (for example, the current change in Change Management or the current request in Service Request Management). The following special object instances are available. Object Description Examples current_user Returns the person who is currently logged in. Note: When the currently logged in user is the suite system integration user (bo-integration@dummy.com; this user is used in integration scenarios such as syncing UCMDB CIs), only the following fields are supported for current_user: current_user.Id, current_user. Locale current_user. Upn current_user. IsPermitted current_user. is_permitted_to_domain",
    "url": "objectpropertyexp",
    "filename": "objectpropertyexp",
    "headings": [
      "Current update",
      "Summary of object expressions",
      "Complex type fields",
      "Related topics"
    ],
    "keywords": [
      "association_end2.Id",
      "tenant.Type",
      "dummy.com",
      "tenant.Id",
      "current_user.Name",
      "Assignee.Upn",
      "current_user.Upn",
      "entity.Id",
      "association_end1.Id",
      "current_user.Id",
      "object",
      "property",
      "expressions",
      "current",
      "update",
      "summary",
      "complex",
      "type",
      "fields",
      "related",
      "topics",
      "form",
      "expression",
      "refer",
      "record",
      "term",
      "entity.",
      "phrase",
      "entity",
      "returns",
      "itself.",
      "id",
      "record.",
      "same",
      "language",
      "business",
      "rules",
      "workflow",
      "tasks",
      "notifications.",
      "instance",
      "refers",
      "example",
      "change",
      "management",
      "request",
      "service",
      "following",
      "special",
      "instances",
      "available.",
      "description",
      "examples",
      "person",
      "currently",
      "logged",
      "in.",
      "note",
      "user",
      "suite",
      "system",
      "integration",
      "bo-integration",
      "scenarios",
      "such",
      "syncing",
      "ucmdb",
      "cis",
      "supported",
      "locale",
      "upn",
      "ispermitted",
      "return",
      "null",
      "during",
      "dsl",
      "parsing",
      "user.",
      "cases",
      "add",
      "beginning",
      "condition",
      "work",
      "around",
      "issue.",
      "name",
      "string",
      "domain",
      "boolean",
      "value",
      "indicating",
      "whether",
      "permission",
      "any",
      "specified",
      "data",
      "domains",
      "true",
      "least",
      "one"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "object property expressions",
    "contentLower": "when you form an object property expression you refer to the current record by the term entity. the phrase ${entity} returns the current record itself. the phrase ${entity.id} returns the id property of the current record. the same expression language phrase can be used in business rules in a workflow, in tasks, or in notifications. in each instance, the term entity refers to the current record (for example, the current change in change management or the current request in service request management). the following special object instances are available. object description examples current_user returns the person who is currently logged in. note: when the currently logged in user is the suite system integration user (bo-integration@dummy.com; this user is used in integration scenarios such as syncing ucmdb cis), only the following fields are supported for current_user: current_user.id, current_user. locale current_user. upn current_user. ispermitted current_user. is_permitted_to_domain",
    "keywordsLower": [
      "association_end2.id",
      "tenant.type",
      "dummy.com",
      "tenant.id",
      "current_user.name",
      "assignee.upn",
      "current_user.upn",
      "entity.id",
      "association_end1.id",
      "current_user.id",
      "object",
      "property",
      "expressions",
      "current",
      "update",
      "summary",
      "complex",
      "type",
      "fields",
      "related",
      "topics",
      "form",
      "expression",
      "refer",
      "record",
      "term",
      "entity.",
      "phrase",
      "entity",
      "returns",
      "itself.",
      "id",
      "record.",
      "same",
      "language",
      "business",
      "rules",
      "workflow",
      "tasks",
      "notifications.",
      "instance",
      "refers",
      "example",
      "change",
      "management",
      "request",
      "service",
      "following",
      "special",
      "instances",
      "available.",
      "description",
      "examples",
      "person",
      "currently",
      "logged",
      "in.",
      "note",
      "user",
      "suite",
      "system",
      "integration",
      "bo-integration",
      "scenarios",
      "such",
      "syncing",
      "ucmdb",
      "cis",
      "supported",
      "locale",
      "upn",
      "ispermitted",
      "return",
      "null",
      "during",
      "dsl",
      "parsing",
      "user.",
      "cases",
      "add",
      "beginning",
      "condition",
      "work",
      "around",
      "issue.",
      "name",
      "string",
      "domain",
      "boolean",
      "value",
      "indicating",
      "whether",
      "permission",
      "any",
      "specified",
      "data",
      "domains",
      "true",
      "least",
      "one"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Not able to connect GKE from bastion",
    "content": "When you attempt to install the suite on GCP, you may encounter issues connecting to the bastion node. Cause The problem arises due to the unavailability of the required subnet network range Solution Perform the following steps in the bastion node Run the following command to describe the cluster details and get the CIDR block of the authorized network gcloud container clusters describe ap-esm2-gke-cluster --<region name> --format=\"value(masterAuthorizedNetworksConfig.cidrBlocks)\" {'cidrBlock': '0.0.0.0/0', 'displayName': 'ap-esm2-gke-network'} Log in to the GCP console. Open the newly created Bastion VM. The overview page displays the public IP address and region. Run the following command to connect to the authorized network. gcloud container clusters update ap-esm2-gke-cluster \\ --region <region> \\ --enable-master-authorized-networks \\ --master-authorized-networks <BASTION_PUBLIC_IP>/32 Run the following commands to list the pods and nodes. kubectl get nodes -A kubectl get pods -A",
    "url": "noconnectgkefrombastion",
    "filename": "noconnectgkefrombastion",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "0.0.0",
      "0.0.0.0",
      "able",
      "connect",
      "gke",
      "bastion",
      "cause",
      "solution",
      "attempt",
      "install",
      "suite",
      "gcp",
      "encounter",
      "issues",
      "connecting",
      "node.",
      "problem",
      "arises",
      "due",
      "unavailability",
      "required",
      "subnet",
      "network",
      "range",
      "perform",
      "following",
      "steps",
      "node",
      "run",
      "command",
      "describe",
      "cluster",
      "details",
      "get",
      "cidr",
      "block",
      "authorized",
      "gcloud",
      "container",
      "clusters",
      "ap-esm2-gke-cluster",
      "--format",
      "value",
      "masterauthorizednetworksconfig.cidrblocks",
      "cidrblock",
      "displayname",
      "ap-esm2-gke-network",
      "log",
      "console.",
      "open",
      "newly",
      "created",
      "vm.",
      "overview",
      "page",
      "displays",
      "public",
      "ip",
      "address",
      "region.",
      "network.",
      "update",
      "--region",
      "--enable-master-authorized-networks",
      "--master-authorized-networks",
      "32",
      "commands",
      "list",
      "pods",
      "nodes.",
      "kubectl",
      "nodes",
      "-a"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "not able to connect gke from bastion",
    "contentLower": "when you attempt to install the suite on gcp, you may encounter issues connecting to the bastion node. cause the problem arises due to the unavailability of the required subnet network range solution perform the following steps in the bastion node run the following command to describe the cluster details and get the cidr block of the authorized network gcloud container clusters describe ap-esm2-gke-cluster --<region name> --format=\"value(masterauthorizednetworksconfig.cidrblocks)\" {'cidrblock': '0.0.0.0/0', 'displayname': 'ap-esm2-gke-network'} log in to the gcp console. open the newly created bastion vm. the overview page displays the public ip address and region. run the following command to connect to the authorized network. gcloud container clusters update ap-esm2-gke-cluster \\ --region <region> \\ --enable-master-authorized-networks \\ --master-authorized-networks <bastion_public_ip>/32 run the following commands to list the pods and nodes. kubectl get nodes -a kubectl get pods -a",
    "keywordsLower": [
      "0.0.0",
      "0.0.0.0",
      "able",
      "connect",
      "gke",
      "bastion",
      "cause",
      "solution",
      "attempt",
      "install",
      "suite",
      "gcp",
      "encounter",
      "issues",
      "connecting",
      "node.",
      "problem",
      "arises",
      "due",
      "unavailability",
      "required",
      "subnet",
      "network",
      "range",
      "perform",
      "following",
      "steps",
      "node",
      "run",
      "command",
      "describe",
      "cluster",
      "details",
      "get",
      "cidr",
      "block",
      "authorized",
      "gcloud",
      "container",
      "clusters",
      "ap-esm2-gke-cluster",
      "--format",
      "value",
      "masterauthorizednetworksconfig.cidrblocks",
      "cidrblock",
      "displayname",
      "ap-esm2-gke-network",
      "log",
      "console.",
      "open",
      "newly",
      "created",
      "vm.",
      "overview",
      "page",
      "displays",
      "public",
      "ip",
      "address",
      "region.",
      "network.",
      "update",
      "--region",
      "--enable-master-authorized-networks",
      "--master-authorized-networks",
      "32",
      "commands",
      "list",
      "pods",
      "nodes.",
      "kubectl",
      "nodes",
      "-a"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Platform pods fail to start after OMT upgrade",
    "content": "Platform pods fail to start after an upgrade to OMT 24.1. Cause This issue occurs due to a known issue (OCTCR19M1959201). Applying the suite patch 2023.05.P1 or later before upgrading to OMT 24.1 will prevent this issue from occurring. However, if you forgot to do this before upgrading to OMT 24.1, use the solution below to manually restart the platform pods. Solution Use the following steps to restart the platform pods. Check the platform and platform-offline pod status. kubectl get pod -n <itsma_namespace>|grep platform Describe the platform pods to get more error details. kubectl describe pod itom-xruntime-platform-xxxxxxxxxxxx -n <itsma_namespace> Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Pulled 48m (x225 over 19h) kubelet Container image \"itom-docker.svsartifactory.swinfra.net:443/hpeswitomsandbox/itom-itsmax-lookup:4.18.1.5550-9850\" already present on machine Warning BackOff 3m28s (x5327 over 19h) kubelet Back-off restarting failed container lookup",
    "url": "platformpodfailstostart",
    "filename": "platformpodfailstostart",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "2023.05",
      "4.18.1",
      "1.5550",
      "4.18",
      "swinfra.net",
      "24.1",
      "platform",
      "pods",
      "fail",
      "start",
      "after",
      "omt",
      "upgrade",
      "cause",
      "solution",
      "24.1.",
      "issue",
      "occurs",
      "due",
      "known",
      "octcr19m1959201",
      "applying",
      "suite",
      "patch",
      "2023.05.p1",
      "later",
      "before",
      "upgrading",
      "prevent",
      "occurring.",
      "however",
      "forgot",
      "below",
      "manually",
      "restart",
      "pods.",
      "following",
      "steps",
      "check",
      "platform-offline",
      "pod",
      "status.",
      "kubectl",
      "get",
      "-n",
      "grep",
      "describe",
      "error",
      "details.",
      "itom-xruntime-platform-xxxxxxxxxxxx",
      "events",
      "type",
      "reason",
      "age",
      "message",
      "normal",
      "pulled",
      "48m",
      "x225",
      "over",
      "19h",
      "kubelet",
      "container",
      "image",
      "itom-docker.svsartifactory.swinfra.net",
      "443",
      "hpeswitomsandbox",
      "itom-itsmax-lookup",
      "4.18.1.5550-9850",
      "already",
      "present",
      "machine",
      "warning",
      "backoff",
      "3m28s",
      "x5327",
      "back-off",
      "restarting",
      "failed",
      "lookup-install",
      "f855fba4-3922-4f86-a702-f83033a1a194",
      "delete",
      "itom-xruntime-platform-offline-xxxxxxxxxxxx"
    ],
    "language": "en",
    "word_count": 113,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "platform pods fail to start after omt upgrade",
    "contentLower": "platform pods fail to start after an upgrade to omt 24.1. cause this issue occurs due to a known issue (octcr19m1959201). applying the suite patch 2023.05.p1 or later before upgrading to omt 24.1 will prevent this issue from occurring. however, if you forgot to do this before upgrading to omt 24.1, use the solution below to manually restart the platform pods. solution use the following steps to restart the platform pods. check the platform and platform-offline pod status. kubectl get pod -n <itsma_namespace>|grep platform describe the platform pods to get more error details. kubectl describe pod itom-xruntime-platform-xxxxxxxxxxxx -n <itsma_namespace> events: type reason age from message ---- ------ ---- ---- ------- normal pulled 48m (x225 over 19h) kubelet container image \"itom-docker.svsartifactory.swinfra.net:443/hpeswitomsandbox/itom-itsmax-lookup:4.18.1.5550-9850\" already present on machine warning backoff 3m28s (x5327 over 19h) kubelet back-off restarting failed container lookup",
    "keywordsLower": [
      "2023.05",
      "4.18.1",
      "1.5550",
      "4.18",
      "swinfra.net",
      "24.1",
      "platform",
      "pods",
      "fail",
      "start",
      "after",
      "omt",
      "upgrade",
      "cause",
      "solution",
      "24.1.",
      "issue",
      "occurs",
      "due",
      "known",
      "octcr19m1959201",
      "applying",
      "suite",
      "patch",
      "2023.05.p1",
      "later",
      "before",
      "upgrading",
      "prevent",
      "occurring.",
      "however",
      "forgot",
      "below",
      "manually",
      "restart",
      "pods.",
      "following",
      "steps",
      "check",
      "platform-offline",
      "pod",
      "status.",
      "kubectl",
      "get",
      "-n",
      "grep",
      "describe",
      "error",
      "details.",
      "itom-xruntime-platform-xxxxxxxxxxxx",
      "events",
      "type",
      "reason",
      "age",
      "message",
      "normal",
      "pulled",
      "48m",
      "x225",
      "over",
      "19h",
      "kubelet",
      "container",
      "image",
      "itom-docker.svsartifactory.swinfra.net",
      "443",
      "hpeswitomsandbox",
      "itom-itsmax-lookup",
      "4.18.1.5550-9850",
      "already",
      "present",
      "machine",
      "warning",
      "backoff",
      "3m28s",
      "x5327",
      "back-off",
      "restarting",
      "failed",
      "lookup-install",
      "f855fba4-3922-4f86-a702-f83033a1a194",
      "delete",
      "itom-xruntime-platform-offline-xxxxxxxxxxxx"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Prometheus pod is in \"Pending\" state after you upgrade OMT",
    "content": "After you upgrade OMT, the Prometheus pod stays in the \"Pending\" state. Cause This issue occurs because the prometheus-operator Helm chart was originally installed with a release name that wasn't “cdf-prometheus.”  Therefore, the chart’s dynamic rules created the Persistent Volume Claim for Prometheus with the wrong name. Solution Run the following command to find the PVC name. Make a note of the returned value. kubectl get pvc -n $CDF_NAMESPACE | grep 'prometheus-0' |awk '{print $1}'|xargs Run the following command to find the PV name. Update the command with the value that you found in step 1. Make a note of the returned value. kubectl get pv | grep <PVC_name>| awk '{print $1}'| xargs Run the following command to delete the PVC. Update the command with the value that you found in step 1. kubectl delete pvc -n $CDF_NAMESPACE <PVC_name> Run the following command to reset the PV’s status. Update the command with the value that you found in step 2. kubectl patch pv <PV_name> -p '{\"spec\":",
    "url": "prometheuspodpendingafterupgrade",
    "filename": "prometheuspodpendingafterupgrade",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "prometheus",
      "pod",
      "pending",
      "state",
      "after",
      "upgrade",
      "omt",
      "cause",
      "solution",
      "stays",
      "state.",
      "issue",
      "occurs",
      "because",
      "prometheus-operator",
      "helm",
      "chart",
      "originally",
      "installed",
      "release",
      "name",
      "wasn",
      "cdf-prometheus.",
      "therefore",
      "dynamic",
      "rules",
      "created",
      "persistent",
      "volume",
      "claim",
      "wrong",
      "name.",
      "run",
      "following",
      "command",
      "find",
      "pvc",
      "make",
      "note",
      "returned",
      "value.",
      "kubectl",
      "get",
      "-n",
      "grep",
      "prometheus-0",
      "awk",
      "print",
      "xargs",
      "pv",
      "update",
      "value",
      "found",
      "step",
      "1.",
      "delete",
      "pvc.",
      "reset",
      "status.",
      "2.",
      "patch",
      "-p",
      "spec",
      "claimref",
      "null",
      "wait",
      "restart."
    ],
    "language": "en",
    "word_count": 121,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "prometheus pod is in \"pending\" state after you upgrade omt",
    "contentLower": "after you upgrade omt, the prometheus pod stays in the \"pending\" state. cause this issue occurs because the prometheus-operator helm chart was originally installed with a release name that wasn't “cdf-prometheus.”  therefore, the chart’s dynamic rules created the persistent volume claim for prometheus with the wrong name. solution run the following command to find the pvc name. make a note of the returned value. kubectl get pvc -n $cdf_namespace | grep 'prometheus-0' |awk '{print $1}'|xargs run the following command to find the pv name. update the command with the value that you found in step 1. make a note of the returned value. kubectl get pv | grep <pvc_name>| awk '{print $1}'| xargs run the following command to delete the pvc. update the command with the value that you found in step 1. kubectl delete pvc -n $cdf_namespace <pvc_name> run the following command to reset the pv’s status. update the command with the value that you found in step 2. kubectl patch pv <pv_name> -p '{\"spec\":",
    "keywordsLower": [
      "prometheus",
      "pod",
      "pending",
      "state",
      "after",
      "upgrade",
      "omt",
      "cause",
      "solution",
      "stays",
      "state.",
      "issue",
      "occurs",
      "because",
      "prometheus-operator",
      "helm",
      "chart",
      "originally",
      "installed",
      "release",
      "name",
      "wasn",
      "cdf-prometheus.",
      "therefore",
      "dynamic",
      "rules",
      "created",
      "persistent",
      "volume",
      "claim",
      "wrong",
      "name.",
      "run",
      "following",
      "command",
      "find",
      "pvc",
      "make",
      "note",
      "returned",
      "value.",
      "kubectl",
      "get",
      "-n",
      "grep",
      "prometheus-0",
      "awk",
      "print",
      "xargs",
      "pv",
      "update",
      "value",
      "found",
      "step",
      "1.",
      "delete",
      "pvc.",
      "reset",
      "status.",
      "2.",
      "patch",
      "-p",
      "spec",
      "claimref",
      "null",
      "wait",
      "restart."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OMT service status check fails",
    "content": "As part of the suite upgrade precheck, the OMT service status check is mainly focused on the API Server, IDM Server, and Vault Service of OMT. The suite version update relies on these services. If the OMT service status check fails, you can't continue with the suite version update. Cause Errors that are most likely to occur are related to your database connection pool. All of the three services described above need a database. If the database can't provide enough connections, the services may have errors. When such errors occur, you should see a message that resembles the following one: Could not open JPA EntityManager for transaction; nested exception is javax.persistence.PersistenceException: org.hibernate.exception.GenericJDBCException: Unable to acquire JDBC Connection Solution Restart the three pods to release database connections. To do this, follow these steps: Run the following command to get the OMT pod names: kubectl get pods -n core Run the following commands to restart the ",
    "url": "cdfservicestatuscheckfails",
    "filename": "cdfservicestatuscheckfails",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "omt",
      "service",
      "status",
      "check",
      "fails",
      "cause",
      "solution",
      "part",
      "suite",
      "upgrade",
      "precheck",
      "mainly",
      "focused",
      "api",
      "server",
      "idm",
      "vault",
      "omt.",
      "version",
      "update",
      "relies",
      "services.",
      "continue",
      "update.",
      "errors",
      "most",
      "likely",
      "occur",
      "related",
      "database",
      "connection",
      "pool.",
      "all",
      "three",
      "services",
      "described",
      "above",
      "need",
      "database.",
      "provide",
      "enough",
      "connections",
      "errors.",
      "such",
      "see",
      "message",
      "resembles",
      "following",
      "one",
      "open",
      "jpa",
      "entitymanager",
      "transaction",
      "nested",
      "exception",
      "javax.persistence.persistenceexception",
      "org.hibernate.exception.genericjdbcexception",
      "unable",
      "acquire",
      "jdbc",
      "restart",
      "pods",
      "release",
      "connections.",
      "follow",
      "steps",
      "run",
      "command",
      "get",
      "pod",
      "names",
      "kubectl",
      "-n",
      "core",
      "commands",
      "delete",
      "cdf-apiserver-",
      "itom-postgresql-default-",
      "suite-db-",
      "wait",
      "while",
      "allow",
      "restart."
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "omt service status check fails",
    "contentLower": "as part of the suite upgrade precheck, the omt service status check is mainly focused on the api server, idm server, and vault service of omt. the suite version update relies on these services. if the omt service status check fails, you can't continue with the suite version update. cause errors that are most likely to occur are related to your database connection pool. all of the three services described above need a database. if the database can't provide enough connections, the services may have errors. when such errors occur, you should see a message that resembles the following one: could not open jpa entitymanager for transaction; nested exception is javax.persistence.persistenceexception: org.hibernate.exception.genericjdbcexception: unable to acquire jdbc connection solution restart the three pods to release database connections. to do this, follow these steps: run the following command to get the omt pod names: kubectl get pods -n core run the following commands to restart the ",
    "keywordsLower": [
      "omt",
      "service",
      "status",
      "check",
      "fails",
      "cause",
      "solution",
      "part",
      "suite",
      "upgrade",
      "precheck",
      "mainly",
      "focused",
      "api",
      "server",
      "idm",
      "vault",
      "omt.",
      "version",
      "update",
      "relies",
      "services.",
      "continue",
      "update.",
      "errors",
      "most",
      "likely",
      "occur",
      "related",
      "database",
      "connection",
      "pool.",
      "all",
      "three",
      "services",
      "described",
      "above",
      "need",
      "database.",
      "provide",
      "enough",
      "connections",
      "errors.",
      "such",
      "see",
      "message",
      "resembles",
      "following",
      "one",
      "open",
      "jpa",
      "entitymanager",
      "transaction",
      "nested",
      "exception",
      "javax.persistence.persistenceexception",
      "org.hibernate.exception.genericjdbcexception",
      "unable",
      "acquire",
      "jdbc",
      "restart",
      "pods",
      "release",
      "connections.",
      "follow",
      "steps",
      "run",
      "command",
      "get",
      "pod",
      "names",
      "kubectl",
      "-n",
      "core",
      "commands",
      "delete",
      "cdf-apiserver-",
      "itom-postgresql-default-",
      "suite-db-",
      "wait",
      "while",
      "allow",
      "restart."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Post-upgrade actions failed",
    "content": "During the suite upgrade, after the suite services have been successfully upgraded, an error occurs indicating that the system fails to perform one or more post-upgrade actions. Cause This issue may occur in one of the following scenarios: The post-upgrade actions have been completed successfully; however, the system fails to retrieve the correct status of the actions because of a browser refresh made by you. You should not refresh the browser during the suite upgrade. The system fails to complete one or more of the post-upgrade actions. Solution Log in to the OMT Management Portal as admin: https://<external access host>:5443. Navigate to DEPLOYMENT > Deployments, and then do the following: Check that the suite version is updated to the new version. Check that the suite deployment status is Installed. Click the More Actions icon for the suite, and check that the Change option is available. If everything is fine, the suite upgrade has been completed successfully. Close the upgrade wiza",
    "url": "postupgradeactionfailure",
    "filename": "postupgradeactionfailure",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "https://<external",
      "post-upgrade",
      "actions",
      "failed",
      "cause",
      "solution",
      "during",
      "suite",
      "upgrade",
      "after",
      "services",
      "successfully",
      "upgraded",
      "error",
      "occurs",
      "indicating",
      "system",
      "fails",
      "perform",
      "one",
      "actions.",
      "issue",
      "occur",
      "following",
      "scenarios",
      "completed",
      "however",
      "retrieve",
      "correct",
      "status",
      "because",
      "browser",
      "refresh",
      "made",
      "you.",
      "upgrade.",
      "complete",
      "log",
      "omt",
      "management",
      "portal",
      "admin",
      "https",
      "5443.",
      "navigate",
      "deployment",
      "deployments",
      "check",
      "version",
      "updated",
      "new",
      "version.",
      "installed.",
      "click",
      "icon",
      "change",
      "option",
      "available.",
      "everything",
      "fine",
      "successfully.",
      "close",
      "wizard",
      "window.",
      "anything",
      "wrong",
      "example",
      "updating",
      "isn",
      "available",
      "contact",
      "support",
      "team",
      "assistance."
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "post-upgrade actions failed",
    "contentLower": "during the suite upgrade, after the suite services have been successfully upgraded, an error occurs indicating that the system fails to perform one or more post-upgrade actions. cause this issue may occur in one of the following scenarios: the post-upgrade actions have been completed successfully; however, the system fails to retrieve the correct status of the actions because of a browser refresh made by you. you should not refresh the browser during the suite upgrade. the system fails to complete one or more of the post-upgrade actions. solution log in to the omt management portal as admin: https://<external access host>:5443. navigate to deployment > deployments, and then do the following: check that the suite version is updated to the new version. check that the suite deployment status is installed. click the more actions icon for the suite, and check that the change option is available. if everything is fine, the suite upgrade has been completed successfully. close the upgrade wiza",
    "keywordsLower": [
      "https://<external",
      "post-upgrade",
      "actions",
      "failed",
      "cause",
      "solution",
      "during",
      "suite",
      "upgrade",
      "after",
      "services",
      "successfully",
      "upgraded",
      "error",
      "occurs",
      "indicating",
      "system",
      "fails",
      "perform",
      "one",
      "actions.",
      "issue",
      "occur",
      "following",
      "scenarios",
      "completed",
      "however",
      "retrieve",
      "correct",
      "status",
      "because",
      "browser",
      "refresh",
      "made",
      "you.",
      "upgrade.",
      "complete",
      "log",
      "omt",
      "management",
      "portal",
      "admin",
      "https",
      "5443.",
      "navigate",
      "deployment",
      "deployments",
      "check",
      "version",
      "updated",
      "new",
      "version.",
      "installed.",
      "click",
      "icon",
      "change",
      "option",
      "available.",
      "everything",
      "fine",
      "successfully.",
      "close",
      "wizard",
      "window.",
      "anything",
      "wrong",
      "example",
      "updating",
      "isn",
      "available",
      "contact",
      "support",
      "team",
      "assistance."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OO business rule upgrade step fails during crawling upgrade",
    "content": "After an upgrade to version 24.1, you perform a crawling upgrade. In the maas_crawling_upgrade.log file, you find messages that resemble the following: SupportOOBRDataUpdateCrawlingStep,in the tenant:555500001,failed: <error message> SupportOOBRDataUpdateCrawlingStep end, duration: 30304 This indicates the OO business rule upgrade step (SupportOOBRDataUpdateCrawlingStep) failed for the tenant. If your tenant isn't using OO business rules, you can ignore this failure; otherwise, you need to follow the instructions below. In the maas_crawling_upgrade.log file, you may also find messages that resemble the following, which indicate the OO business rule upgrade step was a success for the tenant: SupportOOBRDataUpdateCrawlingStep, change data finished in tenant:555500001 SupportOOBRDataUpdateCrawlingStep end, duration: 30304 Cause The cause of this issue is unknown. Solution Manually run the OO business rule upgrade step by using an integration user to trigger a REST API call: Parameter Valu",
    "url": "supportoobrdataupdatecrawlingstepfails",
    "filename": "supportoobrdataupdatecrawlingstepfails",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "https://{host}/rest/{tenantId}/oo-integration/businessrule-workflow/update",
      "integration.log",
      "maas_crawling_upgrade.log",
      "24.1",
      "oo",
      "business",
      "rule",
      "upgrade",
      "step",
      "fails",
      "during",
      "crawling",
      "cause",
      "solution",
      "after",
      "version",
      "perform",
      "upgrade.",
      "file",
      "find",
      "messages",
      "resemble",
      "following",
      "supportoobrdataupdatecrawlingstep",
      "tenant",
      "555500001",
      "failed",
      "end",
      "duration",
      "30304",
      "indicates",
      "tenant.",
      "isn",
      "rules",
      "ignore",
      "failure",
      "otherwise",
      "need",
      "follow",
      "instructions",
      "below.",
      "indicate",
      "success",
      "change",
      "data",
      "finished",
      "issue",
      "unknown.",
      "manually",
      "run",
      "integration",
      "user",
      "trigger",
      "rest",
      "api",
      "call",
      "parameter",
      "value",
      "method",
      "post",
      "url",
      "https",
      "host",
      "tenantid",
      "oo-integration",
      "businessrule-workflow",
      "update",
      "body",
      "none",
      "cookie",
      "place",
      "token",
      "see",
      "response",
      "businessrule",
      "workflow",
      "successfully",
      "message",
      "oobusinessruleworkflowserviceimpl",
      "xxxx",
      "happens",
      "contact",
      "support",
      "help."
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "oo business rule upgrade step fails during crawling upgrade",
    "contentLower": "after an upgrade to version 24.1, you perform a crawling upgrade. in the maas_crawling_upgrade.log file, you find messages that resemble the following: supportoobrdataupdatecrawlingstep,in the tenant:555500001,failed: <error message> supportoobrdataupdatecrawlingstep end, duration: 30304 this indicates the oo business rule upgrade step (supportoobrdataupdatecrawlingstep) failed for the tenant. if your tenant isn't using oo business rules, you can ignore this failure; otherwise, you need to follow the instructions below. in the maas_crawling_upgrade.log file, you may also find messages that resemble the following, which indicate the oo business rule upgrade step was a success for the tenant: supportoobrdataupdatecrawlingstep, change data finished in tenant:555500001 supportoobrdataupdatecrawlingstep end, duration: 30304 cause the cause of this issue is unknown. solution manually run the oo business rule upgrade step by using an integration user to trigger a rest api call: parameter valu",
    "keywordsLower": [
      "https://{host}/rest/{tenantid}/oo-integration/businessrule-workflow/update",
      "integration.log",
      "maas_crawling_upgrade.log",
      "24.1",
      "oo",
      "business",
      "rule",
      "upgrade",
      "step",
      "fails",
      "during",
      "crawling",
      "cause",
      "solution",
      "after",
      "version",
      "perform",
      "upgrade.",
      "file",
      "find",
      "messages",
      "resemble",
      "following",
      "supportoobrdataupdatecrawlingstep",
      "tenant",
      "555500001",
      "failed",
      "end",
      "duration",
      "30304",
      "indicates",
      "tenant.",
      "isn",
      "rules",
      "ignore",
      "failure",
      "otherwise",
      "need",
      "follow",
      "instructions",
      "below.",
      "indicate",
      "success",
      "change",
      "data",
      "finished",
      "issue",
      "unknown.",
      "manually",
      "run",
      "integration",
      "user",
      "trigger",
      "rest",
      "api",
      "call",
      "parameter",
      "value",
      "method",
      "post",
      "url",
      "https",
      "host",
      "tenantid",
      "oo-integration",
      "businessrule-workflow",
      "update",
      "body",
      "none",
      "cookie",
      "place",
      "token",
      "see",
      "response",
      "businessrule",
      "workflow",
      "successfully",
      "message",
      "oobusinessruleworkflowserviceimpl",
      "xxxx",
      "happens",
      "contact",
      "support",
      "help."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Post upgrade issues after OPB auto-upgrade",
    "content": "Permission denial error occurs during post upgrade process on the On-Premises Bridge (OPB) agent after an auto-upgrade. This is due to a change in file permission during the auto-upgrade. Cause Auto-upgrade changes file permission resulting in OPB execution issues during the post upgrade process. The permission of the <Agent_installation_directory>/product/util/opb/ folder changes from 755 to 644, blocking the util tools usage. Solution After the OPB auto-upgrade is complete, check the OPB utils to verify the permission is 755. If not, use below command to update it: sudo chown -R 755 <Agent_installation_directory>/product/util/opb/",
    "url": "opbpostupgradeissueafterautoupgrade",
    "filename": "opbpostupgradeissueafterautoupgrade",
    "headings": [
      "Cause"
    ],
    "keywords": [
      "post",
      "upgrade",
      "issues",
      "after",
      "opb",
      "auto-upgrade",
      "cause",
      "permission",
      "denial",
      "error",
      "occurs",
      "during",
      "process",
      "on-premises",
      "bridge",
      "agent",
      "auto-upgrade.",
      "due",
      "change",
      "file",
      "changes",
      "resulting",
      "execution",
      "process.",
      "product",
      "util",
      "folder",
      "755",
      "644",
      "blocking",
      "tools",
      "usage.",
      "solution",
      "complete",
      "check",
      "utils",
      "verify",
      "755.",
      "below",
      "command",
      "update",
      "sudo",
      "chown",
      "-r"
    ],
    "language": "en",
    "word_count": 72,
    "importance_score": 1.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "post upgrade issues after opb auto-upgrade",
    "contentLower": "permission denial error occurs during post upgrade process on the on-premises bridge (opb) agent after an auto-upgrade. this is due to a change in file permission during the auto-upgrade. cause auto-upgrade changes file permission resulting in opb execution issues during the post upgrade process. the permission of the <agent_installation_directory>/product/util/opb/ folder changes from 755 to 644, blocking the util tools usage. solution after the opb auto-upgrade is complete, check the opb utils to verify the permission is 755. if not, use below command to update it: sudo chown -r 755 <agent_installation_directory>/product/util/opb/",
    "keywordsLower": [
      "post",
      "upgrade",
      "issues",
      "after",
      "opb",
      "auto-upgrade",
      "cause",
      "permission",
      "denial",
      "error",
      "occurs",
      "during",
      "process",
      "on-premises",
      "bridge",
      "agent",
      "auto-upgrade.",
      "due",
      "change",
      "file",
      "changes",
      "resulting",
      "execution",
      "process.",
      "product",
      "util",
      "folder",
      "755",
      "644",
      "blocking",
      "tools",
      "usage.",
      "solution",
      "complete",
      "check",
      "utils",
      "verify",
      "755.",
      "below",
      "command",
      "update",
      "sudo",
      "chown",
      "-r"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OPB doesn't work after suite upgrade",
    "content": "After the suite is upgraded to a new version, the suite can't connect to the On-Premises Bridge (OPB) agent. The system sends notification emails to the predefined admin users about the error. Cause During the suite upgrade, OPB is automatically upgraded if the new suite version comes with a newer version of OPB. As such the OPB service is automatically restarted during the upgrade. Normally, an OPB upgrade can be completed in less than 10 minutes depending on the network speed. You can monitor the OPB upgrade status in the <Agent_installation_directory>\\product\\log\\controller\\upgrade.log file. This file contains data only when an OPB upgrade has taken place. This issue occurs if the OPB upgrade fails during the suite upgrade. The OPB upgrade may fail due to certain reasons. For example, the OPB may fail to work if the upgrade process is stuck because it failed to download the correct agent-update.zip.  You may find the following error message in the <Agent_installation_directory>\\prod",
    "url": "opbfailsaftersuiteupgrade",
    "filename": "opbfailsaftersuiteupgrade",
    "headings": [
      "Cause",
      "Solution",
      "Make sure no OPB installation folders or files are being locked",
      "Reinstall OPB",
      "Back up the old OPB",
      "Reinstall OPB",
      "Restore the old configuration files and certificate",
      "Start the new OPB"
    ],
    "keywords": [
      "doesnt",
      "upgrade.log",
      "AgentUpgrader.java",
      "https://myhost.mycompany.com:443/v22/install/on-premise-bridge/agent-update.zip",
      "confdump.conf",
      "mycompany.com",
      "controller.log",
      "1.0.0",
      "AgentUpgraderMain.java",
      "java.io",
      "3006.jar",
      "update.zip",
      "opb",
      "doesn",
      "work",
      "after",
      "suite",
      "upgrade",
      "cause",
      "solution",
      "make",
      "sure",
      "installation",
      "folders",
      "files",
      "locked",
      "reinstall",
      "back",
      "old",
      "restore",
      "configuration",
      "certificate",
      "start",
      "new",
      "upgraded",
      "version",
      "connect",
      "on-premises",
      "bridge",
      "agent.",
      "system",
      "sends",
      "notification",
      "emails",
      "predefined",
      "admin",
      "users",
      "about",
      "error.",
      "during",
      "automatically",
      "comes",
      "newer",
      "opb.",
      "such",
      "service",
      "restarted",
      "upgrade.",
      "normally",
      "completed",
      "less",
      "10",
      "minutes",
      "depending",
      "network",
      "speed.",
      "monitor",
      "status",
      "product",
      "log",
      "controller",
      "file.",
      "file",
      "contains",
      "data",
      "taken",
      "place.",
      "issue",
      "occurs",
      "fails",
      "fail",
      "due",
      "certain",
      "reasons.",
      "example",
      "process",
      "stuck",
      "because",
      "failed",
      "download",
      "correct",
      "agent-update.zip.",
      "find",
      "following",
      "error",
      "message",
      "opbservicesampler-1-thread",
      "229",
      "update",
      "agent"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "opb doesn't work after suite upgrade",
    "contentLower": "after the suite is upgraded to a new version, the suite can't connect to the on-premises bridge (opb) agent. the system sends notification emails to the predefined admin users about the error. cause during the suite upgrade, opb is automatically upgraded if the new suite version comes with a newer version of opb. as such the opb service is automatically restarted during the upgrade. normally, an opb upgrade can be completed in less than 10 minutes depending on the network speed. you can monitor the opb upgrade status in the <agent_installation_directory>\\product\\log\\controller\\upgrade.log file. this file contains data only when an opb upgrade has taken place. this issue occurs if the opb upgrade fails during the suite upgrade. the opb upgrade may fail due to certain reasons. for example, the opb may fail to work if the upgrade process is stuck because it failed to download the correct agent-update.zip.  you may find the following error message in the <agent_installation_directory>\\prod",
    "keywordsLower": [
      "doesnt",
      "upgrade.log",
      "agentupgrader.java",
      "https://myhost.mycompany.com:443/v22/install/on-premise-bridge/agent-update.zip",
      "confdump.conf",
      "mycompany.com",
      "controller.log",
      "1.0.0",
      "agentupgradermain.java",
      "java.io",
      "3006.jar",
      "update.zip",
      "opb",
      "doesn",
      "work",
      "after",
      "suite",
      "upgrade",
      "cause",
      "solution",
      "make",
      "sure",
      "installation",
      "folders",
      "files",
      "locked",
      "reinstall",
      "back",
      "old",
      "restore",
      "configuration",
      "certificate",
      "start",
      "new",
      "upgraded",
      "version",
      "connect",
      "on-premises",
      "bridge",
      "agent.",
      "system",
      "sends",
      "notification",
      "emails",
      "predefined",
      "admin",
      "users",
      "about",
      "error.",
      "during",
      "automatically",
      "comes",
      "newer",
      "opb.",
      "such",
      "service",
      "restarted",
      "upgrade.",
      "normally",
      "completed",
      "less",
      "10",
      "minutes",
      "depending",
      "network",
      "speed.",
      "monitor",
      "status",
      "product",
      "log",
      "controller",
      "file.",
      "file",
      "contains",
      "data",
      "taken",
      "place.",
      "issue",
      "occurs",
      "fails",
      "fail",
      "due",
      "certain",
      "reasons.",
      "example",
      "process",
      "stuck",
      "because",
      "failed",
      "download",
      "correct",
      "agent-update.zip.",
      "find",
      "following",
      "error",
      "message",
      "opbservicesampler-1-thread",
      "229",
      "update",
      "agent"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "New emails aren't processed",
    "content": "New emails aren't processed. Cause There might be problems with the On-Premises Bridge email integration task. Solution In the endpoint configuration dialog box, check if the last SyncEmailTask failed. If so, check the \\\\<OPB_folder>\\product\\log\\email-integration\\email-integration.log and \\\\<OPB_folder>\\product\\log\\controller\\controller.log files. The new task will start at the next cycle (at xx:00 or xx:30).",
    "url": "emailintegnewemailsnotprocessed",
    "filename": "emailintegnewemailsnotprocessed",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "arent",
      "integration.log",
      "controller.log",
      "new",
      "emails",
      "aren",
      "processed",
      "cause",
      "solution",
      "processed.",
      "there",
      "problems",
      "on-premises",
      "bridge",
      "email",
      "integration",
      "task.",
      "endpoint",
      "configuration",
      "dialog",
      "box",
      "check",
      "last",
      "syncemailtask",
      "failed.",
      "product",
      "log",
      "email-integration",
      "email-integration.log",
      "controller",
      "files.",
      "task",
      "start",
      "next",
      "cycle",
      "xx",
      "00",
      "30"
    ],
    "language": "en",
    "word_count": 46,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "new emails aren't processed",
    "contentLower": "new emails aren't processed. cause there might be problems with the on-premises bridge email integration task. solution in the endpoint configuration dialog box, check if the last syncemailtask failed. if so, check the \\\\<opb_folder>\\product\\log\\email-integration\\email-integration.log and \\\\<opb_folder>\\product\\log\\controller\\controller.log files. the new task will start at the next cycle (at xx:00 or xx:30).",
    "keywordsLower": [
      "arent",
      "integration.log",
      "controller.log",
      "new",
      "emails",
      "aren",
      "processed",
      "cause",
      "solution",
      "processed.",
      "there",
      "problems",
      "on-premises",
      "bridge",
      "email",
      "integration",
      "task.",
      "endpoint",
      "configuration",
      "dialog",
      "box",
      "check",
      "last",
      "syncemailtask",
      "failed.",
      "product",
      "log",
      "email-integration",
      "email-integration.log",
      "controller",
      "files.",
      "task",
      "start",
      "next",
      "cycle",
      "xx",
      "00",
      "30"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Pod stuck in Terminating status",
    "content": "A pod is stuck in Terminating status. Cause The cause for this issue is unknown. Solution If a pod gets stuck in Terminating status, run the following command to remove the pod: kubectl delete pod [POD NAME] -n [NAMESPACE] --grace-period=0 --force",
    "url": "podsterminatingstatus",
    "filename": "podsterminatingstatus",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "pod",
      "stuck",
      "terminating",
      "status",
      "cause",
      "solution",
      "status.",
      "issue",
      "unknown.",
      "gets",
      "run",
      "following",
      "command",
      "remove",
      "kubectl",
      "delete",
      "name",
      "-n",
      "namespace",
      "--grace-period",
      "--force"
    ],
    "language": "en",
    "word_count": 33,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "pod stuck in terminating status",
    "contentLower": "a pod is stuck in terminating status. cause the cause for this issue is unknown. solution if a pod gets stuck in terminating status, run the following command to remove the pod: kubectl delete pod [pod name] -n [namespace] --grace-period=0 --force",
    "keywordsLower": [
      "pod",
      "stuck",
      "terminating",
      "status",
      "cause",
      "solution",
      "status.",
      "issue",
      "unknown.",
      "gets",
      "run",
      "following",
      "command",
      "remove",
      "kubectl",
      "delete",
      "name",
      "-n",
      "namespace",
      "--grace-period",
      "--force"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Pods in Pending state after a worker node reboot",
    "content": "The pods status changes to Pending when you reboot a worker node. Cause This is the expected behavior, not an issue. The pods are inactive because the worker node isn't up and running. Kubernetes will check the worker node status periodically and if needed assign a new worker node for these pods. Solution Try the following solutions one by one. Solution 1 Wait for Kubernetes to automatically restart these pods and if needed reassign a worker node. Solution 2 You can also manually run the following commands on the worker node to reduce the pending time: cd $CDF_HOME/bin ./kube-restart.sh If you need to restart a worker node, do it in a safe way by running the following command before restarting the Linux machine: cd $CDF_HOME/bin ./kube-stop.sh",
    "url": "podspendingstate",
    "filename": "podspendingstate",
    "headings": [
      "Cause",
      "Solution",
      "Solution 1",
      "Solution 2"
    ],
    "keywords": [
      "restart.sh",
      "stop.sh",
      "pods",
      "pending",
      "state",
      "after",
      "worker",
      "node",
      "reboot",
      "cause",
      "solution",
      "status",
      "changes",
      "node.",
      "expected",
      "behavior",
      "issue.",
      "inactive",
      "because",
      "isn",
      "running.",
      "kubernetes",
      "check",
      "periodically",
      "needed",
      "assign",
      "new",
      "pods.",
      "try",
      "following",
      "solutions",
      "one",
      "one.",
      "wait",
      "automatically",
      "restart",
      "reassign",
      "manually",
      "run",
      "commands",
      "reduce",
      "time",
      "cd",
      "bin",
      "kube-restart.sh",
      "need",
      "safe",
      "way",
      "running",
      "command",
      "before",
      "restarting",
      "linux",
      "machine",
      "kube-stop.sh"
    ],
    "language": "en",
    "word_count": 87,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "pods in pending state after a worker node reboot",
    "contentLower": "the pods status changes to pending when you reboot a worker node. cause this is the expected behavior, not an issue. the pods are inactive because the worker node isn't up and running. kubernetes will check the worker node status periodically and if needed assign a new worker node for these pods. solution try the following solutions one by one. solution 1 wait for kubernetes to automatically restart these pods and if needed reassign a worker node. solution 2 you can also manually run the following commands on the worker node to reduce the pending time: cd $cdf_home/bin ./kube-restart.sh if you need to restart a worker node, do it in a safe way by running the following command before restarting the linux machine: cd $cdf_home/bin ./kube-stop.sh",
    "keywordsLower": [
      "restart.sh",
      "stop.sh",
      "pods",
      "pending",
      "state",
      "after",
      "worker",
      "node",
      "reboot",
      "cause",
      "solution",
      "status",
      "changes",
      "node.",
      "expected",
      "behavior",
      "issue.",
      "inactive",
      "because",
      "isn",
      "running.",
      "kubernetes",
      "check",
      "periodically",
      "needed",
      "assign",
      "new",
      "pods.",
      "try",
      "following",
      "solutions",
      "one",
      "one.",
      "wait",
      "automatically",
      "restart",
      "reassign",
      "manually",
      "run",
      "commands",
      "reduce",
      "time",
      "cd",
      "bin",
      "kube-restart.sh",
      "need",
      "safe",
      "way",
      "running",
      "command",
      "before",
      "restarting",
      "linux",
      "machine",
      "kube-stop.sh"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Performance: the Linux buffer isn't released",
    "content": "The Linux buffer on the control plane node or worker node isn't released during suite installation and operation even when the free memory is low. Cause The cause for this issue is unknown. Solution To resolve the issue, use the following commands to release the buffer manually: echo 1 > /proc/sys/vm/drop_caches echo 2 > /proc/sys/vm/drop_caches echo 3 > /proc/sys/vm/drop_caches sync",
    "url": "linuxbuffernotreleased",
    "filename": "linuxbuffernotreleased",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "isnt",
      "performance",
      "linux",
      "buffer",
      "isn",
      "released",
      "cause",
      "solution",
      "control",
      "plane",
      "node",
      "worker",
      "during",
      "suite",
      "installation",
      "operation",
      "even",
      "free",
      "memory",
      "low.",
      "issue",
      "unknown.",
      "resolve",
      "following",
      "commands",
      "release",
      "manually",
      "echo",
      "proc",
      "sys",
      "vm",
      "sync"
    ],
    "language": "en",
    "word_count": 55,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "performance: the linux buffer isn't released",
    "contentLower": "the linux buffer on the control plane node or worker node isn't released during suite installation and operation even when the free memory is low. cause the cause for this issue is unknown. solution to resolve the issue, use the following commands to release the buffer manually: echo 1 > /proc/sys/vm/drop_caches echo 2 > /proc/sys/vm/drop_caches echo 3 > /proc/sys/vm/drop_caches sync",
    "keywordsLower": [
      "isnt",
      "performance",
      "linux",
      "buffer",
      "isn",
      "released",
      "cause",
      "solution",
      "control",
      "plane",
      "node",
      "worker",
      "during",
      "suite",
      "installation",
      "operation",
      "even",
      "free",
      "memory",
      "low.",
      "issue",
      "unknown.",
      "resolve",
      "following",
      "commands",
      "release",
      "manually",
      "echo",
      "proc",
      "sys",
      "vm",
      "sync"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Performance: kernel soft lockup issue",
    "content": "If the server is the Kubernetes control plane node or an NFS server, the entire system will stop responding for a while. The system displays the following error message: kernel:NMI watchdog: BUG: soft lockup - CPU#3 stuck for 22s Cause The cause for this issue is unknown. Solution Try the following solutions to resolve this issue: Reduce the load or add more computing resource. Optimize the disk usage. For example, change the NFS server from \"sync\" mode to \"async\" mode.",
    "url": "kernelsoftlockupissue",
    "filename": "kernelsoftlockupissue",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "performance",
      "kernel",
      "soft",
      "lockup",
      "issue",
      "cause",
      "solution",
      "server",
      "kubernetes",
      "control",
      "plane",
      "node",
      "nfs",
      "entire",
      "system",
      "stop",
      "responding",
      "while.",
      "displays",
      "following",
      "error",
      "message",
      "nmi",
      "watchdog",
      "bug",
      "cpu",
      "stuck",
      "22s",
      "unknown.",
      "try",
      "solutions",
      "resolve",
      "reduce",
      "load",
      "add",
      "computing",
      "resource.",
      "optimize",
      "disk",
      "usage.",
      "example",
      "change",
      "sync",
      "mode",
      "async",
      "mode."
    ],
    "language": "en",
    "word_count": 58,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "performance: kernel soft lockup issue",
    "contentLower": "if the server is the kubernetes control plane node or an nfs server, the entire system will stop responding for a while. the system displays the following error message: kernel:nmi watchdog: bug: soft lockup - cpu#3 stuck for 22s cause the cause for this issue is unknown. solution try the following solutions to resolve this issue: reduce the load or add more computing resource. optimize the disk usage. for example, change the nfs server from \"sync\" mode to \"async\" mode.",
    "keywordsLower": [
      "performance",
      "kernel",
      "soft",
      "lockup",
      "issue",
      "cause",
      "solution",
      "server",
      "kubernetes",
      "control",
      "plane",
      "node",
      "nfs",
      "entire",
      "system",
      "stop",
      "responding",
      "while.",
      "displays",
      "following",
      "error",
      "message",
      "nmi",
      "watchdog",
      "bug",
      "cpu",
      "stuck",
      "22s",
      "unknown.",
      "try",
      "solutions",
      "resolve",
      "reduce",
      "load",
      "add",
      "computing",
      "resource.",
      "optimize",
      "disk",
      "usage.",
      "example",
      "change",
      "sync",
      "mode",
      "async",
      "mode."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Performance: high CPU ready issues on ESX",
    "content": "You may find the following performance issues on ESX: High response time spikes during a load test or abnormal response time in production. High CPU ready found on VM level. All the servers on the ESX host experience a slowdown period. Some of the servers running kubernetes pods may become NodeNotReady or NodeLost. Cause The cause for this issue is unknown. Solution Try the following solutions to resolve this issue: Enable Hyperthreading. Use \"High Performance\" for your Power Management Policy. Revisit the overall size for the ESX host, and make sure there is no overcommitment. Determine the size for the VMs correctly. Or, use VMware vCenter Operations Manager to assist on determining the right size for VMs. Use DRS anti-affinity rules to keep non-complementary workloads apart.",
    "url": "highcpureadyissues",
    "filename": "highcpureadyissues",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "performance",
      "high",
      "cpu",
      "ready",
      "issues",
      "esx",
      "cause",
      "solution",
      "find",
      "following",
      "response",
      "time",
      "spikes",
      "during",
      "load",
      "test",
      "abnormal",
      "production.",
      "found",
      "vm",
      "level.",
      "all",
      "servers",
      "host",
      "experience",
      "slowdown",
      "period.",
      "running",
      "kubernetes",
      "pods",
      "become",
      "nodenotready",
      "nodelost.",
      "issue",
      "unknown.",
      "try",
      "solutions",
      "resolve",
      "enable",
      "hyperthreading.",
      "power",
      "management",
      "policy.",
      "revisit",
      "overall",
      "size",
      "make",
      "sure",
      "there",
      "overcommitment.",
      "determine",
      "vms",
      "correctly.",
      "vmware",
      "vcenter",
      "operations",
      "manager",
      "assist",
      "determining",
      "right",
      "vms.",
      "drs",
      "anti-affinity",
      "rules",
      "keep",
      "non-complementary",
      "workloads",
      "apart."
    ],
    "language": "en",
    "word_count": 88,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "performance: high cpu ready issues on esx",
    "contentLower": "you may find the following performance issues on esx: high response time spikes during a load test or abnormal response time in production. high cpu ready found on vm level. all the servers on the esx host experience a slowdown period. some of the servers running kubernetes pods may become nodenotready or nodelost. cause the cause for this issue is unknown. solution try the following solutions to resolve this issue: enable hyperthreading. use \"high performance\" for your power management policy. revisit the overall size for the esx host, and make sure there is no overcommitment. determine the size for the vms correctly. or, use vmware vcenter operations manager to assist on determining the right size for vms. use drs anti-affinity rules to keep non-complementary workloads apart.",
    "keywordsLower": [
      "performance",
      "high",
      "cpu",
      "ready",
      "issues",
      "esx",
      "cause",
      "solution",
      "find",
      "following",
      "response",
      "time",
      "spikes",
      "during",
      "load",
      "test",
      "abnormal",
      "production.",
      "found",
      "vm",
      "level.",
      "all",
      "servers",
      "host",
      "experience",
      "slowdown",
      "period.",
      "running",
      "kubernetes",
      "pods",
      "become",
      "nodenotready",
      "nodelost.",
      "issue",
      "unknown.",
      "try",
      "solutions",
      "resolve",
      "enable",
      "hyperthreading.",
      "power",
      "management",
      "policy.",
      "revisit",
      "overall",
      "size",
      "make",
      "sure",
      "there",
      "overcommitment.",
      "determine",
      "vms",
      "correctly.",
      "vmware",
      "vcenter",
      "operations",
      "manager",
      "assist",
      "determining",
      "right",
      "vms.",
      "drs",
      "anti-affinity",
      "rules",
      "keep",
      "non-complementary",
      "workloads",
      "apart."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Platform Pods going to 1/2 state when sample data deployment is triggered",
    "content": "Cause When the sample data deployment is triggered, the existing load may affect connections to RabbitMQ. This can, in turn, cause the platform pods to enter a 1/2 state. Solution When RabbitMQ imposes restrictions on the connection, the platform pod goes to a 1/2 status and will return after a few minutes. Therefore, if this issue arises, wait for 15 minutes before checking if it resolves automatically. If the problem persists, check the system resources and network connectivity. If there's no resource or network issue, manually restart RabbitMQ. You can do either a hard or soft restart.  To do this, follow these steps: Soft restart : For a soft restart, follow the steps outlined in the hard restart instructions, but skip step 3 and do not delete any data. Hard restart : Run the following command on a master node (embedded Kubernetes) or the bastion node (managed Kubernetes) to stop RabbitMQ: kubectl scale statefulset infra-rabbitmq -n <suite namespace> --replicas=0 Wait until all Rab",
    "url": "platformgoingdownforsampledata",
    "filename": "platformgoingdownforsampledata",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "12",
      "x.xx",
      "platform",
      "pods",
      "going",
      "state",
      "sample",
      "data",
      "deployment",
      "triggered",
      "cause",
      "solution",
      "existing",
      "load",
      "affect",
      "connections",
      "rabbitmq.",
      "turn",
      "enter",
      "state.",
      "rabbitmq",
      "imposes",
      "restrictions",
      "connection",
      "pod",
      "goes",
      "status",
      "return",
      "after",
      "few",
      "minutes.",
      "therefore",
      "issue",
      "arises",
      "wait",
      "15",
      "minutes",
      "before",
      "checking",
      "resolves",
      "automatically.",
      "problem",
      "persists",
      "check",
      "system",
      "resources",
      "network",
      "connectivity.",
      "there",
      "resource",
      "manually",
      "restart",
      "either",
      "hard",
      "soft",
      "restart.",
      "follow",
      "steps",
      "outlined",
      "instructions",
      "skip",
      "step",
      "delete",
      "any",
      "data.",
      "run",
      "following",
      "command",
      "master",
      "node",
      "embedded",
      "kubernetes",
      "bastion",
      "managed",
      "stop",
      "kubectl",
      "scale",
      "statefulset",
      "infra-rabbitmq",
      "-n",
      "--replicas",
      "until",
      "all",
      "terminated.",
      "remove",
      "xservices",
      "rabbitmq-n",
      "x.x.x.xx",
      "mnesia",
      "folders",
      "nfs",
      "server.",
      "example",
      "var",
      "vols",
      "itom",
      "itsma",
      "rabbitmq-infra-rabbitmq-0",
      "rabbitmq-infra-rabbitmq-1",
      "rabbitmq-infra-rabbitmq-2"
    ],
    "language": "en",
    "word_count": 116,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "platform pods going to 1/2 state when sample data deployment is triggered",
    "contentLower": "cause when the sample data deployment is triggered, the existing load may affect connections to rabbitmq. this can, in turn, cause the platform pods to enter a 1/2 state. solution when rabbitmq imposes restrictions on the connection, the platform pod goes to a 1/2 status and will return after a few minutes. therefore, if this issue arises, wait for 15 minutes before checking if it resolves automatically. if the problem persists, check the system resources and network connectivity. if there's no resource or network issue, manually restart rabbitmq. you can do either a hard or soft restart.  to do this, follow these steps: soft restart : for a soft restart, follow the steps outlined in the hard restart instructions, but skip step 3 and do not delete any data. hard restart : run the following command on a master node (embedded kubernetes) or the bastion node (managed kubernetes) to stop rabbitmq: kubectl scale statefulset infra-rabbitmq -n <suite namespace> --replicas=0 wait until all rab",
    "keywordsLower": [
      "12",
      "x.xx",
      "platform",
      "pods",
      "going",
      "state",
      "sample",
      "data",
      "deployment",
      "triggered",
      "cause",
      "solution",
      "existing",
      "load",
      "affect",
      "connections",
      "rabbitmq.",
      "turn",
      "enter",
      "state.",
      "rabbitmq",
      "imposes",
      "restrictions",
      "connection",
      "pod",
      "goes",
      "status",
      "return",
      "after",
      "few",
      "minutes.",
      "therefore",
      "issue",
      "arises",
      "wait",
      "15",
      "minutes",
      "before",
      "checking",
      "resolves",
      "automatically.",
      "problem",
      "persists",
      "check",
      "system",
      "resources",
      "network",
      "connectivity.",
      "there",
      "resource",
      "manually",
      "restart",
      "either",
      "hard",
      "soft",
      "restart.",
      "follow",
      "steps",
      "outlined",
      "instructions",
      "skip",
      "step",
      "delete",
      "any",
      "data.",
      "run",
      "following",
      "command",
      "master",
      "node",
      "embedded",
      "kubernetes",
      "bastion",
      "managed",
      "stop",
      "kubectl",
      "scale",
      "statefulset",
      "infra-rabbitmq",
      "-n",
      "--replicas",
      "until",
      "all",
      "terminated.",
      "remove",
      "xservices",
      "rabbitmq-n",
      "x.x.x.xx",
      "mnesia",
      "folders",
      "nfs",
      "server.",
      "example",
      "var",
      "vols",
      "itom",
      "itsma",
      "rabbitmq-infra-rabbitmq-0",
      "rabbitmq-infra-rabbitmq-1",
      "rabbitmq-infra-rabbitmq-2"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OO flow Get Guest Info fails with error java.lang.NullPointerException",
    "content": "After upgrading the solution to 2020.11, if you try to provision a VM in vCenter, OO flow vCenter Deploy VM / vCenter Provision Server / Collect Properties / Get All IP Addresses / Get Guest Info fails with the following error. java.lang.NullPointerException at com.opsware.content.actions.vmware.guest.GetGuestInfo.execute(GetGuestInfo.java:70) at sun.reflect.GeneratedMethodAccessor1970.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at com.hp.oo.sdk.plugins.abstracts.BaseActionPlugin.execute(BaseActionPlugin.java:53) ..... Cause This issue occurs by the upgrade of CSA VMWare vCenter Sequenced content pack. Solution Downgrade the content pack from 18.3.0 to 18.2.1. To downgrade, visit ITOM Marketplace and download the content pack VMWare vCenter Sequenced 18.2.1. Redeploy the content pack to HCMX. For instructions, see VMWare vCenter Sequenced release notes. Related topics",
    "url": "ooflowgetuestinfofails",
    "filename": "ooflowgetuestinfofails",
    "headings": [
      "Cause",
      "Solution",
      "Related topics"
    ],
    "keywords": [
      "oo.sdk",
      "2020.11",
      "GetGuestInfo.java",
      "java.lang",
      "Method.java",
      "18.3.0",
      "DelegatingMethodAccessorImpl.java",
      "com.hp",
      "BaseActionPlugin.java",
      "18.2.1",
      "oo",
      "flow",
      "get",
      "guest",
      "info",
      "fails",
      "error",
      "java.lang.nullpointerexception",
      "cause",
      "solution",
      "related",
      "topics",
      "after",
      "upgrading",
      "try",
      "provision",
      "vm",
      "vcenter",
      "deploy",
      "server",
      "collect",
      "properties",
      "all",
      "ip",
      "addresses",
      "following",
      "error.",
      "com.opsware.content.actions.vmware.guest.getguestinfo.execute",
      "70",
      "sun.reflect.generatedmethodaccessor1970.invoke",
      "unknown",
      "source",
      "sun.reflect.delegatingmethodaccessorimpl.invoke",
      "43",
      "java.lang.reflect.method.invoke",
      "498",
      "com.hp.oo.sdk.plugins.abstracts.baseactionplugin.execute",
      "53",
      "issue",
      "occurs",
      "upgrade",
      "csa",
      "vmware",
      "sequenced",
      "content",
      "pack.",
      "downgrade",
      "pack",
      "18.2.1.",
      "visit",
      "itom",
      "marketplace",
      "download",
      "redeploy",
      "hcmx.",
      "instructions",
      "see",
      "release",
      "notes.",
      "capsule",
      "deployment",
      "capsules",
      "marketplace."
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "oo flow get guest info fails with error java.lang.nullpointerexception",
    "contentLower": "after upgrading the solution to 2020.11, if you try to provision a vm in vcenter, oo flow vcenter deploy vm / vcenter provision server / collect properties / get all ip addresses / get guest info fails with the following error. java.lang.nullpointerexception at com.opsware.content.actions.vmware.guest.getguestinfo.execute(getguestinfo.java:70) at sun.reflect.generatedmethodaccessor1970.invoke(unknown source) at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43) at java.lang.reflect.method.invoke(method.java:498) at com.hp.oo.sdk.plugins.abstracts.baseactionplugin.execute(baseactionplugin.java:53) ..... cause this issue occurs by the upgrade of csa vmware vcenter sequenced content pack. solution downgrade the content pack from 18.3.0 to 18.2.1. to downgrade, visit itom marketplace and download the content pack vmware vcenter sequenced 18.2.1. redeploy the content pack to hcmx. for instructions, see vmware vcenter sequenced release notes. related topics",
    "keywordsLower": [
      "oo.sdk",
      "2020.11",
      "getguestinfo.java",
      "java.lang",
      "method.java",
      "18.3.0",
      "delegatingmethodaccessorimpl.java",
      "com.hp",
      "baseactionplugin.java",
      "18.2.1",
      "oo",
      "flow",
      "get",
      "guest",
      "info",
      "fails",
      "error",
      "java.lang.nullpointerexception",
      "cause",
      "solution",
      "related",
      "topics",
      "after",
      "upgrading",
      "try",
      "provision",
      "vm",
      "vcenter",
      "deploy",
      "server",
      "collect",
      "properties",
      "all",
      "ip",
      "addresses",
      "following",
      "error.",
      "com.opsware.content.actions.vmware.guest.getguestinfo.execute",
      "70",
      "sun.reflect.generatedmethodaccessor1970.invoke",
      "unknown",
      "source",
      "sun.reflect.delegatingmethodaccessorimpl.invoke",
      "43",
      "java.lang.reflect.method.invoke",
      "498",
      "com.hp.oo.sdk.plugins.abstracts.baseactionplugin.execute",
      "53",
      "issue",
      "occurs",
      "upgrade",
      "csa",
      "vmware",
      "sequenced",
      "content",
      "pack.",
      "downgrade",
      "pack",
      "18.2.1.",
      "visit",
      "itom",
      "marketplace",
      "download",
      "redeploy",
      "hcmx.",
      "instructions",
      "see",
      "release",
      "notes.",
      "capsule",
      "deployment",
      "capsules",
      "marketplace."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Orchestration Management session expires even though Service Designer session is active",
    "content": "Orchestration Management session expires even though Service Designer session is active Cause The cause is unknown. Solution Close the Orchestration Management tab and relaunch it from Deployment Operations UI.",
    "url": "oocentralsessionexpires",
    "filename": "oocentralsessionexpires",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "orchestration",
      "management",
      "session",
      "expires",
      "even",
      "though",
      "service",
      "designer",
      "active",
      "cause",
      "solution",
      "unknown.",
      "close",
      "tab",
      "relaunch",
      "deployment",
      "operations",
      "ui."
    ],
    "language": "en",
    "word_count": 32,
    "importance_score": 1.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "orchestration management session expires even though service designer session is active",
    "contentLower": "orchestration management session expires even though service designer session is active cause the cause is unknown. solution close the orchestration management tab and relaunch it from deployment operations ui.",
    "keywordsLower": [
      "orchestration",
      "management",
      "session",
      "expires",
      "even",
      "though",
      "service",
      "designer",
      "active",
      "cause",
      "solution",
      "unknown.",
      "close",
      "tab",
      "relaunch",
      "deployment",
      "operations",
      "ui."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Paused subscriptions are moved to terminated state after upgrade",
    "content": "If a Service Instance is in a Paused state and the corresponding Portal Subscription in a Pending state, the Portal Subscriptions will move to a Terminated State after an upgrade. Cause Failure to connect with the DND service will move the portal subscription to the terminated state. Solution Before performing the suite upgrade, resolve the Service Instances in a Paused state. For more details, see Service Instance Management. If a subscription becomes terminated by the upgrade process, an administrator can fail the provisioning or modification in Deployment Operations to align with the Service Portal state.  For more information on failing provisioning or modification when paused, see Service instance management. The subscriber that had requested the instance that paused and was later terminated will need to resubscribe to the offering.",
    "url": "pausetoterminatedafterupgd",
    "filename": "pausetoterminatedafterupgd",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "paused",
      "subscriptions",
      "moved",
      "terminated",
      "state",
      "after",
      "upgrade",
      "cause",
      "solution",
      "service",
      "instance",
      "corresponding",
      "portal",
      "subscription",
      "pending",
      "move",
      "upgrade.",
      "failure",
      "connect",
      "dnd",
      "state.",
      "before",
      "performing",
      "suite",
      "resolve",
      "instances",
      "details",
      "see",
      "management.",
      "becomes",
      "process",
      "administrator",
      "fail",
      "provisioning",
      "modification",
      "deployment",
      "operations",
      "align",
      "information",
      "failing",
      "subscriber",
      "requested",
      "later",
      "need",
      "resubscribe",
      "offering."
    ],
    "language": "en",
    "word_count": 81,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "paused subscriptions are moved to terminated state after upgrade",
    "contentLower": "if a service instance is in a paused state and the corresponding portal subscription in a pending state, the portal subscriptions will move to a terminated state after an upgrade. cause failure to connect with the dnd service will move the portal subscription to the terminated state. solution before performing the suite upgrade, resolve the service instances in a paused state. for more details, see service instance management. if a subscription becomes terminated by the upgrade process, an administrator can fail the provisioning or modification in deployment operations to align with the service portal state.  for more information on failing provisioning or modification when paused, see service instance management. the subscriber that had requested the instance that paused and was later terminated will need to resubscribe to the offering.",
    "keywordsLower": [
      "paused",
      "subscriptions",
      "moved",
      "terminated",
      "state",
      "after",
      "upgrade",
      "cause",
      "solution",
      "service",
      "instance",
      "corresponding",
      "portal",
      "subscription",
      "pending",
      "move",
      "upgrade.",
      "failure",
      "connect",
      "dnd",
      "state.",
      "before",
      "performing",
      "suite",
      "resolve",
      "instances",
      "details",
      "see",
      "management.",
      "becomes",
      "process",
      "administrator",
      "fail",
      "provisioning",
      "modification",
      "deployment",
      "operations",
      "align",
      "information",
      "failing",
      "subscriber",
      "requested",
      "later",
      "need",
      "resubscribe",
      "offering."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Provider validation failed",
    "content": "While configuring resource providers with an HTTPS endpoint, you get the following override message: Provider Validation Failed The provider information you have entered was unable to be validated successfully. This is most likely due to an invalid/unreachable URL or an untrusted provider certificate. Continue anyway? This override appears even if you have configured the certificate in the DND truststore and entered the correct Resource Provider URL and credentials on the UI. Cause This issue occurs because of the custom trust manager as FIPS has a restriction on it. Solution Select Yes on the Provider Validation Failure override message to continue the resource provider configuration.",
    "url": "providervalidationfailed",
    "filename": "providervalidationfailed",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "provider",
      "validation",
      "failed",
      "cause",
      "solution",
      "while",
      "configuring",
      "resource",
      "providers",
      "https",
      "endpoint",
      "get",
      "following",
      "override",
      "message",
      "information",
      "entered",
      "unable",
      "validated",
      "successfully.",
      "most",
      "likely",
      "due",
      "invalid",
      "unreachable",
      "url",
      "untrusted",
      "certificate.",
      "continue",
      "anyway",
      "appears",
      "even",
      "configured",
      "certificate",
      "dnd",
      "truststore",
      "correct",
      "credentials",
      "ui.",
      "issue",
      "occurs",
      "because",
      "custom",
      "trust",
      "manager",
      "fips",
      "restriction",
      "it.",
      "select",
      "failure",
      "configuration."
    ],
    "language": "en",
    "word_count": 68,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "provider validation failed",
    "contentLower": "while configuring resource providers with an https endpoint, you get the following override message: provider validation failed the provider information you have entered was unable to be validated successfully. this is most likely due to an invalid/unreachable url or an untrusted provider certificate. continue anyway? this override appears even if you have configured the certificate in the dnd truststore and entered the correct resource provider url and credentials on the ui. cause this issue occurs because of the custom trust manager as fips has a restriction on it. solution select yes on the provider validation failure override message to continue the resource provider configuration.",
    "keywordsLower": [
      "provider",
      "validation",
      "failed",
      "cause",
      "solution",
      "while",
      "configuring",
      "resource",
      "providers",
      "https",
      "endpoint",
      "get",
      "following",
      "override",
      "message",
      "information",
      "entered",
      "unable",
      "validated",
      "successfully.",
      "most",
      "likely",
      "due",
      "invalid",
      "unreachable",
      "url",
      "untrusted",
      "certificate.",
      "continue",
      "anyway",
      "appears",
      "even",
      "configured",
      "certificate",
      "dnd",
      "truststore",
      "correct",
      "credentials",
      "ui.",
      "issue",
      "occurs",
      "because",
      "custom",
      "trust",
      "manager",
      "fips",
      "restriction",
      "it.",
      "select",
      "failure",
      "configuration."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "PostgreSQL logs display warnings while OO Containerized is running",
    "content": "While OO Containerized is running, PostgreSQL’s logs will display periodically warnings similar to maas_admin@<component> WARNING: there is no transaction in progress. Cause OO Containerized and its components use static database connection pools to cover all tenant capacity regardless of the number of active tenants. As a result, in periods of low usage when connection refresh occurs, all the idle connections from the connection pool will result in such a message within the PostgreSQL logs. Solution There is no cause for concern regarding these warnings, which are a normal side effect of the static connection pool.",
    "url": "oopglogs",
    "filename": "oopglogs",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "postgresql",
      "logs",
      "display",
      "warnings",
      "while",
      "oo",
      "containerized",
      "running",
      "cause",
      "solution",
      "periodically",
      "similar",
      "warning",
      "there",
      "transaction",
      "progress.",
      "components",
      "static",
      "database",
      "connection",
      "pools",
      "cover",
      "all",
      "tenant",
      "capacity",
      "regardless",
      "number",
      "active",
      "tenants.",
      "result",
      "periods",
      "low",
      "usage",
      "refresh",
      "occurs",
      "idle",
      "connections",
      "pool",
      "such",
      "message",
      "logs.",
      "concern",
      "regarding",
      "normal",
      "side",
      "effect",
      "pool."
    ],
    "language": "en",
    "word_count": 69,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "postgresql logs display warnings while oo containerized is running",
    "contentLower": "while oo containerized is running, postgresql’s logs will display periodically warnings similar to maas_admin@<component> warning: there is no transaction in progress. cause oo containerized and its components use static database connection pools to cover all tenant capacity regardless of the number of active tenants. as a result, in periods of low usage when connection refresh occurs, all the idle connections from the connection pool will result in such a message within the postgresql logs. solution there is no cause for concern regarding these warnings, which are a normal side effect of the static connection pool.",
    "keywordsLower": [
      "postgresql",
      "logs",
      "display",
      "warnings",
      "while",
      "oo",
      "containerized",
      "running",
      "cause",
      "solution",
      "periodically",
      "similar",
      "warning",
      "there",
      "transaction",
      "progress.",
      "components",
      "static",
      "database",
      "connection",
      "pools",
      "cover",
      "all",
      "tenant",
      "capacity",
      "regardless",
      "number",
      "active",
      "tenants.",
      "result",
      "periods",
      "low",
      "usage",
      "refresh",
      "occurs",
      "idle",
      "connections",
      "pool",
      "such",
      "message",
      "logs.",
      "concern",
      "regarding",
      "normal",
      "side",
      "effect",
      "pool."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OO flows fail to execute with the error \"vault secret load failed\"",
    "content": "DND flows that need to be executed in OO will fail due to the error vault secret load failed. You can find the error in the following log file: <logging-volume>/dnd/<namespace>/<namespace>_<podName>_itom-dnd-controller_<workerName>/csa/csa.log Cause This issue occurs when the pods aren't restarted after importing OO certificates to DND controller. Solution Log in to the control plane node/bastion host as the root user. Run the following commands to restart the cmp-config-controller pod: kubectl scale deployment.v1.apps/itom-cmp-config-controller -n itsma-helm --replicas=0 kubectl scale deployment.v1.apps/itom-cmp-config-controller -n itsma-helm --replicas=1",
    "url": "flowexecutionfailsvaulterror",
    "filename": "flowexecutionfailsvaulterror",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "csa.log",
      "deployment.v1",
      "oo",
      "flows",
      "fail",
      "execute",
      "error",
      "vault",
      "secret",
      "load",
      "failed",
      "cause",
      "solution",
      "dnd",
      "need",
      "executed",
      "due",
      "failed.",
      "find",
      "following",
      "log",
      "file",
      "csa",
      "issue",
      "occurs",
      "pods",
      "aren",
      "restarted",
      "after",
      "importing",
      "certificates",
      "controller.",
      "control",
      "plane",
      "node",
      "bastion",
      "host",
      "root",
      "user.",
      "run",
      "commands",
      "restart",
      "cmp-config-controller",
      "pod",
      "kubectl",
      "scale",
      "deployment.v1.apps",
      "itom-cmp-config-controller",
      "-n",
      "itsma-helm",
      "--replicas"
    ],
    "language": "en",
    "word_count": 74,
    "importance_score": 1.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "oo flows fail to execute with the error \"vault secret load failed\"",
    "contentLower": "dnd flows that need to be executed in oo will fail due to the error vault secret load failed. you can find the error in the following log file: <logging-volume>/dnd/<namespace>/<namespace>_<podname>_itom-dnd-controller_<workername>/csa/csa.log cause this issue occurs when the pods aren't restarted after importing oo certificates to dnd controller. solution log in to the control plane node/bastion host as the root user. run the following commands to restart the cmp-config-controller pod: kubectl scale deployment.v1.apps/itom-cmp-config-controller -n itsma-helm --replicas=0 kubectl scale deployment.v1.apps/itom-cmp-config-controller -n itsma-helm --replicas=1",
    "keywordsLower": [
      "csa.log",
      "deployment.v1",
      "oo",
      "flows",
      "fail",
      "execute",
      "error",
      "vault",
      "secret",
      "load",
      "failed",
      "cause",
      "solution",
      "dnd",
      "need",
      "executed",
      "due",
      "failed.",
      "find",
      "following",
      "log",
      "file",
      "csa",
      "issue",
      "occurs",
      "pods",
      "aren",
      "restarted",
      "after",
      "importing",
      "certificates",
      "controller.",
      "control",
      "plane",
      "node",
      "bastion",
      "host",
      "root",
      "user.",
      "run",
      "commands",
      "restart",
      "cmp-config-controller",
      "pod",
      "kubectl",
      "scale",
      "deployment.v1.apps",
      "itom-cmp-config-controller",
      "-n",
      "itsma-helm",
      "--replicas"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Operations Orchestration doesn't support ping operation",
    "content": "Operations Orchestration which is internal to Design and Deploy doesn't support ping operation. The following error is displayed when you run the ping command. bash: ping: command not found Cause The ping command is available in the iputils package. This package isn't available in OpenSUSE by default. Solution Install the iputils package to start using the ping command.",
    "url": "oopingoperationnotsupported",
    "filename": "oopingoperationnotsupported",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "doesnt",
      "operations",
      "orchestration",
      "doesn",
      "support",
      "ping",
      "operation",
      "cause",
      "solution",
      "internal",
      "design",
      "deploy",
      "operation.",
      "following",
      "error",
      "displayed",
      "run",
      "command.",
      "bash",
      "command",
      "found",
      "available",
      "iputils",
      "package.",
      "package",
      "isn",
      "opensuse",
      "default.",
      "install",
      "start"
    ],
    "language": "en",
    "word_count": 46,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "operations orchestration doesn't support ping operation",
    "contentLower": "operations orchestration which is internal to design and deploy doesn't support ping operation. the following error is displayed when you run the ping command. bash: ping: command not found cause the ping command is available in the iputils package. this package isn't available in opensuse by default. solution install the iputils package to start using the ping command.",
    "keywordsLower": [
      "doesnt",
      "operations",
      "orchestration",
      "doesn",
      "support",
      "ping",
      "operation",
      "cause",
      "solution",
      "internal",
      "design",
      "deploy",
      "operation.",
      "following",
      "error",
      "displayed",
      "run",
      "command.",
      "bash",
      "command",
      "found",
      "available",
      "iputils",
      "package.",
      "package",
      "isn",
      "opensuse",
      "default.",
      "install",
      "start"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OO start up is stuck due to insufficient entropy pool",
    "content": "When installing OO on Linux OS, the product start up gets stuck. Cause This occurs due to an insufficient entropy pool. Entropy value less than 1000 will lead to processes being blocked, waiting for more entropy. To check the current entropy value, run the following command as a root/sudo user: cat /proc/sys/kernel/random/entropy_avail A result between 800-1000 or lower can indicate an insufficient entropy pool. Solution Run either of the following commands as a user with appropriate privileges to update the entropy pool with values from the /dev/urandom directory. Solution 1 Run the following command: ln -sf /dev/urandom /dev/random Solution 2 Install rng-tools package for your Linux distribution. To enable rng-tools for entropy pool increase: Create or edit the file /etc/sysconfig/rngd. Add the following line and save the file. EXTRAOPTIONS=\"--rng-device=drng --no-tpm=1\". Restart the rng-tools service. Run the following command: cat /proc/sys/kernel/random/entropy_avail The entropy l",
    "url": "insufficiententropypool",
    "filename": "insufficiententropypool",
    "headings": [
      "Cause",
      "Solution",
      "Solution 1",
      "Solution 2"
    ],
    "keywords": [
      "oo",
      "start",
      "stuck",
      "due",
      "insufficient",
      "entropy",
      "pool",
      "cause",
      "solution",
      "installing",
      "linux",
      "os",
      "product",
      "gets",
      "stuck.",
      "occurs",
      "pool.",
      "value",
      "less",
      "1000",
      "lead",
      "processes",
      "blocked",
      "waiting",
      "entropy.",
      "check",
      "current",
      "run",
      "following",
      "command",
      "root",
      "sudo",
      "user",
      "cat",
      "proc",
      "sys",
      "kernel",
      "random",
      "result",
      "between",
      "800-1000",
      "lower",
      "indicate",
      "either",
      "commands",
      "appropriate",
      "privileges",
      "update",
      "values",
      "dev",
      "urandom",
      "directory.",
      "ln",
      "-sf",
      "install",
      "rng-tools",
      "package",
      "distribution.",
      "enable",
      "increase",
      "create",
      "edit",
      "file",
      "etc",
      "sysconfig",
      "rngd.",
      "add",
      "line",
      "save",
      "file.",
      "extraoptions",
      "--rng-device",
      "drng",
      "--no-tpm",
      "restart",
      "service.",
      "level",
      "increases",
      "about",
      "3000-3200",
      "method."
    ],
    "language": "en",
    "word_count": 122,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "oo start up is stuck due to insufficient entropy pool",
    "contentLower": "when installing oo on linux os, the product start up gets stuck. cause this occurs due to an insufficient entropy pool. entropy value less than 1000 will lead to processes being blocked, waiting for more entropy. to check the current entropy value, run the following command as a root/sudo user: cat /proc/sys/kernel/random/entropy_avail a result between 800-1000 or lower can indicate an insufficient entropy pool. solution run either of the following commands as a user with appropriate privileges to update the entropy pool with values from the /dev/urandom directory. solution 1 run the following command: ln -sf /dev/urandom /dev/random solution 2 install rng-tools package for your linux distribution. to enable rng-tools for entropy pool increase: create or edit the file /etc/sysconfig/rngd. add the following line and save the file. extraoptions=\"--rng-device=drng --no-tpm=1\". restart the rng-tools service. run the following command: cat /proc/sys/kernel/random/entropy_avail the entropy l",
    "keywordsLower": [
      "oo",
      "start",
      "stuck",
      "due",
      "insufficient",
      "entropy",
      "pool",
      "cause",
      "solution",
      "installing",
      "linux",
      "os",
      "product",
      "gets",
      "stuck.",
      "occurs",
      "pool.",
      "value",
      "less",
      "1000",
      "lead",
      "processes",
      "blocked",
      "waiting",
      "entropy.",
      "check",
      "current",
      "run",
      "following",
      "command",
      "root",
      "sudo",
      "user",
      "cat",
      "proc",
      "sys",
      "kernel",
      "random",
      "result",
      "between",
      "800-1000",
      "lower",
      "indicate",
      "either",
      "commands",
      "appropriate",
      "privileges",
      "update",
      "values",
      "dev",
      "urandom",
      "directory.",
      "ln",
      "-sf",
      "install",
      "rng-tools",
      "package",
      "distribution.",
      "enable",
      "increase",
      "create",
      "edit",
      "file",
      "etc",
      "sysconfig",
      "rngd.",
      "add",
      "line",
      "save",
      "file.",
      "extraoptions",
      "--rng-device",
      "drng",
      "--no-tpm",
      "restart",
      "service.",
      "level",
      "increases",
      "about",
      "3000-3200",
      "method."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Python 3.12 syntax changes",
    "content": "Starting with Python 3.12 special characters are causing flows using Python expressions or scripts to remain stuck in running. Cause Starting with Python 3.12, any unescaped special character will no longer be treated as a deprecation warning but as a syntax warning. This can cause standalone scripts to fail to complete or remain stuck in a crash loop. Solution In Python expressions or operations, unescaped characters must be escaped, otherwise the flow might fail to complete or get stuck in running. For example, '\\' should be replaced with '\\\\' or '/'.",
    "url": "pythonsyntaxerror",
    "filename": "pythonsyntaxerror",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "3.12",
      "python",
      "syntax",
      "changes",
      "cause",
      "solution",
      "starting",
      "special",
      "characters",
      "causing",
      "flows",
      "expressions",
      "scripts",
      "remain",
      "stuck",
      "running.",
      "any",
      "unescaped",
      "character",
      "longer",
      "treated",
      "deprecation",
      "warning",
      "warning.",
      "standalone",
      "fail",
      "complete",
      "crash",
      "loop.",
      "operations",
      "escaped",
      "otherwise",
      "flow",
      "get",
      "example",
      "replaced"
    ],
    "language": "en",
    "word_count": 56,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "python 3.12 syntax changes",
    "contentLower": "starting with python 3.12 special characters are causing flows using python expressions or scripts to remain stuck in running. cause starting with python 3.12, any unescaped special character will no longer be treated as a deprecation warning but as a syntax warning. this can cause standalone scripts to fail to complete or remain stuck in a crash loop. solution in python expressions or operations, unescaped characters must be escaped, otherwise the flow might fail to complete or get stuck in running. for example, '\\' should be replaced with '\\\\' or '/'.",
    "keywordsLower": [
      "3.12",
      "python",
      "syntax",
      "changes",
      "cause",
      "solution",
      "starting",
      "special",
      "characters",
      "causing",
      "flows",
      "expressions",
      "scripts",
      "remain",
      "stuck",
      "running.",
      "any",
      "unescaped",
      "character",
      "longer",
      "treated",
      "deprecation",
      "warning",
      "warning.",
      "standalone",
      "fail",
      "complete",
      "crash",
      "loop.",
      "operations",
      "escaped",
      "otherwise",
      "flow",
      "get",
      "example",
      "replaced"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OO Central pod restarts when the environment is idle",
    "content": "Kubernetes restarts the OO Central pod even when the system is idle or under load. This is due to the pod not responding to Kubernetes health probes in time, leading to a restart. Cause The root cause is that the Java memory allocation for the OO Central application is too close to the total memory available for the pod. Any fluctuations in Java memory usage cause the system to hit memory limits, prompting Kubernetes to restart the pod to ensure stability. Understanding memory requests and limits In Kubernetes, each pod has two important memory configurations: memory requests and memory limits. These control how much memory a pod can use and how Kubernetes manages resources. Memory requests: This is the minimum amount of memory that Kubernetes guarantees for the pod. Kubernetes will reserve this amount of memory for the pod, ensuring it always has access to at least this much.Memory limits: This is the maximum amount of memory a pod can use. If the pod tries to exceed this limit, Kuber",
    "url": "podrestartedbyk8s",
    "filename": "podrestartedbyk8s",
    "headings": [
      "Cause",
      "Understanding memory requests and limits",
      "Java memory allocation for the application",
      "Solution",
      "Idle system (no load)",
      "System under load"
    ],
    "keywords": [
      "previous_deployment_values.yaml",
      "oo_chart.tgz",
      "values.yaml",
      "oo",
      "central",
      "pod",
      "restarts",
      "environment",
      "idle",
      "cause",
      "understanding",
      "memory",
      "requests",
      "limits",
      "java",
      "allocation",
      "application",
      "solution",
      "system",
      "load",
      "under",
      "kubernetes",
      "even",
      "load.",
      "due",
      "responding",
      "health",
      "probes",
      "time",
      "leading",
      "restart.",
      "root",
      "too",
      "close",
      "total",
      "available",
      "pod.",
      "any",
      "fluctuations",
      "usage",
      "hit",
      "prompting",
      "restart",
      "ensure",
      "stability.",
      "two",
      "important",
      "configurations",
      "limits.",
      "control",
      "much",
      "manages",
      "resources.",
      "minimum",
      "amount",
      "guarantees",
      "reserve",
      "ensuring",
      "always",
      "access",
      "least",
      "much.memory",
      "maximum",
      "use.",
      "tries",
      "exceed",
      "limit",
      "stop",
      "prevent",
      "affecting",
      "pods",
      "cluster.",
      "example",
      "5-tenant",
      "configuration",
      "request",
      "set",
      "5120mi",
      "5gb",
      "5120mi.",
      "15-tenant",
      "23040mi",
      "23gb",
      "same.",
      "allocated",
      "separately",
      "managed",
      "default",
      "4092mi",
      "4gb",
      "18432mi",
      "18gb",
      "values",
      "just",
      "examples",
      "adapted",
      "according",
      "resource",
      "needs.",
      "adjust"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "oo central pod restarts when the environment is idle",
    "contentLower": "kubernetes restarts the oo central pod even when the system is idle or under load. this is due to the pod not responding to kubernetes health probes in time, leading to a restart. cause the root cause is that the java memory allocation for the oo central application is too close to the total memory available for the pod. any fluctuations in java memory usage cause the system to hit memory limits, prompting kubernetes to restart the pod to ensure stability. understanding memory requests and limits in kubernetes, each pod has two important memory configurations: memory requests and memory limits. these control how much memory a pod can use and how kubernetes manages resources. memory requests: this is the minimum amount of memory that kubernetes guarantees for the pod. kubernetes will reserve this amount of memory for the pod, ensuring it always has access to at least this much.memory limits: this is the maximum amount of memory a pod can use. if the pod tries to exceed this limit, kuber",
    "keywordsLower": [
      "previous_deployment_values.yaml",
      "oo_chart.tgz",
      "values.yaml",
      "oo",
      "central",
      "pod",
      "restarts",
      "environment",
      "idle",
      "cause",
      "understanding",
      "memory",
      "requests",
      "limits",
      "java",
      "allocation",
      "application",
      "solution",
      "system",
      "load",
      "under",
      "kubernetes",
      "even",
      "load.",
      "due",
      "responding",
      "health",
      "probes",
      "time",
      "leading",
      "restart.",
      "root",
      "too",
      "close",
      "total",
      "available",
      "pod.",
      "any",
      "fluctuations",
      "usage",
      "hit",
      "prompting",
      "restart",
      "ensure",
      "stability.",
      "two",
      "important",
      "configurations",
      "limits.",
      "control",
      "much",
      "manages",
      "resources.",
      "minimum",
      "amount",
      "guarantees",
      "reserve",
      "ensuring",
      "always",
      "access",
      "least",
      "much.memory",
      "maximum",
      "use.",
      "tries",
      "exceed",
      "limit",
      "stop",
      "prevent",
      "affecting",
      "pods",
      "cluster.",
      "example",
      "5-tenant",
      "configuration",
      "request",
      "set",
      "5120mi",
      "5gb",
      "5120mi.",
      "15-tenant",
      "23040mi",
      "23gb",
      "same.",
      "allocated",
      "separately",
      "managed",
      "default",
      "4092mi",
      "4gb",
      "18432mi",
      "18gb",
      "values",
      "just",
      "examples",
      "adapted",
      "according",
      "resource",
      "needs.",
      "adjust"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Monitoring lines for resource limits and requests on smarta-saw-con pods are too close",
    "content": "The Prometheus dashboard shows that the monitoring line for resource limits is too close to the monitoring line for resource requests on the smarta-saw-con pods. Cause This issue might occur due to insufficient memory of the smarta-saw-con pods. Solution Run the following command to open the smarta-saw-con resource editor. kubectl edit sts smarta-saw-con -n itsma-a4iax Change the value of the memory field under limits to a larger value, and then save the update. After this change, the saw content pods will restart automatically. Go to the Prometheus dashboard and check if the monitoring lines for limits and requests are normal.",
    "url": "closemonitoringlines",
    "filename": "closemonitoringlines",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "monitoring",
      "lines",
      "resource",
      "limits",
      "requests",
      "smarta-saw-con",
      "pods",
      "too",
      "close",
      "cause",
      "solution",
      "prometheus",
      "dashboard",
      "shows",
      "line",
      "pods.",
      "issue",
      "occur",
      "due",
      "insufficient",
      "memory",
      "run",
      "following",
      "command",
      "open",
      "editor.",
      "kubectl",
      "edit",
      "sts",
      "-n",
      "itsma-a4iax",
      "change",
      "value",
      "field",
      "under",
      "larger",
      "save",
      "update.",
      "after",
      "saw",
      "content",
      "restart",
      "automatically.",
      "go",
      "check",
      "normal."
    ],
    "language": "en",
    "word_count": 72,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "monitoring lines for resource limits and requests on smarta-saw-con pods are too close",
    "contentLower": "the prometheus dashboard shows that the monitoring line for resource limits is too close to the monitoring line for resource requests on the smarta-saw-con pods. cause this issue might occur due to insufficient memory of the smarta-saw-con pods. solution run the following command to open the smarta-saw-con resource editor. kubectl edit sts smarta-saw-con -n itsma-a4iax change the value of the memory field under limits to a larger value, and then save the update. after this change, the saw content pods will restart automatically. go to the prometheus dashboard and check if the monitoring lines for limits and requests are normal.",
    "keywordsLower": [
      "monitoring",
      "lines",
      "resource",
      "limits",
      "requests",
      "smarta-saw-con",
      "pods",
      "too",
      "close",
      "cause",
      "solution",
      "prometheus",
      "dashboard",
      "shows",
      "line",
      "pods.",
      "issue",
      "occur",
      "due",
      "insufficient",
      "memory",
      "run",
      "following",
      "command",
      "open",
      "editor.",
      "kubectl",
      "edit",
      "sts",
      "-n",
      "itsma-a4iax",
      "change",
      "value",
      "field",
      "under",
      "larger",
      "save",
      "update.",
      "after",
      "saw",
      "content",
      "restart",
      "automatically.",
      "go",
      "check",
      "normal."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage Persons API stops processing",
    "content": "The Manage Persons API stops processing. If you check the database, you can see the following symptoms: The number of records in the job table (WQProvisionUsers_857561481) never decreases. The number of 'Pending' records in the status table (ProvisionUsersState_<tenant_id>) never decreases. Cause The two tables become too large so that the Manage Persons API stops processing. Solution You can check both the job table and the status table every 24 hours. When the issue happens, clean both tables. Query Group status: select body -> 'provisionUsersBulkResultJSON' ->> 'BulkStatus' AS \"BulkStatus\", count(*), MIN(body ->> 'lastUpdateTime') AS \"minLastUpdateTime\" FROM ProvisionUsersState_<tenant_id> where body -> GROUP BY body -> 'provisionUsersBulkResultJSON' ->> 'BulkStatus'; Query RUNNING status: SELECT id, body FROM ProvisionUsersState_<tenant_id> where body -> 'provisionUsersBulkResultJSON' ->> 'BulkStatus' = 'RUNNING'; Delete the records in the job table: truncate table ProvisionUsersSt",
    "url": "tsmanagepersonsstopprocessing",
    "filename": "tsmanagepersonsstopprocessing",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "manage",
      "persons",
      "api",
      "stops",
      "processing",
      "cause",
      "solution",
      "processing.",
      "check",
      "database",
      "see",
      "following",
      "symptoms",
      "number",
      "records",
      "job",
      "table",
      "never",
      "decreases.",
      "pending",
      "status",
      "two",
      "tables",
      "become",
      "too",
      "large",
      "both",
      "every",
      "24",
      "hours.",
      "issue",
      "happens",
      "clean",
      "tables.",
      "query",
      "group",
      "select",
      "body",
      "provisionusersbulkresultjson",
      "bulkstatus",
      "count",
      "min",
      "lastupdatetime",
      "minlastupdatetime",
      "running",
      "id",
      "delete",
      "truncate",
      "note",
      "scheduled",
      "runs",
      "midnight",
      "utc",
      "time",
      "deletes",
      "older",
      "days",
      "table.",
      "cannot",
      "wait",
      "deleted",
      "want",
      "all",
      "instead",
      "just",
      "manually",
      "command",
      "above."
    ],
    "language": "en",
    "word_count": 93,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage persons api stops processing",
    "contentLower": "the manage persons api stops processing. if you check the database, you can see the following symptoms: the number of records in the job table (wqprovisionusers_857561481) never decreases. the number of 'pending' records in the status table (provisionusersstate_<tenant_id>) never decreases. cause the two tables become too large so that the manage persons api stops processing. solution you can check both the job table and the status table every 24 hours. when the issue happens, clean both tables. query group status: select body -> 'provisionusersbulkresultjson' ->> 'bulkstatus' as \"bulkstatus\", count(*), min(body ->> 'lastupdatetime') as \"minlastupdatetime\" from provisionusersstate_<tenant_id> where body -> group by body -> 'provisionusersbulkresultjson' ->> 'bulkstatus'; query running status: select id, body from provisionusersstate_<tenant_id> where body -> 'provisionusersbulkresultjson' ->> 'bulkstatus' = 'running'; delete the records in the job table: truncate table provisionusersst",
    "keywordsLower": [
      "manage",
      "persons",
      "api",
      "stops",
      "processing",
      "cause",
      "solution",
      "processing.",
      "check",
      "database",
      "see",
      "following",
      "symptoms",
      "number",
      "records",
      "job",
      "table",
      "never",
      "decreases.",
      "pending",
      "status",
      "two",
      "tables",
      "become",
      "too",
      "large",
      "both",
      "every",
      "24",
      "hours.",
      "issue",
      "happens",
      "clean",
      "tables.",
      "query",
      "group",
      "select",
      "body",
      "provisionusersbulkresultjson",
      "bulkstatus",
      "count",
      "min",
      "lastupdatetime",
      "minlastupdatetime",
      "running",
      "id",
      "delete",
      "truncate",
      "note",
      "scheduled",
      "runs",
      "midnight",
      "utc",
      "time",
      "deletes",
      "older",
      "days",
      "table.",
      "cannot",
      "wait",
      "deleted",
      "want",
      "all",
      "instead",
      "just",
      "manually",
      "command",
      "above."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "On Azure deployment - Dynamic subscriber options are not available in drop-down while submitting subscription request",
    "content": "Log in to the Service Portal that's deployed on Azure environment. Decide on the offering and select Request Service. The drop-down menus on the offering details page aren't populated with any options. Cause This issue occurs when the Azure self-signed certificate isn't added to the trust store. Solution Perform the following steps when the product deployment is on Azure using the Azure Kubernetes Service (AKS). Obtain the server certificate by exporting it from the browser. Copy the server certificate into the NFS shared folder <config-volume>/certificate/cmp/source. Example: /var/vols/itom/itsma/config-volume/certificate/cmp/source Delete and restart the DND pods by running the following commands: kubectl scale deployment itom-dnd-controller-deployment --replicas=0 -n <namespace> kubectl scale deployment itom-dnd-controller-deployment --replicas=1 -n <namespace>",
    "url": "subscriptionoptionsnotavailableonaks",
    "filename": "subscriptionoptionsnotavailableonaks",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "azure",
      "deployment",
      "dynamic",
      "subscriber",
      "options",
      "available",
      "drop-down",
      "while",
      "submitting",
      "subscription",
      "request",
      "cause",
      "solution",
      "log",
      "service",
      "portal",
      "deployed",
      "environment.",
      "decide",
      "offering",
      "select",
      "service.",
      "menus",
      "details",
      "page",
      "aren",
      "populated",
      "any",
      "options.",
      "issue",
      "occurs",
      "self-signed",
      "certificate",
      "isn",
      "added",
      "trust",
      "store.",
      "perform",
      "following",
      "steps",
      "product",
      "kubernetes",
      "aks",
      "obtain",
      "server",
      "exporting",
      "browser.",
      "copy",
      "nfs",
      "shared",
      "folder",
      "cmp",
      "source.",
      "example",
      "var",
      "vols",
      "itom",
      "itsma",
      "config-volume",
      "source",
      "delete",
      "restart",
      "dnd",
      "pods",
      "running",
      "commands",
      "kubectl",
      "scale",
      "itom-dnd-controller-deployment",
      "--replicas",
      "-n"
    ],
    "language": "en",
    "word_count": 99,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "on azure deployment - dynamic subscriber options are not available in drop-down while submitting subscription request",
    "contentLower": "log in to the service portal that's deployed on azure environment. decide on the offering and select request service. the drop-down menus on the offering details page aren't populated with any options. cause this issue occurs when the azure self-signed certificate isn't added to the trust store. solution perform the following steps when the product deployment is on azure using the azure kubernetes service (aks). obtain the server certificate by exporting it from the browser. copy the server certificate into the nfs shared folder <config-volume>/certificate/cmp/source. example: /var/vols/itom/itsma/config-volume/certificate/cmp/source delete and restart the dnd pods by running the following commands: kubectl scale deployment itom-dnd-controller-deployment --replicas=0 -n <namespace> kubectl scale deployment itom-dnd-controller-deployment --replicas=1 -n <namespace>",
    "keywordsLower": [
      "azure",
      "deployment",
      "dynamic",
      "subscriber",
      "options",
      "available",
      "drop-down",
      "while",
      "submitting",
      "subscription",
      "request",
      "cause",
      "solution",
      "log",
      "service",
      "portal",
      "deployed",
      "environment.",
      "decide",
      "offering",
      "select",
      "service.",
      "menus",
      "details",
      "page",
      "aren",
      "populated",
      "any",
      "options.",
      "issue",
      "occurs",
      "self-signed",
      "certificate",
      "isn",
      "added",
      "trust",
      "store.",
      "perform",
      "following",
      "steps",
      "product",
      "kubernetes",
      "aks",
      "obtain",
      "server",
      "exporting",
      "browser.",
      "copy",
      "nfs",
      "shared",
      "folder",
      "cmp",
      "source.",
      "example",
      "var",
      "vols",
      "itom",
      "itsma",
      "config-volume",
      "source",
      "delete",
      "restart",
      "dnd",
      "pods",
      "running",
      "commands",
      "kubectl",
      "scale",
      "itom-dnd-controller-deployment",
      "--replicas",
      "-n"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "On-call shifts aren't displayed in the on-call calendar",
    "content": "On-call shifts aren't displayed in the on-call calendar. Cause By design, the maximum number of on-call shifts in a group is 3000. However, the system doesn't clean up outdated on-call shifts, once the accumulated number of on-call shifts exceeds the limit, new on-call shifts won't show up in the on-call calendar. Resolution To work around this issue, manually clean up old on-call shifts by running the following command: UPDATE user_work_schedule_<tenant ID> SET is_deleted = true WHERE group_id = '<group ID>' and schedule_end_time < <timestamp>; Make sure that you replace placeholders <tenant ID>, <group ID> and <timestamp> with the actual tenant ID, group ID, and UNIX timestamp. For example, the following command deletes the on-call shift data for the group ID 10005 before July 1st, 2023 in the tenant 100000002: UPDATE user_work_schedule_100000002 SET is_deleted = true WHERE group_id = '10005' and schedule_end_time < 1688140800000",
    "url": "tsoncallshiftsnotdisplayed",
    "filename": "tsoncallshiftsnotdisplayed",
    "headings": [
      "Cause",
      "Resolution"
    ],
    "keywords": [
      "arent",
      "on-call",
      "shifts",
      "aren",
      "displayed",
      "calendar",
      "cause",
      "resolution",
      "calendar.",
      "design",
      "maximum",
      "number",
      "group",
      "3000.",
      "however",
      "system",
      "doesn",
      "clean",
      "outdated",
      "once",
      "accumulated",
      "exceeds",
      "limit",
      "new",
      "won",
      "show",
      "work",
      "around",
      "issue",
      "manually",
      "old",
      "running",
      "following",
      "command",
      "update",
      "set",
      "true",
      "make",
      "sure",
      "replace",
      "placeholders",
      "actual",
      "tenant",
      "id",
      "unix",
      "timestamp.",
      "example",
      "deletes",
      "shift",
      "data",
      "10005",
      "before",
      "july",
      "1st",
      "2023",
      "100000002",
      "1688140800000"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "on-call shifts aren't displayed in the on-call calendar",
    "contentLower": "on-call shifts aren't displayed in the on-call calendar. cause by design, the maximum number of on-call shifts in a group is 3000. however, the system doesn't clean up outdated on-call shifts, once the accumulated number of on-call shifts exceeds the limit, new on-call shifts won't show up in the on-call calendar. resolution to work around this issue, manually clean up old on-call shifts by running the following command: update user_work_schedule_<tenant id> set is_deleted = true where group_id = '<group id>' and schedule_end_time < <timestamp>; make sure that you replace placeholders <tenant id>, <group id> and <timestamp> with the actual tenant id, group id, and unix timestamp. for example, the following command deletes the on-call shift data for the group id 10005 before july 1st, 2023 in the tenant 100000002: update user_work_schedule_100000002 set is_deleted = true where group_id = '10005' and schedule_end_time < 1688140800000",
    "keywordsLower": [
      "arent",
      "on-call",
      "shifts",
      "aren",
      "displayed",
      "calendar",
      "cause",
      "resolution",
      "calendar.",
      "design",
      "maximum",
      "number",
      "group",
      "3000.",
      "however",
      "system",
      "doesn",
      "clean",
      "outdated",
      "once",
      "accumulated",
      "exceeds",
      "limit",
      "new",
      "won",
      "show",
      "work",
      "around",
      "issue",
      "manually",
      "old",
      "running",
      "following",
      "command",
      "update",
      "set",
      "true",
      "make",
      "sure",
      "replace",
      "placeholders",
      "actual",
      "tenant",
      "id",
      "unix",
      "timestamp.",
      "example",
      "deletes",
      "shift",
      "data",
      "10005",
      "before",
      "july",
      "1st",
      "2023",
      "100000002",
      "1688140800000"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Per-schedule rules experience slow performance",
    "content": "Your per-schedule rules experience slow performance. Normally, a per-schedule rule is expected to start in no more than 5 minutes after the scheduled start time. However, the limited velocity of one single thread will cause accumulated execution delay when the incoming volume of the per-schedule rules is high. In this scenario, the accumulated rule execution delay may be much longer than 5 minutes. Cause You can use only one thread to consume per-schedule rules and the limited velocity of the single thread may cause delay. Resolution Check the per-schedule rule and make sure the slow performance isn't caused by any other configuration issues. Monitor the per-schedule rule and see if the execution delay persists. Run the following command on the control plane node to create a configmap key: kubectl patch configmap itom-xruntime-infra-config --patch '{\"data\": {\"WORKFLOW_TIMER_THREAD_COUNT\": \"<ThreadNumber>\"}}' -n <SuiteNamespace> Note: Replace <ThreadNumber> and <SuiteNamespace> with the",
    "url": "perscheduleruleslowperformance",
    "filename": "perscheduleruleslowperformance",
    "headings": [
      "Cause",
      "Resolution"
    ],
    "keywords": [
      "per-schedule",
      "rules",
      "experience",
      "slow",
      "performance",
      "cause",
      "resolution",
      "performance.",
      "normally",
      "rule",
      "expected",
      "start",
      "minutes",
      "after",
      "scheduled",
      "time.",
      "however",
      "limited",
      "velocity",
      "one",
      "single",
      "thread",
      "accumulated",
      "execution",
      "delay",
      "incoming",
      "volume",
      "high.",
      "scenario",
      "much",
      "longer",
      "minutes.",
      "consume",
      "delay.",
      "check",
      "make",
      "sure",
      "isn",
      "caused",
      "any",
      "configuration",
      "issues.",
      "monitor",
      "see",
      "persists.",
      "run",
      "following",
      "command",
      "control",
      "plane",
      "node",
      "create",
      "configmap",
      "key",
      "kubectl",
      "patch",
      "itom-xruntime-infra-config",
      "--patch",
      "data",
      "-n",
      "note",
      "replace",
      "number",
      "threads",
      "suite",
      "namespace.",
      "minimum",
      "value",
      "maximum",
      "total",
      "cpu",
      "core",
      "offline",
      "pod.",
      "set",
      "larger",
      "platform",
      "pod",
      "count.",
      "suitable",
      "threadnumber",
      "improves",
      "reduce",
      "even",
      "eliminate",
      "example",
      "setting",
      "doubles",
      "velocity.",
      "doesn",
      "restart",
      "automatically",
      "manually",
      "running",
      "rollout",
      "deployment",
      "itom-xruntime-platform-offline",
      "actual"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "per-schedule rules experience slow performance",
    "contentLower": "your per-schedule rules experience slow performance. normally, a per-schedule rule is expected to start in no more than 5 minutes after the scheduled start time. however, the limited velocity of one single thread will cause accumulated execution delay when the incoming volume of the per-schedule rules is high. in this scenario, the accumulated rule execution delay may be much longer than 5 minutes. cause you can use only one thread to consume per-schedule rules and the limited velocity of the single thread may cause delay. resolution check the per-schedule rule and make sure the slow performance isn't caused by any other configuration issues. monitor the per-schedule rule and see if the execution delay persists. run the following command on the control plane node to create a configmap key: kubectl patch configmap itom-xruntime-infra-config --patch '{\"data\": {\"workflow_timer_thread_count\": \"<threadnumber>\"}}' -n <suitenamespace> note: replace <threadnumber> and <suitenamespace> with the",
    "keywordsLower": [
      "per-schedule",
      "rules",
      "experience",
      "slow",
      "performance",
      "cause",
      "resolution",
      "performance.",
      "normally",
      "rule",
      "expected",
      "start",
      "minutes",
      "after",
      "scheduled",
      "time.",
      "however",
      "limited",
      "velocity",
      "one",
      "single",
      "thread",
      "accumulated",
      "execution",
      "delay",
      "incoming",
      "volume",
      "high.",
      "scenario",
      "much",
      "longer",
      "minutes.",
      "consume",
      "delay.",
      "check",
      "make",
      "sure",
      "isn",
      "caused",
      "any",
      "configuration",
      "issues.",
      "monitor",
      "see",
      "persists.",
      "run",
      "following",
      "command",
      "control",
      "plane",
      "node",
      "create",
      "configmap",
      "key",
      "kubectl",
      "patch",
      "itom-xruntime-infra-config",
      "--patch",
      "data",
      "-n",
      "note",
      "replace",
      "number",
      "threads",
      "suite",
      "namespace.",
      "minimum",
      "value",
      "maximum",
      "total",
      "cpu",
      "core",
      "offline",
      "pod.",
      "set",
      "larger",
      "platform",
      "pod",
      "count.",
      "suitable",
      "threadnumber",
      "improves",
      "reduce",
      "even",
      "eliminate",
      "example",
      "setting",
      "doubles",
      "velocity.",
      "doesn",
      "restart",
      "automatically",
      "manually",
      "running",
      "rollout",
      "deployment",
      "itom-xruntime-platform-offline",
      "actual"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "No CIs displayed in a grid in Service Management",
    "content": "No CIs are displayed when you load Devices in a Service Management grid and the filter or order contains non-federated fields. Cause This issue may occur because the data includes old Devices that don't have bitmap or bitposition values, which are part of the primary key of device. When a filter or order contains non-federated fields, the query needs to make a join on the unique key. Solution Make sure that all Devices have a bitmap and bit position value. To do this, you can run the updateEmptyBitmapIds function in the JMX console in UCMDB.",
    "url": "dataloadingissuesacm",
    "filename": "dataloadingissuesacm",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "cis",
      "displayed",
      "grid",
      "service",
      "management",
      "cause",
      "solution",
      "load",
      "devices",
      "filter",
      "order",
      "contains",
      "non-federated",
      "fields.",
      "issue",
      "occur",
      "because",
      "data",
      "includes",
      "old",
      "don",
      "bitmap",
      "bitposition",
      "values",
      "part",
      "primary",
      "key",
      "device.",
      "fields",
      "query",
      "needs",
      "make",
      "join",
      "unique",
      "key.",
      "sure",
      "all",
      "bit",
      "position",
      "value.",
      "run",
      "updateemptybitmapids",
      "function",
      "jmx",
      "console",
      "ucmdb."
    ],
    "language": "en",
    "word_count": 60,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "no cis displayed in a grid in service management",
    "contentLower": "no cis are displayed when you load devices in a service management grid and the filter or order contains non-federated fields. cause this issue may occur because the data includes old devices that don't have bitmap or bitposition values, which are part of the primary key of device. when a filter or order contains non-federated fields, the query needs to make a join on the unique key. solution make sure that all devices have a bitmap and bit position value. to do this, you can run the updateemptybitmapids function in the jmx console in ucmdb.",
    "keywordsLower": [
      "cis",
      "displayed",
      "grid",
      "service",
      "management",
      "cause",
      "solution",
      "load",
      "devices",
      "filter",
      "order",
      "contains",
      "non-federated",
      "fields.",
      "issue",
      "occur",
      "because",
      "data",
      "includes",
      "old",
      "don",
      "bitmap",
      "bitposition",
      "values",
      "part",
      "primary",
      "key",
      "device.",
      "fields",
      "query",
      "needs",
      "make",
      "join",
      "unique",
      "key.",
      "sure",
      "all",
      "bit",
      "position",
      "value.",
      "run",
      "updateemptybitmapids",
      "function",
      "jmx",
      "console",
      "ucmdb."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Newly created attributes in UD/UCMDB aren't exposed in Service Management Studio",
    "content": "Newly created attributes in UD/UCMDB aren't exposed to Service Management Studio. Solution Open the config.properties file in the UCMDB Gateway installer folder, locate the [ucmdb] section. Make sure you have enablePoll set to true, also make sure userName and password are set correctly. Then, restart UCMDB Gateway.",
    "url": "attributescmsnotexposedsmastudio",
    "filename": "attributescmsnotexposedsmastudio",
    "headings": [
      "Solution"
    ],
    "keywords": [
      "arent",
      "uducmdb",
      "newly",
      "created",
      "attributes",
      "ud",
      "ucmdb",
      "aren",
      "exposed",
      "service",
      "management",
      "studio",
      "solution",
      "studio.",
      "open",
      "config.properties",
      "file",
      "gateway",
      "installer",
      "folder",
      "locate",
      "section.",
      "make",
      "sure",
      "enablepoll",
      "set",
      "true",
      "username",
      "password",
      "correctly.",
      "restart",
      "gateway."
    ],
    "language": "en",
    "word_count": 47,
    "importance_score": 0.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "newly created attributes in ud/ucmdb aren't exposed in service management studio",
    "contentLower": "newly created attributes in ud/ucmdb aren't exposed to service management studio. solution open the config.properties file in the ucmdb gateway installer folder, locate the [ucmdb] section. make sure you have enablepoll set to true, also make sure username and password are set correctly. then, restart ucmdb gateway.",
    "keywordsLower": [
      "arent",
      "uducmdb",
      "newly",
      "created",
      "attributes",
      "ud",
      "ucmdb",
      "aren",
      "exposed",
      "service",
      "management",
      "studio",
      "solution",
      "studio.",
      "open",
      "config.properties",
      "file",
      "gateway",
      "installer",
      "folder",
      "locate",
      "section.",
      "make",
      "sure",
      "enablepoll",
      "set",
      "true",
      "username",
      "password",
      "correctly.",
      "restart",
      "gateway."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OO content pack import failure",
    "content": "Capsule installation fails if the OO content pack import fails. You may come across any one of the following error messages in the Content Store UI: Upload of OO content deployment process file failed with status code: '<4XX ,5XX>' Unable to continue asynchronous deployment of OO contents. The following error messages appear in the Content Store log files: Asynchronous deployment of OO content is in conflict state as another deployment with one or more content(s) is in progress. Retrying.... Asynchronous deployment of OO content is in conflict state as another deployment with one or more content(s) is in progress. Reached maximum number of retry, exiting... Possible causes Cause 1: There is an issue in the network connection. Cause 2: Any of the required flows in the content packs are missing. Cause 3: Any of the dependent content packs are missing. Cause 4: If an installation is in progress and another installation request is made, then you see the conflict messages. If the failure is",
    "url": "oocpimportfail",
    "filename": "oocpimportfail",
    "headings": [
      "Possible causes",
      "If the failure is caused by a network issue",
      "If the failure is caused by a missing flow",
      "If the failure is caused by a missing content pack",
      "If the failure is caused by a conflict state"
    ],
    "keywords": [
      "oo",
      "content",
      "pack",
      "import",
      "failure",
      "possible",
      "causes",
      "caused",
      "network",
      "issue",
      "missing",
      "flow",
      "conflict",
      "state",
      "capsule",
      "installation",
      "fails",
      "fails.",
      "come",
      "across",
      "any",
      "one",
      "following",
      "error",
      "messages",
      "store",
      "ui",
      "upload",
      "deployment",
      "process",
      "file",
      "failed",
      "status",
      "code",
      "unable",
      "continue",
      "asynchronous",
      "contents.",
      "appear",
      "log",
      "files",
      "another",
      "progress.",
      "retrying....",
      "reached",
      "maximum",
      "number",
      "retry",
      "exiting...",
      "cause",
      "there",
      "connection.",
      "required",
      "flows",
      "packs",
      "missing.",
      "dependent",
      "progress",
      "request",
      "made",
      "see",
      "messages.",
      "check",
      "connection",
      "stable.",
      "identify",
      "update",
      "pack.",
      "wait",
      "first",
      "complete.",
      "doesn",
      "proceed",
      "logs",
      "details",
      "take",
      "necessary",
      "action."
    ],
    "language": "en",
    "word_count": 111,
    "importance_score": 5.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "oo content pack import failure",
    "contentLower": "capsule installation fails if the oo content pack import fails. you may come across any one of the following error messages in the content store ui: upload of oo content deployment process file failed with status code: '<4xx ,5xx>' unable to continue asynchronous deployment of oo contents. the following error messages appear in the content store log files: asynchronous deployment of oo content is in conflict state as another deployment with one or more content(s) is in progress. retrying.... asynchronous deployment of oo content is in conflict state as another deployment with one or more content(s) is in progress. reached maximum number of retry, exiting... possible causes cause 1: there is an issue in the network connection. cause 2: any of the required flows in the content packs are missing. cause 3: any of the dependent content packs are missing. cause 4: if an installation is in progress and another installation request is made, then you see the conflict messages. if the failure is",
    "keywordsLower": [
      "oo",
      "content",
      "pack",
      "import",
      "failure",
      "possible",
      "causes",
      "caused",
      "network",
      "issue",
      "missing",
      "flow",
      "conflict",
      "state",
      "capsule",
      "installation",
      "fails",
      "fails.",
      "come",
      "across",
      "any",
      "one",
      "following",
      "error",
      "messages",
      "store",
      "ui",
      "upload",
      "deployment",
      "process",
      "file",
      "failed",
      "status",
      "code",
      "unable",
      "continue",
      "asynchronous",
      "contents.",
      "appear",
      "log",
      "files",
      "another",
      "progress.",
      "retrying....",
      "reached",
      "maximum",
      "number",
      "retry",
      "exiting...",
      "cause",
      "there",
      "connection.",
      "required",
      "flows",
      "packs",
      "missing.",
      "dependent",
      "progress",
      "request",
      "made",
      "see",
      "messages.",
      "check",
      "connection",
      "stable.",
      "identify",
      "update",
      "pack.",
      "wait",
      "first",
      "complete.",
      "doesn",
      "proceed",
      "logs",
      "details",
      "take",
      "necessary",
      "action."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "On subscription creation screen 'OS Image' drop-down list is empty",
    "content": "You export the Microsoft Azure design from CSA version 4.92 and then import it to HCMX. If you create a subscription from the imported Azure design, the OS Image drop-down list is empty. Cause The Azure design file Ms-Azure-Images-17.9.0.jsp contains unsupported configurations. Solution From CSA, export the design SERVICE_DESIGN_Microsoft_Azure_Compute_<version>.zip to the local file system and extract the contents. Copy the Ms-Azure-Images-17.9.0.jsp file in the extracted folder to a temporary location and then edit the file to remove the following lines: String adminuseridentifier = CSAReadResourceOfferingProperty.getUserIdentifier(\"admin\"); CSAIntegrationHelper.executeCSAREST(\"csa/rest/catalog?userIdentifier=\" + adminuseridentifier); Note that, the custom designs in the CSA environment shouldn't contain these lines. Make sure you remove these lines, before importing any custom designs. After editing, rename the file to Ms-Azure-Images-17.9.0-modified.jsp. Copy the file Ms-Azure-Imag",
    "url": "imagefieldnonalue",
    "filename": "imagefieldnonalue",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "modified.jsp",
      "4.92",
      "17.9.0",
      "csa.war",
      "0.jsp",
      "subscription",
      "creation",
      "screen",
      "os",
      "image",
      "drop-down",
      "list",
      "empty",
      "cause",
      "solution",
      "export",
      "microsoft",
      "azure",
      "design",
      "csa",
      "version",
      "import",
      "hcmx.",
      "create",
      "imported",
      "empty.",
      "file",
      "ms-azure-images-17.9.0.jsp",
      "contains",
      "unsupported",
      "configurations.",
      "local",
      "system",
      "extract",
      "contents.",
      "copy",
      "extracted",
      "folder",
      "temporary",
      "location",
      "edit",
      "remove",
      "following",
      "lines",
      "string",
      "adminuseridentifier",
      "csareadresourceofferingproperty.getuseridentifier",
      "admin",
      "csaintegrationhelper.executecsarest",
      "rest",
      "catalog",
      "useridentifier",
      "note",
      "custom",
      "designs",
      "environment",
      "shouldn",
      "contain",
      "lines.",
      "make",
      "sure",
      "before",
      "importing",
      "any",
      "designs.",
      "after",
      "editing",
      "rename",
      "ms-azure-images-17.9.0-modified.jsp.",
      "ms-azure-images-17.9.0-modified.jsp",
      "data",
      "dnd",
      "sync",
      "app-server",
      "deployments",
      "propertysources",
      "downloaded",
      "zip",
      "file.",
      "published",
      "state",
      "unpublish",
      "it.",
      "service",
      "designer",
      "go",
      "configuration",
      "options.",
      "select",
      "ms",
      "click",
      "option",
      "navigate",
      "set.",
      "configurations",
      "panel",
      "image.",
      "properties",
      "items",
      "done."
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "on subscription creation screen 'os image' drop-down list is empty",
    "contentLower": "you export the microsoft azure design from csa version 4.92 and then import it to hcmx. if you create a subscription from the imported azure design, the os image drop-down list is empty. cause the azure design file ms-azure-images-17.9.0.jsp contains unsupported configurations. solution from csa, export the design service_design_microsoft_azure_compute_<version>.zip to the local file system and extract the contents. copy the ms-azure-images-17.9.0.jsp file in the extracted folder to a temporary location and then edit the file to remove the following lines: string adminuseridentifier = csareadresourceofferingproperty.getuseridentifier(\"admin\"); csaintegrationhelper.executecsarest(\"csa/rest/catalog?useridentifier=\" + adminuseridentifier); note that, the custom designs in the csa environment shouldn't contain these lines. make sure you remove these lines, before importing any custom designs. after editing, rename the file to ms-azure-images-17.9.0-modified.jsp. copy the file ms-azure-imag",
    "keywordsLower": [
      "modified.jsp",
      "4.92",
      "17.9.0",
      "csa.war",
      "0.jsp",
      "subscription",
      "creation",
      "screen",
      "os",
      "image",
      "drop-down",
      "list",
      "empty",
      "cause",
      "solution",
      "export",
      "microsoft",
      "azure",
      "design",
      "csa",
      "version",
      "import",
      "hcmx.",
      "create",
      "imported",
      "empty.",
      "file",
      "ms-azure-images-17.9.0.jsp",
      "contains",
      "unsupported",
      "configurations.",
      "local",
      "system",
      "extract",
      "contents.",
      "copy",
      "extracted",
      "folder",
      "temporary",
      "location",
      "edit",
      "remove",
      "following",
      "lines",
      "string",
      "adminuseridentifier",
      "csareadresourceofferingproperty.getuseridentifier",
      "admin",
      "csaintegrationhelper.executecsarest",
      "rest",
      "catalog",
      "useridentifier",
      "note",
      "custom",
      "designs",
      "environment",
      "shouldn",
      "contain",
      "lines.",
      "make",
      "sure",
      "before",
      "importing",
      "any",
      "designs.",
      "after",
      "editing",
      "rename",
      "ms-azure-images-17.9.0-modified.jsp.",
      "ms-azure-images-17.9.0-modified.jsp",
      "data",
      "dnd",
      "sync",
      "app-server",
      "deployments",
      "propertysources",
      "downloaded",
      "zip",
      "file.",
      "published",
      "state",
      "unpublish",
      "it.",
      "service",
      "designer",
      "go",
      "configuration",
      "options.",
      "select",
      "ms",
      "click",
      "option",
      "navigate",
      "set.",
      "configurations",
      "panel",
      "image.",
      "properties",
      "items",
      "done."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Overview of the REST API",
    "content": "The following API services are available: Entity Management Service (EMS). Enables you to perform Create/Read/Update operations on records and manage records. See Record bulk update and collection APIs and REST API queries. Analytics. Enables you to use reporting services to analyze your record data. Manage Persons. Enables you to manage users and contacts. Business Intelligence (BI). Enables you to integrate Service Management with a third-party business intelligence system. Case Exchange. Enables you to exchange records with an external system. Encryption domain. Enables you to manage encryption domains. Comments. Enables you to post comments to records and access record comments. User lock. Enables you to get the lock status of a user and lock or unlock a user. Important Service Management only supports the public APIs documented in this section and doesn't support other APIs. Caution When entering characters in fields in REST calls, the following characters are invalid: 0x0 to 0x8 ",
    "url": "restapi",
    "filename": "restapi",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "overview",
      "rest",
      "api",
      "related",
      "topics",
      "following",
      "services",
      "available",
      "entity",
      "management",
      "service",
      "ems",
      "enables",
      "perform",
      "create",
      "read",
      "update",
      "operations",
      "records",
      "manage",
      "records.",
      "see",
      "record",
      "bulk",
      "collection",
      "apis",
      "queries.",
      "analytics.",
      "reporting",
      "analyze",
      "data.",
      "persons.",
      "users",
      "contacts.",
      "business",
      "intelligence",
      "bi",
      "integrate",
      "third-party",
      "system.",
      "case",
      "exchange.",
      "exchange",
      "external",
      "encryption",
      "domain.",
      "domains.",
      "comments.",
      "post",
      "comments",
      "access",
      "user",
      "lock.",
      "get",
      "lock",
      "status",
      "unlock",
      "user.",
      "important",
      "supports",
      "public",
      "documented",
      "section",
      "doesn",
      "support",
      "apis.",
      "caution",
      "entering",
      "characters",
      "fields",
      "calls",
      "invalid",
      "0x0",
      "0x8",
      "0xb",
      "0xc",
      "0xe",
      "0x1f",
      "0xd8",
      "0xdfff",
      "0xfffe",
      "0xffff",
      "all",
      "after",
      "0x110000",
      "connect",
      "authentication",
      "endpoint",
      "single",
      "queries",
      "query",
      "protocol",
      "scenario",
      "import"
    ],
    "language": "en",
    "word_count": 107,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "overview of the rest api",
    "contentLower": "the following api services are available: entity management service (ems). enables you to perform create/read/update operations on records and manage records. see record bulk update and collection apis and rest api queries. analytics. enables you to use reporting services to analyze your record data. manage persons. enables you to manage users and contacts. business intelligence (bi). enables you to integrate service management with a third-party business intelligence system. case exchange. enables you to exchange records with an external system. encryption domain. enables you to manage encryption domains. comments. enables you to post comments to records and access record comments. user lock. enables you to get the lock status of a user and lock or unlock a user. important service management only supports the public apis documented in this section and doesn't support other apis. caution when entering characters in fields in rest calls, the following characters are invalid: 0x0 to 0x8 ",
    "keywordsLower": [
      "overview",
      "rest",
      "api",
      "related",
      "topics",
      "following",
      "services",
      "available",
      "entity",
      "management",
      "service",
      "ems",
      "enables",
      "perform",
      "create",
      "read",
      "update",
      "operations",
      "records",
      "manage",
      "records.",
      "see",
      "record",
      "bulk",
      "collection",
      "apis",
      "queries.",
      "analytics.",
      "reporting",
      "analyze",
      "data.",
      "persons.",
      "users",
      "contacts.",
      "business",
      "intelligence",
      "bi",
      "integrate",
      "third-party",
      "system.",
      "case",
      "exchange.",
      "exchange",
      "external",
      "encryption",
      "domain.",
      "domains.",
      "comments.",
      "post",
      "comments",
      "access",
      "user",
      "lock.",
      "get",
      "lock",
      "status",
      "unlock",
      "user.",
      "important",
      "supports",
      "public",
      "documented",
      "section",
      "doesn",
      "support",
      "apis.",
      "caution",
      "entering",
      "characters",
      "fields",
      "calls",
      "invalid",
      "0x0",
      "0x8",
      "0xb",
      "0xc",
      "0xe",
      "0x1f",
      "0xd8",
      "0xdfff",
      "0xfffe",
      "0xffff",
      "all",
      "after",
      "0x110000",
      "connect",
      "authentication",
      "endpoint",
      "single",
      "queries",
      "query",
      "protocol",
      "scenario",
      "import"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Manage Persons API",
    "content": "The Manage Persons API is designed for the following scenarios: Initialization of users and contacts: Import and synchronize users and contacts from third-party systems, such as LDAP and HR systems Incremental updates: Add or update users and contacts based on data changes in an external system User authorization: Assign users to different groups and roles as required. You can use the Manage Persons API for bulk operations when creating, updating, or deleting users or contacts in your tenant. You can create or update both users and contacts in one API call. If the operation involves a user, the changes will automatically be reflected in Suite Administration. Each bulk request can contain multiple records and relationship types, but it can only perform a single type of operation at a time. We recommend creating or updating multiple records in a single API call. The maximum number of records per bulk request is 100. For example, you can create 50 users simultaneously in one API call. A t",
    "url": "managepersonsapi",
    "filename": "managepersonsapi",
    "headings": [
      "Manage users",
      "Create/Update users",
      "Supported language fields",
      "Convert user to contact",
      "Manage contacts",
      "Create/Update contacts",
      "Delete contacts",
      "Get job status",
      "Related topics"
    ],
    "keywords": [
      "https://<External",
      "example.com",
      "manage",
      "persons",
      "api",
      "users",
      "create",
      "update",
      "supported",
      "language",
      "fields",
      "convert",
      "user",
      "contact",
      "contacts",
      "delete",
      "get",
      "job",
      "status",
      "related",
      "topics",
      "designed",
      "following",
      "scenarios",
      "initialization",
      "import",
      "synchronize",
      "third-party",
      "systems",
      "such",
      "ldap",
      "hr",
      "incremental",
      "updates",
      "add",
      "based",
      "data",
      "changes",
      "external",
      "system",
      "authorization",
      "assign",
      "different",
      "groups",
      "roles",
      "required.",
      "bulk",
      "operations",
      "creating",
      "updating",
      "deleting",
      "tenant.",
      "both",
      "one",
      "call.",
      "operation",
      "involves",
      "automatically",
      "reflected",
      "suite",
      "administration.",
      "request",
      "contain",
      "multiple",
      "records",
      "relationship",
      "types",
      "perform",
      "single",
      "type",
      "time.",
      "recommend",
      "maximum",
      "number",
      "per",
      "100.",
      "example",
      "50",
      "simultaneously",
      "tenant",
      "admin",
      "role",
      "required",
      "access",
      "url.",
      "default",
      "provisioning",
      "200",
      "000.",
      "limit",
      "reached",
      "calling",
      "fail",
      "error.",
      "scheduled",
      "run",
      "daily",
      "older",
      "days.",
      "needed"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "manage persons api",
    "contentLower": "the manage persons api is designed for the following scenarios: initialization of users and contacts: import and synchronize users and contacts from third-party systems, such as ldap and hr systems incremental updates: add or update users and contacts based on data changes in an external system user authorization: assign users to different groups and roles as required. you can use the manage persons api for bulk operations when creating, updating, or deleting users or contacts in your tenant. you can create or update both users and contacts in one api call. if the operation involves a user, the changes will automatically be reflected in suite administration. each bulk request can contain multiple records and relationship types, but it can only perform a single type of operation at a time. we recommend creating or updating multiple records in a single api call. the maximum number of records per bulk request is 100. for example, you can create 50 users simultaneously in one api call. a t",
    "keywordsLower": [
      "https://<external",
      "example.com",
      "manage",
      "persons",
      "api",
      "users",
      "create",
      "update",
      "supported",
      "language",
      "fields",
      "convert",
      "user",
      "contact",
      "contacts",
      "delete",
      "get",
      "job",
      "status",
      "related",
      "topics",
      "designed",
      "following",
      "scenarios",
      "initialization",
      "import",
      "synchronize",
      "third-party",
      "systems",
      "such",
      "ldap",
      "hr",
      "incremental",
      "updates",
      "add",
      "based",
      "data",
      "changes",
      "external",
      "system",
      "authorization",
      "assign",
      "different",
      "groups",
      "roles",
      "required.",
      "bulk",
      "operations",
      "creating",
      "updating",
      "deleting",
      "tenant.",
      "both",
      "one",
      "call.",
      "operation",
      "involves",
      "automatically",
      "reflected",
      "suite",
      "administration.",
      "request",
      "contain",
      "multiple",
      "records",
      "relationship",
      "types",
      "perform",
      "single",
      "type",
      "time.",
      "recommend",
      "maximum",
      "number",
      "per",
      "100.",
      "example",
      "50",
      "simultaneously",
      "tenant",
      "admin",
      "role",
      "required",
      "access",
      "url.",
      "default",
      "provisioning",
      "200",
      "000.",
      "limit",
      "reached",
      "calling",
      "fail",
      "error.",
      "scheduled",
      "run",
      "daily",
      "older",
      "days.",
      "needed"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "OO Content development",
    "content": "This topic provides content developers with guidelines for developing new operations for Operations Orchestration (OO). Operations are used to create new flows to execute inside Operations Orchestration's flow execution engine. To create new content, build an extension and deploy it to OO Central or Workflow Designer. An extension is an artifact that contains actions written in Java,  .NET, or python. Actions are code blocks used to create new operations for OO in Advanced Flow Language (AFL) or CloudSlang language.",
    "url": "contentpackdevelopment",
    "filename": "contentpackdevelopment",
    "headings": [],
    "keywords": [
      "oo",
      "content",
      "development",
      "topic",
      "provides",
      "developers",
      "guidelines",
      "developing",
      "new",
      "operations",
      "orchestration",
      "create",
      "flows",
      "execute",
      "inside",
      "flow",
      "execution",
      "engine.",
      "build",
      "extension",
      "deploy",
      "central",
      "workflow",
      "designer.",
      "artifact",
      "contains",
      "actions",
      "written",
      "java",
      ".net",
      "python.",
      "code",
      "blocks",
      "advanced",
      "language",
      "afl",
      "cloudslang",
      "language."
    ],
    "language": "en",
    "word_count": 57,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "oo content development",
    "contentLower": "this topic provides content developers with guidelines for developing new operations for operations orchestration (oo). operations are used to create new flows to execute inside operations orchestration's flow execution engine. to create new content, build an extension and deploy it to oo central or workflow designer. an extension is an artifact that contains actions written in java,  .net, or python. actions are code blocks used to create new operations for oo in advanced flow language (afl) or cloudslang language.",
    "keywordsLower": [
      "oo",
      "content",
      "development",
      "topic",
      "provides",
      "developers",
      "guidelines",
      "developing",
      "new",
      "operations",
      "orchestration",
      "create",
      "flows",
      "execute",
      "inside",
      "flow",
      "execution",
      "engine.",
      "build",
      "extension",
      "deploy",
      "central",
      "workflow",
      "designer.",
      "artifact",
      "contains",
      "actions",
      "written",
      "java",
      ".net",
      "python.",
      "code",
      "blocks",
      "advanced",
      "language",
      "afl",
      "cloudslang",
      "language."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "Operations",
    "content": "This section contains recommendations for: Designing an operation Reusing out-of-the-box operations Executing against multiple versions of a integration product Using standard outputs across OO Content Using proper responses and providing meaningful error messages Avoid creating multiple operations that run the same command If at all possible, avoid creating multiple operations that run the same command. For example, you can get both packet loss and maximum latency from a Ping operation. It is a best practice to capture both pieces of information in one operation by using multiple outputs of one Ping operation, rather than create multiple operations that use the ping command. Following are the available outputs for a Ping operation: Exceptions to this principle are operations that are extremely generic, such as an operation that runs a WMI command. It is better to create WMI command operations that are specific to particular functions, instead of a single operation that has a very gene",
    "url": "operations",
    "filename": "operations",
    "headings": [
      "Avoid creating multiple operations that run the same command",
      "Soft copies instead of hard copies",
      "JavaScript “Do Nothing” operation",
      "Inputs",
      "Assignment",
      "Handling multiple versions of the same product",
      "Handling versions",
      "Timeouts",
      "Outputs",
      "Responses",
      "Error messages"
    ],
    "keywords": [
      "10.10.10.257",
      "10.10",
      "studio.ui",
      "1.0",
      "10.257",
      "6.0",
      "5.5",
      "6.2",
      "2.0",
      "10.10.10",
      "operations",
      "avoid",
      "creating",
      "multiple",
      "run",
      "same",
      "command",
      "soft",
      "copies",
      "instead",
      "hard",
      "javascript",
      "nothing",
      "operation",
      "inputs",
      "assignment",
      "handling",
      "versions",
      "product",
      "timeouts",
      "outputs",
      "responses",
      "error",
      "messages",
      "section",
      "contains",
      "recommendations",
      "designing",
      "reusing",
      "out-of-the-box",
      "executing",
      "against",
      "integration",
      "standard",
      "across",
      "oo",
      "content",
      "proper",
      "providing",
      "meaningful",
      "all",
      "possible",
      "command.",
      "example",
      "get",
      "both",
      "packet",
      "loss",
      "maximum",
      "latency",
      "ping",
      "operation.",
      "best",
      "practice",
      "capture",
      "pieces",
      "information",
      "one",
      "rather",
      "create",
      "following",
      "available",
      "exceptions",
      "principle",
      "extremely",
      "generic",
      "such",
      "runs",
      "wmi",
      "better",
      "specific",
      "particular",
      "functions",
      "single",
      "very",
      "input",
      "outputs.",
      "search",
      "event",
      "log",
      "active",
      "users",
      "ensures",
      "future",
      "updates",
      "fixes",
      "reflected",
      "flows.",
      "details",
      "vs"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "operations",
    "contentLower": "this section contains recommendations for: designing an operation reusing out-of-the-box operations executing against multiple versions of a integration product using standard outputs across oo content using proper responses and providing meaningful error messages avoid creating multiple operations that run the same command if at all possible, avoid creating multiple operations that run the same command. for example, you can get both packet loss and maximum latency from a ping operation. it is a best practice to capture both pieces of information in one operation by using multiple outputs of one ping operation, rather than create multiple operations that use the ping command. following are the available outputs for a ping operation: exceptions to this principle are operations that are extremely generic, such as an operation that runs a wmi command. it is better to create wmi command operations that are specific to particular functions, instead of a single operation that has a very gene",
    "keywordsLower": [
      "10.10.10.257",
      "10.10",
      "studio.ui",
      "1.0",
      "10.257",
      "6.0",
      "5.5",
      "6.2",
      "2.0",
      "10.10.10",
      "operations",
      "avoid",
      "creating",
      "multiple",
      "run",
      "same",
      "command",
      "soft",
      "copies",
      "instead",
      "hard",
      "javascript",
      "nothing",
      "operation",
      "inputs",
      "assignment",
      "handling",
      "versions",
      "product",
      "timeouts",
      "outputs",
      "responses",
      "error",
      "messages",
      "section",
      "contains",
      "recommendations",
      "designing",
      "reusing",
      "out-of-the-box",
      "executing",
      "against",
      "integration",
      "standard",
      "across",
      "oo",
      "content",
      "proper",
      "providing",
      "meaningful",
      "all",
      "possible",
      "command.",
      "example",
      "get",
      "both",
      "packet",
      "loss",
      "maximum",
      "latency",
      "ping",
      "operation.",
      "best",
      "practice",
      "capture",
      "pieces",
      "information",
      "one",
      "rather",
      "create",
      "following",
      "available",
      "exceptions",
      "principle",
      "extremely",
      "generic",
      "such",
      "runs",
      "wmi",
      "better",
      "specific",
      "particular",
      "functions",
      "single",
      "very",
      "input",
      "outputs.",
      "search",
      "event",
      "log",
      "active",
      "users",
      "ensures",
      "future",
      "updates",
      "fixes",
      "reflected",
      "flows.",
      "details",
      "vs"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  }
]