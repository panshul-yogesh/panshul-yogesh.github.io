[
  {
    "title": "(Optional) Install containerized UD/UCMDB",
    "content": "The Native SACM and SAM solutions require a UD/UCMDB system to work with Service Management. Once you have completed the Service Management installation, you can continue to install UD/UCMDB on the same OMT cluster as the next deployment (Service Management is the first deployment). The following diagram shows the multi-deployment structure.Using Multi-Customer modeUD/UCMDB and Service Management both have the concept of \"tenant,\" but it's important not to mix up the ideas of UCMDB tenants and Service Management tenants. Service Management tenants correspond with UCMDB customers, while Service Management data domains correspond with UCMDB tenants. When you have multiple tenants in Service Management, each of them needs to be associated with a UCMDB customer. Service Management conceptsCorrelated UCMDB conceptsService Management tenantUCMDB customerService Management data domainUCMDB tenantsCustomer in UCMDB is a data isolation mechanism. The Multi-Customer solution enables the manageme",
    "url": "installcmshelm",
    "filename": "installcmshelm",
    "headings": [
      "Using Multi-Customer mode",
      "Category of customers in UCMDB",
      "Default customer",
      "Provider customer",
      "Consumer customer",
      "Using converged authorization solution",
      "Prepare NFS and PostgreSQL servers and add worker nodes",
      "Determine your load balancing option",
      "Option 1",
      "Option 2",
      "Option 3",
      "Configure load balancers",
      "Prerequisites",
      "Option1: Configure Load balancers in a DMZ network",
      "Install the Nginx server",
      "Prepare certificates for each Nginx server",
      "Configure Nginx",
      "Configure nginx-smax",
      "Configure nginx-cms",
      "Configure nginx-integration"
    ],
    "keywords": [
      "uducmdb",
      "443https://smax.example-domain-1.com:443URL",
      "5443https://myhost.example-domain.com:5443Suite",
      "https://management",
      "server.key",
      "2.com",
      "gatewayhttps://cms.example-domain-2.com:443/cms-gatewayTip",
      "1.com",
      "443https://cms.example-domain-2.com:443URL",
      "UCMDBhttps://<nginx-cms",
      "3443https://intl.gtw.mf.com:3443URL",
      "suitehttps://<external",
      "443https://myhost.example-domain.com:443URL",
      "UCMDBhttps://<nginx-integration",
      "UCMDBhttps://<external",
      "suitehttps://<nginx-integration",
      "gatewayhttps://myhost.example-domain.com:3443/cms-gateway",
      "values.yaml",
      "nginx.conf",
      "settings.http",
      "443https://smax.example-domain-1.com:443UD/UCMDB",
      "suitehttps://<nginx-smax",
      "gatewayhttps://intl.gtw.mf.com:3443/cms-gatewayTip",
      "https://cms-service",
      "Gatewayhttps://<nginx-integration",
      "443https://myhost.example-domain.com:443UD/UCMDB",
      "https://suite-service",
      "3443https://myhost.example-domain.com:3443URL",
      "hostname.For",
      "mf.com",
      "domain.com",
      "hostname.Copy",
      "cmshttps://cms.example-domain-2.com:443/cmsIntegration",
      "intl.gtw",
      "server.crt",
      "URLhttps://<external",
      "443https://intl.gtw.mf.com:443URL",
      "smax.For",
      "network.The",
      "balancers.The",
      "host.One",
      "URLhttps://<nginx-cms",
      "customer.In",
      "Gatewayhttps://<external",
      "5443https://smax.example-domain-1.com:5443Suite",
      "server.One",
      "proceed.Port",
      "Gatewayhttps://<nginx-cms",
      "cmshttps://myhost.example-domain.com:3443/cmsIntegration",
      "optional",
      "install",
      "containerized",
      "ud",
      "ucmdb",
      "multi-customer",
      "mode",
      "category",
      "customers",
      "default",
      "customer",
      "provider",
      "consumer",
      "converged",
      "authorization",
      "solution",
      "prepare",
      "nfs",
      "postgresql",
      "servers",
      "add",
      "worker",
      "nodes",
      "determine",
      "load",
      "balancing",
      "option",
      "configure",
      "balancers",
      "prerequisites",
      "option1",
      "dmz",
      "network",
      "nginx",
      "server",
      "certificates",
      "nginx-smax",
      "nginx-cms",
      "nginx-integration",
      "login",
      "urls",
      "integration",
      "external",
      "existing",
      "keepalived",
      "structure",
      "next",
      "deployment",
      "same",
      "omt",
      "related"
    ],
    "language": "en",
    "word_count": 98,
    "importance_score": 6.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) install containerized ud/ucmdb",
    "contentLower": "the native sacm and sam solutions require a ud/ucmdb system to work with service management. once you have completed the service management installation, you can continue to install ud/ucmdb on the same omt cluster as the next deployment (service management is the first deployment). the following diagram shows the multi-deployment structure.using multi-customer modeud/ucmdb and service management both have the concept of \"tenant,\" but it's important not to mix up the ideas of ucmdb tenants and service management tenants. service management tenants correspond with ucmdb customers, while service management data domains correspond with ucmdb tenants. when you have multiple tenants in service management, each of them needs to be associated with a ucmdb customer. service management conceptscorrelated ucmdb conceptsservice management tenantucmdb customerservice management data domainucmdb tenantscustomer in ucmdb is a data isolation mechanism. the multi-customer solution enables the manageme",
    "keywordsLower": [
      "uducmdb",
      "443https://smax.example-domain-1.com:443url",
      "5443https://myhost.example-domain.com:5443suite",
      "https://management",
      "server.key",
      "2.com",
      "gatewayhttps://cms.example-domain-2.com:443/cms-gatewaytip",
      "1.com",
      "443https://cms.example-domain-2.com:443url",
      "ucmdbhttps://<nginx-cms",
      "3443https://intl.gtw.mf.com:3443url",
      "suitehttps://<external",
      "443https://myhost.example-domain.com:443url",
      "ucmdbhttps://<nginx-integration",
      "ucmdbhttps://<external",
      "suitehttps://<nginx-integration",
      "gatewayhttps://myhost.example-domain.com:3443/cms-gateway",
      "values.yaml",
      "nginx.conf",
      "settings.http",
      "443https://smax.example-domain-1.com:443ud/ucmdb",
      "suitehttps://<nginx-smax",
      "gatewayhttps://intl.gtw.mf.com:3443/cms-gatewaytip",
      "https://cms-service",
      "gatewayhttps://<nginx-integration",
      "443https://myhost.example-domain.com:443ud/ucmdb",
      "https://suite-service",
      "3443https://myhost.example-domain.com:3443url",
      "hostname.for",
      "mf.com",
      "domain.com",
      "hostname.copy",
      "cmshttps://cms.example-domain-2.com:443/cmsintegration",
      "intl.gtw",
      "server.crt",
      "urlhttps://<external",
      "443https://intl.gtw.mf.com:443url",
      "smax.for",
      "network.the",
      "balancers.the",
      "host.one",
      "urlhttps://<nginx-cms",
      "customer.in",
      "gatewayhttps://<external",
      "5443https://smax.example-domain-1.com:5443suite",
      "server.one",
      "proceed.port",
      "gatewayhttps://<nginx-cms",
      "cmshttps://myhost.example-domain.com:3443/cmsintegration",
      "optional",
      "install",
      "containerized",
      "ud",
      "ucmdb",
      "multi-customer",
      "mode",
      "category",
      "customers",
      "default",
      "customer",
      "provider",
      "consumer",
      "converged",
      "authorization",
      "solution",
      "prepare",
      "nfs",
      "postgresql",
      "servers",
      "add",
      "worker",
      "nodes",
      "determine",
      "load",
      "balancing",
      "option",
      "configure",
      "balancers",
      "prerequisites",
      "option1",
      "dmz",
      "network",
      "nginx",
      "server",
      "certificates",
      "nginx-smax",
      "nginx-cms",
      "nginx-integration",
      "login",
      "urls",
      "integration",
      "external",
      "existing",
      "keepalived",
      "structure",
      "next",
      "deployment",
      "same",
      "omt",
      "related"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "(Optional) Install the Audit service on-premises",
    "content": "The Audit service installation is optional. The Audit service is an independent service that enables your system to store security audits. Audit is mainly composed of the following components: Audit client: A library embedded with each Audit producer service. Audit service engine: A microservice that you deploy using a Helm installer in its own namespace. In addition to Audit service engine microservice, the Helm installer deploys the Audit gateway microservice. Audit gateway: A microservice that displays audit data on the UI. Audit collector: A microservice installed using Helm in each namespace where Audit producer services are deployed. This microservice is responsible for handling failure scenarios when an Audit producer service is unable to reach the Audit service engine. In this scenario, the audit records are stored temporarily in NFS by the Audit producer service. The job of the Audit collector service is to monitor the audit records in NFS and send them to the Audit service en",
    "url": "installauditservice",
    "filename": "installauditservice",
    "headings": [
      "Install the Audit service on the same OMT"
    ],
    "keywords": [
      "values.yaml",
      "optional",
      "install",
      "audit",
      "service",
      "on-premises",
      "same",
      "omt",
      "installation",
      "optional.",
      "independent",
      "enables",
      "system",
      "store",
      "security",
      "audits.",
      "mainly",
      "composed",
      "following",
      "components",
      "client",
      "library",
      "embedded",
      "producer",
      "service.",
      "engine",
      "microservice",
      "deploy",
      "helm",
      "installer",
      "own",
      "namespace.",
      "addition",
      "deploys",
      "gateway",
      "microservice.",
      "displays",
      "data",
      "ui.",
      "collector",
      "installed",
      "namespace",
      "services",
      "deployed.",
      "responsible",
      "handling",
      "failure",
      "scenarios",
      "unable",
      "reach",
      "engine.",
      "scenario",
      "records",
      "stored",
      "temporarily",
      "nfs",
      "job",
      "monitor",
      "send",
      "reachable.",
      "predefined",
      "event",
      "happens",
      "creates",
      "log",
      "record",
      "database",
      "what",
      "cluster",
      "stores",
      "events",
      "system.",
      "example",
      "management",
      "platform",
      "idm",
      "suite",
      "administration",
      "ucmdb.",
      "sequence",
      "component",
      "follows",
      "configure",
      "services.",
      "namespaces",
      "contain",
      "perform",
      "steps",
      "download",
      "chart",
      "images",
      "upload",
      "container",
      "registry",
      "create",
      "deployment",
      "volumes",
      "prepare",
      "persistent",
      "external"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) install the audit service on-premises",
    "contentLower": "the audit service installation is optional. the audit service is an independent service that enables your system to store security audits. audit is mainly composed of the following components: audit client: a library embedded with each audit producer service. audit service engine: a microservice that you deploy using a helm installer in its own namespace. in addition to audit service engine microservice, the helm installer deploys the audit gateway microservice. audit gateway: a microservice that displays audit data on the ui. audit collector: a microservice installed using helm in each namespace where audit producer services are deployed. this microservice is responsible for handling failure scenarios when an audit producer service is unable to reach the audit service engine. in this scenario, the audit records are stored temporarily in nfs by the audit producer service. the job of the audit collector service is to monitor the audit records in nfs and send them to the audit service en",
    "keywordsLower": [
      "values.yaml",
      "optional",
      "install",
      "audit",
      "service",
      "on-premises",
      "same",
      "omt",
      "installation",
      "optional.",
      "independent",
      "enables",
      "system",
      "store",
      "security",
      "audits.",
      "mainly",
      "composed",
      "following",
      "components",
      "client",
      "library",
      "embedded",
      "producer",
      "service.",
      "engine",
      "microservice",
      "deploy",
      "helm",
      "installer",
      "own",
      "namespace.",
      "addition",
      "deploys",
      "gateway",
      "microservice.",
      "displays",
      "data",
      "ui.",
      "collector",
      "installed",
      "namespace",
      "services",
      "deployed.",
      "responsible",
      "handling",
      "failure",
      "scenarios",
      "unable",
      "reach",
      "engine.",
      "scenario",
      "records",
      "stored",
      "temporarily",
      "nfs",
      "job",
      "monitor",
      "send",
      "reachable.",
      "predefined",
      "event",
      "happens",
      "creates",
      "log",
      "record",
      "database",
      "what",
      "cluster",
      "stores",
      "events",
      "system.",
      "example",
      "management",
      "platform",
      "idm",
      "suite",
      "administration",
      "ucmdb.",
      "sequence",
      "component",
      "follows",
      "configure",
      "services.",
      "namespaces",
      "contain",
      "perform",
      "steps",
      "download",
      "chart",
      "images",
      "upload",
      "container",
      "registry",
      "create",
      "deployment",
      "volumes",
      "prepare",
      "persistent",
      "external"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "(Optional) Install containerized UD/UCMDB on GovCloud (FIPS mode)",
    "content": "Complete these tasks to prepare and install containerized UD/UCMDB on GovCloud with FIPS mode enabled. Sr No. Task Where to perform Role 1 Create EKS cluster worker nodes for UD/UCMDB AWS Management Console IT / Kubernetes Cluster administrator 2 Download UD/UCMDB chart package Bastion node Suite administrator 3 Download and upload container images for UD/UCMDB (FIPS mode) Bastion node Suite administrator 4 Configure EFS for UD/UCMDB AWS Management Console and bastion node Storage administrator 5 Prepare persistent volumes for UD/UCMDB Bastion node Suite administrator 6 Launch RDS for UD/UCMDB (FIPS mode) AWS Management Console Database administrator 7 Create UD/UCMDB integration admin user Suite IdM Suite administrator 8 Create my-values.yaml (FIPS mode) Bastion node Suite administrator 9 Create a new deployment for UD/UCMDB Bastion node Suite administrator 10 Generate vault secrets Bastion node Suite administrator 11 Install UD/UCMDB on AWS Bastion node Suite administrator 12 Verify ",
    "url": "installcmshelmgovcloudfips",
    "filename": "installcmshelmgovcloudfips",
    "headings": [],
    "keywords": [
      "uducmdb",
      "values.yaml",
      "optional",
      "install",
      "containerized",
      "ud",
      "ucmdb",
      "govcloud",
      "fips",
      "mode",
      "complete",
      "tasks",
      "prepare",
      "enabled.",
      "sr",
      "no.",
      "task",
      "perform",
      "role",
      "create",
      "eks",
      "cluster",
      "worker",
      "nodes",
      "aws",
      "management",
      "console",
      "kubernetes",
      "administrator",
      "download",
      "chart",
      "package",
      "bastion",
      "node",
      "suite",
      "upload",
      "container",
      "images",
      "configure",
      "efs",
      "storage",
      "persistent",
      "volumes",
      "launch",
      "rds",
      "database",
      "integration",
      "admin",
      "user",
      "idm",
      "my-values.yaml",
      "new",
      "deployment",
      "10",
      "generate",
      "vault",
      "secrets",
      "11",
      "12",
      "verify",
      "installation",
      "13",
      "load",
      "balancers",
      "cloud",
      "14",
      "application",
      "balancer",
      "15",
      "associate",
      "superadmin"
    ],
    "language": "en",
    "word_count": 144,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) install containerized ud/ucmdb on govcloud (fips mode)",
    "contentLower": "complete these tasks to prepare and install containerized ud/ucmdb on govcloud with fips mode enabled. sr no. task where to perform role 1 create eks cluster worker nodes for ud/ucmdb aws management console it / kubernetes cluster administrator 2 download ud/ucmdb chart package bastion node suite administrator 3 download and upload container images for ud/ucmdb (fips mode) bastion node suite administrator 4 configure efs for ud/ucmdb aws management console and bastion node storage administrator 5 prepare persistent volumes for ud/ucmdb bastion node suite administrator 6 launch rds for ud/ucmdb (fips mode) aws management console database administrator 7 create ud/ucmdb integration admin user suite idm suite administrator 8 create my-values.yaml (fips mode) bastion node suite administrator 9 create a new deployment for ud/ucmdb bastion node suite administrator 10 generate vault secrets bastion node suite administrator 11 install ud/ucmdb on aws bastion node suite administrator 12 verify ",
    "keywordsLower": [
      "uducmdb",
      "values.yaml",
      "optional",
      "install",
      "containerized",
      "ud",
      "ucmdb",
      "govcloud",
      "fips",
      "mode",
      "complete",
      "tasks",
      "prepare",
      "enabled.",
      "sr",
      "no.",
      "task",
      "perform",
      "role",
      "create",
      "eks",
      "cluster",
      "worker",
      "nodes",
      "aws",
      "management",
      "console",
      "kubernetes",
      "administrator",
      "download",
      "chart",
      "package",
      "bastion",
      "node",
      "suite",
      "upload",
      "container",
      "images",
      "configure",
      "efs",
      "storage",
      "persistent",
      "volumes",
      "launch",
      "rds",
      "database",
      "integration",
      "admin",
      "user",
      "idm",
      "my-values.yaml",
      "new",
      "deployment",
      "10",
      "generate",
      "vault",
      "secrets",
      "11",
      "12",
      "verify",
      "installation",
      "13",
      "load",
      "balancers",
      "cloud",
      "14",
      "application",
      "balancer",
      "15",
      "associate",
      "superadmin"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "(Optional) Install containerized UD/UCMDB (EKS)",
    "content": "The Native SACM and SAM solutions require a UD/UCMDB system to work with Service Management. Once you have completed the Service Management installation, you can continue to install UD/UCMDB on the same OMT cluster as the next deployment (Service Management is the first deployment). Additionally, to enable Service Management to work with the containerized UD/UCMDB, you must configure a load balancer between Service Management and UD/UCMDB. Note: This task is optional. If you plan to use classic UD/UCMDB, skip this task and follow the classic UD/UCMDB documentation instead. Step1: Install the containerized UD/UCMDB Perform the following steps to install the containerized UD/UCMDB: Topic Description Plan Review the required resources and create a deployment plan for the installation of UD/UCMDB with EKS: Review support matrix Review sizing considerations Prepare and deploy Prepare the required resources for the installation of UD/UCMDB with EKS: Launch EKS cluster worker nodes for UD/UCM",
    "url": "installcmshelmeks",
    "filename": "installcmshelmeks",
    "headings": [
      "Step1: Install the containerized UD/UCMDB",
      "Step 2: Configure an Application Load Balancer for integration",
      "Step 3: Associate UD/UCMDB SuperAdmin role",
      "Related topics"
    ],
    "keywords": [
      "uducmdb",
      "values.yaml",
      "optional",
      "install",
      "containerized",
      "ud",
      "ucmdb",
      "eks",
      "step1",
      "step",
      "configure",
      "application",
      "load",
      "balancer",
      "integration",
      "associate",
      "superadmin",
      "role",
      "related",
      "topics",
      "native",
      "sacm",
      "sam",
      "solutions",
      "require",
      "system",
      "work",
      "service",
      "management.",
      "once",
      "completed",
      "management",
      "installation",
      "continue",
      "same",
      "omt",
      "cluster",
      "next",
      "deployment",
      "first",
      "additionally",
      "enable",
      "between",
      "ucmdb.",
      "note",
      "task",
      "optional.",
      "plan",
      "classic",
      "skip",
      "follow",
      "documentation",
      "instead.",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "review",
      "required",
      "resources",
      "create",
      "support",
      "matrix",
      "sizing",
      "considerations",
      "prepare",
      "deploy",
      "launch",
      "worker",
      "nodes",
      "download",
      "chart",
      "package",
      "upload",
      "images",
      "efs",
      "persistent",
      "volumes",
      "rds",
      "admin",
      "user",
      "suite",
      "idm",
      "my-values.yaml",
      "new",
      "generate",
      "vault",
      "secrets",
      "aws",
      "verify",
      "balancers",
      "uninstall",
      "installed.",
      "release",
      "separate",
      "instances",
      "together",
      "need",
      "internal"
    ],
    "language": "en",
    "word_count": 113,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) install containerized ud/ucmdb (eks)",
    "contentLower": "the native sacm and sam solutions require a ud/ucmdb system to work with service management. once you have completed the service management installation, you can continue to install ud/ucmdb on the same omt cluster as the next deployment (service management is the first deployment). additionally, to enable service management to work with the containerized ud/ucmdb, you must configure a load balancer between service management and ud/ucmdb. note: this task is optional. if you plan to use classic ud/ucmdb, skip this task and follow the classic ud/ucmdb documentation instead. step1: install the containerized ud/ucmdb perform the following steps to install the containerized ud/ucmdb: topic description plan review the required resources and create a deployment plan for the installation of ud/ucmdb with eks: review support matrix review sizing considerations prepare and deploy prepare the required resources for the installation of ud/ucmdb with eks: launch eks cluster worker nodes for ud/ucm",
    "keywordsLower": [
      "uducmdb",
      "values.yaml",
      "optional",
      "install",
      "containerized",
      "ud",
      "ucmdb",
      "eks",
      "step1",
      "step",
      "configure",
      "application",
      "load",
      "balancer",
      "integration",
      "associate",
      "superadmin",
      "role",
      "related",
      "topics",
      "native",
      "sacm",
      "sam",
      "solutions",
      "require",
      "system",
      "work",
      "service",
      "management.",
      "once",
      "completed",
      "management",
      "installation",
      "continue",
      "same",
      "omt",
      "cluster",
      "next",
      "deployment",
      "first",
      "additionally",
      "enable",
      "between",
      "ucmdb.",
      "note",
      "task",
      "optional.",
      "plan",
      "classic",
      "skip",
      "follow",
      "documentation",
      "instead.",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "review",
      "required",
      "resources",
      "create",
      "support",
      "matrix",
      "sizing",
      "considerations",
      "prepare",
      "deploy",
      "launch",
      "worker",
      "nodes",
      "download",
      "chart",
      "package",
      "upload",
      "images",
      "efs",
      "persistent",
      "volumes",
      "rds",
      "admin",
      "user",
      "suite",
      "idm",
      "my-values.yaml",
      "new",
      "generate",
      "vault",
      "secrets",
      "aws",
      "verify",
      "balancers",
      "uninstall",
      "installed.",
      "release",
      "separate",
      "instances",
      "together",
      "need",
      "internal"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "(Optional) Install the Audit service on EKS",
    "content": "The Audit service installation is optional. The Audit service is an independent service that enables your system to store security audits. Audit is mainly composed of the following components: Audit client: A library embedded with each Audit producer service. Audit service engine: A microservice that you deploy using a Helm installer in its own namespace. In addition to Audit service engine microservice, the Helm installer deploys the Audit gateway microservice. Audit gateway: A microservice that displays audit data on the UI. Audit collector: A microservice installed using Helm in each namespace where Audit producer services are deployed. This microservice is responsible for handling failure scenarios when an Audit producer service is unable to reach the Audit service engine. In this scenario, the audit records are stored temporarily in NFS by the Audit producer service. The job of the Audit collector service is to monitor the audit records in NFS and send them to the Audit service en",
    "url": "installauditserviceeks",
    "filename": "installauditserviceeks",
    "headings": [
      "Install the Audit service on the same OMT as the suite"
    ],
    "keywords": [
      "values.yaml",
      "optional",
      "install",
      "audit",
      "service",
      "eks",
      "same",
      "omt",
      "suite",
      "installation",
      "optional.",
      "independent",
      "enables",
      "system",
      "store",
      "security",
      "audits.",
      "mainly",
      "composed",
      "following",
      "components",
      "client",
      "library",
      "embedded",
      "producer",
      "service.",
      "engine",
      "microservice",
      "deploy",
      "helm",
      "installer",
      "own",
      "namespace.",
      "addition",
      "deploys",
      "gateway",
      "microservice.",
      "displays",
      "data",
      "ui.",
      "collector",
      "installed",
      "namespace",
      "services",
      "deployed.",
      "responsible",
      "handling",
      "failure",
      "scenarios",
      "unable",
      "reach",
      "engine.",
      "scenario",
      "records",
      "stored",
      "temporarily",
      "nfs",
      "job",
      "monitor",
      "send",
      "reachable.",
      "predefined",
      "event",
      "happens",
      "creates",
      "log",
      "record",
      "database",
      "what",
      "cluster",
      "stores",
      "events",
      "system.",
      "example",
      "management",
      "platform",
      "idm",
      "administration",
      "ucmdb.",
      "sequence",
      "component",
      "follows",
      "configure",
      "namespaces",
      "contain",
      "services.",
      "perform",
      "steps",
      "download",
      "chart",
      "upload",
      "images",
      "efs",
      "prepare",
      "persistent",
      "volumes",
      "launch",
      "rds",
      "create",
      "new"
    ],
    "language": "en",
    "word_count": 101,
    "importance_score": 2.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) install the audit service on eks",
    "contentLower": "the audit service installation is optional. the audit service is an independent service that enables your system to store security audits. audit is mainly composed of the following components: audit client: a library embedded with each audit producer service. audit service engine: a microservice that you deploy using a helm installer in its own namespace. in addition to audit service engine microservice, the helm installer deploys the audit gateway microservice. audit gateway: a microservice that displays audit data on the ui. audit collector: a microservice installed using helm in each namespace where audit producer services are deployed. this microservice is responsible for handling failure scenarios when an audit producer service is unable to reach the audit service engine. in this scenario, the audit records are stored temporarily in nfs by the audit producer service. the job of the audit collector service is to monitor the audit records in nfs and send them to the audit service en",
    "keywordsLower": [
      "values.yaml",
      "optional",
      "install",
      "audit",
      "service",
      "eks",
      "same",
      "omt",
      "suite",
      "installation",
      "optional.",
      "independent",
      "enables",
      "system",
      "store",
      "security",
      "audits.",
      "mainly",
      "composed",
      "following",
      "components",
      "client",
      "library",
      "embedded",
      "producer",
      "service.",
      "engine",
      "microservice",
      "deploy",
      "helm",
      "installer",
      "own",
      "namespace.",
      "addition",
      "deploys",
      "gateway",
      "microservice.",
      "displays",
      "data",
      "ui.",
      "collector",
      "installed",
      "namespace",
      "services",
      "deployed.",
      "responsible",
      "handling",
      "failure",
      "scenarios",
      "unable",
      "reach",
      "engine.",
      "scenario",
      "records",
      "stored",
      "temporarily",
      "nfs",
      "job",
      "monitor",
      "send",
      "reachable.",
      "predefined",
      "event",
      "happens",
      "creates",
      "log",
      "record",
      "database",
      "what",
      "cluster",
      "stores",
      "events",
      "system.",
      "example",
      "management",
      "platform",
      "idm",
      "administration",
      "ucmdb.",
      "sequence",
      "component",
      "follows",
      "configure",
      "namespaces",
      "contain",
      "services.",
      "perform",
      "steps",
      "download",
      "chart",
      "upload",
      "images",
      "efs",
      "prepare",
      "persistent",
      "volumes",
      "launch",
      "rds",
      "create",
      "new"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "(Optional) Install containerized UD/UCMDB (AKS)",
    "content": "The Native SACM and SAM solutions require a CMS system to work with the suite. Once you have completed the suite installation, you can continue to install UD/UCMDB on the same OMT cluster as the next deployment (the suite is the first deployment). This task is optional. If you plan to use classic CMS, skip this step and follow the classic CMS documentation instead. Install the containerized CMS Perform the following steps to install the containerized CMS: Topic Description Plan Review the required resources and create a deployment plan for the installation of CMS with AKS: Review support matrix Review sizing considerations Prepare and deploy Prepare the required resources for the installation of CMS with AKS: Download CMS charts package (AKS) Download and upload images for CMS (AKS) Launch AKS cluster worker nodes for CMS Create a new deployment for CMS (AKS) Subscribe premium Azure Files for CMS Prepare external databases for CMS (AKS) Create CMS integration admin user in SMAX IdM (AK",
    "url": "installcmshelmaks",
    "filename": "installcmshelmaks",
    "headings": [
      "Install the containerized CMS",
      "Related topics"
    ],
    "keywords": [
      "uducmdb",
      "values.yaml",
      "optional",
      "install",
      "containerized",
      "ud",
      "ucmdb",
      "aks",
      "cms",
      "related",
      "topics",
      "native",
      "sacm",
      "sam",
      "solutions",
      "require",
      "system",
      "work",
      "suite.",
      "once",
      "completed",
      "suite",
      "installation",
      "continue",
      "same",
      "omt",
      "cluster",
      "next",
      "deployment",
      "first",
      "task",
      "optional.",
      "plan",
      "classic",
      "skip",
      "step",
      "follow",
      "documentation",
      "instead.",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "review",
      "required",
      "resources",
      "create",
      "support",
      "matrix",
      "sizing",
      "considerations",
      "prepare",
      "deploy",
      "download",
      "charts",
      "package",
      "upload",
      "images",
      "launch",
      "worker",
      "nodes",
      "new",
      "subscribe",
      "premium",
      "azure",
      "files",
      "external",
      "databases",
      "integration",
      "admin",
      "user",
      "smax",
      "idm",
      "my-values.yaml",
      "generate",
      "vault",
      "secrets",
      "configure",
      "application",
      "gateway",
      "associate",
      "superadmin",
      "role",
      "uninstall",
      "aws",
      "sma",
      "installed.",
      "release",
      "separate",
      "postgresql",
      "storage",
      "service",
      "instances",
      "cms.",
      "enable"
    ],
    "language": "en",
    "word_count": 117,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) install containerized ud/ucmdb (aks)",
    "contentLower": "the native sacm and sam solutions require a cms system to work with the suite. once you have completed the suite installation, you can continue to install ud/ucmdb on the same omt cluster as the next deployment (the suite is the first deployment). this task is optional. if you plan to use classic cms, skip this step and follow the classic cms documentation instead. install the containerized cms perform the following steps to install the containerized cms: topic description plan review the required resources and create a deployment plan for the installation of cms with aks: review support matrix review sizing considerations prepare and deploy prepare the required resources for the installation of cms with aks: download cms charts package (aks) download and upload images for cms (aks) launch aks cluster worker nodes for cms create a new deployment for cms (aks) subscribe premium azure files for cms prepare external databases for cms (aks) create cms integration admin user in smax idm (ak",
    "keywordsLower": [
      "uducmdb",
      "values.yaml",
      "optional",
      "install",
      "containerized",
      "ud",
      "ucmdb",
      "aks",
      "cms",
      "related",
      "topics",
      "native",
      "sacm",
      "sam",
      "solutions",
      "require",
      "system",
      "work",
      "suite.",
      "once",
      "completed",
      "suite",
      "installation",
      "continue",
      "same",
      "omt",
      "cluster",
      "next",
      "deployment",
      "first",
      "task",
      "optional.",
      "plan",
      "classic",
      "skip",
      "step",
      "follow",
      "documentation",
      "instead.",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "review",
      "required",
      "resources",
      "create",
      "support",
      "matrix",
      "sizing",
      "considerations",
      "prepare",
      "deploy",
      "download",
      "charts",
      "package",
      "upload",
      "images",
      "launch",
      "worker",
      "nodes",
      "new",
      "subscribe",
      "premium",
      "azure",
      "files",
      "external",
      "databases",
      "integration",
      "admin",
      "user",
      "smax",
      "idm",
      "my-values.yaml",
      "generate",
      "vault",
      "secrets",
      "configure",
      "application",
      "gateway",
      "associate",
      "superadmin",
      "role",
      "uninstall",
      "aws",
      "sma",
      "installed.",
      "release",
      "separate",
      "postgresql",
      "storage",
      "service",
      "instances",
      "cms.",
      "enable"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "(Optional) Install containerized UD/UCMDB on GCP (GKE)",
    "content": "The Native SACM and SAM solutions require a UD/UCMDB system to work with the suite. Once you have completed the suite installation, you can continue to install UD/UCMDB on the same K8s cluster as the next deployment (the suite is the first deployment). Additionally, to enable Service Management to work with the containerized UD/UCMDB, you must configure a load balancer between Service Management and UD/UCMDB. This task is optional. If you plan to use classic UD/UCMDB, skip this step and follow the classic UD/UCMDB documentation. Step1: Install the containerized UD/UCMDB Perform the following steps to install the containerized UD/UCMDB: Topic Description Plan Review the required resources and create a deployment plan for the installation of UD/UCMDB on GCP: Review support matrix Review sizing considerations Prepare and deploy Prepare the required resources for the installation of UD/UCMDB on GCP: Set up IP and external access hostname Create a node pool for UD/UCMDB Download installatio",
    "url": "installucmdbgcp",
    "filename": "installucmdbgcp",
    "headings": [
      "Step1: Install the containerized UD/UCMDB",
      "Step 2: Configure an Application Load Balancer for integration"
    ],
    "keywords": [
      "uducmdb",
      "values.yaml",
      "optional",
      "install",
      "containerized",
      "ud",
      "ucmdb",
      "gcp",
      "gke",
      "step1",
      "step",
      "configure",
      "application",
      "load",
      "balancer",
      "integration",
      "native",
      "sacm",
      "sam",
      "solutions",
      "require",
      "system",
      "work",
      "suite.",
      "once",
      "completed",
      "suite",
      "installation",
      "continue",
      "same",
      "k8s",
      "cluster",
      "next",
      "deployment",
      "first",
      "additionally",
      "enable",
      "service",
      "management",
      "between",
      "ucmdb.",
      "task",
      "optional.",
      "plan",
      "classic",
      "skip",
      "follow",
      "documentation.",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "review",
      "required",
      "resources",
      "create",
      "support",
      "matrix",
      "sizing",
      "considerations",
      "prepare",
      "deploy",
      "set",
      "ip",
      "external",
      "access",
      "hostname",
      "node",
      "pool",
      "download",
      "packages",
      "persistent",
      "volumes",
      "subscribe",
      "cloud",
      "filestore",
      "databases",
      "upload",
      "container",
      "images",
      "registry",
      "omt",
      "my-values.yaml",
      "new",
      "generate",
      "vault",
      "secrets",
      "verify",
      "balancers",
      "uninstall",
      "installed.",
      "together",
      "need",
      "internal",
      "ingress",
      "details",
      "see"
    ],
    "language": "en",
    "word_count": 114,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) install containerized ud/ucmdb on gcp (gke)",
    "contentLower": "the native sacm and sam solutions require a ud/ucmdb system to work with the suite. once you have completed the suite installation, you can continue to install ud/ucmdb on the same k8s cluster as the next deployment (the suite is the first deployment). additionally, to enable service management to work with the containerized ud/ucmdb, you must configure a load balancer between service management and ud/ucmdb. this task is optional. if you plan to use classic ud/ucmdb, skip this step and follow the classic ud/ucmdb documentation. step1: install the containerized ud/ucmdb perform the following steps to install the containerized ud/ucmdb: topic description plan review the required resources and create a deployment plan for the installation of ud/ucmdb on gcp: review support matrix review sizing considerations prepare and deploy prepare the required resources for the installation of ud/ucmdb on gcp: set up ip and external access hostname create a node pool for ud/ucmdb download installatio",
    "keywordsLower": [
      "uducmdb",
      "values.yaml",
      "optional",
      "install",
      "containerized",
      "ud",
      "ucmdb",
      "gcp",
      "gke",
      "step1",
      "step",
      "configure",
      "application",
      "load",
      "balancer",
      "integration",
      "native",
      "sacm",
      "sam",
      "solutions",
      "require",
      "system",
      "work",
      "suite.",
      "once",
      "completed",
      "suite",
      "installation",
      "continue",
      "same",
      "k8s",
      "cluster",
      "next",
      "deployment",
      "first",
      "additionally",
      "enable",
      "service",
      "management",
      "between",
      "ucmdb.",
      "task",
      "optional.",
      "plan",
      "classic",
      "skip",
      "follow",
      "documentation.",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "review",
      "required",
      "resources",
      "create",
      "support",
      "matrix",
      "sizing",
      "considerations",
      "prepare",
      "deploy",
      "set",
      "ip",
      "external",
      "access",
      "hostname",
      "node",
      "pool",
      "download",
      "packages",
      "persistent",
      "volumes",
      "subscribe",
      "cloud",
      "filestore",
      "databases",
      "upload",
      "container",
      "images",
      "registry",
      "omt",
      "my-values.yaml",
      "new",
      "generate",
      "vault",
      "secrets",
      "verify",
      "balancers",
      "uninstall",
      "installed.",
      "together",
      "need",
      "internal",
      "ingress",
      "details",
      "see"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "(Optional) Install Audit service on GCP",
    "content": "Perform the following steps to install the Audit service on GCP: Topic Description Plan Review the required resources and create a deployment plan for the installation of Audit service with GCP: Review system requirements Prepare and deploy Prepare the required resources for the installation of Audit service with GCP: Download the Audit Helm chart Download and upload images for Audit service Configure EFS for Audit service Prepare persistent volumes for Audit service Launch RDS for Audit service Create a new deployment for Audit service Configure load balancer for Audit service Create application load balancer for Audit service Configure values.yaml Deploy Audit service on GCP Enable Audit Enable indexing for existing tenants Uninstall Uninstall Audit service on GCP Audit collector Install Audit collector",
    "url": "131-installauditgcp",
    "filename": "131-installauditgcp",
    "headings": [],
    "keywords": [
      "values.yaml",
      "optional",
      "install",
      "audit",
      "service",
      "gcp",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "plan",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "installation",
      "system",
      "requirements",
      "prepare",
      "deploy",
      "download",
      "helm",
      "chart",
      "upload",
      "images",
      "configure",
      "efs",
      "persistent",
      "volumes",
      "launch",
      "rds",
      "new",
      "load",
      "balancer",
      "application",
      "enable",
      "indexing",
      "existing",
      "tenants",
      "uninstall",
      "collector"
    ],
    "language": "en",
    "word_count": 97,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) install audit service on gcp",
    "contentLower": "perform the following steps to install the audit service on gcp: topic description plan review the required resources and create a deployment plan for the installation of audit service with gcp: review system requirements prepare and deploy prepare the required resources for the installation of audit service with gcp: download the audit helm chart download and upload images for audit service configure efs for audit service prepare persistent volumes for audit service launch rds for audit service create a new deployment for audit service configure load balancer for audit service create application load balancer for audit service configure values.yaml deploy audit service on gcp enable audit enable indexing for existing tenants uninstall uninstall audit service on gcp audit collector install audit collector",
    "keywordsLower": [
      "values.yaml",
      "optional",
      "install",
      "audit",
      "service",
      "gcp",
      "perform",
      "following",
      "steps",
      "topic",
      "description",
      "plan",
      "review",
      "required",
      "resources",
      "create",
      "deployment",
      "installation",
      "system",
      "requirements",
      "prepare",
      "deploy",
      "download",
      "helm",
      "chart",
      "upload",
      "images",
      "configure",
      "efs",
      "persistent",
      "volumes",
      "launch",
      "rds",
      "new",
      "load",
      "balancer",
      "application",
      "enable",
      "indexing",
      "existing",
      "tenants",
      "uninstall",
      "collector"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "(Optional) Install Audit collector on GCP",
    "content": "Services integrated with the Audit service store audits on the NFS file system when the Audit service isn't reachable. The Audit collector is a microservice that collects audits from the NFS file system and sends them to the Audit service through REST. You can deploy the Audit collector using Helm charts. Install and configure the Audit collector in each namespace that contains Audit producer services, such as Service Management and UD/UCMDB. The Audit collector requires the Audit producer file system and uses the Audit producer PVC and subpath. This section explains how to install the Audit collector.",
    "url": "131-installauditcollectorhelmgcp",
    "filename": "131-installauditcollectorhelmgcp",
    "headings": [],
    "keywords": [
      "optional",
      "install",
      "audit",
      "collector",
      "gcp",
      "services",
      "integrated",
      "service",
      "store",
      "audits",
      "nfs",
      "file",
      "system",
      "isn",
      "reachable.",
      "microservice",
      "collects",
      "sends",
      "through",
      "rest.",
      "deploy",
      "helm",
      "charts.",
      "configure",
      "namespace",
      "contains",
      "producer",
      "such",
      "management",
      "ud",
      "ucmdb.",
      "requires",
      "uses",
      "pvc",
      "subpath.",
      "section",
      "explains",
      "collector."
    ],
    "language": "en",
    "word_count": 68,
    "importance_score": 1.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) install audit collector on gcp",
    "contentLower": "services integrated with the audit service store audits on the nfs file system when the audit service isn't reachable. the audit collector is a microservice that collects audits from the nfs file system and sends them to the audit service through rest. you can deploy the audit collector using helm charts. install and configure the audit collector in each namespace that contains audit producer services, such as service management and ud/ucmdb. the audit collector requires the audit producer file system and uses the audit producer pvc and subpath. this section explains how to install the audit collector.",
    "keywordsLower": [
      "optional",
      "install",
      "audit",
      "collector",
      "gcp",
      "services",
      "integrated",
      "service",
      "store",
      "audits",
      "nfs",
      "file",
      "system",
      "isn",
      "reachable.",
      "microservice",
      "collects",
      "sends",
      "through",
      "rest.",
      "deploy",
      "helm",
      "charts.",
      "configure",
      "namespace",
      "contains",
      "producer",
      "such",
      "management",
      "ud",
      "ucmdb.",
      "requires",
      "uses",
      "pvc",
      "subpath.",
      "section",
      "explains",
      "collector."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "(Optional) Configure the SAM deployment size after installation",
    "content": "To change the SAM deployment size after you deploy SAM, run the kubectl edit deployment sam-backend-deployment -n <ESM namespace> command to update sam-backend-deployment based on the following table. SAM deployment size Parameters Small SAM_BACKEND_JVM_PERF_OPTION -Xmx4000m -Xms4000m -Xmn1500m CPU limits 3 CPU requests 50 million Memory limits 8 GB Memory requests 2 GB Medium SAM_BACKEND_JVM_PERF_OPTION -Xmx9000m -Xms9000m -Xmn3375m CPU limits 4 CPU requests 50 million Memory limits 15 GB Memory requests 4 GB Large SAM_BACKEND_JVM_PERF_OPTION -Xmx18000m -Xms18000m -Xmn6750m CPU limits 4 CPU requests 50 million Memory limits 24 GB Memory requests 8 GB Related topics Plan the deployment",
    "url": "changesamsizingafterinstallation",
    "filename": "changesamsizingafterinstallation",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "optional",
      "configure",
      "sam",
      "deployment",
      "size",
      "after",
      "installation",
      "related",
      "topics",
      "change",
      "deploy",
      "run",
      "kubectl",
      "edit",
      "sam-backend-deployment",
      "-n",
      "command",
      "update",
      "based",
      "following",
      "table.",
      "parameters",
      "small",
      "-xmx4000m",
      "-xms4000m",
      "-xmn1500m",
      "cpu",
      "limits",
      "requests",
      "50",
      "million",
      "memory",
      "gb",
      "medium",
      "-xmx9000m",
      "-xms9000m",
      "-xmn3375m",
      "15",
      "large",
      "-xmx18000m",
      "-xms18000m",
      "-xmn6750m",
      "24",
      "plan"
    ],
    "language": "en",
    "word_count": 94,
    "importance_score": 1.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) configure the sam deployment size after installation",
    "contentLower": "to change the sam deployment size after you deploy sam, run the kubectl edit deployment sam-backend-deployment -n <esm namespace> command to update sam-backend-deployment based on the following table. sam deployment size parameters small sam_backend_jvm_perf_option -xmx4000m -xms4000m -xmn1500m cpu limits 3 cpu requests 50 million memory limits 8 gb memory requests 2 gb medium sam_backend_jvm_perf_option -xmx9000m -xms9000m -xmn3375m cpu limits 4 cpu requests 50 million memory limits 15 gb memory requests 4 gb large sam_backend_jvm_perf_option -xmx18000m -xms18000m -xmn6750m cpu limits 4 cpu requests 50 million memory limits 24 gb memory requests 8 gb related topics plan the deployment",
    "keywordsLower": [
      "optional",
      "configure",
      "sam",
      "deployment",
      "size",
      "after",
      "installation",
      "related",
      "topics",
      "change",
      "deploy",
      "run",
      "kubectl",
      "edit",
      "sam-backend-deployment",
      "-n",
      "command",
      "update",
      "based",
      "following",
      "table.",
      "parameters",
      "small",
      "-xmx4000m",
      "-xms4000m",
      "-xmn1500m",
      "cpu",
      "limits",
      "requests",
      "50",
      "million",
      "memory",
      "gb",
      "medium",
      "-xmx9000m",
      "-xms9000m",
      "-xmn3375m",
      "15",
      "large",
      "-xmx18000m",
      "-xms18000m",
      "-xmn6750m",
      "24",
      "plan"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "(Optional) Configure data domain segmentation for Native SACM",
    "content": "If you aren't using and don't plan to use the data domain segmentation feature in OpenText Service Management, you can skip the steps in this topic. By skipping this topic, the solution will have an out-of-the-box active data domain called \"Public\" and a UCMDB setup without any tenants assigned to CIs. If you are using data domain segmentation, you must enable UCMDB multi-tenancy as described in (Optional) Enable UCMDB Multi-Tenancy and you must also initially synchronize data domains and UCMDB tenants. By following the steps below, the solution will have an out-of-the-box active data domain called \"Public\" as well as additional configured domains and the UCMDB will have the default tenant and all synchronized Service Management data domains as UCMDB tenants. After enabling Native SACM, the Service Management data domain segmentation, and the UCMDB Tenant feature, follow these steps to synchronize Service Management data domains and UCMDB tenants. Push Service Management data domains t",
    "url": "configdsnativesacm",
    "filename": "configdsnativesacm",
    "headings": [
      "Push Service Management data domains to UCMDB",
      "Create tenant association rules in UCMDB"
    ],
    "keywords": [
      "optional",
      "configure",
      "data",
      "domain",
      "segmentation",
      "native",
      "sacm",
      "push",
      "service",
      "management",
      "domains",
      "ucmdb",
      "create",
      "tenant",
      "association",
      "rules",
      "aren",
      "don",
      "plan",
      "feature",
      "opentext",
      "skip",
      "steps",
      "topic.",
      "skipping",
      "topic",
      "solution",
      "out-of-the-box",
      "active",
      "called",
      "public",
      "setup",
      "any",
      "tenants",
      "assigned",
      "cis.",
      "enable",
      "multi-tenancy",
      "described",
      "initially",
      "synchronize",
      "tenants.",
      "following",
      "below",
      "well",
      "additional",
      "configured",
      "default",
      "all",
      "synchronized",
      "after",
      "enabling",
      "follow",
      "perform",
      "agent",
      "interface.",
      "studio",
      "go",
      "list",
      "page",
      "click",
      "sync.",
      "consistency",
      "check",
      "detects",
      "doesn",
      "exist",
      "load",
      "management.",
      "set",
      "feature.",
      "make",
      "sure",
      "save",
      "created",
      "manually",
      "domains.",
      "save.",
      "re",
      "assign",
      "permissions",
      "persons",
      "groups",
      "roles.",
      "users",
      "classic",
      "ud",
      "containzerized",
      "local",
      "client",
      "customer",
      "newly",
      "discovered",
      "ci",
      "assigned.",
      "value",
      "automatically",
      "corresponding",
      "record",
      "enrichment"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) configure data domain segmentation for native sacm",
    "contentLower": "if you aren't using and don't plan to use the data domain segmentation feature in opentext service management, you can skip the steps in this topic. by skipping this topic, the solution will have an out-of-the-box active data domain called \"public\" and a ucmdb setup without any tenants assigned to cis. if you are using data domain segmentation, you must enable ucmdb multi-tenancy as described in (optional) enable ucmdb multi-tenancy and you must also initially synchronize data domains and ucmdb tenants. by following the steps below, the solution will have an out-of-the-box active data domain called \"public\" as well as additional configured domains and the ucmdb will have the default tenant and all synchronized service management data domains as ucmdb tenants. after enabling native sacm, the service management data domain segmentation, and the ucmdb tenant feature, follow these steps to synchronize service management data domains and ucmdb tenants. push service management data domains t",
    "keywordsLower": [
      "optional",
      "configure",
      "data",
      "domain",
      "segmentation",
      "native",
      "sacm",
      "push",
      "service",
      "management",
      "domains",
      "ucmdb",
      "create",
      "tenant",
      "association",
      "rules",
      "aren",
      "don",
      "plan",
      "feature",
      "opentext",
      "skip",
      "steps",
      "topic.",
      "skipping",
      "topic",
      "solution",
      "out-of-the-box",
      "active",
      "called",
      "public",
      "setup",
      "any",
      "tenants",
      "assigned",
      "cis.",
      "enable",
      "multi-tenancy",
      "described",
      "initially",
      "synchronize",
      "tenants.",
      "following",
      "below",
      "well",
      "additional",
      "configured",
      "default",
      "all",
      "synchronized",
      "after",
      "enabling",
      "follow",
      "perform",
      "agent",
      "interface.",
      "studio",
      "go",
      "list",
      "page",
      "click",
      "sync.",
      "consistency",
      "check",
      "detects",
      "doesn",
      "exist",
      "load",
      "management.",
      "set",
      "feature.",
      "make",
      "sure",
      "save",
      "created",
      "manually",
      "domains.",
      "save.",
      "re",
      "assign",
      "permissions",
      "persons",
      "groups",
      "roles.",
      "users",
      "classic",
      "ud",
      "containzerized",
      "local",
      "client",
      "customer",
      "newly",
      "discovered",
      "ci",
      "assigned.",
      "value",
      "automatically",
      "corresponding",
      "record",
      "enrichment"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "(Optional) Enable UCMDB Multi-Tenancy",
    "content": "If you aren't using and don't plan to use the Service Management data domain segmentation feature, skip the steps in this topic. Please be aware that if you choose to use Native SACM without UCMDB multi-tenancy enabled, once Native SACM is enabled, you won't be able to use the data domain segmentation feature in Service Management. If Service Management data domain segmentation is already in use (you have data domains set up besides the Public domain), you must enable multi-tenancy in UCMDB in order for Native SACM. In this scenario, Native SACM won't work anymore without enabling multi-tenancy in UCMDB. By enabling multi-tenancy in UCMDB, Service Management, and UCMDB are configured to support the data domain segmentation feature. For more information about Data Domain segmentation for Native SACM, see Adopt data domain segmentation for Native SACM. If you have already enabled UCMDB multi-tenancy in UCMDB and you want to change the default tenant to All Tenants for your existing UCMDB",
    "url": "enableucmdbmt",
    "filename": "enableucmdbmt",
    "headings": [
      "Related topics"
    ],
    "keywords": [
      "tenancy.ci",
      "optional",
      "enable",
      "ucmdb",
      "multi-tenancy",
      "related",
      "topics",
      "aren",
      "don",
      "plan",
      "service",
      "management",
      "data",
      "domain",
      "segmentation",
      "feature",
      "skip",
      "steps",
      "topic.",
      "please",
      "aware",
      "choose",
      "native",
      "sacm",
      "enabled",
      "once",
      "won",
      "able",
      "management.",
      "already",
      "domains",
      "set",
      "besides",
      "public",
      "order",
      "sacm.",
      "scenario",
      "work",
      "anymore",
      "enabling",
      "ucmdb.",
      "configured",
      "support",
      "feature.",
      "information",
      "about",
      "see",
      "adopt",
      "want",
      "change",
      "default",
      "tenant",
      "all",
      "tenants",
      "existing",
      "customers",
      "customer.",
      "follow",
      "disable",
      "ci",
      "owner",
      "consumers",
      "mix",
      "up.",
      "open",
      "server",
      "jmx",
      "console",
      "provider",
      "customer",
      "search",
      "setglobalsettingvalue.",
      "enter",
      "multi.tenancy.ci.ownertenant.disabled",
      "name",
      "value",
      "true.",
      "click",
      "invoke.",
      "multi-tenancy.",
      "enabletenant.",
      "enabletenant",
      "section",
      "tenantname",
      "tenants.",
      "create",
      "later",
      "multi-customer",
      "consumer",
      "fall",
      "back",
      "system",
      "name.",
      "limitation",
      "addressed",
      "message",
      "displays",
      "indicating",
      "mode",
      "working"
    ],
    "language": "en",
    "word_count": 106,
    "importance_score": 3.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "(optional) enable ucmdb multi-tenancy",
    "contentLower": "if you aren't using and don't plan to use the service management data domain segmentation feature, skip the steps in this topic. please be aware that if you choose to use native sacm without ucmdb multi-tenancy enabled, once native sacm is enabled, you won't be able to use the data domain segmentation feature in service management. if service management data domain segmentation is already in use (you have data domains set up besides the public domain), you must enable multi-tenancy in ucmdb in order for native sacm. in this scenario, native sacm won't work anymore without enabling multi-tenancy in ucmdb. by enabling multi-tenancy in ucmdb, service management, and ucmdb are configured to support the data domain segmentation feature. for more information about data domain segmentation for native sacm, see adopt data domain segmentation for native sacm. if you have already enabled ucmdb multi-tenancy in ucmdb and you want to change the default tenant to all tenants for your existing ucmdb",
    "keywordsLower": [
      "tenancy.ci",
      "optional",
      "enable",
      "ucmdb",
      "multi-tenancy",
      "related",
      "topics",
      "aren",
      "don",
      "plan",
      "service",
      "management",
      "data",
      "domain",
      "segmentation",
      "feature",
      "skip",
      "steps",
      "topic.",
      "please",
      "aware",
      "choose",
      "native",
      "sacm",
      "enabled",
      "once",
      "won",
      "able",
      "management.",
      "already",
      "domains",
      "set",
      "besides",
      "public",
      "order",
      "sacm.",
      "scenario",
      "work",
      "anymore",
      "enabling",
      "ucmdb.",
      "configured",
      "support",
      "feature.",
      "information",
      "about",
      "see",
      "adopt",
      "want",
      "change",
      "default",
      "tenant",
      "all",
      "tenants",
      "existing",
      "customers",
      "customer.",
      "follow",
      "disable",
      "ci",
      "owner",
      "consumers",
      "mix",
      "up.",
      "open",
      "server",
      "jmx",
      "console",
      "provider",
      "customer",
      "search",
      "setglobalsettingvalue.",
      "enter",
      "multi.tenancy.ci.ownertenant.disabled",
      "name",
      "value",
      "true.",
      "click",
      "invoke.",
      "multi-tenancy.",
      "enabletenant.",
      "enabletenant",
      "section",
      "tenantname",
      "tenants.",
      "create",
      "later",
      "multi-customer",
      "consumer",
      "fall",
      "back",
      "system",
      "name.",
      "limitation",
      "addressed",
      "message",
      "displays",
      "indicating",
      "mode",
      "working"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "\"Create purchase order\" business rule",
    "content": "We provide an out-of-the-box business rule named \"Create purchase order\". If this rule is defined in the fulfill task plan of an offering and someone requests this offering, a purchase order (PO) is then created for the requester. To enable automatic PO creation for an offering, follow these steps: From the main menu, select Plan > Service Catalog > Offerings. Open the offering for which you want to implement automatic PO creation. Navigate to Task plan. Click the Fulfill tab. Open the task plan editor and select Automated Task. In the Business rule section, click Set rule. Click Actions > Create purchase order. Click Save. In the Business rule section, a message \"Created purchase order with delivered location and delivered stock\" is then displayed , as shown below: where: delivered location is mapped to the Delivered to field on the purchase order delivered stock is mapped to the Delivered Stock ID field on the purchase order You can specify the delivered location and delivered stock ",
    "url": "pobizrules",
    "filename": "pobizrules",
    "headings": [],
    "keywords": [
      "create",
      "purchase",
      "order",
      "business",
      "rule",
      "provide",
      "out-of-the-box",
      "named",
      "defined",
      "fulfill",
      "task",
      "plan",
      "offering",
      "someone",
      "requests",
      "po",
      "created",
      "requester.",
      "enable",
      "automatic",
      "creation",
      "follow",
      "steps",
      "main",
      "menu",
      "select",
      "service",
      "catalog",
      "offerings.",
      "open",
      "want",
      "implement",
      "creation.",
      "navigate",
      "plan.",
      "click",
      "tab.",
      "editor",
      "automated",
      "task.",
      "section",
      "set",
      "rule.",
      "actions",
      "order.",
      "save.",
      "message",
      "delivered",
      "location",
      "stock",
      "displayed",
      "shown",
      "below",
      "mapped",
      "field",
      "id",
      "specify",
      "expressions.",
      "don",
      "system",
      "default",
      "vary",
      "depending",
      "actual",
      "scenario",
      "empty.",
      "first",
      "non-empty",
      "values",
      "following",
      "deliver",
      "request",
      "record",
      "people",
      "requested",
      "neither",
      "entity",
      "above",
      "room",
      "empty",
      "value.",
      "save",
      "again.",
      "generate",
      "line",
      "populate",
      "vendor",
      "item",
      "fields",
      "line.",
      "asset",
      "model",
      "same",
      "request.",
      "vendor.",
      "preferred",
      "generated"
    ],
    "language": "en",
    "word_count": 109,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "\"create purchase order\" business rule",
    "contentLower": "we provide an out-of-the-box business rule named \"create purchase order\". if this rule is defined in the fulfill task plan of an offering and someone requests this offering, a purchase order (po) is then created for the requester. to enable automatic po creation for an offering, follow these steps: from the main menu, select plan > service catalog > offerings. open the offering for which you want to implement automatic po creation. navigate to task plan. click the fulfill tab. open the task plan editor and select automated task. in the business rule section, click set rule. click actions > create purchase order. click save. in the business rule section, a message \"created purchase order with delivered location and delivered stock\" is then displayed , as shown below: where: delivered location is mapped to the delivered to field on the purchase order delivered stock is mapped to the delivered stock id field on the purchase order you can specify the delivered location and delivered stock ",
    "keywordsLower": [
      "create",
      "purchase",
      "order",
      "business",
      "rule",
      "provide",
      "out-of-the-box",
      "named",
      "defined",
      "fulfill",
      "task",
      "plan",
      "offering",
      "someone",
      "requests",
      "po",
      "created",
      "requester.",
      "enable",
      "automatic",
      "creation",
      "follow",
      "steps",
      "main",
      "menu",
      "select",
      "service",
      "catalog",
      "offerings.",
      "open",
      "want",
      "implement",
      "creation.",
      "navigate",
      "plan.",
      "click",
      "tab.",
      "editor",
      "automated",
      "task.",
      "section",
      "set",
      "rule.",
      "actions",
      "order.",
      "save.",
      "message",
      "delivered",
      "location",
      "stock",
      "displayed",
      "shown",
      "below",
      "mapped",
      "field",
      "id",
      "specify",
      "expressions.",
      "don",
      "system",
      "default",
      "vary",
      "depending",
      "actual",
      "scenario",
      "empty.",
      "first",
      "non-empty",
      "values",
      "following",
      "deliver",
      "request",
      "record",
      "people",
      "requested",
      "neither",
      "entity",
      "above",
      "room",
      "empty",
      "value.",
      "save",
      "again.",
      "generate",
      "line",
      "populate",
      "vendor",
      "item",
      "fields",
      "line.",
      "asset",
      "model",
      "same",
      "request.",
      "vendor.",
      "preferred",
      "generated"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "\"NodeInstanceRole is not authorized to perform\" error during the installation on AWS",
    "content": "You fail to provision the Application Load Balancer by using the AWS Load Balancer Controller. When running the kubectl logs aws-load-balancer-controller-xxx -n kube-system command, you see the following error message: {\"level\":\"error\",\"ts\":xx.xx,\"logger\":\"controller\",\"msg\":\"Reconciler error\",\"controller\":\"ingress\",\"name\":\"meic-loadbalancer\", \"namespace\":\"\",\"error\":\":sts::xx:assumed-role/meic-eks-NodeInstanceRole-xx/i-xx is not authorized to perform: elasticloadbalancing:DescribeTargetGroupsst id: xx\"} Additionally, when you run the kubectl get sa aws-load-balancer-controller -n kube-system -o yaml command, the annotation section is not included in the results. The annotation section looks as below: eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/AmazonEKSLoadBalancerControllerRole Cause You didn't delete the ServiceAccount section in the YAML file as requested. Solution Create an IAM role and annotate the Kubernetes service account that's named aws-load-balancer-controller ",
    "url": "nodeinstancerolenotauthorized",
    "filename": "nodeinstancerolenotauthorized",
    "headings": [
      "Cause",
      "Solution",
      "Related topics"
    ],
    "keywords": [
      "controller.html",
      "amazon.com",
      "https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html",
      "amazonaws.com",
      "xx.xx",
      "docs.aws",
      "nodeinstancerole",
      "authorized",
      "perform",
      "error",
      "during",
      "installation",
      "aws",
      "cause",
      "solution",
      "related",
      "topics",
      "fail",
      "provision",
      "application",
      "load",
      "balancer",
      "controller.",
      "running",
      "kubectl",
      "logs",
      "aws-load-balancer-controller-xxx",
      "-n",
      "kube-system",
      "command",
      "see",
      "following",
      "message",
      "level",
      "ts",
      "logger",
      "controller",
      "msg",
      "reconciler",
      "ingress",
      "name",
      "meic-loadbalancer",
      "namespace",
      "sts",
      "xx",
      "assumed-role",
      "meic-eks-nodeinstancerole-xx",
      "i-xx",
      "elasticloadbalancing",
      "describetargetgroupsst",
      "id",
      "additionally",
      "run",
      "get",
      "sa",
      "aws-load-balancer-controller",
      "-o",
      "yaml",
      "annotation",
      "section",
      "included",
      "results.",
      "looks",
      "below",
      "eks.amazonaws.com",
      "role-arn",
      "arn",
      "iam",
      "111122223333",
      "role",
      "amazoneksloadbalancercontrollerrole",
      "didn",
      "delete",
      "serviceaccount",
      "file",
      "requested.",
      "create",
      "annotate",
      "kubernetes",
      "service",
      "account",
      "named",
      "eksctl",
      "management",
      "console",
      "document",
      "https",
      "docs.aws.amazon.com",
      "eks",
      "latest",
      "userguide",
      "aws-load-balancer-controller.html.",
      "deploy",
      "bootstrap"
    ],
    "language": "en",
    "word_count": 100,
    "importance_score": 1.5,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "\"nodeinstancerole is not authorized to perform\" error during the installation on aws",
    "contentLower": "you fail to provision the application load balancer by using the aws load balancer controller. when running the kubectl logs aws-load-balancer-controller-xxx -n kube-system command, you see the following error message: {\"level\":\"error\",\"ts\":xx.xx,\"logger\":\"controller\",\"msg\":\"reconciler error\",\"controller\":\"ingress\",\"name\":\"meic-loadbalancer\", \"namespace\":\"\",\"error\":\":sts::xx:assumed-role/meic-eks-nodeinstancerole-xx/i-xx is not authorized to perform: elasticloadbalancing:describetargetgroupsst id: xx\"} additionally, when you run the kubectl get sa aws-load-balancer-controller -n kube-system -o yaml command, the annotation section is not included in the results. the annotation section looks as below: eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/amazoneksloadbalancercontrollerrole cause you didn't delete the serviceaccount section in the yaml file as requested. solution create an iam role and annotate the kubernetes service account that's named aws-load-balancer-controller ",
    "keywordsLower": [
      "controller.html",
      "amazon.com",
      "https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html",
      "amazonaws.com",
      "xx.xx",
      "docs.aws",
      "nodeinstancerole",
      "authorized",
      "perform",
      "error",
      "during",
      "installation",
      "aws",
      "cause",
      "solution",
      "related",
      "topics",
      "fail",
      "provision",
      "application",
      "load",
      "balancer",
      "controller.",
      "running",
      "kubectl",
      "logs",
      "aws-load-balancer-controller-xxx",
      "-n",
      "kube-system",
      "command",
      "see",
      "following",
      "message",
      "level",
      "ts",
      "logger",
      "controller",
      "msg",
      "reconciler",
      "ingress",
      "name",
      "meic-loadbalancer",
      "namespace",
      "sts",
      "xx",
      "assumed-role",
      "meic-eks-nodeinstancerole-xx",
      "i-xx",
      "elasticloadbalancing",
      "describetargetgroupsst",
      "id",
      "additionally",
      "run",
      "get",
      "sa",
      "aws-load-balancer-controller",
      "-o",
      "yaml",
      "annotation",
      "section",
      "included",
      "results.",
      "looks",
      "below",
      "eks.amazonaws.com",
      "role-arn",
      "arn",
      "iam",
      "111122223333",
      "role",
      "amazoneksloadbalancercontrollerrole",
      "didn",
      "delete",
      "serviceaccount",
      "file",
      "requested.",
      "create",
      "annotate",
      "kubernetes",
      "service",
      "account",
      "named",
      "eksctl",
      "management",
      "console",
      "document",
      "https",
      "docs.aws.amazon.com",
      "eks",
      "latest",
      "userguide",
      "aws-load-balancer-controller.html.",
      "deploy",
      "bootstrap"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "\"Device is busy\" or \"waiting for IO/CPU to become free\" error messages during upgrade",
    "content": "During OMT upgrade, you receive some error message that resemble one the following error messages: Error:Device is busy Error: waiting for IO/CPU to become free The OMT upgrade process isn't suspended. Cause This issue occurs because some errors occurs in the Linux kernel. Solution You can safely ignore these error messages and continue the upgrade process.",
    "url": "deviceisbusyerror",
    "filename": "deviceisbusyerror",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "iocpu",
      "device",
      "busy",
      "waiting",
      "io",
      "cpu",
      "become",
      "free",
      "error",
      "messages",
      "during",
      "upgrade",
      "cause",
      "solution",
      "omt",
      "receive",
      "message",
      "resemble",
      "one",
      "following",
      "process",
      "isn",
      "suspended.",
      "issue",
      "occurs",
      "because",
      "errors",
      "linux",
      "kernel.",
      "safely",
      "ignore",
      "continue",
      "process."
    ],
    "language": "en",
    "word_count": 53,
    "importance_score": 1.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "\"device is busy\" or \"waiting for io/cpu to become free\" error messages during upgrade",
    "contentLower": "during omt upgrade, you receive some error message that resemble one the following error messages: error:device is busy error: waiting for io/cpu to become free the omt upgrade process isn't suspended. cause this issue occurs because some errors occurs in the linux kernel. solution you can safely ignore these error messages and continue the upgrade process.",
    "keywordsLower": [
      "iocpu",
      "device",
      "busy",
      "waiting",
      "io",
      "cpu",
      "become",
      "free",
      "error",
      "messages",
      "during",
      "upgrade",
      "cause",
      "solution",
      "omt",
      "receive",
      "message",
      "resemble",
      "one",
      "following",
      "process",
      "isn",
      "suspended.",
      "issue",
      "occurs",
      "because",
      "errors",
      "linux",
      "kernel.",
      "safely",
      "ignore",
      "continue",
      "process."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "\"Failed to find CDF_HOME\" error",
    "content": "When you try to upgrade OMT or the OMT tools in an external Kubernetes environment, the upgrade fails with an error message that resembles the following: Failed to find CDF_HOME. Please make sure CDF_HOME is in <current_user_home>/itom-cdf.sh Cause This issue occurs if you change the bastion node but don't restore the OMT environment values. Solution Make sure that you are logged in to the bastion node with the correct user who has permission to connect the Kubernetes cluster. Check whether the ${HOME}/itom-cdf.sh file exists. If it doesn't, run the following commands to create it: touch ${HOME}/itom-cdf.sh chmod 644 ${HOME}/itom-cdf.sh Run the following commands to restore the CDF_HOME variable to the ${HOME}/itom-cdf.sh file: CDF_HOME=${HOME}/cdf echo \"export CDF_HOME=${CDF_HOME}\" >> ${HOME}/itom-cdf.sh If the directory $CDF_HOME doesn't exist, run the following command to create it: mkdir $CDF_HOME If you installed OMT on the Kubernetes cluster, you must also restore the CDF_NAMESPA",
    "url": "failedtofindcdfhome",
    "filename": "failedtofindcdfhome",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "cdf_home",
      "cdf.sh",
      "failed",
      "find",
      "error",
      "cause",
      "solution",
      "try",
      "upgrade",
      "omt",
      "tools",
      "external",
      "kubernetes",
      "environment",
      "fails",
      "message",
      "resembles",
      "following",
      "please",
      "make",
      "sure",
      "itom-cdf.sh",
      "issue",
      "occurs",
      "change",
      "bastion",
      "node",
      "don",
      "restore",
      "values.",
      "logged",
      "correct",
      "user",
      "permission",
      "connect",
      "cluster.",
      "check",
      "whether",
      "home",
      "file",
      "exists.",
      "doesn",
      "run",
      "commands",
      "create",
      "touch",
      "chmod",
      "644",
      "variable",
      "cdf",
      "echo",
      "export",
      "directory",
      "exist",
      "command",
      "mkdir",
      "installed",
      "cluster",
      "set",
      "namespace",
      "capability",
      "ignore",
      "step"
    ],
    "language": "en",
    "word_count": 104,
    "importance_score": 4.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "\"failed to find cdf_home\" error",
    "contentLower": "when you try to upgrade omt or the omt tools in an external kubernetes environment, the upgrade fails with an error message that resembles the following: failed to find cdf_home. please make sure cdf_home is in <current_user_home>/itom-cdf.sh cause this issue occurs if you change the bastion node but don't restore the omt environment values. solution make sure that you are logged in to the bastion node with the correct user who has permission to connect the kubernetes cluster. check whether the ${home}/itom-cdf.sh file exists. if it doesn't, run the following commands to create it: touch ${home}/itom-cdf.sh chmod 644 ${home}/itom-cdf.sh run the following commands to restore the cdf_home variable to the ${home}/itom-cdf.sh file: cdf_home=${home}/cdf echo \"export cdf_home=${cdf_home}\" >> ${home}/itom-cdf.sh if the directory $cdf_home doesn't exist, run the following command to create it: mkdir $cdf_home if you installed omt on the kubernetes cluster, you must also restore the cdf_namespa",
    "keywordsLower": [
      "cdf_home",
      "cdf.sh",
      "failed",
      "find",
      "error",
      "cause",
      "solution",
      "try",
      "upgrade",
      "omt",
      "tools",
      "external",
      "kubernetes",
      "environment",
      "fails",
      "message",
      "resembles",
      "following",
      "please",
      "make",
      "sure",
      "itom-cdf.sh",
      "issue",
      "occurs",
      "change",
      "bastion",
      "node",
      "don",
      "restore",
      "values.",
      "logged",
      "correct",
      "user",
      "permission",
      "connect",
      "cluster.",
      "check",
      "whether",
      "home",
      "file",
      "exists.",
      "doesn",
      "run",
      "commands",
      "create",
      "touch",
      "chmod",
      "644",
      "variable",
      "cdf",
      "echo",
      "export",
      "directory",
      "exist",
      "command",
      "mkdir",
      "installed",
      "cluster",
      "set",
      "namespace",
      "capability",
      "ignore",
      "step"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "\"UPGRADE FAILED\" error after updating certificates from OMT Management Portal",
    "content": "After uploading new certificates from the Management Portal, you might get the following error: Error: UPGRADE FAILED: template: apphub/charts/portalIngress/templates/secret.yaml:4:24: executing \"apphub/charts/portalIngress/templates/secret.yaml\" at <index $secret.metadata.annotations \"deployments.microfocus.com/ingore-tls-cert\">: error calling index: index of untyped nil Cause Some values in nginx-default-secret are missing. Solution After uploading certificates from the Management Portal, follow these steps to run the commands on one of the control plane nodes or worker nodes. Run the following command to get the <release-name>: helm list -n $CDF_NAMESPACE -a 2>/dev/null | grep -E 'apphub-[0-9]+\\.[0-9]+\\.[0-9]\\+[0-9]+' | awk '{print $1}' | xargs Run the following command to patch nginx-default-secret and replace <release-name> with the result you get from step 1: kubectl patch secret nginx-default-secret -n $CDF_NAMESPACE -p \"{ \\\"metadata\\\": { \\\"annotations\\\": { \\\"meta.helm.sh/releas",
    "url": "upgradefailed",
    "filename": "upgradefailed",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "kubernetes.io",
      "meta.helm",
      "microfocus.com",
      "secret.yaml",
      "upgrade",
      "failed",
      "error",
      "after",
      "updating",
      "certificates",
      "omt",
      "management",
      "portal",
      "cause",
      "solution",
      "uploading",
      "new",
      "get",
      "following",
      "template",
      "apphub",
      "charts",
      "portalingress",
      "templates",
      "24",
      "executing",
      "calling",
      "index",
      "untyped",
      "nil",
      "values",
      "nginx-default-secret",
      "missing.",
      "follow",
      "steps",
      "run",
      "commands",
      "one",
      "control",
      "plane",
      "nodes",
      "worker",
      "nodes.",
      "command",
      "helm",
      "list",
      "-n",
      "-a",
      "dev",
      "null",
      "grep",
      "-e",
      "apphub-",
      "0-9",
      "awk",
      "print",
      "xargs",
      "patch",
      "replace",
      "result",
      "step",
      "kubectl",
      "secret",
      "-p",
      "metadata",
      "annotations",
      "meta.helm.sh",
      "release-name",
      "release-namespace",
      "labels",
      "app.kubernetes.io",
      "managed-by",
      "nginx-frontend-secret"
    ],
    "language": "en",
    "word_count": 105,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "\"upgrade failed\" error after updating certificates from omt management portal",
    "contentLower": "after uploading new certificates from the management portal, you might get the following error: error: upgrade failed: template: apphub/charts/portalingress/templates/secret.yaml:4:24: executing \"apphub/charts/portalingress/templates/secret.yaml\" at <index $secret.metadata.annotations \"deployments.microfocus.com/ingore-tls-cert\">: error calling index: index of untyped nil cause some values in nginx-default-secret are missing. solution after uploading certificates from the management portal, follow these steps to run the commands on one of the control plane nodes or worker nodes. run the following command to get the <release-name>: helm list -n $cdf_namespace -a 2>/dev/null | grep -e 'apphub-[0-9]+\\.[0-9]+\\.[0-9]\\+[0-9]+' | awk '{print $1}' | xargs run the following command to patch nginx-default-secret and replace <release-name> with the result you get from step 1: kubectl patch secret nginx-default-secret -n $cdf_namespace -p \"{ \\\"metadata\\\": { \\\"annotations\\\": { \\\"meta.helm.sh/releas",
    "keywordsLower": [
      "kubernetes.io",
      "meta.helm",
      "microfocus.com",
      "secret.yaml",
      "upgrade",
      "failed",
      "error",
      "after",
      "updating",
      "certificates",
      "omt",
      "management",
      "portal",
      "cause",
      "solution",
      "uploading",
      "new",
      "get",
      "following",
      "template",
      "apphub",
      "charts",
      "portalingress",
      "templates",
      "24",
      "executing",
      "calling",
      "index",
      "untyped",
      "nil",
      "values",
      "nginx-default-secret",
      "missing.",
      "follow",
      "steps",
      "run",
      "commands",
      "one",
      "control",
      "plane",
      "nodes",
      "worker",
      "nodes.",
      "command",
      "helm",
      "list",
      "-n",
      "-a",
      "dev",
      "null",
      "grep",
      "-e",
      "apphub-",
      "0-9",
      "awk",
      "print",
      "xargs",
      "patch",
      "replace",
      "result",
      "step",
      "kubectl",
      "secret",
      "-p",
      "metadata",
      "annotations",
      "meta.helm.sh",
      "release-name",
      "release-namespace",
      "labels",
      "app.kubernetes.io",
      "managed-by",
      "nginx-frontend-secret"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "\"Waiting for upgrade finished timeout\" error during OMT upgrade",
    "content": "OMT upgrade fails with a Waiting for upgrade finished timeout error message. Cause This issue occurs when an exception is thrown when a suite is uninstalled. In this situation, the status of the deployment isn't updated. Therefore, when you try to create a new deployment, more than one deployment exists in the system. Solution Remove the references to the old deployment. To do this, follow these steps: Run the following command to identify the cdfapiserver-db database pod: # kubectl get pods -n $CDF_NAMESPACE The output resembles the following. Make a note of the cdfapiserver-db database pod name. NAME READY STATUS RESTARTS AGE ……. cdfapiserver-postgresql-75ccd5dc97-jmsbs 2/2 Running 9 3d14h …….. Run the following command to log in to the cdfapiserver-db database pod: kubectl exec -it -n $CDF_NAMESPACE <cdfapiserver-db name> bash For example: kubectl exec -it -n $CDF_NAMESPACE cdfapiserver-postgresql-75ccd5dc97-jmsbs bash Run the following command to get the cdfapiserver-db user passwo",
    "url": "waitingupgradetimeout",
    "filename": "waitingupgradetimeout",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "49.167",
      "07.553",
      "waiting",
      "upgrade",
      "finished",
      "timeout",
      "error",
      "during",
      "omt",
      "cause",
      "solution",
      "fails",
      "message.",
      "issue",
      "occurs",
      "exception",
      "thrown",
      "suite",
      "uninstalled.",
      "situation",
      "status",
      "deployment",
      "isn",
      "updated.",
      "therefore",
      "try",
      "create",
      "new",
      "one",
      "exists",
      "system.",
      "remove",
      "references",
      "old",
      "deployment.",
      "follow",
      "steps",
      "run",
      "following",
      "command",
      "identify",
      "cdfapiserver-db",
      "database",
      "pod",
      "kubectl",
      "get",
      "pods",
      "-n",
      "output",
      "resembles",
      "following.",
      "make",
      "note",
      "name.",
      "name",
      "ready",
      "restarts",
      "age",
      "cdfapiserver-postgresql-75ccd5dc97-jmsbs",
      "running",
      "3d14h",
      "log",
      "exec",
      "-it",
      "bash",
      "example",
      "user",
      "password",
      "password.",
      "pass",
      "d22j6hafyh",
      "2uw",
      "psql",
      "-u",
      "cdfapiserver",
      "-d",
      "cdfapiserverdb",
      "-h",
      "localhost",
      "enter",
      "noted",
      "earlier.",
      "list",
      "deployments",
      "select",
      "resemble",
      "id",
      "updates",
      "uuid",
      "5444",
      "5443",
      "2020-01-09",
      "07",
      "58",
      "itsma-favgh",
      "primary",
      "itsma",
      "7793f630-60b0-4389-96e3-c7ff48551906",
      "itsma-qnjg6",
      "938cfd2c-be67-4ccf-9186-c8e19f3ff607"
    ],
    "language": "en",
    "word_count": 107,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "\"waiting for upgrade finished timeout\" error during omt upgrade",
    "contentLower": "omt upgrade fails with a waiting for upgrade finished timeout error message. cause this issue occurs when an exception is thrown when a suite is uninstalled. in this situation, the status of the deployment isn't updated. therefore, when you try to create a new deployment, more than one deployment exists in the system. solution remove the references to the old deployment. to do this, follow these steps: run the following command to identify the cdfapiserver-db database pod: # kubectl get pods -n $cdf_namespace the output resembles the following. make a note of the cdfapiserver-db database pod name. name ready status restarts age ……. cdfapiserver-postgresql-75ccd5dc97-jmsbs 2/2 running 9 3d14h …….. run the following command to log in to the cdfapiserver-db database pod: kubectl exec -it -n $cdf_namespace <cdfapiserver-db name> bash for example: kubectl exec -it -n $cdf_namespace cdfapiserver-postgresql-75ccd5dc97-jmsbs bash run the following command to get the cdfapiserver-db user passwo",
    "keywordsLower": [
      "49.167",
      "07.553",
      "waiting",
      "upgrade",
      "finished",
      "timeout",
      "error",
      "during",
      "omt",
      "cause",
      "solution",
      "fails",
      "message.",
      "issue",
      "occurs",
      "exception",
      "thrown",
      "suite",
      "uninstalled.",
      "situation",
      "status",
      "deployment",
      "isn",
      "updated.",
      "therefore",
      "try",
      "create",
      "new",
      "one",
      "exists",
      "system.",
      "remove",
      "references",
      "old",
      "deployment.",
      "follow",
      "steps",
      "run",
      "following",
      "command",
      "identify",
      "cdfapiserver-db",
      "database",
      "pod",
      "kubectl",
      "get",
      "pods",
      "-n",
      "output",
      "resembles",
      "following.",
      "make",
      "note",
      "name.",
      "name",
      "ready",
      "restarts",
      "age",
      "cdfapiserver-postgresql-75ccd5dc97-jmsbs",
      "running",
      "3d14h",
      "log",
      "exec",
      "-it",
      "bash",
      "example",
      "user",
      "password",
      "password.",
      "pass",
      "d22j6hafyh",
      "2uw",
      "psql",
      "-u",
      "cdfapiserver",
      "-d",
      "cdfapiserverdb",
      "-h",
      "localhost",
      "enter",
      "noted",
      "earlier.",
      "list",
      "deployments",
      "select",
      "resemble",
      "id",
      "updates",
      "uuid",
      "5444",
      "5443",
      "2020-01-09",
      "07",
      "58",
      "itsma-favgh",
      "primary",
      "itsma",
      "7793f630-60b0-4389-96e3-c7ff48551906",
      "itsma-qnjg6",
      "938cfd2c-be67-4ccf-9186-c8e19f3ff607"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "\"Warning FailedCreatePodSandBox\" message and pods do not start during upgrade",
    "content": "When you run the upgrade -u command to perform a manual upgrade on a node that has many network adapters, the pods on the node don't start. If you run the kubectl describe pod <pod_name> -n <namespace> command to check the pod status when this occurs, you see warning messages that resemble the following: Warning FailedCreatePodSandBox 2m2s (x2438 over 91m) kubelet (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container \"c4b3b01869bbd664efc6c5b8e4f9a6b1a0fa2e2838fe947b07ccfb023316096c\" network for pod \"itom-cdf-upgrade-deployer-202105-9qvwn\": networkPlugin cni failed to set up pod \"itom-cdf-up grade-deployer-202105-9qvwn_core\" network: failed to set bridge addr: \"cni0\" already has an IP address different from 172.16.60.1/24, failed to clean up sandbox container \"c4b3b01869bbd66 4efc6c5b8e4f9a6b1a0fa2e2838fe947b07ccfb023316096c\" network for pod \"itom-cdf-upgrade-deployer-202105-9qvwn\": networkPlugin cni failed to ",
    "url": "failedcreatepodsandbox",
    "filename": "failedcreatepodsandbox",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "172.16.60",
      "678.910",
      "v1.4.21",
      "172.16.60.171",
      "678.911",
      "12.345.678.911",
      "12.345.678",
      "4.21",
      "12.345",
      "172.16",
      "12.345.678.910",
      "172.16.60.1",
      "mfswlab.net",
      "cfgKubeViaMulNetAdapters.sh",
      "upgrade.sh",
      "60.171",
      "warning",
      "failedcreatepodsandbox",
      "message",
      "pods",
      "start",
      "during",
      "upgrade",
      "cause",
      "solution",
      "run",
      "-u",
      "command",
      "perform",
      "manual",
      "node",
      "many",
      "network",
      "adapters",
      "don",
      "start.",
      "kubectl",
      "describe",
      "pod",
      "-n",
      "check",
      "status",
      "occurs",
      "see",
      "messages",
      "resemble",
      "following",
      "2m2s",
      "x2438",
      "over",
      "91m",
      "kubelet",
      "combined",
      "similar",
      "events",
      "failed",
      "create",
      "sandbox",
      "rpc",
      "error",
      "code",
      "unknown",
      "desc",
      "set",
      "container",
      "c4b3b01869bbd664efc6c5b8e4f9a6b1a0fa2e2838fe947b07ccfb023316096c",
      "itom-cdf-upgrade-deployer-202105-9qvwn",
      "networkplugin",
      "cni",
      "itom-cdf-up",
      "bridge",
      "addr",
      "cni0",
      "already",
      "ip",
      "address",
      "different",
      "24",
      "clean",
      "c4b3b01869bbd66",
      "4efc6c5b8e4f9a6b1a0fa2e2838fe947b07ccfb023316096c",
      "teardown",
      "itom-cdf-upgrade-deployer-202105-9qv",
      "running",
      "usr",
      "sbin",
      "iptables",
      "-t",
      "nat",
      "-d",
      "postrouting",
      "-s",
      "-j",
      "cni-22a8642b3b26382c0b66d1e8",
      "-m",
      "comment",
      "--comment",
      "name",
      "cbr0",
      "id"
    ],
    "language": "en",
    "word_count": 96,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "\"warning failedcreatepodsandbox\" message and pods do not start during upgrade",
    "contentLower": "when you run the upgrade -u command to perform a manual upgrade on a node that has many network adapters, the pods on the node don't start. if you run the kubectl describe pod <pod_name> -n <namespace> command to check the pod status when this occurs, you see warning messages that resemble the following: warning failedcreatepodsandbox 2m2s (x2438 over 91m) kubelet (combined from similar events): failed to create pod sandbox: rpc error: code = unknown desc = [failed to set up sandbox container \"c4b3b01869bbd664efc6c5b8e4f9a6b1a0fa2e2838fe947b07ccfb023316096c\" network for pod \"itom-cdf-upgrade-deployer-202105-9qvwn\": networkplugin cni failed to set up pod \"itom-cdf-up grade-deployer-202105-9qvwn_core\" network: failed to set bridge addr: \"cni0\" already has an ip address different from 172.16.60.1/24, failed to clean up sandbox container \"c4b3b01869bbd66 4efc6c5b8e4f9a6b1a0fa2e2838fe947b07ccfb023316096c\" network for pod \"itom-cdf-upgrade-deployer-202105-9qvwn\": networkplugin cni failed to ",
    "keywordsLower": [
      "172.16.60",
      "678.910",
      "v1.4.21",
      "172.16.60.171",
      "678.911",
      "12.345.678.911",
      "12.345.678",
      "4.21",
      "12.345",
      "172.16",
      "12.345.678.910",
      "172.16.60.1",
      "mfswlab.net",
      "cfgkubeviamulnetadapters.sh",
      "upgrade.sh",
      "60.171",
      "warning",
      "failedcreatepodsandbox",
      "message",
      "pods",
      "start",
      "during",
      "upgrade",
      "cause",
      "solution",
      "run",
      "-u",
      "command",
      "perform",
      "manual",
      "node",
      "many",
      "network",
      "adapters",
      "don",
      "start.",
      "kubectl",
      "describe",
      "pod",
      "-n",
      "check",
      "status",
      "occurs",
      "see",
      "messages",
      "resemble",
      "following",
      "2m2s",
      "x2438",
      "over",
      "91m",
      "kubelet",
      "combined",
      "similar",
      "events",
      "failed",
      "create",
      "sandbox",
      "rpc",
      "error",
      "code",
      "unknown",
      "desc",
      "set",
      "container",
      "c4b3b01869bbd664efc6c5b8e4f9a6b1a0fa2e2838fe947b07ccfb023316096c",
      "itom-cdf-upgrade-deployer-202105-9qvwn",
      "networkplugin",
      "cni",
      "itom-cdf-up",
      "bridge",
      "addr",
      "cni0",
      "already",
      "ip",
      "address",
      "different",
      "24",
      "clean",
      "c4b3b01869bbd66",
      "4efc6c5b8e4f9a6b1a0fa2e2838fe947b07ccfb023316096c",
      "teardown",
      "itom-cdf-upgrade-deployer-202105-9qv",
      "running",
      "usr",
      "sbin",
      "iptables",
      "-t",
      "nat",
      "-d",
      "postrouting",
      "-s",
      "-j",
      "cni-22a8642b3b26382c0b66d1e8",
      "-m",
      "comment",
      "--comment",
      "name",
      "cbr0",
      "id"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "\"Failed to pull image\" error when you try to upgrade OMT",
    "content": "When you try to upgrade OMT, the upgrade fails and some pods enter an abnormal state. If you run the kubectl describe pod <pod_name> -n <namespace> command to query a pod that's in an abnormal state, you see errors that resemble the following: Failed to pull image \"localhost:5000/hpeswitom/golang:1.15.3-esgz\": rpc error: code = Unknown desc = failed to pull and unpack image \"localhost:5000/hpeswitom/golang:1.15.3-esgz\": failed to resolve reference \"localhost:5000/hpeswitom/golang:1.15.3-esgz\": failed to do request: Head \"https://localhost:5000/v2/hpeswitom/golang/manifests/1.15.3-esgz\": x509: certificate signed by unknown authority Cause This issue occurs because your deployment uses a private registry CA certificate that has no authority. During installation, OMT didn’t add this private registry CA certificate to every node. Therefore, when you try to upgrade OMT, you can't pull images from a private registry when there is no image cache on the current node. Solution Run the following",
    "url": "upgradefailedtopullimage",
    "filename": "upgradefailedtopullimage",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "1.15",
      "1.15.3",
      "restart.sh",
      "https://localhost:5000/v2/hpeswitom/golang/manifests/1.15.3-esgz",
      "failed",
      "pull",
      "image",
      "error",
      "try",
      "upgrade",
      "omt",
      "cause",
      "solution",
      "fails",
      "pods",
      "enter",
      "abnormal",
      "state.",
      "run",
      "kubectl",
      "describe",
      "pod",
      "-n",
      "command",
      "query",
      "state",
      "see",
      "errors",
      "resemble",
      "following",
      "localhost",
      "5000",
      "hpeswitom",
      "golang",
      "1.15.3-esgz",
      "rpc",
      "code",
      "unknown",
      "desc",
      "unpack",
      "resolve",
      "reference",
      "request",
      "head",
      "https",
      "v2",
      "manifests",
      "x509",
      "certificate",
      "signed",
      "authority",
      "issue",
      "occurs",
      "because",
      "deployment",
      "uses",
      "private",
      "registry",
      "ca",
      "authority.",
      "during",
      "installation",
      "didn",
      "add",
      "every",
      "node.",
      "therefore",
      "images",
      "there",
      "cache",
      "current",
      "copy",
      "openssl",
      "directory",
      "cp",
      "etc",
      "pki",
      "ca-trust",
      "source",
      "anchors",
      "example",
      "path",
      "load",
      "update-ca-trust",
      "extract",
      "restart",
      "kubernetes",
      "services",
      "import",
      "bin",
      "kube-restart.sh",
      "repeat",
      "steps",
      "1-3",
      "node",
      "cluster."
    ],
    "language": "en",
    "word_count": 117,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "\"failed to pull image\" error when you try to upgrade omt",
    "contentLower": "when you try to upgrade omt, the upgrade fails and some pods enter an abnormal state. if you run the kubectl describe pod <pod_name> -n <namespace> command to query a pod that's in an abnormal state, you see errors that resemble the following: failed to pull image \"localhost:5000/hpeswitom/golang:1.15.3-esgz\": rpc error: code = unknown desc = failed to pull and unpack image \"localhost:5000/hpeswitom/golang:1.15.3-esgz\": failed to resolve reference \"localhost:5000/hpeswitom/golang:1.15.3-esgz\": failed to do request: head \"https://localhost:5000/v2/hpeswitom/golang/manifests/1.15.3-esgz\": x509: certificate signed by unknown authority cause this issue occurs because your deployment uses a private registry ca certificate that has no authority. during installation, omt didn’t add this private registry ca certificate to every node. therefore, when you try to upgrade omt, you can't pull images from a private registry when there is no image cache on the current node. solution run the following",
    "keywordsLower": [
      "1.15",
      "1.15.3",
      "restart.sh",
      "https://localhost:5000/v2/hpeswitom/golang/manifests/1.15.3-esgz",
      "failed",
      "pull",
      "image",
      "error",
      "try",
      "upgrade",
      "omt",
      "cause",
      "solution",
      "fails",
      "pods",
      "enter",
      "abnormal",
      "state.",
      "run",
      "kubectl",
      "describe",
      "pod",
      "-n",
      "command",
      "query",
      "state",
      "see",
      "errors",
      "resemble",
      "following",
      "localhost",
      "5000",
      "hpeswitom",
      "golang",
      "1.15.3-esgz",
      "rpc",
      "code",
      "unknown",
      "desc",
      "unpack",
      "resolve",
      "reference",
      "request",
      "head",
      "https",
      "v2",
      "manifests",
      "x509",
      "certificate",
      "signed",
      "authority",
      "issue",
      "occurs",
      "because",
      "deployment",
      "uses",
      "private",
      "registry",
      "ca",
      "authority.",
      "during",
      "installation",
      "didn",
      "add",
      "every",
      "node.",
      "therefore",
      "images",
      "there",
      "cache",
      "current",
      "copy",
      "openssl",
      "directory",
      "cp",
      "etc",
      "pki",
      "ca-trust",
      "source",
      "anchors",
      "example",
      "path",
      "load",
      "update-ca-trust",
      "extract",
      "restart",
      "kubernetes",
      "services",
      "import",
      "bin",
      "kube-restart.sh",
      "repeat",
      "steps",
      "1-3",
      "node",
      "cluster."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "\"Insufficient Disk Space\" error occurs",
    "content": "After a keyword-based search, the \"Insufficient Disk Space\" error occurs in the index.log file of saw-dih or other DIH pods. Cause The following errors may lead to this error: The persistent volume (PV) or disk associated with the IDOL content pod is full. There are file permission issues with the storage folder. There is a mounting issue. Solution 1 If the persistent volume (PV) or disk associated with the IDOL content pod is full, refer to Manage persistent storage for the suite and extend the storage. If necessary, submit a PCS ticket for further assistance. Sometimes, the disk may show available space (for example, a few GB left), yet IDOL can still display the above error log. This issue is related to the content's utilization of the file system and the resource claiming methods. It is also connected to various implementations and management practices of Kubernetes NFS storage, such as Azure Disk or AWS EBS. If no other problems are detected, consider extending the file system as ",
    "url": "insufficientdiskspace",
    "filename": "insufficientdiskspace",
    "headings": [
      "Cause",
      "Solution 1",
      "Solution 2",
      "Solution 3"
    ],
    "keywords": [
      "index.log",
      "insufficient",
      "disk",
      "space",
      "error",
      "occurs",
      "cause",
      "solution",
      "after",
      "keyword-based",
      "search",
      "file",
      "saw-dih",
      "dih",
      "pods.",
      "following",
      "errors",
      "lead",
      "persistent",
      "volume",
      "pv",
      "associated",
      "idol",
      "content",
      "pod",
      "full.",
      "there",
      "permission",
      "issues",
      "storage",
      "folder.",
      "mounting",
      "issue.",
      "full",
      "refer",
      "manage",
      "suite",
      "extend",
      "storage.",
      "necessary",
      "submit",
      "pcs",
      "ticket",
      "further",
      "assistance.",
      "sometimes",
      "show",
      "available",
      "example",
      "few",
      "gb",
      "left",
      "yet",
      "still",
      "display",
      "above",
      "log.",
      "issue",
      "related",
      "utilization",
      "system",
      "resource",
      "claiming",
      "methods.",
      "connected",
      "various",
      "implementations",
      "management",
      "practices",
      "kubernetes",
      "nfs",
      "such",
      "azure",
      "aws",
      "ebs.",
      "problems",
      "detected",
      "consider",
      "extending",
      "first",
      "step.",
      "folder",
      "follow",
      "steps",
      "access",
      "filesystem",
      "pod.",
      "go",
      "change",
      "ownership",
      "recursively.",
      "cd",
      "itsma-smarta-saw-con-0",
      "chown",
      "-r",
      "1999",
      "here",
      "user",
      "id",
      "group"
    ],
    "language": "en",
    "word_count": 110,
    "importance_score": 5.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "\"insufficient disk space\" error occurs",
    "contentLower": "after a keyword-based search, the \"insufficient disk space\" error occurs in the index.log file of saw-dih or other dih pods. cause the following errors may lead to this error: the persistent volume (pv) or disk associated with the idol content pod is full. there are file permission issues with the storage folder. there is a mounting issue. solution 1 if the persistent volume (pv) or disk associated with the idol content pod is full, refer to manage persistent storage for the suite and extend the storage. if necessary, submit a pcs ticket for further assistance. sometimes, the disk may show available space (for example, a few gb left), yet idol can still display the above error log. this issue is related to the content's utilization of the file system and the resource claiming methods. it is also connected to various implementations and management practices of kubernetes nfs storage, such as azure disk or aws ebs. if no other problems are detected, consider extending the file system as ",
    "keywordsLower": [
      "index.log",
      "insufficient",
      "disk",
      "space",
      "error",
      "occurs",
      "cause",
      "solution",
      "after",
      "keyword-based",
      "search",
      "file",
      "saw-dih",
      "dih",
      "pods.",
      "following",
      "errors",
      "lead",
      "persistent",
      "volume",
      "pv",
      "associated",
      "idol",
      "content",
      "pod",
      "full.",
      "there",
      "permission",
      "issues",
      "storage",
      "folder.",
      "mounting",
      "issue.",
      "full",
      "refer",
      "manage",
      "suite",
      "extend",
      "storage.",
      "necessary",
      "submit",
      "pcs",
      "ticket",
      "further",
      "assistance.",
      "sometimes",
      "show",
      "available",
      "example",
      "few",
      "gb",
      "left",
      "yet",
      "still",
      "display",
      "above",
      "log.",
      "issue",
      "related",
      "utilization",
      "system",
      "resource",
      "claiming",
      "methods.",
      "connected",
      "various",
      "implementations",
      "management",
      "practices",
      "kubernetes",
      "nfs",
      "such",
      "azure",
      "aws",
      "ebs.",
      "problems",
      "detected",
      "consider",
      "extending",
      "first",
      "step.",
      "folder",
      "follow",
      "steps",
      "access",
      "filesystem",
      "pod.",
      "go",
      "change",
      "ownership",
      "recursively.",
      "cd",
      "itsma-smarta-saw-con-0",
      "chown",
      "-r",
      "1999",
      "here",
      "user",
      "id",
      "group"
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "\"No License Found\" error during user login",
    "content": "The user gets a \"No License Found\" error when logging in to the Service Portal, and the user's Person record has an empty License field. Cause There are many possible causes for this issue. One common cause is that the LicenseUsers_<TENANTID> table in the RMS database has some wrong data with empty users value. To determine this is indeed the cause for the problem, connect to the xruntime RMS database, and run the following SQL: select jsonb_typeof(body->'users') from \"LicenseUsers_<TENANTID>\" order by id DESC If the query result is NULL, then this is the cause of the problem. Solution Contact support and provide the query results. Support will help fix the wrong data accordingly.",
    "url": "nolicensefound",
    "filename": "nolicensefound",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "license",
      "found",
      "error",
      "during",
      "user",
      "login",
      "cause",
      "solution",
      "gets",
      "logging",
      "service",
      "portal",
      "person",
      "record",
      "empty",
      "field.",
      "there",
      "many",
      "possible",
      "causes",
      "issue.",
      "one",
      "common",
      "table",
      "rms",
      "database",
      "wrong",
      "data",
      "users",
      "value.",
      "determine",
      "indeed",
      "problem",
      "connect",
      "xruntime",
      "run",
      "following",
      "sql",
      "select",
      "body-",
      "order",
      "id",
      "desc",
      "query",
      "result",
      "null",
      "problem.",
      "contact",
      "support",
      "provide",
      "results.",
      "help",
      "fix",
      "accordingly."
    ],
    "language": "en",
    "word_count": 75,
    "importance_score": 3.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "\"no license found\" error during user login",
    "contentLower": "the user gets a \"no license found\" error when logging in to the service portal, and the user's person record has an empty license field. cause there are many possible causes for this issue. one common cause is that the licenseusers_<tenantid> table in the rms database has some wrong data with empty users value. to determine this is indeed the cause for the problem, connect to the xruntime rms database, and run the following sql: select jsonb_typeof(body->'users') from \"licenseusers_<tenantid>\" order by id desc if the query result is null, then this is the cause of the problem. solution contact support and provide the query results. support will help fix the wrong data accordingly.",
    "keywordsLower": [
      "license",
      "found",
      "error",
      "during",
      "user",
      "login",
      "cause",
      "solution",
      "gets",
      "logging",
      "service",
      "portal",
      "person",
      "record",
      "empty",
      "field.",
      "there",
      "many",
      "possible",
      "causes",
      "issue.",
      "one",
      "common",
      "table",
      "rms",
      "database",
      "wrong",
      "data",
      "users",
      "value.",
      "determine",
      "indeed",
      "problem",
      "connect",
      "xruntime",
      "run",
      "following",
      "sql",
      "select",
      "body-",
      "order",
      "id",
      "desc",
      "query",
      "result",
      "null",
      "problem.",
      "contact",
      "support",
      "provide",
      "results.",
      "help",
      "fix",
      "accordingly."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  },
  {
    "title": "\"User not found\" error when assigning role to user",
    "content": "A \"User not found\" error appears when you assign roles to a user. Cause One possible cause is that the user record is absent from IdM. Solution In Suite Administration, go to your tenant's IdM settings, and verify that the user is absent from IdM. Contact support to fix the user data for you.",
    "url": "usernotfoundauthconvergence",
    "filename": "usernotfoundauthconvergence",
    "headings": [
      "Cause",
      "Solution"
    ],
    "keywords": [
      "user",
      "found",
      "error",
      "assigning",
      "role",
      "cause",
      "solution",
      "appears",
      "assign",
      "roles",
      "user.",
      "one",
      "possible",
      "record",
      "absent",
      "idm.",
      "suite",
      "administration",
      "go",
      "tenant",
      "idm",
      "settings",
      "verify",
      "contact",
      "support",
      "fix",
      "data",
      "you."
    ],
    "language": "en",
    "word_count": 39,
    "importance_score": 2.0,
    "tf_idf_scores": {},
    "ngrams": [],
    "stemmed_words": [],
    "titleLower": "\"user not found\" error when assigning role to user",
    "contentLower": "a \"user not found\" error appears when you assign roles to a user. cause one possible cause is that the user record is absent from idm. solution in suite administration, go to your tenant's idm settings, and verify that the user is absent from idm. contact support to fix the user data for you.",
    "keywordsLower": [
      "user",
      "found",
      "error",
      "assigning",
      "role",
      "cause",
      "solution",
      "appears",
      "assign",
      "roles",
      "user.",
      "one",
      "possible",
      "record",
      "absent",
      "idm.",
      "suite",
      "administration",
      "go",
      "tenant",
      "idm",
      "settings",
      "verify",
      "contact",
      "support",
      "fix",
      "data",
      "you."
    ],
    "bm25f_precomputed": true,
    "semantic_vector_available": true,
    "field_weights": {
      "title": 3.0,
      "headings": 2.0,
      "keywords": 2.5,
      "content": 1.0
    }
  }
]